# RAG-CPGQL: Code Property Graph Query Language Generation via RAG

**âœ… Status: Production Ready - 100% Query Validity Achieved**

Advanced Retrieval-Augmented Generation system for PostgreSQL source code analysis using CPGQL queries.

## ðŸŽ¯ Key Achievements

- **100% Query Validity** on 30-question validation set
- **4-Agent Architecture** (Analyzer â†’ Retriever â†’ Enrichment â†’ Generator)
- **RAGAS Metrics** integrated for comprehensive evaluation
- **200-Question Test Suite** with statistical significance analysis
- **Joern Integration** for real CPG query execution

## Overview

This system translates natural language questions about PostgreSQL internals into CPGQL queries using a sophisticated RAG pipeline with semantic enrichment.

### Architecture

```
Question â†’ Analyzer â†’ Retriever â†’ Enrichment â†’ Generator â†’ CPGQL Query
                â†“                                    â†“
           Domain Analysis              12-Layer Semantic Tags
                â†“                                    â†“
          ChromaDB (24k items)           Query Validation
```

**4-Agent Pipeline:**
1. **AnalyzerAgent** - Extracts domain, keywords, intent from question
2. **RetrieverAgent** - Retrieves relevant Q&A pairs (23k) and CPGQL examples (1k)
3. **EnrichmentAgent** - Maps to 12 semantic layers (transaction, lock, memory, etc.)
4. **GeneratorAgent** - Generates valid CPGQL with enrichment hints

## Dataset

- **Q&A Pairs:** 23,156 (indexed in ChromaDB)
- **CPGQL Examples:** 1,072 curated examples
- **Total Indexed Items:** 24,228
- **CPG:** PostgreSQL 17.6 (~450k vertices)
- **Test Sets:** 30 questions (validation), 200 questions (statistical)

## Performance Metrics

### 30-Question Validation
- **Validity Rate:** 100% (30/30)
- **Avg Generation Time:** 4.1s
- **Enrichment Coverage:** 0.44
- **Uses Enrichment Tags:** 46.7%
- **Uses Name Filters:** 80%

### RAGAS Evaluation
- **Q&A Similarity:** 0.791
- **CPGQL Similarity:** 0.230
- **Context Coverage:** 0.442
- **Throughput:** 0.24 qps

### Domain Coverage
Tested across 12 PostgreSQL domains:
- MVCC, WAL, Parallel Query, Partitioning
- Vacuum, Security, Indexes, Query Planning
- Memory Management, Background Workers, Extensions

## Installation

### 1. Install Dependencies

```bash
cd rag_cpgql
pip install -r requirements.txt
```

### 2. Initialize Vector Store

```bash
python src/retrieval/vector_store_real.py
```

This will:
- Initialize ChromaDB
- Index 23,156 Q&A pairs
- Index 1,072 CPGQL examples
- Create embeddings (takes ~5 minutes)

### 3. Download Model

**Qwen3-Coder-30B** (base model, no fine-tuning needed):
```bash
# Model path (adjust in code if different)
~/.lmstudio/models/lmstudio-community/Qwen2.5-Coder-32B-Instruct-GGUF/
```

## Quick Start

### 30-Question Validation Test

```bash
cd rag_cpgql
python experiments/test_30_questions.py
```

**Expected Output:**
```
Questions tested:      30
Valid queries:         30/30 (100.0%)
Avg generation time:   4.1s
Avg enrich coverage:   0.44
Total test time:       ~2 minutes
```

### 200-Question Statistical Test

```bash
# With checkpoint support
python experiments/run_200_questions_test.py

# Direct execution
python experiments/test_200_questions.py
```

**Features:**
- Auto-checkpoint every 50 questions
- Resume on interruption (Ctrl+C)
- Statistical analysis (95% CI)
- Expected duration: 15-20 minutes

### RAGAS Evaluation

```bash
python experiments/test_with_ragas.py
```

Requires existing test results (30 or 200 questions).

### Joern Client Test

```bash
# Start Joern server first
cd C:/Users/user/joern
./joern.bat --server --server-host localhost --server-port 8080

# Test connection
cd C:/Users/user/pg_copilot
python experiments/test_joern_client.py
```

## Project Structure

```
rag_cpgql/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ analyzer_agent.py       # Domain & keyword extraction
â”‚   â”‚   â”œâ”€â”€ retriever_agent.py      # RAG retrieval
â”‚   â”‚   â”œâ”€â”€ enrichment_agent.py     # Semantic enrichment (12 layers)
â”‚   â”‚   â””â”€â”€ generator_agent.py      # CPGQL generation
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ vector_store_real.py    # ChromaDB (24k items)
â”‚   â”‚   â””â”€â”€ enrichment_mappings.py  # 12-layer semantic tags
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â”œâ”€â”€ llm_interface.py        # LLM wrapper
â”‚   â”‚   â”œâ”€â”€ cpgql_generator.py      # Query generator
â”‚   â”‚   â””â”€â”€ prompts.py              # Prompt templates
â”‚   â”œâ”€â”€ execution/
â”‚   â”‚   â””â”€â”€ joern_client.py         # Joern server client
â”‚   â””â”€â”€ evaluation/
â”‚       â””â”€â”€ ragas_evaluator.py      # RAGAS metrics
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_split_merged.jsonl    # Training Q&A (23,156)
â”‚   â””â”€â”€ cpgql_examples.json         # CPGQL examples (1,072)
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ test_30_questions.py        # 30-question validation
â”‚   â”œâ”€â”€ test_200_questions.py       # 200-question statistical test
â”‚   â”œâ”€â”€ run_200_questions_test.py   # Test runner with checks
â”‚   â”œâ”€â”€ test_with_ragas.py          # RAGAS evaluation
â”‚   â”œâ”€â”€ test_joern_client.py        # Joern connection test
â”‚   â””â”€â”€ README.md                   # Experiments documentation
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ test_30_questions_results.json
â”‚   â”œâ”€â”€ test_200_questions_results.json
â”‚   â””â”€â”€ ragas_evaluation.json
â”œâ”€â”€ chroma_db/                      # ChromaDB storage (auto-created)
â””â”€â”€ README.md                       # This file
```

## 12-Layer Semantic Enrichment

Enrichment tags improve query specificity:

1. **transaction** - MVCC, XID, snapshot isolation
2. **lock** - Locking mechanisms, deadlocks
3. **memory** - Memory contexts, buffers
4. **io** - I/O operations, WAL writes
5. **network** - Client/server communication
6. **security** - Authentication, permissions
7. **optimization** - Query planner, indexes
8. **parallel** - Parallel query execution
9. **replication** - Streaming, logical replication
10. **vacuum** - VACUUM, autovacuum
11. **partition** - Table partitioning
12. **extension** - Extension framework

**Example:**
```
Question: "How does PostgreSQL handle transaction isolation?"
â†’ Enrichment: ["transaction", "lock", "memory"]
â†’ Query: cpg.method.name(".*HeapTuple.*").tag.name("transaction").l
```

## Usage Examples

### Example 1: Domain-Aware Query

```python
from agents.analyzer_agent import AnalyzerAgent
from agents.retriever_agent import RetrieverAgent
from agents.enrichment_agent import EnrichmentAgent
from agents.generator_agent import GeneratorAgent

question = "How does PostgreSQL validate transaction IDs?"

# 1. Analyze
analyzer = AnalyzerAgent()
analysis = analyzer.analyze(question)
# â†’ domain: "mvcc", keywords: ["transaction", "validate", "id"]

# 2. Retrieve
retriever = RetrieverAgent(vector_store, analyzer)
context = retriever.retrieve(question, analysis, top_k_qa=3, top_k_cpgql=5)

# 3. Enrich
enrichment = EnrichmentAgent()
hints = enrichment.get_enrichment_hints(question, analysis)
# â†’ tags: ["transaction", "memory"], coverage: 0.67

# 4. Generate
generator = GeneratorAgent(cpgql_gen, use_grammar=False)
query, valid, error = generator.generate(question, context)
# â†’ cpg.method.name("TransactionIdIsValid").tag.name("transaction").l
```

### Example 2: RAGAS Evaluation

```python
from evaluation.ragas_evaluator import RAGASEvaluator

evaluator = RAGASEvaluator(use_local_llm=True)
metrics = evaluator.evaluate_rag_pipeline(test_results, output_file)

print(f"Q&A Similarity: {metrics['retrieval_quality']['avg_qa_similarity']}")
print(f"Validity Rate: {metrics['generation_quality']['validity_rate']}")
```

## Evaluation Metrics

### Query Metrics
- **Query Validity Rate** - % of syntactically valid CPGQL
- **Enrichment Coverage** - Average enrichment tag usage
- **Query Complexity** - Average query length, clauses used

### RAGAS Metrics
- **Context Precision** - Relevance of retrieved context
- **Context Recall** - Coverage of retrieved context
- **Answer Relevancy** - Quality of generated answers
- **Faithfulness** - Accuracy to retrieved context

### Retrieval Metrics
- **Q&A Similarity** - Cosine similarity to reference Q&A
- **CPGQL Similarity** - Similarity to example queries
- **Retrieval Time** - Speed of vector search

## Statistical Analysis (200-Question Test)

With 200 questions:
- **Sample Size:** 200
- **Confidence Level:** 95%
- **Margin of Error:** Â±1.96 * SE

**Example Output:**
```
Validity rate: 97.5% Â± 2.2%
95% CI: [95.3%, 99.7%]

Domain Performance:
  mvcc                25/25 (100.0%)
  wal                 18/18 (100.0%)
  parallel            22/23 ( 95.7%)
  ...
```

## Troubleshooting

### ChromaDB Issues
```bash
# Reinitialize if corrupted
rm -rf chroma_db/
python src/retrieval/vector_store_real.py
```

### Model Loading Issues
- Ensure sufficient RAM (32GB+ recommended)
- Reduce `n_ctx` if OOM errors occur
- Use CPU if GPU unavailable (slower)

### Joern Server Issues
```bash
# Check server status
netstat -ano | findstr :8080

# Restart server
cd C:/Users/user/joern
./joern.bat --server --server-host localhost --server-port 8080
```

## Results Files

### test_30_questions_results.json
```json
{
  "total_questions": 30,
  "valid_queries": 30,
  "validity_rate": 100.0,
  "avg_generation_time": 4.11,
  "query_patterns": {...},
  "results": [...]
}
```

### test_200_questions_results.json
```json
{
  "statistical_analysis": {
    "sample_size": 200,
    "validity_rate": 0.975,
    "confidence_interval_95": [0.953, 0.997]
  },
  "domains": {...},
  "results": [...]
}
```

### ragas_evaluation.json
```json
{
  "retrieval_quality": {
    "avg_qa_similarity": 0.791,
    "avg_cpgql_similarity": 0.230
  },
  "generation_quality": {
    "validity_rate": 1.0,
    "uses_enrichment_tags_rate": 0.467
  }
}
```

## Next Steps

1. âœ… Run 200-question test for statistical validation
2. âœ… RAGAS evaluation on test results
3. â¬œ Full LangGraph integration (9-agent workflow)
4. â¬œ Production deployment with FastAPI
5. â¬œ Real-time query execution on PostgreSQL CPG

## Citation

```bibtex
@article{ragcpgql2025,
  title={RAG-Based CPGQL Query Generation with Semantic Enrichment for PostgreSQL Source Code Analysis},
  author={Research Team},
  year={2025},
  note={100% query validity achieved through 4-agent architecture with 12-layer semantic enrichment}
}
```

## License

Research project - see individual component licenses.

## Documentation

- **Experiments:** `experiments/README.md`
- **Architecture:** `LANGGRAPH_ARCHITECTURE.md`
- **Enrichment Layers:** `src/retrieval/enrichment_mappings.py`
- **Test Results:** `results/*.json`
