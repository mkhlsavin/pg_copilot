INFO:__main__:Connecting to Joern at localhost:8080...
INFO:src.execution.joern_client:JoernClient initialized (endpoint=localhost:8080)
INFO:src.execution.joern_client:Connecting to Joern server at localhost:8080
INFO:src.execution.joern_client:Query executed successfully in 12.28s
INFO:src.execution.joern_client:Connected to Joern server (CPG has [33mval[0m [36mres330[0m: [32mInt[0m = 52303

 methods)
INFO:__main__:Connected successfully to Joern CPG
INFO:__main__:Starting batched documentation extraction (total=30000, batch_size=100)...
INFO:__main__:Fetching batch: offset=0, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=0, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres331[0m: [32mList[0m[[32mString[0m] = List(
  """brinhandler|||backend\access\brin\brin.c|||246|||/*
 * BRIN handler function: return IndexAmRoutine with access method parameters
 * and callbacks.
 */""",
  """initialize_brin_insertstate|||backend\access\brin\brin.c|||305|||/*
 * Initialize a BrinInsertState to maintain state to be used across multiple
 * tuple inserts, within the same command.
 */""",
  """brininsert|||backend\access\brin\brin.c|||334|||/*
 * A tuple in the heap is being inserted.  To keep a brin index up to date,
 * we need to obtain the relevant index tuple and compare its stored values
 * with those of the new tuple.  If the tuple values are not consistent with
 * the summary tuple, we need to update the index tuple.
 *
 * If autosummarization is enabled, check if we need to summarize the previous
 * page range.
 *
 * If the range is not currently summarized (i.e. the revmap returns NULL for
 * it), there's nothing to do for this tuple.
 */""",
  """brininsertcleanup|||backend\access\brin\brin.c|||502|||/*
 * Callback to clean up the BrinInsertState once all tuple inserts are done.
 */""",
  """brinbeginscan|||backend\access\brin\brin.c|||529|||/*
 * Initialize state for a BRIN index scan.
 *
 * We read the metapage here to determine the pages-per-range number that this
 * index was built with.  Note that since this cannot be changed while we're
 * holding lock on index, it's not necessary to recompute it during brinrescan.
 */""",
  """bringetbitmap|||backend\access\brin\brin.c|||557|||/*
 * Execute the index scan.
 *
 * This works by reading index TIDs from the revmap, and obtaining the index
 * tuples pointed to by them; the summary values in the index tuples are
 * compared to the scan keys.  We return into the TID bitmap all the pages in
 * ranges corresponding to index tuples that match the scan keys.
 *
 * If a TID from the revmap is read as InvalidTID, we know that range is
 * unsummarized.  Pages in those ranges need to be returned regardless of scan
 * keys.
 */""",
  """brinrescan|||backend\access\brin\brin.c|||947|||/*
 * Re-initialize state for a BRIN index scan
 */""",
  """brinendscan|||backend\access\brin\brin.c|||967|||/*
 * Close down a BRIN index scan
 */""",
  """brinbuildCallback|||backend\access\brin\brin.c|||984|||/*
 * Per-heap-tuple callback for table_index_build_scan.
 *
 * Note we don't worry about the page range at the end of the table here; it is
 * present in the build state struct after we're called the last time, but not
 * inserted into the index.  Caller must ensure to do so, if appropriate.
 */""",
  """brinbuildCallbackParallel|||backend\access\brin\brin.c|||1035|||/*
 * Per-heap-tuple callback for table_index_build_scan with parallelism.
 *
 * A version of the callback used by parallel index builds. The main difference
 * is that instead of writing the BRIN tuples into the index, we write them
 * into a shared tuplesort, and leave the insertion up to the leader (which may
 * reorder them a bit etc.). The callback also does not generate empty ranges,
 * those will be added by the leader when merging results from workers.
 */""",
  """brinbuild|||backend\access\brin\brin.c|||1094|||/*
 * brinbuild() -- build a new BRIN index.
 */""",
  """brinbulkdelete|||backend\access\brin\brin.c|||1292|||/*
 * brinbulkdelete
 *		Since there are no per-heap-tuple index tuples in BRIN indexes,
 *		there's not a lot we can do here.
 *
 * XXX we could mark item tuples as "dirty" (when a minimum or maximum heap
 * tuple is deleted), meaning the need to re-run summarization on the affected
 * range.  Would need to add an extra flag in brintuples for that.
 */""",
  """brinvacuumcleanup|||backend\access\brin\brin.c|||1307|||/*
 * This routine is in charge of "vacuuming" a BRIN index: we just summarize
 * ranges that are currently unsummarized.
 */""",
  """brinoptions|||backend\access\brin\brin.c|||1337|||/*
 * reloptions processor for BRIN indexes
 */""",
  """brin_summarize_new_values|||backend\access\brin\brin.c|||1355|||/*
 * SQL-callable function to scan through an index and summarize all ranges
 * that are not currently summarized.
 */""",
  """brin_summarize_range|||backend\access\brin\brin.c|||1370|||/*
 * SQL-callable function to summarize the indicated page range, if not already
 * summarized.  If the second argument is BRIN_ALL_BLOCKRANGES, all
 * unsummarized ranges are summarized.
 */""",
  """brin_desummarize_range|||backend\access\brin\brin.c|||1481|||/*
 * SQL-callable interface to mark a range as no longer summarized
 */""",
  """brin_build_desc|||backend\access\brin\brin.c|||1571|||/*
 * Build a BrinDesc used to create or scan a BRIN index
 */""",
  """brin_free_desc|||backend\access\brin\brin.c|||1626|||	/* make sure the tupdesc is still valid */""",
  """brinGetStats|||backend\access\brin\brin.c|||1638|||/*
 * Fetch index's statistical data into *stats
 */""",
  """initialize_brin_buildstate|||backend\access\brin\brin.c|||1659|||/*
 * Initialize a BrinBuildState appropriate to create tuples on the given index.
 */""",
  """terminate_brin_buildstate|||backend\access\brin\brin.c|||1706|||/*
 * Release resources associated with a BrinBuildState.
 */""",
  """summarize_range|||backend\access\brin\brin.c|||1751|||/*
 * On the given BRIN index, summarize the heap page range that corresponds
 * to the heap block number given.
 *
 * This routine can run in parallel with insertions into the heap.  To avoid
 * missing those values from the summary tuple, we first insert a placeholder
 * index tuple into the index, then execute the heap scan; transactions
 * concurrent with the scan update the placeholder tuple.  After the scan, we
 * union the placeholder tuple with the one computed by this routine.  The
 * update of the index value happens in a loop, so that if somebody updates
 * the placeholder tuple after we read it, we detect the case and try again.
 * This ensures that the concurrently inserted tuples are not lost.
 *
 * A further corner case is this routine being asked to summarize the partial
 * range at the end of the table.  heapNumBlocks is the (possibly outdated)
 * table size; if we notice that the requested range lies beyond that size,
 * we re-compute the table size after inserting the placeholder tuple, to
 * avoid missing pages that were appended recently.
 */""",
  """brinsummarize|||backend\access\brin\brin.c|||1877|||/*
 * Summarize page ranges that are not already summarized.  If pageRange is
 * BRIN_ALL_BLOCKRANGES then the whole table is scanned; otherwise, only the
 * page range containing the given heap page number is scanned.
 * If include_partial is true, then the partial range at the end of the table
 * is summarized, otherwise not.
 *
 * For each new index tuple inserted, *numSummarized (if not NULL) is
 * incremented; for each existing tuple, *numExisting (if not NULL) is
 * incremented.
 */""",
  """form_and_insert_tuple|||backend\access\brin\brin.c|||1975|||/*
 * Given a deformed tuple in the build state, convert it into the on-disk
 * format and insert it into the index, making the revmap point to it.
 */""",
  """form_and_spill_tuple|||backend\access\brin\brin.c|||1996|||/*
 * Given a deformed tuple in the build state, convert it into the on-disk
 * format and write it to a (shared) tuplesort (the leader will insert it
 * into the index later).
 */""",
  """union_tuples|||backend\access\brin\brin.c|||2021|||/*
 * Given two deformed tuples, adjust the first one so that it's consistent
 * with the summary values in both.
 */""",
  """brin_vacuum_scan|||backend\access\brin\brin.c|||2162|||/*
 * brin_vacuum_scan
 *		Do a complete scan of the index during VACUUM.
 *
 * This routine scans the complete index looking for uncataloged index pages,
 * i.e. those that might have been lost due to a crash after index extension
 * and such.
 */""",
  """_brin_begin_parallel|||backend\access\brin\brin.c|||2353|||/*
 * Create parallel context, and launch workers for leader.
 *
 * buildstate argument should be initialized (with the exception of the
 * tuplesort states, which may later be created based on shared
 * state initially set up here).
 *
 * isconcurrent indicates if operation is CREATE INDEX CONCURRENTLY.
 *
 * request is the target number of parallel worker processes to launch.
 *
 * Sets buildstate's BrinLeader, which caller must use to shut down parallel
 * mode by passing it to _brin_end_parallel() at the very end of its index
 * build.  If not even a single worker process can be launched, this is
 * never set, and caller should proceed with a serial index build.
 */""",
  """_brin_end_parallel|||backend\access\brin\brin.c|||2537|||/*
 * Shut down workers, destroy parallel context, and end parallel mode.
 */""",
  """_brin_parallel_heapscan|||backend\access\brin\brin.c|||2568|||/*
 * Within leader, wait for end of heap scan.
 *
 * When called, parallel heap scan started by _brin_begin_parallel() will
 * already be underway within worker processes (when leader participates
 * as a worker, we should end up here just as workers are finishing).
 *
 * Returns the total number of heap tuples scanned.
 */""",
  """_brin_parallel_merge|||backend\access\brin\brin.c|||2609|||/*
 * Within leader, wait for end of heap scan and merge per-worker results.
 *
 * After waiting for all workers to finish, merge the per-worker results into
 * the complete index. The results from each worker are sorted by block number
 * (start of the page range). While combining the per-worker results we merge
 * summaries for the same page range, and also fill-in empty summaries for
 * ranges without any tuples.
 *
 * Returns the total number of heap tuples scanned.
 */""",
  """_brin_parallel_estimate_shared|||backend\access\brin\brin.c|||2756|||/*
 * Returns size of shared memory required to store state for a parallel
 * brin index build based on the snapshot its parallel scan will use.
 */""",
  """_brin_leader_participate_as_worker|||backend\access\brin\brin.c|||2767|||/*
 * Within leader, participate as a parallel worker.
 */""",
  """_brin_parallel_scan_and_build|||backend\access\brin\brin.c|||2795|||/*
 * Perform a worker's portion of a parallel sort.
 *
 * This generates a tuplesort for the worker portion of the table.
 *
 * sortmem is the amount of working memory to use within each worker,
 * expressed in KBs.
 *
 * When this returns, workers are done, and need only release resources.
 */""",
  """_brin_parallel_build_main|||backend\access\brin\brin.c|||2852|||/*
 * Perform work within a launched parallel process.
 */""",
  """brin_build_empty_tuple|||backend\access\brin\brin.c|||2942|||/*
 * brin_build_empty_tuple
 *		Maybe initialize a BRIN tuple representing empty range.
 *
 * Returns a BRIN tuple representing an empty page range starting at the
 * specified block number. The empty tuple is initialized only once, when it's
 * needed for the first time, stored in the memory context bs_context to ensure
 * proper life span, and reused on following calls. All empty tuples are
 * exactly the same except for the bt_blkno field, which is set to the value
 * in blkno parameter.
 */""",
  """brin_fill_empty_ranges|||backend\access\brin\brin.c|||2979|||/*
 * brin_fill_empty_ranges
 *		Add BRIN index tuples representing empty page ranges.
 *
 * prevRange/nextRange determine for which page ranges to add empty summaries.
 * Both boundaries are exclusive, i.e. only ranges starting at blkno for which
 * (prevRange < blkno < nextRange) will be added to the index.
 *
 * If prevRange is InvalidBlockNumber, this means there was no previous page
 * range (i.e. the first empty range to add is for blkno=0).
 *
 * The empty tuple is built only once, and then reused for all future calls.
 */""",
  """<clinit>|||backend\access\brin\brin_bloom.c|||242|||/*
 * Bloom Filter
 *
 * Represents a bloom filter, built on hashes of the indexed values. That is,
 * we compute a uint32 hash of the value, and then store this hash into the
 * bloom filter (and compute additional hashes on it).
 *
 * XXX We could implement "sparse" bloom filters, keeping only the bytes that
 * are not entirely 0. But while indexes don't support TOAST, the varlena can
 * still be compressed. So this seems unnecessary, because the compression
 * should do the same job.
 *
 * XXX We can also watch the number of bits set in the bloom filter, and then
 * stop using it (and not store the bitmap, to save space) when the false
 * positive rate gets too high. But even if the false positive rate exceeds the
 * desired value, it still can eliminate some page ranges.
 */""",
  """bloom_filter_size|||backend\access\brin\brin_bloom.c|||270|||/*
 * bloom_filter_size
 *		Calculate Bloom filter parameters (nbits, nbytes, nhashes).
 *
 * Given expected number of distinct values and desired false positive rate,
 * calculates the optimal parameters of the Bloom filter.
 *
 * The resulting parameters are returned through nbytesp (number of bytes),
 * nbitsp (number of bits) and nhashesp (number of hash functions). If a
 * pointer is NULL, the parameter is not returned.
 */""",
  """bloom_init|||backend\access\brin\brin_bloom.c|||309|||/*
 * bloom_init
 * 		Initialize the Bloom Filter, allocate all the memory.
 *
 * The filter is initialized with optimal size for ndistinct expected values
 * and the requested false positive rate. The filter is stored as varlena.
 */""",
  """bloom_add_value|||backend\access\brin\brin_bloom.c|||369|||/*
 * bloom_add_value
 * 		Add value to the bloom filter.
 */""",
  """bloom_contains_value|||backend\access\brin\brin_bloom.c|||406|||/*
 * bloom_contains_value
 * 		Check if the bloom filter contains a particular value.
 */""",
  """<clinit>|||backend\access\brin\brin_bloom.c|||434|||	/*
	 * XXX At this point we only need a single proc (to compute the hash), but
	 * let's keep the array just like inclusion and minmax opclasses, for
	 * consistency. We may need additional procs in the future.
	 */""",
  """brin_bloom_get_ndistinct|||backend\access\brin\brin_bloom.c|||495|||/*
 * brin_bloom_get_ndistinct
 *		Determine the ndistinct value used to size bloom filter.
 *
 * Adjust the ndistinct value based on the pagesPerRange value. First,
 * if it's negative, it's assumed to be relative to maximum number of
 * tuples in the range (assuming each page gets MaxHeapTuplesPerPage
 * tuples, which is likely a significant over-estimate). We also clamp
 * the value, not to over-size the bloom filter unnecessarily.
 *
 * XXX We can only do this when the pagesPerRange value was supplied.
 * If it wasn't, it has to be a read-only access to the index, in which
 * case we don't really care. But perhaps we should fall-back to the
 * default pagesPerRange value?
 *
 * XXX We might also fetch info about ndistinct estimate for the column,
 * and compute the expected number of distinct values in a range. But
 * that may be tricky due to data being sorted in various ways, so it
 * seems better to rely on the upper estimate.
 *
 * XXX We might also calculate a better estimate of rows per BRIN range,
 * instead of using MaxHeapTuplesPerPage (which probably produces values
 * much higher than reality).
 */""",
  """brin_bloom_add_value|||backend\access\brin\brin_bloom.c|||538|||/*
 * Examine the given index tuple (which contains partial status of a certain
 * page range) by comparing it to the given value that comes from another heap
 * tuple.  If the new value is outside the bloom filter specified by the
 * existing tuple values, update the index tuple and return true.  Otherwise,
 * return false and do not modify in this case.
 */""",
  """brin_bloom_consistent|||backend\access\brin\brin_bloom.c|||593|||/*
 * Given an index tuple corresponding to a certain page range and a scan key,
 * return whether the scan key is consistent with the index tuple's bloom
 * filter.  Return true if so, false otherwise.
 */""",
  """brin_bloom_union|||backend\access\brin\brin_bloom.c|||665|||/*
 * Given two BrinValues, update the first of them as a union of the summary
 * values contained in both.  The second one is untouched.
 *
 * XXX We assume the bloom filters have the same parameters for now. In the
 * future we should have 'can union' function, to decide if we can combine
 * two particular bloom filters.
 */""",
  """bloom_get_procinfo|||backend\access\brin\brin_bloom.c|||716|||/*
 * Cache and return inclusion opclass support procedure
 *
 * Return the procedure corresponding to the given function support number
 * or null if it does not exist.
 */""",
  """brin_bloom_summary_in|||backend\access\brin\brin_bloom.c|||776|||/*
 * brin_bloom_summary_in
 *		- input routine for type brin_bloom_summary.
 *
 * brin_bloom_summary is only used internally to represent summaries
 * in BRIN bloom indexes, so it has no operations of its own, and we
 * disallow input too.
 */""",
  """brin_bloom_summary_out|||backend\access\brin\brin_bloom.c|||798|||/*
 * brin_bloom_summary_out
 *		- output routine for type brin_bloom_summary.
 *
 * BRIN bloom summaries are serialized into a bytea value, but we want
 * to output something nicer humans can understand.
 */""",
  """brin_bloom_summary_recv|||backend\access\brin\brin_bloom.c|||822|||/*
 * brin_bloom_summary_recv
 *		- binary input routine for type brin_bloom_summary.
 */""",
  """brin_bloom_summary_send|||backend\access\brin\brin_bloom.c|||839|||/*
 * brin_bloom_summary_send
 *		- binary output routine for type brin_bloom_summary.
 *
 * BRIN bloom summaries are serialized in a bytea value (although the
 * type is named differently), so let's just send that.
 */""",
  """<clinit>|||backend\access\brin\brin_inclusion.c|||76|||/*-
 * The values stored in the bv_values arrays correspond to:
 *
 * INCLUSION_UNION
 *		the union of the values in the block range
 * INCLUSION_UNMERGEABLE
 *		whether the values in the block range cannot be merged
 *		(e.g. an IPv6 address amidst IPv4 addresses)
 * INCLUSION_CONTAINS_EMPTY
 *		whether an empty value is present in any tuple
 *		in the block range
 */""",
  """brin_inclusion_opcinfo|||backend\access\brin\brin_inclusion.c|||93|||/*
 * BRIN inclusion OpcInfo function
 */""",
  """brin_inclusion_add_value|||backend\access\brin\brin_inclusion.c|||137|||/*
 * BRIN inclusion add value function
 *
 * Examine the given index tuple (which contains partial status of a certain
 * page range) by comparing it to the given value that comes from another heap
 * tuple.  If the new value is outside the union specified by the existing
 * tuple values, update the index tuple and return true.  Otherwise, return
 * false and do not modify in this case.
 */""",
  """brin_inclusion_consistent|||backend\access\brin\brin_inclusion.c|||249|||/*
 * BRIN inclusion consistent function
 *
 * We're no longer dealing with NULL keys in the consistent function, that is
 * now handled by the AM code. That means we should not get any all-NULL ranges
 * either, because those can't be consistent with regular (not [IS] NULL) keys.
 *
 * All of the strategies are optional.
 */""",
  """brin_inclusion_union|||backend\access\brin\brin_inclusion.c|||473|||/*
 * BRIN inclusion union function
 *
 * Given two BrinValues, update the first of them as a union of the summary
 * values contained in both.  The second one is untouched.
 */""",
  """inclusion_get_procinfo|||backend\access\brin\brin_inclusion.c|||543|||/*
 * Cache and return inclusion opclass support procedure
 *
 * Return the procedure corresponding to the given function support number
 * or null if it is not exists.  If missing_ok is true and the procedure
 * isn't set up for this opclass, return NULL instead of raising an error.
 */""",
  """inclusion_get_strategy_procinfo|||backend\access\brin\brin_inclusion.c|||607|||/*
 * Cache and return the procedure of the given strategy
 *
 * Return the procedure corresponding to the given sub-type and strategy
 * number.  The data type of the index will be used as the left hand side of
 * the operator and the given sub-type will be used as the right hand side.
 * Throws an error if the pg_amop row does not exist, but that should not
 * happen with a properly configured opclass.
 *
 * It always throws an error when the data type of the opclass is different
 * from the data type of the column or the expression.  That happens when the
 * column data type has implicit cast to the opclass data type.  We don't
 * bother casting types, because this situation can easily be avoided by
 * setting storage data type to that of the opclass.  The same problem does not
 * apply to the data type of the right hand side, because the type in the
 * ScanKey always matches the opclass' one.
 *
 * Note: this function mirrors minmax_get_strategy_procinfo; if changes are
 * made here, see that function too.
 */""",
  """<clinit>|||backend\access\brin\brin_minmax.c|||23|||/*
 * brin_minmax.c
 *		Implementation of Min/Max opclass for BRIN
 *
 * Portions Copyright (c) 1996-2024, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 * IDENTIFICATION
 *	  src/backend/access/brin/brin_minmax.c
 */""",
  """brin_minmax_opcinfo|||backend\access\brin\brin_minmax.c|||33|||/*
 * brin_minmax.c
 *		Implementation of Min/Max opclass for BRIN
 *
 * Portions Copyright (c) 1996-2024, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 * IDENTIFICATION
 *	  src/backend/access/brin/brin_minmax.c
 */""",
  """brin_minmax_add_value|||backend\access\brin\brin_minmax.c|||63|||/*
 * Examine the given index tuple (which contains partial status of a certain
 * page range) by comparing it to the given value that comes from another heap
 * tuple.  If the new value is outside the min/max range specified by the
 * existing tuple values, update the index tuple and return true.  Otherwise,
 * return false and do not modify in this case.
 */""",
  """brin_minmax_consistent|||backend\access\brin\brin_minmax.c|||136|||/*
 * Given an index tuple corresponding to a certain page range and a scan key,
 * return whether the scan key is consistent with the index tuple's min/max
 * values.  Return true if so, false otherwise.
 *
 * We're no longer dealing with NULL keys in the consistent function, that is
 * now handled by the AM code. That means we should not get any all-NULL ranges
 * either, because those can't be consistent with regular (not [IS] NULL) keys.
 */""",
  """brin_minmax_union|||backend\access\brin\brin_minmax.c|||207|||/*
 * Given two BrinValues, update the first of them as a union of the summary
 * values contained in both.  The second one is untouched.
 */""",
  """minmax_get_strategy_procinfo|||backend\access\brin\brin_minmax.c|||260|||/*
 * Cache and return the procedure for the given strategy.
 *
 * Note: this function mirrors inclusion_get_strategy_procinfo; see notes
 * there.  If changes are made here, see that function too.
 */""",
  """<clinit>|||backend\access\brin\brin_minmax_multi.c|||111|||/*
 * Sizing the insert buffer - we use 10x the number of values specified
 * in the reloption, but we cap it to 8192 not to get too large. When
 * the buffer gets full, we reduce the number of values by half.
 */""",
  """<clinit>|||backend\access\brin\brin_minmax_multi.c|||169|||/*
 * The summary of minmax-multi indexes has two representations - Ranges for
 * convenient processing, and SerializedRanges for storage in bytea value.
 *
 * The Ranges struct stores the boundary values in a single array, but we
 * treat regular and single-point ranges differently to save space. For
 * regular ranges (with different boundary values) we have to store both
 * the lower and upper bound of the range, while for "single-point ranges"
 * we only need to store a single value.
 *
 * The 'values' array stores boundary values for regular ranges first (there
 * are 2*nranges values to store), and then the nvalues boundary values for
 * single-point ranges. That is, we have (2*nranges + nvalues) boundary
 * values in the array.
 *
 * +-------------------------+----------------------------------+
 * | ranges (2 * nranges of) | single point values (nvalues of) |
 * +-------------------------+----------------------------------+
 *
 * This allows us to quickly add new values, and store outliers without
 * having to widen any of the existing range values.
 *
 * 'nsorted' denotes how many of 'nvalues' in the values[] array are sorted.
 * When nsorted == nvalues, all single point values are sorted.
 *
 * We never store more than maxvalues values (as set by values_per_range
 * reloption). If needed we merge some of the ranges.
 *
 * To minimize palloc overhead, we always allocate the full array with
 * space for maxvalues elements. This should be fine as long as the
 * maxvalues is reasonably small (64 seems fine), which is the case
 * thanks to values_per_range reloption being limited to 256.
 */""",
  """<clinit>|||backend\access\brin\brin_minmax_multi.c|||205|||/*
 * On-disk the summary is stored as a bytea value, with a simple header
 * with basic metadata, followed by the boundary values. It has a varlena
 * header, so can be treated as varlena directly.
 *
 * See brin_range_serialize/brin_range_deserialize for serialization details.
 */""",
  """AssertArrayOrder|||backend\access\brin\brin_minmax_multi.c|||278|||/*
 * Check that the order of the array values is correct, using the cmp
 * function (which should be BTLessStrategyNumber).
 */""",
  """AssertCheckRanges|||backend\access\brin\brin_minmax_multi.c|||295|||/*
 * Comprehensive check of the Ranges structure.
 */""",
  """AssertCheckExpandedRanges|||backend\access\brin\brin_minmax_multi.c|||425|||/*
 * Check that the expanded ranges (built when reducing the number of ranges
 * by combining some of them) are correctly sorted and do not overlap.
 */""",
  """minmax_multi_init|||backend\access\brin\brin_minmax_multi.c|||485|||/*
 * minmax_multi_init
 * 		Initialize the deserialized range list, allocate all the memory.
 *
 * This is only in-memory representation of the ranges, so we allocate
 * enough space for the maximum number of values (so as not to have to do
 * repallocs as the ranges grow).
 */""",
  """range_deduplicate_values|||backend\access\brin\brin_minmax_multi.c|||515|||/*
 * range_deduplicate_values
 *		Deduplicate the part with values in the simple points.
 *
 * This is meant to be a cheaper way of reducing the size of the ranges. It
 * does not touch the ranges, and only sorts the other values - it does not
 * call the distance functions, which may be quite expensive, etc.
 *
 * We do know the values are not duplicate with the ranges, because we check
 * that before adding a new value. Same for the sorted part of values.
 */""",
  """brin_range_serialize|||backend\access\brin\brin_minmax_multi.c|||575|||/*
 * brin_range_serialize
 *	  Serialize the in-memory representation into a compact varlena value.
 *
 * Simply copy the header and then also the individual values, as stored
 * in the in-memory value array.
 */""",
  """brin_range_deserialize|||backend\access\brin\brin_minmax_multi.c|||720|||/*
 * brin_range_deserialize
 *	  Serialize the in-memory representation into a compact varlena value.
 *
 * Simply copy the header and then also the individual values, as stored
 * in the in-memory value array.
 */""",
  """compare_expanded_ranges|||backend\access\brin\brin_minmax_multi.c|||857|||/*
 * compare_expanded_ranges
 *	  Compare the expanded ranges - first by minimum, then by maximum.
 *
 * We do guarantee that ranges in a single Ranges object do not overlap, so it
 * may seem strange that we don't order just by minimum. But when merging two
 * Ranges (which happens in the union function), the ranges may in fact
 * overlap. So we do compare both.
 */""",
  """compare_values|||backend\access\brin\brin_minmax_multi.c|||895|||/*
 * compare_values
 *	  Compare the values.
 */""",
  """has_matching_range|||backend\access\brin\brin_minmax_multi.c|||920|||/*
 * Check if the new value matches one of the existing ranges.
 */""",
  """range_contains_value|||backend\access\brin\brin_minmax_multi.c|||1044|||/*
 * range_contains_value
 * 		See if the new value is already contained in the range list.
 *
 * We first inspect the list of intervals. We use a small trick - we check
 * the value against min/max of the whole range (min of the first interval,
 * max of the last one) first, and only inspect the individual intervals if
 * this passes.
 *
 * If the value matches none of the intervals, we check the exact values.
 * We simply loop through them and invoke equality operator on them.
 *
 * The last parameter (full) determines whether we need to search all the
 * values, including the unsorted part. With full=false, the unsorted part
 * is not searched, which may produce false negatives and duplicate values
 * (in the unsorted part only), but when we're building the range that's
 * fine - we'll deduplicate before serialization, and it can only happen
 * if there already are unsorted values (so it was already modified).
 *
 * Serialized ranges don't have any unsorted values, so this can't cause
 * false negatives during querying.
 */""",
  """fill_expanded_ranges|||backend\access\brin\brin_minmax_multi.c|||1133|||/*
 * Expand ranges from Ranges into ExpandedRange array. This expects the
 * eranges to be pre-allocated and with the correct size - there needs to be
 * (nranges + nvalues) elements.
 *
 * The order of expanded ranges is arbitrary. We do expand the ranges first,
 * and this part is sorted. But then we expand the values, and this part may
 * be unsorted.
 */""",
  """sort_expanded_ranges|||backend\access\brin\brin_minmax_multi.c|||1178|||/*
 * Sort and deduplicate expanded ranges.
 *
 * The ranges may be deduplicated - we're simply appending values, without
 * checking for duplicates etc. So maybe the deduplication will reduce the
 * number of ranges enough, and we won't have to compute the distances etc.
 *
 * Returns the number of expanded ranges.
 */""",
  """merge_overlapping_ranges|||backend\access\brin\brin_minmax_multi.c|||1230|||/*
 * When combining multiple Range values (in union function), some of the
 * ranges may overlap. We simply merge the overlapping ranges to fix that.
 *
 * XXX This assumes the expanded ranges were previously sorted (by minval
 * and then maxval). We leverage this when detecting overlap.
 */""",
  """compare_distances|||backend\access\brin\brin_minmax_multi.c|||1304|||/*
 * Simple comparator for distance values, comparing the double value.
 * This is intentionally sorting the distances in descending order, i.e.
 * the longer gaps will be at the front.
 */""",
  """build_distances|||backend\access\brin\brin_minmax_multi.c|||1328|||/*
 * Given an array of expanded ranges, compute size of the gaps between each
 * range.  For neranges there are (neranges-1) gaps.
 *
 * We simply call the "distance" function to compute the (max-min) for pairs
 * of consecutive ranges. The function may be fairly expensive, so we do that
 * just once (and then use it to pick as many ranges to merge as possible).
 *
 * See reduce_expanded_ranges for details.
 */""",
  """build_expanded_ranges|||backend\access\brin\brin_minmax_multi.c|||1385|||/*
 * Builds expanded ranges for the existing ranges (and single-point ranges),
 * and also the new value (which did not fit into the array).  This expanded
 * representation makes the processing a bit easier, as it allows handling
 * ranges and points the same way.
 *
 * We sort and deduplicate the expanded ranges - this is necessary, because
 * the points may be unsorted. And moreover the two parts (ranges and
 * points) are sorted on their own.
 */""",
  """count_values|||backend\access\brin\brin_minmax_multi.c|||1414|||/*
 * Counts boundary values needed to store the ranges. Each single-point
 * range is stored using a single value, each regular range needs two.
 */""",
  """reduce_expanded_ranges|||backend\access\brin\brin_minmax_multi.c|||1475|||/*
 * reduce_expanded_ranges
 *		reduce the ranges until the number of values is low enough
 *
 * Combines ranges until the number of boundary values drops below the
 * threshold specified by max_values. This happens by merging enough
 * ranges by the distance between them.
 *
 * Returns the number of result ranges.
 *
 * We simply use the global min/max and then add boundaries for enough
 * largest gaps. Each gap adds 2 values, so we simply use (target/2-1)
 * distances. Then we simply sort all the values - each two values are
 * a boundary of a range (possibly collapsed).
 *
 * XXX Some of the ranges may be collapsed (i.e. the min/max values are
 * equal), but we ignore that for now. We could repeat the process,
 * adding a couple more gaps recursively.
 *
 * XXX The ranges to merge are selected solely using the distance. But
 * that may not be the best strategy, for example when multiple gaps
 * are of equal (or very similar) length.
 *
 * Consider for example points 1, 2, 3, .., 64, which have gaps of the
 * same length 1 of course. In that case, we tend to pick the first
 * gap of that length, which leads to this:
 *
 *    step 1:  [1, 2], 3, 4, 5, .., 64
 *    step 2:  [1, 3], 4, 5,    .., 64
 *    step 3:  [1, 4], 5,       .., 64
 *    ...
 *
 * So in the end we'll have one "large" range and multiple small points.
 * That may be fine, but it seems a bit strange and non-optimal. Maybe
 * we should consider other things when picking ranges to merge - e.g.
 * length of the ranges? Or perhaps randomize the choice of ranges, with
 * probability inversely proportional to the distance (the gap lengths
 * may be very close, but not exactly the same).
 *
 * XXX Or maybe we could just handle this by using random value as a
 * tie-break, or by adding random noise to the actual distance.
 */""",
  """store_expanded_ranges|||backend\access\brin\brin_minmax_multi.c|||1557|||/*
 * Store the boundary values from ExpandedRanges back into 'ranges' (using
 * only the minimal number of values needed).
 */""",
  """ensure_free_space_in_buffer|||backend\access\brin\brin_minmax_multi.c|||1600|||/*
 * Consider freeing space in the ranges. Checks if there's space for at least
 * one new value, and performs compaction if needed.
 *
 * Returns true if the value was actually modified.
 */""",
  """range_add_value|||backend\access\brin\brin_minmax_multi.c|||1701|||/*
 * range_add_value
 * 		Add the new value to the minmax-multi range.
 */""",
  """compactify_ranges|||backend\access\brin\brin_minmax_multi.c|||1787|||/*
 * Generate range representation of data collected during "batch mode".
 * This is similar to reduce_expanded_ranges, except that we can't assume
 * the values are sorted and there may be duplicate values.
 */""",
  """brin_minmax_multi_distance_float4|||backend\access\brin\brin_minmax_multi.c|||1882|||/*
 * Compute the distance between two float4 values (plain subtraction).
 */""",
  """brin_minmax_multi_distance_float8|||backend\access\brin\brin_minmax_multi.c|||1908|||/*
 * Compute the distance between two float8 values (plain subtraction).
 */""",
  """brin_minmax_multi_distance_int2|||backend\access\brin\brin_minmax_multi.c|||1934|||/*
 * Compute the distance between two int2 values (plain subtraction).
 */""",
  """brin_minmax_multi_distance_int4|||backend\access\brin\brin_minmax_multi.c|||1952|||/*
 * Compute the distance between two int4 values (plain subtraction).
 */""",
  """brin_minmax_multi_distance_int8|||backend\access\brin\brin_minmax_multi.c|||1970|||/*
 * Compute the distance between two int8 values (plain subtraction).
 */""",
  """brin_minmax_multi_distance_tid|||backend\access\brin\brin_minmax_multi.c|||1989|||/*
 * Compute the distance between two tid values (by mapping them to float8 and
 * then subtracting them).
 */""",
  """brin_minmax_multi_distance_numeric|||backend\access\brin\brin_minmax_multi.c|||2020|||/*
 * Compute the distance between two numeric values (plain subtraction).
 */""",
  """brin_minmax_multi_distance_uuid|||backend\access\brin\brin_minmax_multi.c|||2046|||/*
 * Compute the approximate distance between two UUID values.
 *
 * XXX We do not need a perfectly accurate value, so we approximate the
 * deltas (which would have to be 128-bit integers) with a 64-bit float.
 * The small inaccuracies do not matter in practice, in the worst case
 * we'll decide to merge ranges that are not the closest ones.
 */"""
)
INFO:__main__:Extracted 11 methods with comments in this batch
INFO:__main__:Fetching batch: offset=100, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=100, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.27s
INFO:__main__:Extracted 48 methods with comments in this batch
INFO:__main__:Fetching batch: offset=200, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=200, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.23s
INFO:__main__:Extracted 24 methods with comments in this batch
INFO:__main__:Fetching batch: offset=300, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=300, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres334[0m: [32mList[0m[[32mString[0m] = List(
  """tidstore_iter_extract_tids|||backend\access\common\tidstore.c|||579|||/* Extract TIDs from the given key-value pair */""",
  """pglz_compress_datum|||backend\access\common\toast_compression.c|||39|||/*
 * Compress a varlena using PGLZ.
 *
 * Returns the compressed varlena, or NULL if compression fails.
 */""",
  """pglz_decompress_datum|||backend\access\common\toast_compression.c|||81|||/*
 * Decompress a varlena that was compressed using PGLZ.
 */""",
  """pglz_decompress_datum_slice|||backend\access\common\toast_compression.c|||108|||/*
 * Decompress part of a varlena that was compressed using PGLZ.
 */""",
  """lz4_compress_datum|||backend\access\common\toast_compression.c|||138|||/*
 * Compress a varlena using LZ4.
 *
 * Returns the compressed varlena, or NULL if compression fails.
 */""",
  """lz4_decompress_datum|||backend\access\common\toast_compression.c|||181|||/*
 * Decompress a varlena that was compressed using LZ4.
 */""",
  """lz4_decompress_datum_slice|||backend\access\common\toast_compression.c|||214|||/*
 * Decompress part of a varlena that was compressed using LZ4.
 */""",
  """toast_get_compression_id|||backend\access\common\toast_compression.c|||253|||/*
 * Extract compression ID from a varlena.
 *
 * Returns TOAST_INVALID_COMPRESSION_ID if the varlena is not compressed.
 */""",
  """CompressionNameToMethod|||backend\access\common\toast_compression.c|||284|||/*
 * CompressionNameToMethod - Get compression method from compression name
 *
 * Search in the available built-in methods.  If the compression not found
 * in the built-in methods then return InvalidCompressionMethod.
 */""",
  """GetCompressionMethodName|||backend\access\common\toast_compression.c|||303|||/*
 * GetCompressionMethodName - Get compression method name
 */""",
  """toast_compress_datum|||backend\access\common\toast_internals.c|||45|||/* ----------
 * toast_compress_datum -
 *
 *	Create a compressed version of a varlena datum
 *
 *	If we fail (ie, compressed result is actually bigger than original)
 *	then return NULL.  We must not use compressed data if it'd expand
 *	the tuple!
 *
 *	We use VAR{SIZE,DATA}_ANY so we can handle short varlenas here without
 *	copying them.  But we can't handle external or compressed datums.
 * ----------
 */""",
  """toast_save_datum|||backend\access\common\toast_internals.c|||118|||/* ----------
 * toast_save_datum -
 *
 *	Save one single datum into the secondary relation and return
 *	a Datum reference for it.
 *
 * rel: the main relation we're working with (not the toast rel!)
 * value: datum to be pushed to toast storage
 * oldexternal: if not NULL, toast pointer previously representing the datum
 * options: options to be passed to heap_insert() for toast rows
 * ----------
 */""",
  """toast_delete_datum|||backend\access\common\toast_internals.c|||384|||/* ----------
 * toast_delete_datum -
 *
 *	Delete a single external stored value.
 * ----------
 */""",
  """toastrel_valueid_exists|||backend\access\common\toast_internals.c|||460|||/* ----------
 * toastrel_valueid_exists -
 *
 *	Test whether a toast value with the given ID exists in the toast relation.
 *	For safety, we consider a value to exist if there are either live or dead
 *	toast rows with that ID; see notes for GetNewOidWithIndex().
 * ----------
 */""",
  """toastid_valueid_exists|||backend\access\common\toast_internals.c|||508|||/* ----------
 * toastid_valueid_exists -
 *
 *	As above, but work from toast rel's OID not an open relation
 * ----------
 */""",
  """toast_get_valid_index|||backend\access\common\toast_internals.c|||529|||/* ----------
 * toast_get_valid_index
 *
 *	Get OID of valid index associated to given toast relation. A toast
 *	relation can have only one valid index at the same time.
 */""",
  """toast_open_indexes|||backend\access\common\toast_internals.c|||563|||/* ----------
 * toast_open_indexes
 *
 *	Get an array of the indexes associated to the given toast relation
 *	and return as well the position of the valid index used by the toast
 *	relation in this array. It is the responsibility of the caller of this
 *	function to close the indexes as well as free them.
 */""",
  """toast_close_indexes|||backend\access\common\toast_internals.c|||622|||/* ----------
 * toast_close_indexes
 *
 *	Close an array of indexes for a toast relation and free it. This should
 *	be called for a set of indexes opened previously with toast_open_indexes.
 */""",
  """init_toast_snapshot|||backend\access\common\toast_internals.c|||640|||/* ----------
 * init_toast_snapshot
 *
 *	Initialize an appropriate TOAST snapshot.  We must use an MVCC snapshot
 *	to initialize the TOAST snapshot; since we don't know which one to use,
 *	just use the oldest one.
 */""",
  """convert_tuples_by_position|||backend\access\common\tupconvert.c|||58|||/*
 * Set up for tuple conversion, matching input and output columns by
 * position.  (Dropped columns are ignored in both input and output.)
 */""",
  """convert_tuples_by_name|||backend\access\common\tupconvert.c|||101|||/*
 * Set up for tuple conversion, matching input and output columns by name.
 * (Dropped columns are ignored in both input and output.)	This is intended
 * for use when the rowtypes are related by inheritance, so we expect an exact
 * match of both type and typmod.  The error messages will be a bit unhelpful
 * unless both rowtypes are named composite types.
 */""",
  """convert_tuples_by_name_attrmap|||backend\access\common\tupconvert.c|||123|||/*
 * Set up tuple conversion for input and output TupleDescs using the given
 * AttrMap.
 */""",
  """execute_attr_map_tuple|||backend\access\common\tupconvert.c|||153|||/*
 * Perform conversion of a tuple according to the map.
 */""",
  """execute_attr_map_slot|||backend\access\common\tupconvert.c|||191|||/*
 * Perform conversion of a tuple slot according to the map.
 */""",
  """execute_attr_map_cols|||backend\access\common\tupconvert.c|||251|||/*
 * Perform conversion of bitmap of columns according to the map.
 *
 * The input and output bitmaps are offset by
 * FirstLowInvalidHeapAttributeNumber to accommodate system cols, like the
 * column-bitmaps in RangeTblEntry.
 */""",
  """free_conversion_map|||backend\access\common\tupconvert.c|||298|||/*
 * Free a TupleConversionMap structure.
 */""",
  """ResourceOwnerRememberTupleDesc|||backend\access\common\tupdesc.c|||47|||/* Convenience wrappers over ResourceOwnerRemember/Forget */""",
  """ResourceOwnerForgetTupleDesc|||backend\access\common\tupdesc.c|||53|||/* Convenience wrappers over ResourceOwnerRemember/Forget */""",
  """CreateTemplateTupleDesc|||backend\access\common\tupdesc.c|||66|||/*
 * CreateTemplateTupleDesc
 *		This function allocates an empty tuple descriptor structure.
 *
 * Tuple type ID information is initially set for an anonymous record type;
 * caller can overwrite this if needed.
 */""",
  """CreateTupleDesc|||backend\access\common\tupdesc.c|||111|||/*
 * CreateTupleDesc
 *		This function allocates a new TupleDesc by copying a given
 *		Form_pg_attribute array.
 *
 * Tuple type ID information is initially set for an anonymous record type;
 * caller can overwrite this if needed.
 */""",
  """CreateTupleDescCopy|||backend\access\common\tupdesc.c|||132|||/*
 * CreateTupleDescCopy
 *		This function creates a new TupleDesc by copying from an existing
 *		TupleDesc.
 *
 * !!! Constraints and defaults are not copied !!!
 */""",
  """CreateTupleDescCopyConstr|||backend\access\common\tupdesc.c|||172|||/*
 * CreateTupleDescCopyConstr
 *		This function creates a new TupleDesc by copying from an existing
 *		TupleDesc (including its constraints and defaults).
 */""",
  """TupleDescCopy|||backend\access\common\tupdesc.c|||250|||/*
 * TupleDescCopy
 *		Copy a tuple descriptor into caller-supplied memory.
 *		The memory may be shared memory mapped at any address, and must
 *		be sufficient to hold TupleDescSize(src) bytes.
 *
 * !!! Constraints and defaults are not copied !!!
 */""",
  """TupleDescCopyEntry|||backend\access\common\tupdesc.c|||288|||/*
 * TupleDescCopyEntry
 *		This function copies a single attribute structure from one tuple
 *		descriptor to another.
 *
 * !!! Constraints and defaults are not copied !!!
 */""",
  """FreeTupleDesc|||backend\access\common\tupdesc.c|||330|||/*
 * Free a TupleDesc including all substructure
 */""",
  """IncrTupleDescRefCount|||backend\access\common\tupdesc.c|||387|||/*
 * Increment the reference count of a tupdesc, and log the reference in
 * CurrentResourceOwner.
 *
 * Do not apply this to tupdescs that are not being refcounted.  (Use the
 * macro PinTupleDesc for tupdescs of uncertain status.)
 */""",
  """DecrTupleDescRefCount|||backend\access\common\tupdesc.c|||405|||/*
 * Decrement the reference count of a tupdesc, remove the corresponding
 * reference from CurrentResourceOwner, and free the tupdesc if no more
 * references remain.
 *
 * Do not apply this to tupdescs that are not being refcounted.  (Use the
 * macro ReleaseTupleDesc for tupdescs of uncertain status.)
 */""",
  """equalTupleDescs|||backend\access\common\tupdesc.c|||418|||/*
 * Compare two TupleDesc structures for logical equality
 */""",
  """equalRowTypes|||backend\access\common\tupdesc.c|||585|||/*
 * equalRowTypes
 *
 * This determines whether two tuple descriptors have equal row types.  This
 * only checks those fields in pg_attribute that are applicable for row types,
 * while ignoring those fields that define the physical row storage or those
 * that define table column metadata.
 *
 * Specifically, this checks:
 *
 * - same number of attributes
 * - same composite type ID (but could both be zero)
 * - corresponding attributes (in order) have same the name, type, typmod,
 *   collation
 *
 * This is used to check whether two record types are compatible, whether
 * function return row types are the same, and other similar situations.
 *
 * (XXX There was some discussion whether attndims should be checked here, but
 * for now it has been decided not to.)
 *
 * Note: We deliberately do not check the tdtypmod field.  This allows
 * typcache.c to use this routine to see if a cached record type matches a
 * requested type.
 */""",
  """hashRowType|||backend\access\common\tupdesc.c|||621|||/*
 * hashRowType
 *
 * If two tuple descriptors would be considered equal by equalRowTypes()
 * then their hash value will be equal according to this function.
 */""",
  """TupleDescInitEntry|||backend\access\common\tupdesc.c|||650|||/*
 * TupleDescInitEntry
 *		This function initializes a single attribute structure in
 *		a previously allocated tuple descriptor.
 *
 * If attributeName is NULL, the attname field is set to an empty string
 * (this is for cases where we don't know or need a name for the field).
 * Also, some callers use this function to change the datatype-related fields
 * in an existing tupdesc; they pass attributeName = NameStr(att->attname)
 * to indicate that the attname field shouldn't be modified.
 *
 * Note that attcollation is set to the default for the specified datatype.
 * If a nondefault collation is needed, insert it afterwards using
 * TupleDescInitEntryCollation.
 */""",
  """TupleDescInitBuiltinEntry|||backend\access\common\tupdesc.c|||725|||/*
 * TupleDescInitBuiltinEntry
 *		Initialize a tuple descriptor without catalog access.  Only
 *		a limited range of builtin types are supported.
 */""",
  """TupleDescInitEntryCollation|||backend\access\common\tupdesc.c|||832|||/*
 * TupleDescInitEntryCollation
 *
 * Assign a nondefault collation to a previously initialized tuple descriptor
 * entry.
 */""",
  """BuildDescFromLists|||backend\access\common\tupdesc.c|||857|||/*
 * BuildDescFromLists
 *
 * Build a TupleDesc given lists of column names (as String nodes),
 * column type OIDs, typmods, and collation OIDs.
 *
 * No constraints are generated.
 *
 * This is for use with functions returning RECORD.
 */""",
  """TupleDescGetDefault|||backend\access\common\tupdesc.c|||898|||/*
 * Get default expression (or NULL if none) for the given attribute number.
 */""",
  """ResOwnerReleaseTupleDesc|||backend\access\common\tupdesc.c|||922|||/* ResourceOwner callbacks */""",
  """ResOwnerPrintTupleDesc|||backend\access\common\tupdesc.c|||933|||/* ResourceOwner callbacks */""",
  """ginarrayextract|||backend\access\gin\ginarrayproc.c|||32|||/*
 * extractValue support function
 */""",
  """ginarrayextract_2args|||backend\access\gin\ginarrayproc.c|||67|||/*
 * Formerly, ginarrayextract had only two arguments.  Now it has three,
 * but we still need a pg_proc entry with two args to support reloading
 * pre-9.1 contrib/intarray opclass declarations.  This compatibility
 * function should go away eventually.
 */""",
  """ginqueryarrayextract|||backend\access\gin\ginarrayproc.c|||78|||/*
 * extractQuery support function
 */""",
  """ginarrayconsistent|||backend\access\gin\ginarrayproc.c|||141|||/*
 * consistent support function
 */""",
  """ginarraytriconsistent|||backend\access\gin\ginarrayproc.c|||225|||/*
 * triconsistent support function
 */""",
  """ginTraverseLock|||backend\access\gin\ginbtree.c|||38|||/*
 * Lock buffer by needed method for search.
 */""",
  """ginFindLeafPage|||backend\access\gin\ginbtree.c|||82|||/*
 * Descend the tree to the leaf page that contains or would contain the key
 * we're searching for. The key should already be filled in 'btree', in
 * tree-type specific manner. If btree->fullScan is true, descends to the
 * leftmost leaf page.
 *
 * If 'searchmode' is false, on return stack->buffer is exclusively locked,
 * and the stack represents the full path to the root. Otherwise stack->buffer
 * is share-locked, and stack->parent is NULL.
 *
 * If 'rootConflictCheck' is true, tree root is checked for serialization
 * conflict.
 */""",
  """ginStepRight|||backend\access\gin\ginbtree.c|||176|||/*
 * Step right from current page.
 *
 * The next page is locked first, before releasing the current page. This is
 * crucial to prevent concurrent VACUUM from deleting a page that we are about
 * to step to. (The lock-coupling isn't strictly necessary when we are
 * traversing the tree to find an insert location, because page deletion grabs
 * a cleanup lock on the root to prevent any concurrent inserts. See Page
 * deletion section in the README. But there's no harm in doing it always.)
 */""",
  """freeGinBtreeStack|||backend\access\gin\ginbtree.c|||197|||/*
 * Step right from current page.
 *
 * The next page is locked first, before releasing the current page. This is
 * crucial to prevent concurrent VACUUM from deleting a page that we are about
 * to step to. (The lock-coupling isn't strictly necessary when we are
 * traversing the tree to find an insert location, because page deletion grabs
 * a cleanup lock on the root to prevent any concurrent inserts. See Page
 * deletion section in the README. But there's no harm in doing it always.)
 */""",
  """ginFindParents|||backend\access\gin\ginbtree.c|||217|||/*
 * Try to find parent for current stack position. Returns correct parent and
 * child's offset in stack->parent. The root page is never released, to
 * prevent conflict with vacuum process.
 */""",
  """ginPlaceToPage|||backend\access\gin\ginbtree.c|||336|||/*
 * Insert a new item to a page.
 *
 * Returns true if the insertion was finished. On false, the page was split and
 * the parent needs to be updated. (A root split returns true as it doesn't
 * need any further action by the caller to complete.)
 *
 * When inserting a downlink to an internal page, 'childbuf' contains the
 * child page that was split. Its GIN_INCOMPLETE_SPLIT flag will be cleared
 * atomically with the insert. Also, the existing item at offset stack->off
 * in the target page is updated to point to updateblkno.
 *
 * stack->buffer is locked on entry, and is kept locked.
 * Likewise for childbuf, if given.
 */""",
  """ginFinishSplit|||backend\access\gin\ginbtree.c|||671|||/*
 * Finish a split by inserting the downlink for the new page to parent.
 *
 * On entry, stack->buffer is exclusively locked.
 *
 * If freestack is true, all the buffers are released and unlocked as we
 * crawl up the tree, and 'stack' is freed. Otherwise stack->buffer is kept
 * locked, and stack is unmodified, except for possibly moving right to find
 * the correct parent of page.
 */""",
  """ginFinishOldSplit|||backend\access\gin\ginbtree.c|||778|||/*
 * An entry point to ginFinishSplit() that is used when we stumble upon an
 * existing incompletely split page in the tree, as opposed to completing a
 * split that we just made ourselves. The difference is that stack->buffer may
 * be merely share-locked on entry, and will be upgraded to exclusive mode.
 *
 * Note: Upgrading the lock momentarily releases it. Doing that in a scan
 * would not be OK, because a concurrent VACUUM might delete the page while
 * we're not holding the lock. It's OK in an insert, though, because VACUUM
 * has a different mechanism that prevents it from running concurrently with
 * inserts. (Namely, it holds a cleanup lock on the root.)
 */""",
  """ginInsertValue|||backend\access\gin\ginbtree.c|||815|||/*
 * Insert a value to tree described by stack.
 *
 * The value to be inserted is given in 'insertdata'. Its format depends
 * on whether this is an entry or data tree, ginInsertValue just passes it
 * through to the tree-specific callback function.
 *
 * During an index build, buildStats is non-null and the counters it contains
 * are incremented as needed.
 *
 * NB: the passed-in stack is freed, as though by freeGinBtreeStack.
 */""",
  """ginCombineData|||backend\access\gin\ginbulk.c|||29|||/* Combiner function for rbtree.c */""",
  """cmpEntryAccumulator|||backend\access\gin\ginbulk.c|||71|||/* Comparator function for rbtree.c */""",
  """ginAllocEntryAccumulator|||backend\access\gin\ginbulk.c|||84|||/* Allocator function for rbtree.c */""",
  """ginInitBA|||backend\access\gin\ginbulk.c|||108|||	/* accum->ginstate is intentionally not set here */""",
  """getDatumCopy|||backend\access\gin\ginbulk.c|||127|||/*
 * This is basically the same as datumCopy(), but extended to count
 * palloc'd space in accum->allocatedMemory.
 */""",
  """ginInsertBAEntry|||backend\access\gin\ginbulk.c|||147|||/*
 * Find/store one entry from indexed value.
 */""",
  """ginInsertBAEntries|||backend\access\gin\ginbulk.c|||209|||/*
 * Insert the entries for one heap pointer.
 *
 * Since the entries are being inserted into a balanced binary tree, you
 * might think that the order of insertion wouldn't be critical, but it turns
 * out that inserting the entries in sorted order results in a lot of
 * rebalancing operations and is slow.  To prevent this, we attempt to insert
 * the nodes in an order that will produce a nearly-balanced tree if the input
 * is in fact sorted.
 *
 * We do this as follows.  First, we imagine that we have an array whose size
 * is the smallest power of two greater than or equal to the actual array
 * size.  Second, we insert the middle entry of our virtual array into the
 * tree; then, we insert the middles of each half of our virtual array, then
 * middles of quarters, etc.
 */""",
  """ginBeginBAScan|||backend\access\gin\ginbulk.c|||256|||/* Prepare to read out the rbtree contents using ginGetBAEntry */""",
  """ginGetBAEntry|||backend\access\gin\ginbulk.c|||267|||/*
 * Get the next entry in sequence from the BuildAccumulator's rbtree.
 * This consists of a single key datum and a list (array) of one or more
 * heap TIDs in which that key is found.  The list is guaranteed sorted.
 */""",
  """GinDataLeafPageGetItems|||backend\access\gin\gindatapage.c|||134|||/*
 * Read TIDs from leaf data page to single uncompressed array. The TIDs are
 * returned in ascending order.
 *
 * advancePast is a hint, indicating that the caller is only interested in
 * TIDs > advancePast. To return all items, use ItemPointerSetMin.
 *
 * Note: This function can still return items smaller than advancePast that
 * are in the same posting list as the items of interest, so the caller must
 * still check all the returned items. But passing it allows this function to
 * skip whole posting lists.
 */""",
  """GinDataLeafPageGetItemsToTbm|||backend\access\gin\gindatapage.c|||181|||/*
 * Places all TIDs from leaf data page to bitmap.
 */""",
  """dataLeafPageGetUncompressed|||backend\access\gin\gindatapage.c|||210|||/*
 * Get pointer to the uncompressed array of items on a pre-9.4 format
 * uncompressed leaf page. The number of items in the array is returned in
 * *nitems.
 */""",
  """dataIsMoveRight|||backend\access\gin\gindatapage.c|||233|||/*
 * Check if we should follow the right link to find the item we're searching
 * for.
 *
 * Compares inserting item pointer with the right bound of the current page.
 */""",
  """dataLocateItem|||backend\access\gin\gindatapage.c|||251|||/*
 * Find correct PostingItem in non-leaf page. It is assumed that this is
 * the correct page, and the searched value SHOULD be on the page.
 */""",
  """dataFindChildPtr|||backend\access\gin\gindatapage.c|||318|||/*
 * Find link to blkno on non-leaf page, returns offset of PostingItem
 */""",
  """dataGetLeftMostPage|||backend\access\gin\gindatapage.c|||363|||/*
 * Return blkno of leftmost child
 */""",
  """GinDataPageAddPostingItem|||backend\access\gin\gindatapage.c|||379|||/*
 * Add PostingItem to a non-leaf page.
 */""",
  """GinPageDeletePostingItem|||backend\access\gin\gindatapage.c|||416|||/*
 * Delete posting item from non-leaf page
 */""",
  """dataBeginPlaceToPageLeaf|||backend\access\gin\gindatapage.c|||447|||/*
 * Prepare to insert data on a leaf data page.
 *
 * If it will fit, return GPTP_INSERT after doing whatever setup is needed
 * before we enter the insertion critical section.  *ptp_workspace can be
 * set to pass information along to the execPlaceToPage function.
 *
 * If it won't fit, perform a page split and return two temporary page
 * images into *newlpage and *newrpage, with result GPTP_SPLIT.
 *
 * In neither case should the given page buffer be modified here.
 */""",
  """dataExecPlaceToPageLeaf|||backend\access\gin\gindatapage.c|||715|||/*
 * Perform data insertion after beginPlaceToPage has decided it will fit.
 *
 * This is invoked within a critical section, and XLOG record creation (if
 * needed) is already started.  The target buffer is registered in slot 0.
 */""",
  """ginVacuumPostingTreeLeaf|||backend\access\gin\gindatapage.c|||737|||/*
 * Vacuum a posting tree leaf page.
 */""",
  """computeLeafRecompressWALData|||backend\access\gin\gindatapage.c|||871|||/*
 * Construct a ginxlogRecompressDataLeaf record representing the changes
 * in *leaf.  (Because this requires a palloc, we have to do it before
 * we enter the critical section that actually updates the page.)
 */""",
  """dataPlaceToPageLeafRecompress|||backend\access\gin\gindatapage.c|||977|||/*
 * Assemble a disassembled posting tree leaf page back to a buffer.
 *
 * This just updates the target buffer; WAL stuff is caller's responsibility.
 *
 * NOTE: The segment pointers must not point directly to the same buffer,
 * except for segments that have not been modified and whose preceding
 * segments have not been modified either.
 */""",
  """dataPlaceToPageLeafSplit|||backend\access\gin\gindatapage.c|||1033|||/*
 * Like dataPlaceToPageLeafRecompress, but writes the disassembled leaf
 * segments to two pages instead of one.
 *
 * This is different from the non-split cases in that this does not modify
 * the original page directly, but writes to temporary in-memory copies of
 * the new left and right pages.
 */""",
  """dataBeginPlaceToPageInternal|||backend\access\gin\gindatapage.c|||1118|||/*
 * Prepare to insert data on an internal data page.
 *
 * If it will fit, return GPTP_INSERT after doing whatever setup is needed
 * before we enter the insertion critical section.  *ptp_workspace can be
 * set to pass information along to the execPlaceToPage function.
 *
 * If it won't fit, perform a page split and return two temporary page
 * images into *newlpage and *newrpage, with result GPTP_SPLIT.
 *
 * In neither case should the given page buffer be modified here.
 *
 * Note: on insertion to an internal node, in addition to inserting the given
 * item, the downlink of the existing item at stack->off will be updated to
 * point to updateblkno.
 */""",
  """dataExecPlaceToPageInternal|||backend\access\gin\gindatapage.c|||1144|||/*
 * Perform data insertion after beginPlaceToPage has decided it will fit.
 *
 * This is invoked within a critical section, and XLOG record creation (if
 * needed) is already started.  The target buffer is registered in slot 0.
 */""",
  """dataBeginPlaceToPage|||backend\access\gin\gindatapage.c|||1200|||/*
 * Prepare to insert data on a posting-tree data page.
 *
 * If it will fit, return GPTP_INSERT after doing whatever setup is needed
 * before we enter the insertion critical section.  *ptp_workspace can be
 * set to pass information along to the execPlaceToPage function.
 *
 * If it won't fit, perform a page split and return two temporary page
 * images into *newlpage and *newrpage, with result GPTP_SPLIT.
 *
 * In neither case should the given page buffer be modified here.
 *
 * Note: on insertion to an internal node, in addition to inserting the given
 * item, the downlink of the existing item at stack->off will be updated to
 * point to updateblkno.
 *
 * Calls relevant function for internal or leaf page because they are handled
 * very differently.
 */""",
  """dataExecPlaceToPage|||backend\access\gin\gindatapage.c|||1230|||/*
 * Perform data insertion after beginPlaceToPage has decided it will fit.
 *
 * This is invoked within a critical section, and XLOG record creation (if
 * needed) is already started.  The target buffer is registered in slot 0.
 *
 * Calls relevant function for internal or leaf page because they are handled
 * very differently.
 */""",
  """dataSplitPageInternal|||backend\access\gin\gindatapage.c|||1251|||/*
 * Split internal page and insert new data.
 *
 * Returns new temp pages to *newlpage and *newrpage.
 * The original buffer is left untouched.
 */""",
  """dataPrepareDownlink|||backend\access\gin\gindatapage.c|||1332|||/*
 * Construct insertion payload for inserting the downlink for given buffer.
 */""",
  """ginDataFillRoot|||backend\access\gin\gindatapage.c|||1348|||/*
 * Fills new root by right bound values from child.
 * Also called from ginxlog, should not use btree
 */""",
  """disassembleLeaf|||backend\access\gin\gindatapage.c|||1369|||/*
 * Disassemble page into a disassembledLeaf struct.
 */""",
  """addItemsToLeaf|||backend\access\gin\gindatapage.c|||1443|||/*
 * Distribute newItems to the segments.
 *
 * Any segments that acquire new items are decoded, and the new items are
 * merged with the old items.
 *
 * Returns true if any new items were added. False means they were all
 * duplicates of existing items on the page.
 */""",
  """leafRepackItems|||backend\access\gin\gindatapage.c|||1570|||/*
 * Recompresses all segments that have been modified.
 *
 * If not all the items fit on two pages (ie. after split), we store as
 * many items as fit, and set *remaining to the first item that didn't fit.
 * If all items fit, *remaining is set to invalid.
 *
 * Returns true if the page has to be split.
 */""",
  """createPostingTree|||backend\access\gin\gindatapage.c|||1774|||/*
 * Creates new posting tree containing the given TIDs. Returns the page
 * number of the root of the new posting tree.
 *
 * items[] must be in sorted order with no duplicates.
 */""",
  """ginInsertItemPointers|||backend\access\gin\gindatapage.c|||1907|||/*
 * Inserts array of item pointers, may execute several tree scan (very rare)
 */""",
  """ginScanBeginPostingTree|||backend\access\gin\gindatapage.c|||1935|||/*
 * Starts a new scan on a posting tree.
 */""",
  """GinFormTuple|||backend\access\gin\ginentrypage.c|||43|||/*
 * Form a tuple for entry tree.
 *
 * If the tuple would be too big to be stored, function throws a suitable
 * error if errorTooBig is true, or returns NULL if errorTooBig is false.
 *
 * See src/backend/access/gin/README for a description of the index tuple
 * format that is being built here.  We build on the assumption that we
 * are making a leaf-level key entry containing a posting list of nipd items.
 * If the caller is actually trying to make a posting-tree entry, non-leaf
 * entry, or pending-list entry, it should pass dataSize = 0 and then overwrite
 * the t_tid fields as necessary.  In any case, 'data' can be NULL to skip
 * filling in the posting list; the caller is responsible for filling it
 * afterwards if data = NULL and nipd > 0.
 */""",
  """ginReadTuple|||backend\access\gin\ginentrypage.c|||161|||/*
 * Read item pointers from leaf entry tuple.
 *
 * Returns a palloc'd array of ItemPointers. The number of items is returned
 * in *nitems.
 */"""
)
INFO:__main__:Extracted 100 methods with comments in this batch
INFO:__main__:Fetching batch: offset=400, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=400, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.25s
INFO:__main__:Extracted 1 methods with comments in this batch
INFO:__main__:Fetching batch: offset=500, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=500, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.24s
INFO:__main__:Extracted 64 methods with comments in this batch
INFO:__main__:Fetching batch: offset=600, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=600, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.20s
INFO:__main__:Extracted 4 methods with comments in this batch
INFO:__main__:Fetching batch: offset=700, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=700, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres338[0m: [32mList[0m[[32mString[0m] = List(
  """_hash_pageinit|||backend\access\hash\hashpage.c|||595|||/*
 *	_hash_pageinit() -- Initialize a new hash index page.
 */""",
  """_hash_expandtable|||backend\access\hash\hashpage.c|||613|||/*
 * Attempt to expand the hash table by creating one new bucket.
 *
 * This will silently do nothing if we don't get cleanup lock on old or
 * new bucket.
 *
 * Complete the pending splits and remove the tuples from old bucket,
 * if there are any left over from the previous split.
 *
 * The caller must hold a pin, but no lock, on the metapage buffer.
 * The buffer is returned in the same state.
 */""",
  """_hash_alloc_buckets|||backend\access\hash\hashpage.c|||991|||/*
 * _hash_alloc_buckets -- allocate a new splitpoint's worth of bucket pages
 *
 * This does not need to initialize the new bucket pages; we'll do that as
 * each one is used by _hash_expandtable().  But we have to extend the logical
 * EOF to the end of the splitpoint; this keeps smgr's idea of the EOF in
 * sync with ours, so that we don't get complaints from smgr.
 *
 * We do this by writing a page of zeroes at the end of the splitpoint range.
 * We expect that the filesystem will ensure that the intervening pages read
 * as zeroes too.  On many filesystems this "hole" will not be allocated
 * immediately, which means that the index file may end up more fragmented
 * than if we forced it all to be allocated now; but since we don't scan
 * hash indexes sequentially anyway, that probably doesn't matter.
 *
 * XXX It's annoying that this code is executed with the metapage lock held.
 * We need to interlock against _hash_addovflpage() adding a new overflow page
 * concurrently, but it'd likely be better to use LockRelationForExtension
 * for the purpose.  OTOH, adding a splitpoint is a very infrequent operation,
 * so it may not be worth worrying about.
 *
 * Returns true if successful, or false if allocation failed due to
 * BlockNumber overflow.
 */""",
  """_hash_splitbucket|||backend\access\hash\hashpage.c|||1072|||/*
 * _hash_splitbucket -- split 'obucket' into 'obucket' and 'nbucket'
 *
 * This routine is used to partition the tuples between old and new bucket and
 * is used to finish the incomplete split operations.  To finish the previously
 * interrupted split operation, the caller needs to fill htab.  If htab is set,
 * then we skip the movement of tuples that exists in htab, otherwise NULL
 * value of htab indicates movement of all the tuples that belong to the new
 * bucket.
 *
 * We are splitting a bucket that consists of a base bucket page and zero
 * or more overflow (bucket chain) pages.  We must relocate tuples that
 * belong in the new bucket.
 *
 * The caller must hold cleanup locks on both buckets to ensure that
 * no one else is trying to access them (see README).
 *
 * The caller must hold a pin, but no lock, on the metapage buffer.
 * The buffer is returned in the same state.  (The metapage is only
 * touched if it becomes necessary to add or remove overflow pages.)
 *
 * Split needs to retain pin on primary bucket pages of both old and new
 * buckets till end of operation.  This is to prevent vacuum from starting
 * while a split is in progress.
 *
 * In addition, the caller must have created the new bucket's base page,
 * which is passed in buffer nbuf, pinned and write-locked.  The lock will be
 * released here and pin must be released by the caller.  (The API is set up
 * this way because we must do _hash_getnewbuf() before releasing the metapage
 * write lock.  So instead of passing the new bucket's start block number, we
 * pass an actual buffer.)
 */""",
  """_hash_finish_split|||backend\access\hash\hashpage.c|||1355|||/*
 *	_hash_finish_split() -- Finish the previously interrupted split operation
 *
 * To complete the split operation, we form the hash table of TIDs in new
 * bucket which is then used by split operation to skip tuples that are
 * already moved before the split operation was previously interrupted.
 *
 * The caller must hold a pin, but no lock, on the metapage and old bucket's
 * primary page buffer.  The buffers are returned in the same state.  (The
 * metapage is only touched if it becomes necessary to add or remove overflow
 * pages.)
 */""",
  """log_split_page|||backend\access\hash\hashpage.c|||1473|||/*
 *	log_split_page() -- Log the split operation
 *
 *	We log the split operation when the new page in new bucket gets full,
 *	so we log the entire page.
 *
 *	'buf' must be locked by the caller which is also responsible for unlocking
 *	it.
 */""",
  """_hash_getcachedmetap|||backend\access\hash\hashpage.c|||1500|||/*
 *	_hash_getcachedmetap() -- Returns cached metapage data.
 *
 *	If metabuf is not InvalidBuffer, caller must hold a pin, but no lock, on
 *	the metapage.  If not set, we'll set it before returning if we have to
 *	refresh the cache, and return with a pin but no lock on it; caller is
 *	responsible for releasing the pin.
 *
 *	We refresh the cache if it's not initialized yet or force_refresh is true.
 */""",
  """_hash_getbucketbuf_from_hashkey|||backend\access\hash\hashpage.c|||1558|||/*
 *	_hash_getbucketbuf_from_hashkey() -- Get the bucket's buffer for the given
 *										 hashkey.
 *
 *	Bucket pages do not move or get removed once they are allocated. This give
 *	us an opportunity to use the previously saved metapage contents to reach
 *	the target bucket buffer, instead of reading from the metapage every time.
 *	This saves one buffer access every time we want to reach the target bucket
 *	buffer, which is very helpful savings in bufmgr traffic and contention.
 *
 *	The access type parameter (HASH_READ or HASH_WRITE) indicates whether the
 *	bucket buffer has to be locked for reading or writing.
 *
 *	The out parameter cachedmetap is set with metapage contents used for
 *	hashkey to bucket buffer mapping. Some callers need this info to reach the
 *	old bucket in case of bucket split, see _hash_doinsert().
 */""",
  """_hash_next|||backend\access\hash\hashsearch.c|||47|||/*
 *	_hash_next() -- Get the next item in a scan.
 *
 *		On entry, so->currPos describes the current page, which may
 *		be pinned but not locked, and so->currPos.itemIndex identifies
 *		which item was previously returned.
 *
 *		On successful exit, scan->xs_heaptid is set to the TID of the next
 *		heap tuple.  so->currPos is updated as needed.
 *
 *		On failure exit (no more tuples), we return false with pin
 *		held on bucket page but no pins or locks held on overflow
 *		page.
 */""",
  """_hash_readnext|||backend\access\hash\hashsearch.c|||130|||/*
 * Advance to next page in a bucket, if any.  If we are scanning the bucket
 * being populated during split operation then this function advances to the
 * bucket being split after the last bucket page of bucket being populated.
 */""",
  """_hash_readprev|||backend\access\hash\hashsearch.c|||196|||/*
 * Advance to previous page in a bucket, if any.  If the current scan has
 * started during split operation then this function advances to bucket
 * being populated after the first bucket page of bucket being split.
 */""",
  """_hash_first|||backend\access\hash\hashsearch.c|||287|||/*
 *	_hash_first() -- Find the first item in a scan.
 *
 *		We find the first item (or, if backward scan, the last item) in the
 *		index that satisfies the qualification associated with the scan
 *		descriptor.
 *
 *		On successful exit, if the page containing current index tuple is an
 *		overflow page, both pin and lock are released whereas if it is a bucket
 *		page then it is pinned but not locked and data about the matching
 *		tuple(s) on the page has been loaded into so->currPos,
 *		scan->xs_heaptid is set to the heap TID of the current tuple.
 *
 *		On failure exit (no more tuples), we return false, with pin held on
 *		bucket page but no pins or locks held on overflow page.
 */""",
  """_hash_readpage|||backend\access\hash\hashsearch.c|||445|||/*
 *	_hash_readpage() -- Load data from current index page into so->currPos
 *
 *	We scan all the items in the current index page and save them into
 *	so->currPos if it satisfies the qualification. If no matching items
 *	are found in the current page, we move to the next or previous page
 *	in a bucket chain as indicated by the direction.
 *
 *	Return true if any matching items are found else return false.
 */""",
  """_hash_load_qualified_items|||backend\access\hash\hashsearch.c|||601|||/*
 * Load all the qualified items from a current index page
 * into so->currPos. Helper function for _hash_readpage.
 */""",
  """_hash_saveitem|||backend\access\hash\hashsearch.c|||707|||/* Save an index item into so->currPos.items[itemIndex] */""",
  """_h_spoolinit|||backend\access\hash\hashsort.c|||59|||/*
 * create and initialize a spool structure
 */""",
  """_h_spooldestroy|||backend\access\hash\hashsort.c|||98|||/*
 * clean up a spool structure and its substructures.
 */""",
  """_h_spool|||backend\access\hash\hashsort.c|||108|||/*
 * spool an index entry into the sort file.
 */""",
  """_h_indexbuild|||backend\access\hash\hashsort.c|||119|||/*
 * given a spool loaded by successive calls to _h_spool,
 * create an entire index.
 */""",
  """_hash_checkqual|||backend\access\hash\hashutil.c|||30|||/*
 * _hash_checkqual -- does the index tuple satisfy the scan conditions?
 */""",
  """_hash_datum2hashkey|||backend\access\hash\hashutil.c|||81|||/*
 * _hash_datum2hashkey -- given a Datum, call the index's hash function
 *
 * The Datum is assumed to be of the index's column type, so we can use the
 * "primary" hash function that's tracked for us by the generic index code.
 */""",
  """_hash_datum2hashkey_type|||backend\access\hash\hashutil.c|||101|||/*
 * _hash_datum2hashkey_type -- given a Datum of a specified type,
 *			hash it in a fashion compatible with this index
 *
 * This is much more expensive than _hash_datum2hashkey, so use it only in
 * cross-type situations.
 */""",
  """_hash_hashkey2bucket|||backend\access\hash\hashutil.c|||124|||/*
 * _hash_hashkey2bucket -- determine which bucket the hashkey maps to.
 */""",
  """_hash_spareindex|||backend\access\hash\hashutil.c|||141|||/*
 * _hash_spareindex -- returns spare index / global splitpoint phase of the
 *					   bucket
 */""",
  """_hash_get_totalbuckets|||backend\access\hash\hashutil.c|||173|||/*
 *	_hash_get_totalbuckets -- returns total number of buckets allocated till
 *							the given splitpoint phase.
 */""",
  """_hash_checkpage|||backend\access\hash\hashutil.c|||209|||/*
 * _hash_checkpage -- sanity checks on the format of all hash pages
 *
 * If flags is not zero, it is a bitwise OR of the acceptable page types
 * (values of hasho_flag & LH_PAGE_TYPE).
 */""",
  """_hash_get_indextuple_hashkey|||backend\access\hash\hashutil.c|||290|||/*
 * _hash_get_indextuple_hashkey - get the hash index tuple's hash key value
 */""",
  """_hash_convert_tuple|||backend\access\hash\hashutil.c|||317|||/*
 * _hash_convert_tuple - convert raw index data to hash key
 *
 * Inputs: values and isnull arrays for the user data column(s)
 * Outputs: values and isnull arrays for the index tuple, suitable for
 *		passing to index_form_tuple().
 *
 * Returns true if successful, false if not (because there are null values).
 * On a false result, the given data need not be indexed.
 *
 * Note: callers know that the index-column arrays are always of length 1.
 * In principle, there could be more than one input column, though we do not
 * currently support that.
 */""",
  """_hash_binsearch|||backend\access\hash\hashutil.c|||349|||/*
 * _hash_binsearch - Return the offset number in the page where the
 *					 specified hash value should be sought or inserted.
 *
 * We use binary search, relying on the assumption that the existing entries
 * are ordered by hash key.
 *
 * Returns the offset of the first index entry having hashkey >= hash_value,
 * or the page's max offset plus one if hash_value is greater than all
 * existing hash keys in the page.  This is the appropriate place to start
 * a search, or to insert a new item.
 */""",
  """_hash_binsearch_last|||backend\access\hash\hashutil.c|||387|||/*
 * _hash_binsearch_last
 *
 * Same as above, except that if there are multiple matching items in the
 * page, we return the offset of the last one instead of the first one,
 * and the possible range of outputs is 0..maxoffset not 1..maxoffset+1.
 * This is handy for starting a new page in a backwards scan.
 */""",
  """_hash_get_oldblock_from_newbucket|||backend\access\hash\hashutil.c|||421|||/*
 *	_hash_get_oldblock_from_newbucket() -- get the block number of a bucket
 *			from which current (new) bucket is being split.
 */""",
  """_hash_get_newblock_from_oldbucket|||backend\access\hash\hashutil.c|||460|||/*
 *	_hash_get_newblock_from_oldbucket() -- get the block number of a bucket
 *			that will be generated after split from old bucket.
 *
 * This is used to find the new bucket from old bucket based on current table
 * half.  It is mainly required to finish the incomplete splits where we are
 * sure that not more than one bucket could have split in progress from old
 * bucket.
 */""",
  """_hash_get_newbucket_from_oldbucket|||backend\access\hash\hashutil.c|||493|||/*
 *	_hash_get_newbucket_from_oldbucket() -- get the new bucket that will be
 *			generated after split from current (old) bucket.
 *
 * This is used to find the new bucket from old bucket.  New bucket can be
 * obtained by OR'ing old bucket with most significant bit of current table
 * half (lowmask passed in this function can be used to identify msb of
 * current table half).  There could be multiple buckets that could have
 * been split from current bucket.  We need the first such bucket that exists.
 * Caller must ensure that no more than one split has happened from old
 * bucket.
 */""",
  """_hash_kill_items|||backend\access\hash\hashutil.c|||535|||/*
 * _hash_kill_items - set LP_DEAD state for items an indexscan caller has
 * told us were killed.
 *
 * scan->opaque, referenced locally through so, contains information about the
 * current page and killed tuples thereon (generally, this should only be
 * called if so->numKilled > 0).
 *
 * The caller does not have a lock on the page and may or may not have the
 * page pinned in a buffer.  Note that read-lock is sufficient for setting
 * LP_DEAD status (which is only a hint).
 *
 * The caller must have pin on bucket buffer, but may or may not have pin
 * on overflow buffer, as indicated by HashScanPosIsPinned(so->currPos).
 *
 * We match items by heap TID before assuming they are the right ones to
 * delete.
 *
 * There are never any scans active in a bucket at the time VACUUM begins,
 * because VACUUM takes a cleanup lock on the primary bucket page and scans
 * hold a pin.  A scan can begin after VACUUM leaves the primary bucket page
 * but before it finishes the entire bucket, but it can never pass VACUUM,
 * because VACUUM always locks the next page before releasing the lock on
 * the previous one.  Therefore, we don't have to worry about accidentally
 * killing a TID that has been reused for an unrelated tuple.
 */""",
  """hashvalidate|||backend\access\hash\hashvalidate.c|||46|||/*
 * Validator for a hash opclass.
 *
 * Some of the checks done here cover the whole opfamily, and therefore are
 * redundant when checking each opclass in a family.  But they don't run long
 * enough to be much of a problem, so we accept the duplication rather than
 * complicate the amvalidate API.
 */""",
  """check_hash_func_signature|||backend\access\hash\hashvalidate.c|||274|||/*
 * We need a custom version of check_amproc_signature because of assorted
 * hacks in the core hash opclass definitions.
 */""",
  """hashadjustmembers|||backend\access\hash\hashvalidate.c|||351|||/*
 * Prechecking function for adding operators/functions to a hash opfamily.
 */""",
  """AssertHasSnapshotForToast|||backend\access\heap\heapam.c|||237|||/*
 * Check that we have a valid snapshot if we might need TOAST access.
 */""",
  """heap_scan_stream_read_next_parallel|||backend\access\heap\heapam.c|||275|||/*
 * Streaming read API callback for parallel sequential scans. Returns the next
 * block the caller wants from the read stream or InvalidBlockNumber when done.
 */""",
  """heap_scan_stream_read_next_serial|||backend\access\heap\heapam.c|||313|||/*
 * Streaming read API callback for serial sequential and TID range scans.
 * Returns the next block the caller wants from the read stream or
 * InvalidBlockNumber when done.
 */""",
  """initscan|||backend\access\heap\heapam.c|||337|||/* ----------------
 *		initscan - scan code common to heap_beginscan and heap_rescan
 * ----------------
 */""",
  """heap_setscanlimits|||backend\access\heap\heapam.c|||465|||/*
 * heap_setscanlimits - restrict range of a heapscan
 *
 * startBlk is the page to start at
 * numBlks is number of pages to scan (InvalidBlockNumber means "all")
 */""",
  """page_collect_tuples|||backend\access\heap\heapam.c|||486|||/*
 * Per-tuple loop for heap_prepare_pagescan(). Pulled out so it can be called
 * multiple times, with constant arguments for all_visible,
 * check_serializable.
 */""",
  """heap_prepare_pagescan|||backend\access\heap\heapam.c|||537|||/*
 * heap_prepare_pagescan - Prepare current scan page to be scanned in pagemode
 *
 * Preparation currently consists of 1. prune the scan's rs_cbuf page, and 2.
 * fill the rs_vistuples[] array with the OffsetNumbers of visible tuples.
 */""",
  """heap_fetch_next_buffer|||backend\access\heap\heapam.c|||628|||/*
 * heap_fetch_next_buffer - read and pin the next block from MAIN_FORKNUM.
 *
 * Read the next block of the scan relation from the read stream and save it
 * in the scan descriptor.  It is already pinned.
 */""",
  """heapgettup_initial_block|||backend\access\heap\heapam.c|||673|||/*
 * heapgettup_initial_block - return the first BlockNumber to scan
 *
 * Returns InvalidBlockNumber when there are no blocks to scan.  This can
 * occur with empty tables and in parallel scans when parallel workers get all
 * of the pages before we can get a chance to get our first page.
 */""",
  """heapgettup_start_page|||backend\access\heap\heapam.c|||720|||/*
 * heapgettup_start_page - helper function for heapgettup()
 *
 * Return the next page to scan based on the scan->rs_cbuf and set *linesleft
 * to the number of tuples on this page.  Also set *lineoff to the first
 * offset to scan with forward scans getting the first offset and backward
 * getting the final offset on the page.
 */""",
  """heapgettup_continue_page|||backend\access\heap\heapam.c|||751|||/*
 * heapgettup_continue_page - helper function for heapgettup()
 *
 * Return the next page to scan based on the scan->rs_cbuf and set *linesleft
 * to the number of tuples left to scan on this page.  Also set *lineoff to
 * the next offset to scan according to the ScanDirection in 'dir'.
 */""",
  """heapgettup_advance_block|||backend\access\heap\heapam.c|||797|||/*
 * heapgettup_advance_block - helper for heap_fetch_next_buffer()
 *
 * Given the current block number, the scan direction, and various information
 * contained in the scan descriptor, calculate the BlockNumber to scan next
 * and return it.  If there are no further blocks to scan, return
 * InvalidBlockNumber to indicate this fact to the caller.
 *
 * This should not be called to determine the initial block number -- only for
 * subsequent blocks.
 *
 * This also adjusts rs_numblocks when a limit has been imposed by
 * heap_setscanlimits().
 */""",
  """heapgettup|||backend\access\heap\heapam.c|||881|||/* ----------------
 *		heapgettup - fetch next heap tuple
 *
 *		Initialize the scan if not already done; then advance to the next
 *		tuple as indicated by "dir"; return the next tuple in scan->rs_ctup,
 *		or set scan->rs_ctup.t_data = NULL if no more tuples.
 *
 * Note: the reason nkeys/key are passed separately, even though they are
 * kept in the scan descriptor, is that the caller may not want us to check
 * the scankeys.
 *
 * Note: when we fall off the end of the scan in either direction, we
 * reset rs_inited.  This means that a further request with the same
 * scan direction will restart the scan, which is a bit odd, but a
 * request with the opposite scan direction will start a fresh scan
 * in the proper direction.  The latter is required behavior for cursors,
 * while the former case is generally undefined behavior in Postgres
 * so we don't care too much.
 * ----------------
 */""",
  """heapgettup_pagemode|||backend\access\heap\heapam.c|||991|||/* ----------------
 *		heapgettup_pagemode - fetch next heap tuple in page-at-a-time mode
 *
 *		Same API as heapgettup, but used in page-at-a-time mode
 *
 * The internal logic is much the same as heapgettup's too, but there are some
 * differences: we do not take the buffer content lock (that only needs to
 * happen inside heap_prepare_pagescan), and we iterate through just the
 * tuples listed in rs_vistuples[] rather than all tuples on the page.  Notice
 * that lineindex is 0-based, where the corresponding loop variable lineoff in
 * heapgettup is 1-based.
 * ----------------
 */""",
  """heap_beginscan|||backend\access\heap\heapam.c|||1081|||/* ----------------------------------------------------------------
 *					 heap access method interface
 * ----------------------------------------------------------------
 */""",
  """heap_fetch|||backend\access\heap\heapam.c|||1554|||/*
 *	heap_fetch		- retrieve tuple with given tid
 *
 * On entry, tuple->t_self is the TID to fetch.  We pin the buffer holding
 * the tuple, fill in the remaining fields of *tuple, and check the tuple
 * against the specified snapshot.
 *
 * If successful (tuple found and passes snapshot time qual), then *userbuf
 * is set to the buffer holding the tuple and true is returned.  The caller
 * must unpin the buffer when done with the tuple.
 *
 * If the tuple is not found (ie, item number references a deleted slot),
 * then tuple->t_data is set to NULL, *userbuf is set to InvalidBuffer,
 * and false is returned.
 *
 * If the tuple is found but fails the time qual check, then the behavior
 * depends on the keep_buf parameter.  If keep_buf is false, the results
 * are the same as for the tuple-not-found case.  If keep_buf is true,
 * then tuple->t_data and *userbuf are returned as for the success case,
 * and again the caller must unpin the buffer; but false is returned.
 *
 * heap_fetch does not follow HOT chains: only the exact TID requested will
 * be fetched.
 *
 * It is somewhat inconsistent that we ereport() on invalid block number but
 * return false on invalid item number.  There are a couple of reasons though.
 * One is that the caller can relatively easily check the block number for
 * validity, but cannot check the item number without reading the page
 * himself.  Another is that when we are following a t_ctid link, we can be
 * reasonably confident that the page number is valid (since VACUUM shouldn't
 * truncate off the destination page without having killed the referencing
 * tuple first), but the item number might well not be good.
 */""",
  """heap_hot_search_buffer|||backend\access\heap\heapam.c|||1674|||/*
 *	heap_hot_search_buffer	- search HOT chain for tuple satisfying snapshot
 *
 * On entry, *tid is the TID of a tuple (either a simple tuple, or the root
 * of a HOT chain), and buffer is the buffer holding this tuple.  We search
 * for the first chain member satisfying the given snapshot.  If one is
 * found, we update *tid to reference that tuple's offset number, and
 * return true.  If no match, return false without modifying *tid.
 *
 * heapTuple is a caller-supplied buffer.  When a match is found, we return
 * the tuple here, in addition to updating *tid.  If no match is found, the
 * contents of this buffer on return are undefined.
 *
 * If all_dead is not NULL, we check non-visible tuples to see if they are
 * globally dead; *all_dead is set true if all members of the HOT chain
 * are vacuumable, false if not.
 *
 * Unlike heap_fetch, the caller must already have pin and (at least) share
 * lock on the buffer; it is still pinned/locked at exit.
 */""",
  """heap_get_latest_tid|||backend\access\heap\heapam.c|||1826|||/*
 *	heap_get_latest_tid -  get the latest tid of a specified tuple
 *
 * Actually, this gets the latest version that is visible according to the
 * scan's snapshot.  Create a scan using SnapshotDirty to get the very latest,
 * possibly uncommitted version.
 *
 * *tid is both an input and an output parameter: it is updated to
 * show the latest version of the row.  Note that it will not be changed
 * if no version of the row passes the snapshot test.
 */""",
  """UpdateXmaxHintBits|||backend\access\heap\heapam.c|||1948|||/*
 * UpdateXmaxHintBits - update tuple hint bits after xmax transaction ends
 *
 * This is called after we have waited for the XMAX transaction to terminate.
 * If the transaction aborted, we guarantee the XMAX_INVALID hint bit will
 * be set on exit.  If the transaction committed, we set the XMAX_COMMITTED
 * hint bit if possible --- but beware that that may not yet be possible,
 * if the transaction committed asynchronously.
 *
 * Note that if the transaction was a locker only, we set HEAP_XMAX_INVALID
 * even if it commits.
 *
 * Hence callers should look only at XMAX_INVALID.
 *
 * Note this is not allowed for tuples whose xmax is a multixact.
 */""",
  """GetBulkInsertState|||backend\access\heap\heapam.c|||1970|||/*
 * GetBulkInsertState - prepare status object for a bulk insert
 */""",
  """FreeBulkInsertState|||backend\access\heap\heapam.c|||1987|||/*
 * FreeBulkInsertState - clean up after finishing a bulk insert
 */""",
  """ReleaseBulkInsertStatePin|||backend\access\heap\heapam.c|||1999|||/*
 * ReleaseBulkInsertStatePin - release a buffer currently held in bistate
 */""",
  """heap_insert|||backend\access\heap\heapam.c|||2037|||/*
 *	heap_insert		- insert tuple into a heap
 *
 * The new tuple is stamped with current transaction ID and the specified
 * command ID.
 *
 * See table_tuple_insert for comments about most of the input flags, except
 * that this routine directly takes a tuple rather than a slot.
 *
 * There's corresponding HEAP_INSERT_ options to all the TABLE_INSERT_
 * options, and there additionally is HEAP_INSERT_SPECULATIVE which is used to
 * implement table_tuple_insert_speculative().
 *
 * On return the header fields of *tup are updated to match the stored tuple;
 * in particular tup->t_self receives the actual TID where the tuple was
 * stored.  But note that any toasting of fields within the tuple data is NOT
 * reflected into *tup.
 */""",
  """heap_prepare_insert|||backend\access\heap\heapam.c|||2228|||/*
 * Subroutine for heap_insert(). Prepares a tuple for insertion. This sets the
 * tuple header fields and toasts the tuple if necessary.  Returns a toasted
 * version of the tuple if it was toasted, or the original tuple if not. Note
 * that in any case, the header fields are also set in the original tuple.
 */""",
  """heap_multi_insert_pages|||backend\access\heap\heapam.c|||2276|||/*
 * Helper for heap_multi_insert() that computes the number of entire pages
 * that inserting the remaining heaptuples requires. Used to determine how
 * much the relation needs to be extended by.
 */""",
  """heap_multi_insert|||backend\access\heap\heapam.c|||2308|||/*
 *	heap_multi_insert	- insert multiple tuples into a heap
 *
 * This is like heap_insert(), but inserts multiple tuples in one operation.
 * That's faster than calling heap_insert() in a loop, because when multiple
 * tuples can be inserted on a single page, we can write just a single WAL
 * record covering all of them, and only need to lock/unlock the page once.
 *
 * Note: this leaks memory into the current memory context. You can create a
 * temporary context before calling this, if that's a problem.
 */""",
  """simple_heap_insert|||backend\access\heap\heapam.c|||2672|||/*
 *	simple_heap_insert - insert a tuple
 *
 * Currently, this routine differs from heap_insert only in supplying
 * a default command ID and not allowing access to the speedup options.
 *
 * This should be used rather than using heap_insert directly in most places
 * where we are modifying system catalogs.
 */""",
  """compute_infobits|||backend\access\heap\heapam.c|||2685|||/*
 * Given infomask/infomask2, compute the bits that must be saved in the
 * "infobits" field of xl_heap_delete, xl_heap_update, xl_heap_lock,
 * xl_heap_lock_updated WAL records.
 *
 * See fix_infomask_from_infobits.
 */""",
  """xmax_infomask_changed|||backend\access\heap\heapam.c|||2707|||/*
 * Given two versions of the same t_infomask for a tuple, compare them and
 * return whether the relevant status for a tuple Xmax has changed.  This is
 * used after a buffer lock has been released and reacquired: we want to ensure
 * that the tuple state continues to be the same it was when we previously
 * examined it.
 *
 * Note the Xmax field itself must be compared separately.
 */""",
  """heap_delete|||backend\access\heap\heapam.c|||2730|||/*
 *	heap_delete - delete a tuple
 *
 * See table_tuple_delete() for an explanation of the parameters, except that
 * this routine directly takes a tuple rather than a slot.
 *
 * In the failure cases, the routine fills *tmfd with the tuple's t_ctid,
 * t_xmax (resolving a possible MultiXact, if necessary), and t_cmax (the last
 * only for TM_SelfModified, since we cannot obtain cmax from a combo CID
 * generated by another transaction).
 */""",
  """simple_heap_delete|||backend\access\heap\heapam.c|||3153|||/*
 *	simple_heap_delete - delete a tuple
 *
 * This routine may be used to delete a tuple when concurrent updates of
 * the target tuple are not expected (for example, because we have a lock
 * on the relation associated with the tuple).  Any failure is reported
 * via ereport().
 */""",
  """heap_update|||backend\access\heap\heapam.c|||3199|||/*
 *	heap_update - replace a tuple
 *
 * See table_tuple_update() for an explanation of the parameters, except that
 * this routine directly takes a tuple rather than a slot.
 *
 * In the failure cases, the routine fills *tmfd with the tuple's t_ctid,
 * t_xmax (resolving a possible MultiXact, if necessary), and t_cmax (the last
 * only for TM_SelfModified, since we cannot obtain cmax from a combo CID
 * generated by another transaction).
 */""",
  """check_lock_if_inplace_updateable_rel|||backend\access\heap\heapam.c|||4181|||/*
 * Confirm adequate lock held during heap_update(), per rules from
 * README.tuplock section "Locking to write inplace-updated tables".
 */""",
  """check_inplace_rel_lock|||backend\access\heap\heapam.c|||4264|||/*
 * Confirm adequate relation lock held, per rules from README.tuplock section
 * "Locking to write inplace-updated tables".
 */""",
  """heap_attr_equals|||backend\access\heap\heapam.c|||4302|||/*
 * Check if the specified attribute's values are the same.  Subroutine for
 * HeapDetermineColumnsInfo.
 */""",
  """HeapDetermineColumnsInfo|||backend\access\heap\heapam.c|||4353|||/*
 * Check which columns are being updated.
 *
 * Given an updated tuple, determine (and return into the output bitmapset),
 * from those listed as interesting, the set of columns that changed.
 *
 * has_external indicates if any of the unmodified attributes (from those
 * listed as interesting) of the old tuple is a member of external_cols and is
 * stored externally.
 */""",
  """simple_heap_update|||backend\access\heap\heapam.c|||4443|||/*
 *	simple_heap_update - replace a tuple
 *
 * This routine may be used to update a tuple when concurrent updates of
 * the target tuple are not expected (for example, because we have a lock
 * on the relation associated with the tuple).  Any failure is reported
 * via ereport().
 */""",
  """get_mxact_status_for_lock|||backend\access\heap\heapam.c|||4484|||/*
 * Return the MultiXactStatus corresponding to the given tuple lock mode.
 */""",
  """heap_lock_tuple|||backend\access\heap\heapam.c|||4532|||/*
 *	heap_lock_tuple - lock a tuple in shared or exclusive mode
 *
 * Note that this acquires a buffer pin, which the caller must release.
 *
 * Input parameters:
 *	relation: relation containing tuple (caller must hold suitable lock)
 *	tid: TID of tuple to lock
 *	cid: current command ID (used for visibility test, and stored into
 *		tuple's cmax if lock is successful)
 *	mode: indicates if shared or exclusive tuple lock is desired
 *	wait_policy: what to do if tuple lock is not available
 *	follow_updates: if true, follow the update chain to also lock descendant
 *		tuples.
 *
 * Output parameters:
 *	*tuple: all fields filled in
 *	*buffer: set to buffer holding tuple (pinned but not locked at exit)
 *	*tmfd: filled in failure cases (see below)
 *
 * Function results are the same as the ones for table_tuple_lock().
 *
 * In the failure cases other than TM_Invisible, the routine fills
 * *tmfd with the tuple's t_ctid, t_xmax (resolving a possible MultiXact,
 * if necessary), and t_cmax (the last only for TM_SelfModified,
 * since we cannot obtain cmax from a combo CID generated by another
 * transaction).
 * See comments for struct TM_FailureData for additional info.
 *
 * See README.tuplock for a thorough explanation of this mechanism.
 */""",
  """heap_acquire_tuplock|||backend\access\heap\heapam.c|||5230|||/*
 * Acquire heavyweight lock on the given tuple, in preparation for acquiring
 * its normal, Xmax-based tuple lock.
 *
 * have_tuple_lock is an input and output parameter: on input, it indicates
 * whether the lock has previously been acquired (and this function does
 * nothing in that case).  If this function returns success, have_tuple_lock
 * has been flipped to true.
 *
 * Returns false if it was unable to obtain the lock; this can only happen if
 * wait_policy is Skip.
 */""",
  """compute_new_xmax_infomask|||backend\access\heap\heapam.c|||5279|||/*
 * Given an original set of Xmax and infomask, and a transaction (identified by
 * add_to_xmax) acquiring a new lock of some mode, compute the new Xmax and
 * corresponding infomasks to use on the tuple.
 *
 * Note that this might have side effects such as creating a new MultiXactId.
 *
 * Most callers will have called HeapTupleSatisfiesUpdate before this function;
 * that will have set the HEAP_XMAX_INVALID bit if the xmax was a MultiXactId
 * but it was not running anymore. There is a race condition, which is that the
 * MultiXactId may have finished since then, but that uncommon case is handled
 * either here, or within MultiXactIdExpand.
 *
 * There is a similar race condition possible when the old xmax was a regular
 * TransactionId.  We test TransactionIdIsInProgress again just to narrow the
 * window, but it's still possible to end up creating an unnecessary
 * MultiXactId.  Fortunately this is harmless.
 */""",
  """test_lockmode_for_conflict|||backend\access\heap\heapam.c|||5560|||/*
 * Subroutine for heap_lock_updated_tuple_rec.
 *
 * Given a hypothetical multixact status held by the transaction identified
 * with the given xid, does the current transaction need to wait, fail, or can
 * it continue if it wanted to acquire a lock of the given mode?  "needwait"
 * is set to true if waiting is necessary; if it can continue, then TM_Ok is
 * returned.  If the lock is already held by the current transaction, return
 * TM_SelfModified.  In case of a conflict with another transaction, a
 * different HeapTupleSatisfiesUpdate return code is returned.
 *
 * The held status is said to be hypothetical because it might correspond to a
 * lock held by a single Xid, i.e. not a real MultiXactId; we express it this
 * way for simplicity of API.
 */""",
  """heap_lock_updated_tuple_rec|||backend\access\heap\heapam.c|||5651|||/*
 * Recursive part of heap_lock_updated_tuple
 *
 * Fetch the tuple pointed to by tid in rel, and mark it as locked by the given
 * xid with the given mode; if this tuple is updated, recurse to lock the new
 * version as well.
 */""",
  """heap_lock_updated_tuple|||backend\access\heap\heapam.c|||5996|||/*
 * heap_lock_updated_tuple
 *		Follow update chain when locking an updated tuple, acquiring locks (row
 *		marks) on the updated versions.
 *
 * The initial tuple is assumed to be already locked.
 *
 * This function doesn't check visibility, it just unconditionally marks the
 * tuple(s) as locked.  If any tuple in the updated chain is being deleted
 * concurrently (or updated with the key being modified), sleep until the
 * transaction doing it is finished.
 *
 * Note that we don't acquire heavyweight tuple locks on the tuples we walk
 * when we have to wait for other transactions to release them, as opposed to
 * what heap_lock_tuple does.  The reason is that having more than one
 * transaction walking the chain is probably uncommon enough that risk of
 * starvation is not likely: one of the preconditions for being here is that
 * the snapshot in use predates the update that created this tuple (because we
 * started at an earlier version of the tuple), but at the same time such a
 * transaction cannot be using repeatable read or serializable isolation
 * levels, because that would lead to a serializability failure.
 */""",
  """heap_finish_speculative|||backend\access\heap\heapam.c|||6041|||/*
 *	heap_finish_speculative - mark speculative insertion as successful
 *
 * To successfully finish a speculative insertion we have to clear speculative
 * token from tuple.  To do so the t_ctid field, which will contain a
 * speculative token value, is modified in place to point to the tuple itself,
 * which is characteristic of a newly inserted ordinary tuple.
 *
 * NB: It is not ok to commit without either finishing or aborting a
 * speculative insertion.  We could treat speculative tuples of committed
 * transactions implicitly as completed, but then we would have to be prepared
 * to deal with speculative tokens on committed tuples.  That wouldn't be
 * difficult - no-one looks at the ctid field of a tuple with invalid xmax -
 * but clearing the token at completion isn't very expensive either.
 * An explicit confirmation WAL record also makes logical decoding simpler.
 */""",
  """heap_abort_speculative|||backend\access\heap\heapam.c|||6128|||/*
 *	heap_abort_speculative - kill a speculatively inserted tuple
 *
 * Marks a tuple that was speculatively inserted in the same command as dead,
 * by setting its xmin as invalid.  That makes it immediately appear as dead
 * to all transactions, including our own.  In particular, it makes
 * HeapTupleSatisfiesDirty() regard the tuple as dead, so that another backend
 * inserting a duplicate key value won't unnecessarily wait for our whole
 * transaction to finish (it'll just wait for our speculative insertion to
 * finish).
 *
 * Killing the tuple prevents "unprincipled deadlocks", which are deadlocks
 * that arise due to a mutual dependency that is not user visible.  By
 * definition, unprincipled deadlocks cannot be prevented by the user
 * reordering lock acquisition in client code, because the implementation level
 * lock acquisitions are not under the user's direct control.  If speculative
 * inserters did not take this precaution, then under high concurrency they
 * could deadlock with each other, which would not be acceptable.
 *
 * This is somewhat redundant with heap_delete, but we prefer to have a
 * dedicated routine with stripped down requirements.  Note that this is also
 * used to delete the TOAST tuples created during speculative insertion.
 *
 * This routine does not affect logical decoding as it only looks at
 * confirmation records.
 */""",
  """heap_inplace_lock|||backend\access\heap\heapam.c|||6307|||/*
 * heap_inplace_lock - protect inplace update from concurrent heap_update()
 *
 * Evaluate whether the tuple's state is compatible with a no-key update.
 * Current transaction rowmarks are fine, as is KEY SHARE from any
 * transaction.  If compatible, return true with the buffer exclusive-locked,
 * and the caller must release that by calling
 * heap_inplace_update_and_unlock(), calling heap_inplace_unlock(), or raising
 * an error.  Otherwise, call release_callback(arg), wait for blocking
 * transactions to end, and return false.
 *
 * Since this is intended for system catalogs and SERIALIZABLE doesn't cover
 * DDL, this doesn't guarantee any particular predicate locking.
 *
 * One could modify this to return true for tuples with delete in progress,
 * All inplace updaters take a lock that conflicts with DROP.  If explicit
 * "DELETE FROM pg_class" is in progress, we'll wait for it like we would an
 * update.
 *
 * Readers of inplace-updated fields expect changes to those fields are
 * durable.  For example, vac_truncate_clog() reads datfrozenxid from
 * pg_database tuples via catalog snapshots.  A future snapshot must not
 * return a lower datfrozenxid for the same database OID (lower in the
 * FullTransactionIdPrecedes() sense).  We achieve that since no update of a
 * tuple can start while we hold a lock on its buffer.  In cases like
 * BEGIN;GRANT;CREATE INDEX;COMMIT we're inplace-updating a tuple visible only
 * to this transaction.  ROLLBACK then is one case where it's okay to lose
 * inplace updates.  (Restoring relhasindex=false on ROLLBACK is fine, since
 * any concurrent CREATE INDEX would have blocked, then inplace-updated the
 * committed tuple.)
 *
 * In principle, we could avoid waiting by overwriting every tuple in the
 * updated tuple chain.  Reader expectations permit updating a tuple only if
 * it's aborted, is the tail of the chain, or we already updated the tuple
 * referenced in its t_ctid.  Hence, we would need to overwrite the tuples in
 * order from tail to head.  That would imply either (a) mutating all tuples
 * in one critical section or (b) accepting a chance of partial completion.
 * Partial completion of a relfrozenxid update would have the weird
 * consequence that the table's next VACUUM could see the table's relfrozenxid
 * move forward between vacuum_get_cutoffs() and finishing.
 */""",
  """heap_inplace_update_and_unlock|||backend\access\heap\heapam.c|||6431|||/*
 * heap_inplace_update_and_unlock - core of systable_inplace_update_finish
 *
 * The tuple cannot change size, and therefore its header fields and null
 * bitmap (if any) don't change either.
 *
 * Since we hold LOCKTAG_TUPLE, no updater has a local copy of this tuple.
 */""",
  """heap_inplace_unlock|||backend\access\heap\heapam.c|||6508|||/*
 * heap_inplace_unlock - reverse of heap_inplace_lock
 */""",
  """heap_inplace_update|||backend\access\heap\heapam.c|||6522|||/*
 * heap_inplace_update - deprecated
 *
 * This exists only to keep modules working in back branches.  Affected
 * modules should migrate to systable_inplace_update_begin().
 */""",
  """FreezeMultiXactId|||backend\access\heap\heapam.c|||6658|||/*
 * FreezeMultiXactId
 *		Determine what to do during freezing when a tuple is marked by a
 *		MultiXactId.
 *
 * "flags" is an output value; it's used to tell caller what to do on return.
 * "pagefrz" is an input/output value, used to manage page level freezing.
 *
 * Possible values that we can set in "flags":
 * FRM_NOOP
 *		don't do anything -- keep existing Xmax
 * FRM_INVALIDATE_XMAX
 *		mark Xmax as InvalidTransactionId and set XMAX_INVALID flag.
 * FRM_RETURN_IS_XID
 *		The Xid return value is a single update Xid to set as xmax.
 * FRM_MARK_COMMITTED
 *		Xmax can be marked as HEAP_XMAX_COMMITTED
 * FRM_RETURN_IS_MULTI
 *		The return value is a new MultiXactId to set as new Xmax.
 *		(caller must obtain proper infomask bits using GetMultiXactIdHintBits)
 *
 * Caller delegates control of page freezing to us.  In practice we always
 * force freezing of caller's page unless FRM_NOOP processing is indicated.
 * We help caller ensure that XIDs < FreezeLimit and MXIDs < MultiXactCutoff
 * can never be left behind.  We freely choose when and how to process each
 * Multi, without ever violating the cutoff postconditions for freezing.
 *
 * It's useful to remove Multis on a proactive timeline (relative to freezing
 * XIDs) to keep MultiXact member SLRU buffer misses to a minimum.  It can also
 * be cheaper in the short run, for us, since we too can avoid SLRU buffer
 * misses through eager processing.
 *
 * NB: Creates a _new_ MultiXactId when FRM_RETURN_IS_MULTI is set, though only
 * when FreezeLimit and/or MultiXactCutoff cutoffs leave us with no choice.
 * This can usually be put off, which is usually enough to avoid it altogether.
 * Allocating new multis during VACUUM should be avoided on general principle;
 * only VACUUM can advance relminmxid, so allocating new Multis here comes with
 * its own special risks.
 *
 * NB: Caller must maintain "no freeze" NewRelfrozenXid/NewRelminMxid trackers
 * using heap_tuple_should_freeze when we haven't forced page-level freezing.
 *
 * NB: Caller should avoid needlessly calling heap_tuple_should_freeze when we
 * have already forced page-level freezing, since that might incur the same
 * SLRU buffer misses that we specifically intended to avoid by freezing.
 */""",
  """heap_prepare_freeze_tuple|||backend\access\heap\heapam.c|||7008|||/*
 * heap_prepare_freeze_tuple
 *
 * Check to see whether any of the XID fields of a tuple (xmin, xmax, xvac)
 * are older than the OldestXmin and/or OldestMxact freeze cutoffs.  If so,
 * setup enough state (in the *frz output argument) to enable caller to
 * process this tuple as part of freezing its page, and return true.  Return
 * false if nothing can be changed about the tuple right now.
 *
 * Also sets *totally_frozen to true if the tuple will be totally frozen once
 * caller executes returned freeze plan (or if the tuple was already totally
 * frozen by an earlier VACUUM).  This indicates that there are no remaining
 * XIDs or MultiXactIds that will need to be processed by a future VACUUM.
 *
 * VACUUM caller must assemble HeapTupleFreeze freeze plan entries for every
 * tuple that we returned true for, and then execute freezing.  Caller must
 * initialize pagefrz fields for page as a whole before first call here for
 * each heap page.
 *
 * VACUUM caller decides on whether or not to freeze the page as a whole.
 * We'll often prepare freeze plans for a page that caller just discards.
 * However, VACUUM doesn't always get to make a choice; it must freeze when
 * pagefrz.freeze_required is set, to ensure that any XIDs < FreezeLimit (and
 * MXIDs < MultiXactCutoff) can never be left behind.  We help to make sure
 * that VACUUM always follows that rule.
 *
 * We sometimes force freezing of xmax MultiXactId values long before it is
 * strictly necessary to do so just to ensure the FreezeLimit postcondition.
 * It's worth processing MultiXactIds proactively when it is cheap to do so,
 * and it's convenient to make that happen by piggy-backing it on the "force
 * freezing" mechanism.  Conversely, we sometimes delay freezing MultiXactIds
 * because it is expensive right now (though only when it's still possible to
 * do so without violating the FreezeLimit/MultiXactCutoff postcondition).
 *
 * It is assumed that the caller has checked the tuple with
 * HeapTupleSatisfiesVacuum() and determined that it is not HEAPTUPLE_DEAD
 * (else we should be removing the tuple, not freezing it).
 *
 * NB: This function has side effects: it might allocate a new MultiXactId.
 * It will be set as tuple's new xmax when our *frz output is processed within
 * heap_execute_freeze_tuple later on.  If the tuple is in a shared buffer
 * then caller had better have an exclusive lock on it already.
 */""",
  """heap_execute_freeze_tuple|||backend\access\heap\heapam.c|||7282|||/*
 * heap_execute_freeze_tuple
 *		Execute the prepared freezing of a tuple with caller's freeze plan.
 *
 * Caller is responsible for ensuring that no other backend can access the
 * storage underlying this tuple, either by holding an exclusive lock on the
 * buffer containing it (which is what lazy VACUUM does), or by having it be
 * in private storage (which is what CLUSTER and friends do).
 */""",
  """heap_pre_freeze_checks|||backend\access\heap\heapam.c|||7305|||/*
 * Perform xmin/xmax XID status sanity checks before actually executing freeze
 * plans.
 *
 * heap_prepare_freeze_tuple doesn't perform these checks directly because
 * pg_xact lookups are relatively expensive.  They shouldn't be repeated by
 * successive VACUUMs that each decide against freezing the same page.
 */""",
  """heap_freeze_prepared_tuples|||backend\access\heap\heapam.c|||7358|||/*
 * Helper which executes freezing of one or more heap tuples on a page on
 * behalf of caller.  Caller passes an array of tuple plans from
 * heap_prepare_freeze_tuple.  Caller must set 'offset' in each plan for us.
 * Must be called in a critical section that also marks the buffer dirty and,
 * if needed, emits WAL.
 */""",
  """heap_freeze_tuple|||backend\access\heap\heapam.c|||7380|||/*
 * heap_freeze_tuple
 *		Freeze tuple in place, without WAL logging.
 *
 * Useful for callers like CLUSTER that perform their own WAL logging.
 */""",
  """GetMultiXactIdHintBits|||backend\access\heap\heapam.c|||7424|||/*
 * For a given MultiXactId, return the hint bits that should be set in the
 * tuple's infomask.
 *
 * Normally this should be called for a multixact that was just created, and
 * so is on our local cache, so the GetMembers call is fast.
 */""",
  """MultiXactIdGetUpdateXid|||backend\access\heap\heapam.c|||7505|||/*
 * MultiXactIdGetUpdateXid
 *
 * Given a multixact Xmax and corresponding infomask, which does not have the
 * HEAP_XMAX_LOCK_ONLY bit set, obtain and return the Xid of the updating
 * transaction.
 *
 * Caller is expected to check the status of the updating transaction, if
 * necessary.
 */""",
  """HeapTupleGetUpdateXid|||backend\access\heap\heapam.c|||7557|||/*
 * HeapTupleGetUpdateXid
 *		As above, but use a HeapTupleHeader
 *
 * See also HeapTupleHeaderGetUpdateXid, which can be used without previously
 * checking the hint bits.
 */""",
  """DoesMultiXactIdConflict|||backend\access\heap\heapam.c|||7573|||/*
 * Does the given multixact conflict with the current transaction grabbing a
 * tuple lock of the given strength?
 *
 * The passed infomask pairs up with the given multixact in the tuple header.
 *
 * If current_is_member is not NULL, it is set to 'true' if the current
 * transaction is a member of the given multixact.
 */""",
  """Do_MultiXactIdWait|||backend\access\heap\heapam.c|||7672|||/*
 * Do_MultiXactIdWait
 *		Actual implementation for the two functions below.
 *
 * 'multi', 'status' and 'infomask' indicate what to sleep on (the status is
 * needed to ensure we only sleep on conflicting members, and the infomask is
 * used to optimize multixact access in case it's a lock-only multi); 'nowait'
 * indicates whether to use conditional lock acquisition, to allow callers to
 * fail if lock is unavailable.  'rel', 'ctid' and 'oper' are used to set up
 * context information for error messages.  'remaining', if not NULL, receives
 * the number of members that are still running, including any (non-aborted)
 * subtransactions of our own transaction.
 *
 * We do this by sleeping on each member using XactLockTableWait.  Any
 * members that belong to the current backend are *not* waited for, however;
 * this would not merely be useless but would lead to Assert failure inside
 * XactLockTableWait.  By the time this returns, it is certain that all
 * transactions *of other backends* that were members of the MultiXactId
 * that conflict with the requested status are dead (and no new ones can have
 * been added, since it is not legal to add members to an existing
 * MultiXactId).
 *
 * But by the time we finish sleeping, someone else may have changed the Xmax
 * of the containing tuple, so the caller needs to iterate on us somehow.
 *
 * Note that in case we return false, the number of remaining members is
 * not to be trusted.
 */""",
  """MultiXactIdWait|||backend\access\heap\heapam.c|||7750|||/*
 * MultiXactIdWait
 *		Sleep on a MultiXactId.
 *
 * By the time we finish sleeping, someone else may have changed the Xmax
 * of the containing tuple, so the caller needs to iterate on us somehow.
 *
 * We return (in *remaining, if not NULL) the number of members that are still
 * running, including any (non-aborted) subtransactions of our own transaction.
 */""",
  """ConditionalMultiXactIdWait|||backend\access\heap\heapam.c|||7772|||/*
 * ConditionalMultiXactIdWait
 *		As above, but only lock if we can get the lock without blocking.
 *
 * By the time we finish sleeping, someone else may have changed the Xmax
 * of the containing tuple, so the caller needs to iterate on us somehow.
 *
 * If the multixact is now all gone, return true.  Returns false if some
 * transactions might still be running.
 *
 * We return (in *remaining, if not NULL) the number of members that are still
 * running, including any (non-aborted) subtransactions of our own transaction.
 */"""
)
INFO:__main__:Extracted 2 methods with comments in this batch
INFO:__main__:Fetching batch: offset=800, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=800, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.20s
INFO:__main__:Extracted 2 methods with comments in this batch
INFO:__main__:Fetching batch: offset=900, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=900, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.31s
INFO:__main__:Extracted 5 methods with comments in this batch
INFO:__main__:Fetching batch: offset=1000, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=1000, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres341[0m: [32mList[0m[[32mString[0m] = List(
  """_bt_singleval_fillfactor|||backend\access\nbtree\nbtdedup.c|||821|||/*
 * Lower maxpostingsize when using "single value" strategy, to avoid a sixth
 * and final maxpostingsize-capped tuple.  The sixth and final posting list
 * tuple will end up somewhat smaller than the first five.  (Note: The first
 * five tuples could actually just be very large duplicate tuples that
 * couldn't be merged together at all.  Deduplication will simply not modify
 * the page when that happens.)
 *
 * When there are six posting lists on the page (after current deduplication
 * pass goes on to create/observe a sixth very large tuple), caller should end
 * its deduplication pass.  It isn't useful to try to deduplicate items that
 * are supposed to end up on the new right sibling page following the
 * anticipated page split.  A future deduplication pass of future right
 * sibling page might take care of it.  (This is why the first single value
 * strategy deduplication pass for a given leaf page will generally find only
 * plain non-pivot tuples -- see _bt_do_singleval() comments.)
 */""",
  """_bt_form_posting|||backend\access\nbtree\nbtdedup.c|||863|||/*
 * Build a posting list tuple based on caller's "base" index tuple and list of
 * heap TIDs.  When nhtids == 1, builds a standard non-pivot tuple without a
 * posting list. (Posting list tuples can never have a single heap TID, partly
 * because that ensures that deduplication always reduces final MAXALIGN()'d
 * size of entire tuple.)
 *
 * Convention is that posting list starts at a MAXALIGN()'d offset (rather
 * than a SHORTALIGN()'d offset), in line with the approach taken when
 * appending a heap TID to new pivot tuple/high key during suffix truncation.
 * This sometimes wastes a little space that was only needed as alignment
 * padding in the original tuple.  Following this convention simplifies the
 * space accounting used when deduplicating a page (the same convention
 * simplifies the accounting for choosing a point to split a page at).
 *
 * Note: Caller's "htids" array must be unique and already in ascending TID
 * order.  Any existing heap TIDs from "base" won't automatically appear in
 * returned posting list tuple (they must be included in htids array.)
 */""",
  """_bt_update_posting|||backend\access\nbtree\nbtdedup.c|||923|||/*
 * Generate a replacement tuple by "updating" a posting list tuple so that it
 * no longer has TIDs that need to be deleted.
 *
 * Used by both VACUUM and index deletion.  Caller's vacposting argument
 * points to the existing posting list tuple to be updated.
 *
 * On return, caller's vacposting argument will point to final "updated"
 * tuple, which will be palloc()'d in caller's memory context.
 */""",
  """_bt_swap_posting|||backend\access\nbtree\nbtdedup.c|||1021|||/*
 * Prepare for a posting list split by swapping heap TID in newitem with heap
 * TID from original posting list (the 'oposting' heap TID located at offset
 * 'postingoff').  Modifies newitem, so caller should pass their own private
 * copy that can safely be modified.
 *
 * Returns new posting list tuple, which is palloc()'d in caller's context.
 * This is guaranteed to be the same size as 'oposting'.  Modified newitem is
 * what caller actually inserts. (This happens inside the same critical
 * section that performs an in-place update of old posting list using new
 * posting list returned here.)
 *
 * While the keys from newitem and oposting must be opclass equal, and must
 * generate identical output when run through the underlying type's output
 * function, it doesn't follow that their representations match exactly.
 * Caller must avoid assuming that there can't be representational differences
 * that make datums from oposting bigger or smaller than the corresponding
 * datums from newitem.  For example, differences in TOAST input state might
 * break a faulty assumption about tuple size (the executor is entitled to
 * apply TOAST compression based on its own criteria).  It also seems possible
 * that further representational variation will be introduced in the future,
 * in order to support nbtree features like page-level prefix compression.
 *
 * See nbtree/README for details on the design of posting list splits.
 */""",
  """_bt_posting_valid|||backend\access\nbtree\nbtdedup.c|||1077|||/*
 * Verify posting list invariants for "posting", which must be a posting list
 * tuple.  Used within assertions.
 */""",
  """_bt_doinsert|||backend\access\nbtree\nbtinsert.c|||101|||/*
 *	_bt_doinsert() -- Handle insertion of a single index tuple in the tree.
 *
 *		This routine is called by the public interface routine, btinsert.
 *		By here, itup is filled in, including the TID.
 *
 *		If checkUnique is UNIQUE_CHECK_NO or UNIQUE_CHECK_PARTIAL, this
 *		will allow duplicates.  Otherwise (UNIQUE_CHECK_YES or
 *		UNIQUE_CHECK_EXISTING) it will throw error for a duplicate.
 *		For UNIQUE_CHECK_EXISTING we merely run the duplicate check, and
 *		don't actually insert.
 *
 *		indexUnchanged executor hint indicates if itup is from an
 *		UPDATE that didn't logically change the indexed value, but
 *		must nevertheless have a new entry to point to a successor
 *		version.
 *
 *		The result value is only significant for UNIQUE_CHECK_PARTIAL:
 *		it must be true if the entry is known unique, else false.
 *		(In the current implementation we'll also return true after a
 *		successful UNIQUE_CHECK_YES or UNIQUE_CHECK_EXISTING call, but
 *		that's just a coding artifact.)
 */""",
  """_bt_search_insert|||backend\access\nbtree\nbtinsert.c|||316|||/*
 *	_bt_search_insert() -- _bt_search() wrapper for inserts
 *
 * Search the tree for a particular scankey, or more precisely for the first
 * leaf page it could be on.  Try to make use of the fastpath optimization's
 * rightmost leaf page cache before actually searching the tree from the root
 * page, though.
 *
 * Return value is a stack of parent-page pointers (though see notes about
 * fastpath optimization and page splits below).  insertstate->buf is set to
 * the address of the leaf-page buffer, which is write-locked and pinned in
 * all cases (if necessary by creating a new empty root page for caller).
 *
 * The fastpath optimization avoids most of the work of searching the tree
 * repeatedly when a single backend inserts successive new tuples on the
 * rightmost leaf page of an index.  A backend cache of the rightmost leaf
 * page is maintained within _bt_insertonpg(), and used here.  The cache is
 * invalidated here when an insert of a non-pivot tuple must take place on a
 * non-rightmost leaf page.
 *
 * The optimization helps with indexes on an auto-incremented field.  It also
 * helps with indexes on datetime columns, as well as indexes with lots of
 * NULL values.  (NULLs usually get inserted in the rightmost page for single
 * column indexes, since they usually get treated as coming after everything
 * else in the key space.  Individual NULL tuples will generally be placed on
 * the rightmost leaf page due to the influence of the heap TID column.)
 *
 * Note that we avoid applying the optimization when there is insufficient
 * space on the rightmost page to fit caller's new item.  This is necessary
 * because we'll need to return a real descent stack when a page split is
 * expected (actually, caller can cope with a leaf page split that uses a NULL
 * stack, but that's very slow and so must be avoided).  Note also that the
 * fastpath optimization acquires the lock on the page conditionally as a way
 * of reducing extra contention when there are concurrent insertions into the
 * rightmost page (we give up if we'd have to wait for the lock).  We assume
 * that it isn't useful to apply the optimization when there is contention,
 * since each per-backend cache won't stay valid for long.
 */""",
  """_bt_check_unique|||backend\access\nbtree\nbtinsert.c|||407|||/*
 *	_bt_check_unique() -- Check for violation of unique index constraint
 *
 * Returns InvalidTransactionId if there is no conflict, else an xact ID
 * we must wait for to see if it commits a conflicting tuple.   If an actual
 * conflict is detected, no return --- just ereport().  If an xact ID is
 * returned, and the conflicting tuple still has a speculative insertion in
 * progress, *speculativeToken is set to non-zero, and the caller can wait for
 * the verdict on the insertion using SpeculativeInsertionWait().
 *
 * However, if checkUnique == UNIQUE_CHECK_PARTIAL, we always return
 * InvalidTransactionId because we don't want to wait.  In this case we
 * set *is_unique to false if there is a potential conflict, and the
 * core code must redo the uniqueness check later.
 *
 * As a side-effect, sets state in insertstate that can later be used by
 * _bt_findinsertloc() to reuse most of the binary search work we do
 * here.
 *
 * This code treats NULLs as equal, unlike the default semantics for unique
 * indexes.  So do not call here when there are NULL values in scan key and
 * the index uses the default NULLS DISTINCT mode.
 */""",
  """_bt_findinsertloc|||backend\access\nbtree\nbtinsert.c|||814|||/*
 *	_bt_findinsertloc() -- Finds an insert location for a tuple
 *
 *		On entry, insertstate buffer contains the page the new tuple belongs
 *		on.  It is exclusive-locked and pinned by the caller.
 *
 *		If 'checkingunique' is true, the buffer on entry is the first page
 *		that contains duplicates of the new key.  If there are duplicates on
 *		multiple pages, the correct insertion position might be some page to
 *		the right, rather than the first page.  In that case, this function
 *		moves right to the correct target page.
 *
 *		(In a !heapkeyspace index, there can be multiple pages with the same
 *		high key, where the new tuple could legitimately be placed on.  In
 *		that case, the caller passes the first page containing duplicates,
 *		just like when checkingunique=true.  If that page doesn't have enough
 *		room for the new tuple, this function moves right, trying to find a
 *		legal page that does.)
 *
 *		If 'indexUnchanged' is true, this is for an UPDATE that didn't
 *		logically change the indexed value, but must nevertheless have a new
 *		entry to point to a successor version.  This hint from the executor
 *		will influence our behavior when the page might have to be split and
 *		we must consider our options.  Bottom-up index deletion can avoid
 *		pathological version-driven page splits, but we only want to go to the
 *		trouble of trying it when we already have moderate confidence that
 *		it's appropriate.  The hint should not significantly affect our
 *		behavior over time unless practically all inserts on to the leaf page
 *		get the hint.
 *
 *		On exit, insertstate buffer contains the chosen insertion page, and
 *		the offset within that page is returned.  If _bt_findinsertloc needed
 *		to move right, the lock and pin on the original page are released, and
 *		the new buffer is exclusively locked and pinned instead.
 *
 *		If insertstate contains cached binary search bounds, we will take
 *		advantage of them.  This avoids repeating comparisons that we made in
 *		_bt_check_unique() already.
 */""",
  """_bt_stepright|||backend\access\nbtree\nbtinsert.c|||1026|||/*
 * Step right to next non-dead page, during insertion.
 *
 * This is a bit more complicated than moving right in a search.  We must
 * write-lock the target page before releasing write lock on current page;
 * else someone else's _bt_check_unique scan could fail to see our insertion.
 * Write locks on intermediate dead pages won't do because we don't know when
 * they will get de-linked from the tree.
 *
 * This is more aggressive than it needs to be for non-unique !heapkeyspace
 * indexes.
 */""",
  """_bt_insertonpg|||backend\access\nbtree\nbtinsert.c|||1104|||/*----------
 *	_bt_insertonpg() -- Insert a tuple on a particular page in the index.
 *
 *		This recursive procedure does the following things:
 *
 *			+  if postingoff != 0, splits existing posting list tuple
 *			   (since it overlaps with new 'itup' tuple).
 *			+  if necessary, splits the target page, using 'itup_key' for
 *			   suffix truncation on leaf pages (caller passes NULL for
 *			   non-leaf pages).
 *			+  inserts the new tuple (might be split from posting list).
 *			+  if the page was split, pops the parent stack, and finds the
 *			   right place to insert the new child pointer (by walking
 *			   right using information stored in the parent stack).
 *			+  invokes itself with the appropriate tuple for the right
 *			   child page on the parent.
 *			+  updates the metapage if a true root or fast root is split.
 *
 *		On entry, we must have the correct buffer in which to do the
 *		insertion, and the buffer must be pinned and write-locked.  On return,
 *		we will have dropped both the pin and the lock on the buffer.
 *
 *		This routine only performs retail tuple insertions.  'itup' should
 *		always be either a non-highkey leaf item, or a downlink (new high
 *		key items are created indirectly, when a page is split).  When
 *		inserting to a non-leaf page, 'cbuf' is the left-sibling of the page
 *		we're inserting the downlink for.  This function will clear the
 *		INCOMPLETE_SPLIT flag on it, and release the buffer.
 *----------
 */""",
  """_bt_split|||backend\access\nbtree\nbtinsert.c|||1466|||/*
 *	_bt_split() -- split a page in the btree.
 *
 *		On entry, buf is the page to split, and is pinned and write-locked.
 *		newitemoff etc. tell us about the new item that must be inserted
 *		along with the data from the original page.
 *
 *		itup_key is used for suffix truncation on leaf pages (internal
 *		page callers pass NULL).  When splitting a non-leaf page, 'cbuf'
 *		is the left-sibling of the page we're inserting the downlink for.
 *		This function will clear the INCOMPLETE_SPLIT flag on it, and
 *		release the buffer.
 *
 *		orignewitem, nposting, and postingoff are needed when an insert of
 *		orignewitem results in both a posting list split and a page split.
 *		These extra posting list split details are used here in the same
 *		way as they are used in the more common case where a posting list
 *		split does not coincide with a page split.  We need to deal with
 *		posting list splits directly in order to ensure that everything
 *		that follows from the insert of orignewitem is handled as a single
 *		atomic operation (though caller's insert of a new pivot/downlink
 *		into parent page will still be a separate operation).  See
 *		nbtree/README for details on the design of posting list splits.
 *
 *		Returns the new right sibling of buf, pinned and write-locked.
 *		The pin and lock on buf are maintained.
 */""",
  """_bt_insert_parent|||backend\access\nbtree\nbtinsert.c|||2098|||/*
 * _bt_insert_parent() -- Insert downlink into parent, completing split.
 *
 * On entry, buf and rbuf are the left and right split pages, which we
 * still hold write locks on.  Both locks will be released here.  We
 * release the rbuf lock once we have a write lock on the page that we
 * intend to insert a downlink to rbuf on (i.e. buf's current parent page).
 * The lock on buf is released at the same point as the lock on the parent
 * page, since buf's INCOMPLETE_SPLIT flag must be cleared by the same
 * atomic operation that completes the split by inserting a new downlink.
 *
 * stack - stack showing how we got here.  Will be NULL when splitting true
 *			root, or during concurrent root split, where we can be inefficient
 * isroot - we split the true root
 * isonly - we split a page alone on its level (might have been fast root)
 */""",
  """_bt_finish_split|||backend\access\nbtree\nbtinsert.c|||2240|||/*
 * _bt_finish_split() -- Finish an incomplete split
 *
 * A crash or other failure can leave a split incomplete.  The insertion
 * routines won't allow to insert on a page that is incompletely split.
 * Before inserting on such a page, call _bt_finish_split().
 *
 * On entry, 'lbuf' must be locked in write-mode.  On exit, it is unlocked
 * and unpinned.
 *
 * Caller must provide a valid heaprel, since finishing a page split requires
 * allocating a new page if and when the parent page splits in turn.
 */""",
  """_bt_getstackbuf|||backend\access\nbtree\nbtinsert.c|||2318|||/*
 *	_bt_getstackbuf() -- Walk back up the tree one step, and find the pivot
 *						 tuple whose downlink points to child page.
 *
 *		Caller passes child's block number, which is used to identify
 *		associated pivot tuple in parent page using a linear search that
 *		matches on pivot's downlink/block number.  The expected location of
 *		the pivot tuple is taken from the stack one level above the child
 *		page.  This is used as a starting point.  Insertions into the
 *		parent level could cause the pivot tuple to move right; deletions
 *		could cause it to move left, but not left of the page we previously
 *		found it on.
 *
 *		Caller can use its stack to relocate the pivot tuple/downlink for
 *		any same-level page to the right of the page found by its initial
 *		descent.  This is necessary because of the possibility that caller
 *		moved right to recover from a concurrent page split.  It's also
 *		convenient for certain callers to be able to step right when there
 *		wasn't a concurrent page split, while still using their original
 *		stack.  For example, the checkingunique _bt_doinsert() case may
 *		have to step right when there are many physical duplicates, and its
 *		scantid forces an insertion to the right of the "first page the
 *		value could be on".  (This is also relied on by all of our callers
 *		when dealing with !heapkeyspace indexes.)
 *
 *		Returns write-locked parent page buffer, or InvalidBuffer if pivot
 *		tuple not found (should not happen).  Adjusts bts_blkno &
 *		bts_offset if changed.  Page split caller should insert its new
 *		pivot tuple for its new right sibling page on parent page, at the
 *		offset number bts_offset + 1.
 */""",
  """_bt_newlevel|||backend\access\nbtree\nbtinsert.c|||2443|||/*
 *	_bt_newlevel() -- Create a new level above root page.
 *
 *		We've just split the old root page and need to create a new one.
 *		In order to do this, we add a new root page to the file, then lock
 *		the metadata page and update it.  This is guaranteed to be deadlock-
 *		free, because all readers release their locks on the metadata page
 *		before trying to lock the root, and all writers lock the root before
 *		trying to lock the metadata page.  We have a write lock on the old
 *		root page, so we have not introduced any cycles into the waits-for
 *		graph.
 *
 *		On entry, lbuf (the old root) and rbuf (its new peer) are write-
 *		locked. On exit, a new root page exists with entries for the
 *		two new children, metapage is updated and unlocked/unpinned.
 *		The new root buffer is returned to caller which has to unlock/unpin
 *		lbuf, rbuf & rootbuf.
 */""",
  """_bt_pgaddtup|||backend\access\nbtree\nbtinsert.c|||2629|||/*
 *	_bt_pgaddtup() -- add a data item to a particular page during split.
 *
 *		The difference between this routine and a bare PageAddItem call is
 *		that this code can deal with the first data item on an internal btree
 *		page in passing.  This data item (which is called "firstright" within
 *		_bt_split()) has a key that must be treated as minus infinity after
 *		the split.  Therefore, we truncate away all attributes when caller
 *		specifies it's the first data item on page (downlink is not changed,
 *		though).  This extra step is only needed for the right page of an
 *		internal page split.  There is no need to do this for the first data
 *		item on the existing/left page, since that will already have been
 *		truncated during an earlier page split.
 *
 *		See _bt_split() for a high level explanation of why we truncate here.
 *		Note that this routine has nothing to do with suffix truncation,
 *		despite using some of the same infrastructure.
 */""",
  """_bt_delete_or_dedup_one_page|||backend\access\nbtree\nbtinsert.c|||2682|||/*
 * _bt_delete_or_dedup_one_page - Try to avoid a leaf page split.
 *
 * There are three operations performed here: simple index deletion, bottom-up
 * index deletion, and deduplication.  If all three operations fail to free
 * enough space for the incoming item then caller will go on to split the
 * page.  We always consider simple deletion first.  If that doesn't work out
 * we consider alternatives.  Callers that only want us to consider simple
 * deletion (without any fallback) ask for that using the 'simpleonly'
 * argument.
 *
 * We usually pick only one alternative "complex" operation when simple
 * deletion alone won't prevent a page split.  The 'checkingunique',
 * 'uniquedup', and 'indexUnchanged' arguments are used for that.
 *
 * Note: We used to only delete LP_DEAD items when the BTP_HAS_GARBAGE page
 * level flag was found set.  The flag was useful back when there wasn't
 * necessarily one single page for a duplicate tuple to go on (before heap TID
 * became a part of the key space in version 4 indexes).  But we don't
 * actually look at the flag anymore (it's not a gating condition for our
 * caller).  That would cause us to miss tuples that are safe to delete,
 * without getting any benefit in return.  We know that the alternative is to
 * split the page; scanning the line pointer array in passing won't have
 * noticeable overhead.  (We still maintain the BTP_HAS_GARBAGE flag despite
 * all this because !heapkeyspace indexes must still do a "getting tired"
 * linear search, and so are likely to get some benefit from using it as a
 * gating condition.)
 */""",
  """_bt_simpledel_pass|||backend\access\nbtree\nbtinsert.c|||2811|||/*
 * _bt_simpledel_pass - Simple index tuple deletion pass.
 *
 * We delete all LP_DEAD-set index tuples on a leaf page.  The offset numbers
 * of all such tuples are determined by caller (caller passes these to us as
 * its 'deletable' argument).
 *
 * We might also delete extra index tuples that turn out to be safe to delete
 * in passing (though they must be cheap to check in passing to begin with).
 * There is no certainty that any extra tuples will be deleted, though.  The
 * high level goal of the approach we take is to get the most out of each call
 * here (without noticeably increasing the per-call overhead compared to what
 * we need to do just to be able to delete the page's LP_DEAD-marked index
 * tuples).
 *
 * The number of extra index tuples that turn out to be deletable might
 * greatly exceed the number of LP_DEAD-marked index tuples due to various
 * locality related effects.  For example, it's possible that the total number
 * of table blocks (pointed to by all TIDs on the leaf page) is naturally
 * quite low, in which case we might end up checking if it's possible to
 * delete _most_ index tuples on the page (without the tableam needing to
 * access additional table blocks).  The tableam will sometimes stumble upon
 * _many_ extra deletable index tuples in indexes where this pattern is
 * common.
 *
 * See nbtree/README for further details on simple index tuple deletion.
 */""",
  """_bt_deadblocks|||backend\access\nbtree\nbtinsert.c|||2937|||/*
 * _bt_deadblocks() -- Get LP_DEAD related table blocks.
 *
 * Builds sorted and unique-ified array of table block numbers from index
 * tuple TIDs whose line pointers are marked LP_DEAD.  Also adds the table
 * block from incoming newitem just in case it isn't among the LP_DEAD-related
 * table blocks.
 *
 * Always counting the newitem's table block as an LP_DEAD related block makes
 * sense because the cost is consistently low; it is practically certain that
 * the table block will not incur a buffer miss in tableam.  On the other hand
 * the benefit is often quite high.  There is a decent chance that there will
 * be some deletable items from this block, since in general most garbage
 * tuples became garbage in the recent past (in many cases this won't be the
 * first logical row that core code added to/modified in table block
 * recently).
 *
 * Returns final array, and sets *nblocks to its final size for caller.
 */""",
  """_bt_blk_cmp|||backend\access\nbtree\nbtinsert.c|||3010|||/*
 * _bt_blk_cmp() -- qsort comparison function for _bt_simpledel_pass
 */""",
  """_bt_initmetapage|||backend\access\nbtree\nbtpage.c|||66|||/*
 *	_bt_initmetapage() -- Fill a page buffer with a correct metapage image
 */""",
  """_bt_upgrademetapage|||backend\access\nbtree\nbtpage.c|||106|||/*
 *	_bt_upgrademetapage() -- Upgrade a meta-page from an old format to version
 *		3, the last version that can be updated without broadly affecting
 *		on-disk compatibility.  (A REINDEX is required to upgrade to v4.)
 *
 *		This routine does purely in-memory image upgrade.  Caller is
 *		responsible for locking, WAL-logging etc.
 */""",
  """_bt_getmeta|||backend\access\nbtree\nbtpage.c|||141|||/*
 * Get metadata from share-locked buffer containing metapage, while performing
 * standard sanity checks.
 *
 * Callers that cache data returned here in local cache should note that an
 * on-the-fly upgrade using _bt_upgrademetapage() can change the version field
 * and BTREE_NOVAC_VERSION specific fields without invalidating local cache.
 */""",
  """_bt_vacuum_needs_cleanup|||backend\access\nbtree\nbtpage.c|||178|||/*
 * _bt_vacuum_needs_cleanup() -- Checks if index needs cleanup
 *
 * Called by btvacuumcleanup when btbulkdelete was never called because no
 * index tuples needed to be deleted.
 */""",
  """_bt_set_cleanup_info|||backend\access\nbtree\nbtpage.c|||231|||/*
 * _bt_set_cleanup_info() -- Update metapage for btvacuumcleanup.
 *
 * Called at the end of btvacuumcleanup, when num_delpages value has been
 * finalized.
 */""",
  """_bt_getroot|||backend\access\nbtree\nbtpage.c|||343|||/*
 *	_bt_getroot() -- Get the root page of the btree.
 *
 *		Since the root page can move around the btree file, we have to read
 *		its location from the metadata page, and then read the root page
 *		itself.  If no root page exists yet, we have to create one.
 *
 *		The access type parameter (BT_READ or BT_WRITE) controls whether
 *		a new root page will be created or not.  If access = BT_READ,
 *		and no root page exists, we just return InvalidBuffer.  For
 *		BT_WRITE, we try to create the root page if it doesn't exist.
 *		NOTE that the returned root page will have only a read lock set
 *		on it even if access = BT_WRITE!
 *
 *		If access = BT_WRITE, heaprel must be set; otherwise caller can just
 *		pass NULL.  See _bt_allocbuf for an explanation.
 *
 *		The returned page is not necessarily the true root --- it could be
 *		a "fast root" (a page that is alone in its level due to deletions).
 *		Also, if the root page is split while we are "in flight" to it,
 *		what we will return is the old root, which is now just the leftmost
 *		page on a probably-not-very-wide level.  For most purposes this is
 *		as good as or better than the true root, so we do not bother to
 *		insist on finding the true root.  We do, however, guarantee to
 *		return a live (not deleted or half-dead) page.
 *
 *		On successful return, the root page is pinned and read-locked.
 *		The metadata page is not locked or pinned on exit.
 */""",
  """_bt_gettrueroot|||backend\access\nbtree\nbtpage.c|||579|||/*
 *	_bt_gettrueroot() -- Get the true root page of the btree.
 *
 *		This is the same as the BT_READ case of _bt_getroot(), except
 *		we follow the true-root link not the fast-root link.
 *
 * By the time we acquire lock on the root page, it might have been split and
 * not be the true root anymore.  This is okay for the present uses of this
 * routine; we only really need to be able to move up at least one tree level
 * from whatever non-root page we were at.  If we ever do need to lock the
 * one true root page, we could loop here, re-reading the metapage on each
 * failure.  (Note that it wouldn't do to hold the lock on the metapage while
 * moving to the root --- that'd deadlock against any concurrent root split.)
 */""",
  """_bt_getrootheight|||backend\access\nbtree\nbtpage.c|||674|||/*
 *	_bt_getrootheight() -- Get the height of the btree search tree.
 *
 *		We return the level (counting from zero) of the current fast root.
 *		This represents the number of tree levels we'd have to descend through
 *		to start any btree index search.
 *
 *		This is used by the planner for cost-estimation purposes.  Since it's
 *		only an estimate, slightly-stale data is fine, hence we don't worry
 *		about updating previously cached data.
 */""",
  """_bt_metaversion|||backend\access\nbtree\nbtpage.c|||738|||/*
 *	_bt_metaversion() -- Get version/status info from metapage.
 *
 *		Sets caller's *heapkeyspace and *allequalimage arguments using data
 *		from the B-Tree metapage (could be locally-cached version).  This
 *		information needs to be stashed in insertion scankey, so we provide a
 *		single function that fetches both at once.
 *
 *		This is used to determine the rules that must be used to descend a
 *		btree.  Version 4 indexes treat heap TID as a tiebreaker attribute.
 *		pg_upgrade'd version 3 indexes need extra steps to preserve reasonable
 *		performance when inserting a new BTScanInsert-wise duplicate tuple
 *		among many leaf pages already full of such duplicates.
 *
 *		Also sets allequalimage field, which indicates whether or not it is
 *		safe to apply deduplication.  We rely on the assumption that
 *		btm_allequalimage will be zero'ed on heapkeyspace indexes that were
 *		pg_upgrade'd from Postgres 12.
 */""",
  """_bt_checkpage|||backend\access\nbtree\nbtpage.c|||796|||/*
 *	_bt_checkpage() -- Verify that a freshly-read page looks sane.
 */""",
  """_bt_getbuf|||backend\access\nbtree\nbtpage.c|||844|||/*
 *	_bt_getbuf() -- Get an existing block in a buffer, for read or write.
 *
 *		The general rule in nbtree is that it's never okay to access a
 *		page without holding both a buffer pin and a buffer lock on
 *		the page's buffer.
 *
 *		When this routine returns, the appropriate lock is set on the
 *		requested buffer and its reference count has been incremented
 *		(ie, the buffer is "locked and pinned").  Also, we apply
 *		_bt_checkpage to sanity-check the page, and perform Valgrind
 *		client requests that help Valgrind detect unsafe page accesses.
 *
 *		Note: raw LockBuffer() calls are disallowed in nbtree; all
 *		buffer lock requests need to go through wrapper functions such
 *		as _bt_lockbuf().
 */""",
  """_bt_allocbuf|||backend\access\nbtree\nbtpage.c|||868|||/*
 *	_bt_allocbuf() -- Allocate a new block/page.
 *
 * Returns a write-locked buffer containing an unallocated nbtree page.
 *
 * Callers are required to pass a valid heaprel.  We need heaprel so that we
 * can handle generating a snapshotConflictHorizon that makes reusing a page
 * from the FSM safe for queries that may be running on standbys.
 */""",
  """_bt_relandgetbuf|||backend\access\nbtree\nbtpage.c|||1002|||/*
 *	_bt_relandgetbuf() -- release a locked buffer and get another one.
 *
 * This is equivalent to _bt_relbuf followed by _bt_getbuf.  Also, if obuf is
 * InvalidBuffer then it reduces to just _bt_getbuf; allowing this case
 * simplifies some callers.
 *
 * The original motivation for using this was to avoid two entries to the
 * bufmgr when one would do.  However, now it's mainly just a notational
 * convenience.  The only case where it saves work over _bt_relbuf/_bt_getbuf
 * is when the target page is the same one already in the buffer.
 */""",
  """_bt_relbuf|||backend\access\nbtree\nbtpage.c|||1022|||/*
 *	_bt_relbuf() -- release a locked buffer.
 *
 * Lock and pin (refcount) are both dropped.
 */""",
  """_bt_lockbuf|||backend\access\nbtree\nbtpage.c|||1038|||/*
 *	_bt_lockbuf() -- lock a pinned buffer.
 *
 * Lock is acquired without acquiring another pin.  This is like a raw
 * LockBuffer() call, but performs extra steps needed by Valgrind.
 *
 * Note: Caller may need to call _bt_checkpage() with buf when pin on buf
 * wasn't originally acquired in _bt_getbuf() or _bt_relandgetbuf().
 */""",
  """_bt_unlockbuf|||backend\access\nbtree\nbtpage.c|||1069|||/*
 *	_bt_unlockbuf() -- unlock a pinned buffer.
 */""",
  """_bt_conditionallockbuf|||backend\access\nbtree\nbtpage.c|||1092|||/*
 *	_bt_conditionallockbuf() -- conditionally BT_WRITE lock pinned
 *	buffer.
 *
 * Note: Caller may need to call _bt_checkpage() with buf when pin on buf
 * wasn't originally acquired in _bt_getbuf() or _bt_relandgetbuf().
 */""",
  """_bt_upgradelockbufcleanup|||backend\access\nbtree\nbtpage.c|||1108|||/*
 *	_bt_upgradelockbufcleanup() -- upgrade lock to a full cleanup lock.
 */""",
  """_bt_pageinit|||backend\access\nbtree\nbtpage.c|||1128|||/*
 *	_bt_pageinit() -- Initialize a new page.
 *
 * On return, the page header is initialized; data space is empty;
 * special space is zeroed out.
 */""",
  """_bt_delitems_vacuum|||backend\access\nbtree\nbtpage.c|||1153|||/*
 * Delete item(s) from a btree leaf page during VACUUM.
 *
 * This routine assumes that the caller already has a full cleanup lock on
 * the buffer.  Also, the given deletable and updatable arrays *must* be
 * sorted in ascending order.
 *
 * Routine deals with deleting TIDs when some (but not all) of the heap TIDs
 * in an existing posting list item are to be removed.  This works by
 * updating/overwriting an existing item with caller's new version of the item
 * (a version that lacks the TIDs that are to be deleted).
 *
 * We record VACUUMs and b-tree deletes differently in WAL.  Deletes must
 * generate their own snapshotConflictHorizon directly from the tableam,
 * whereas VACUUMs rely on the initial VACUUM table scan performing
 * WAL-logging that takes care of the issue for the table's indexes
 * indirectly.  Also, we remove the VACUUM cycle ID from pages, which b-tree
 * deletes don't do.
 */""",
  """_bt_delitems_delete|||backend\access\nbtree\nbtpage.c|||1283|||/*
 * Delete item(s) from a btree leaf page during single-page cleanup.
 *
 * This routine assumes that the caller has pinned and write locked the
 * buffer.  Also, the given deletable and updatable arrays *must* be sorted in
 * ascending order.
 *
 * Routine deals with deleting TIDs when some (but not all) of the heap TIDs
 * in an existing posting list item are to be removed.  This works by
 * updating/overwriting an existing item with caller's new version of the item
 * (a version that lacks the TIDs that are to be deleted).
 *
 * This is nearly the same as _bt_delitems_vacuum as far as what it does to
 * the page, but it needs its own snapshotConflictHorizon and isCatalogRel
 * (from the tableam).  This is used by the REDO routine to generate recovery
 * conflicts.  The other difference is that only _bt_delitems_vacuum will
 * clear page's VACUUM cycle ID.
 */""",
  """_bt_delitems_update|||backend\access\nbtree\nbtpage.c|||1404|||/*
 * Set up state needed to delete TIDs from posting list tuples via "updating"
 * the tuple.  Performs steps common to both _bt_delitems_vacuum and
 * _bt_delitems_delete.  These steps must take place before each function's
 * critical section begins.
 *
 * updatable and nupdatable are inputs, though note that we will use
 * _bt_update_posting() to replace the original itup with a pointer to a final
 * version in palloc()'d memory.  Caller should free the tuples when its done.
 *
 * The first nupdatable entries from updatedoffsets are set to the page offset
 * number for posting list tuples that caller updates.  This is mostly useful
 * because caller may need to WAL-log the page offsets (though we always do
 * this for caller out of convenience).
 *
 * Returns buffer consisting of an array of xl_btree_update structs that
 * describe the steps we perform here for caller (though only when needswal is
 * true).  Also sets *updatedbuflen to the final size of the buffer.  This
 * buffer is used by caller when WAL logging is required.
 */""",
  """_bt_delitems_cmp|||backend\access\nbtree\nbtpage.c|||1463|||/*
 * Comparator used by _bt_delitems_delete_check() to restore deltids array
 * back to its original leaf-page-wise sort order
 */""",
  """_bt_delitems_delete_check|||backend\access\nbtree\nbtpage.c|||1512|||/*
 * Try to delete item(s) from a btree leaf page during single-page cleanup.
 *
 * nbtree interface to table_index_delete_tuples().  Deletes a subset of index
 * tuples from caller's deltids array: those whose TIDs are found safe to
 * delete by the tableam (or already marked LP_DEAD in index, and so already
 * known to be deletable by our simple index deletion caller).  We physically
 * delete index tuples from buf leaf page last of all (for index tuples where
 * that is known to be safe following our table_index_delete_tuples() call).
 *
 * Simple index deletion caller only includes TIDs from index tuples marked
 * LP_DEAD, as well as extra TIDs it found on the same leaf page that can be
 * included without increasing the total number of distinct table blocks for
 * the deletion operation as a whole.  This approach often allows us to delete
 * some extra index tuples that were practically free for tableam to check in
 * passing (when they actually turn out to be safe to delete).  It probably
 * only makes sense for the tableam to go ahead with these extra checks when
 * it is block-oriented (otherwise the checks probably won't be practically
 * free, which we rely on).  The tableam interface requires the tableam side
 * to handle the problem, though, so this is okay (we as an index AM are free
 * to make the simplifying assumption that all tableams must be block-based).
 *
 * Bottom-up index deletion caller provides all the TIDs from the leaf page,
 * without expecting that tableam will check most of them.  The tableam has
 * considerable discretion around which entries/blocks it checks.  Our role in
 * costing the bottom-up deletion operation is strictly advisory.
 *
 * Note: Caller must have added deltids entries (i.e. entries that go in
 * delstate's main array) in leaf-page-wise order: page offset number order,
 * TID order among entries taken from the same posting list tuple (tiebreak on
 * TID).  This order is convenient to work with here.
 *
 * Note: We also rely on the id field of each deltids element "capturing" this
 * original leaf-page-wise order.  That is, we expect to be able to get back
 * to the original leaf-page-wise order just by sorting deltids on the id
 * field (tableam will sort deltids for its own reasons, so we'll need to put
 * it back in leaf-page-wise order afterwards).
 */""",
  """_bt_leftsib_splitflag|||backend\access\nbtree\nbtpage.c|||1694|||/*
 * Check that leftsib page (the btpo_prev of target page) is not marked with
 * INCOMPLETE_SPLIT flag.  Used during page deletion.
 *
 * Returning true indicates that page flag is set in leftsib (which is
 * definitely still the left sibling of target).  When that happens, the
 * target doesn't have a downlink in parent, and the page deletion algorithm
 * isn't prepared to handle that.  Deletion of the target page (or the whole
 * subtree that contains the target page) cannot take place.
 *
 * Caller should not have a lock on the target page itself, since pages on the
 * same level must always be locked left to right to avoid deadlocks.
 */""",
  """_bt_rightsib_halfdeadflag|||backend\access\nbtree\nbtpage.c|||1751|||/*
 * Check that leafrightsib page (the btpo_next of target leaf page) is not
 * marked with ISHALFDEAD flag.  Used during page deletion.
 *
 * Returning true indicates that page flag is set in leafrightsib, so page
 * deletion cannot go ahead.  Our caller is not prepared to deal with the case
 * where the parent page does not have a pivot tuples whose downlink points to
 * leafrightsib (due to an earlier interrupted VACUUM operation).  It doesn't
 * seem worth going to the trouble of teaching our caller to deal with it.
 * The situation will be resolved after VACUUM finishes the deletion of the
 * half-dead page (when a future VACUUM operation reaches the target page
 * again).
 *
 * _bt_leftsib_splitflag() is called for both leaf pages and internal pages.
 * _bt_rightsib_halfdeadflag() is only called for leaf pages, though.  This is
 * okay because of the restriction on deleting pages that are the rightmost
 * page of their parent (i.e. that such deletions can only take place when the
 * entire subtree must be deleted).  The leaf level check made here will apply
 * to a right "cousin" leaf page rather than a simple right sibling leaf page
 * in cases where caller actually goes on to attempt deleting pages that are
 * above the leaf page.  The right cousin leaf page is representative of the
 * left edge of the subtree to the right of the to-be-deleted subtree as a
 * whole, which is exactly the condition that our caller cares about.
 * (Besides, internal pages are never marked half-dead, so it isn't even
 * possible to _directly_ assess if an internal page is part of some other
 * to-be-deleted subtree.)
 */""",
  """_bt_pagedel|||backend\access\nbtree\nbtpage.c|||1801|||/*
 * _bt_pagedel() -- Delete a leaf page from the b-tree, if legal to do so.
 *
 * This action unlinks the leaf page from the b-tree structure, removing all
 * pointers leading to it --- but not touching its own left and right links.
 * The page cannot be physically reclaimed right away, since other processes
 * may currently be trying to follow links leading to the page; they have to
 * be allowed to use its right-link to recover.  See nbtree/README.
 *
 * On entry, the target buffer must be pinned and locked (either read or write
 * lock is OK).  The page must be an empty leaf page, which may be half-dead
 * already (a half-dead page should only be passed to us when an earlier
 * VACUUM operation was interrupted, though).  Note in particular that caller
 * should never pass a buffer containing an existing deleted page here.  The
 * lock and pin on caller's buffer will be dropped before we return.
 *
 * Maintains bulk delete stats for caller, which are taken from vstate.  We
 * need to cooperate closely with caller here so that whole VACUUM operation
 * reliably avoids any double counting of subsidiary-to-leafbuf pages that we
 * delete in passing.  If such pages happen to be from a block number that is
 * ahead of the current scanblkno position, then caller is expected to count
 * them directly later on.  It's simpler for us to understand caller's
 * requirements than it would be for caller to understand when or how a
 * deleted page became deleted after the fact.
 *
 * NOTE: this leaks memory.  Rather than trying to clean up everything
 * carefully, it's better to run it in a temp context that can be reset
 * frequently.
 */""",
  """_bt_mark_page_halfdead|||backend\access\nbtree\nbtpage.c|||2087|||/*
 * First stage of page deletion.
 *
 * Establish the height of the to-be-deleted subtree with leafbuf at its
 * lowest level, remove the downlink to the subtree, and mark leafbuf
 * half-dead.  The final to-be-deleted subtree is usually just leafbuf itself,
 * but may include additional internal pages (at most one per level of the
 * tree below the root).
 *
 * Caller must pass a valid heaprel, since it's just about possible that our
 * call to _bt_lock_subtree_parent will need to allocate a new index page to
 * complete a page split.  Every call to _bt_allocbuf needs to pass a heaprel.
 *
 * Returns 'false' if leafbuf is unsafe to delete, usually because leafbuf is
 * the rightmost child of its parent (and parent has more than one downlink).
 * Returns 'true' when the first stage of page deletion completed
 * successfully.
 */""",
  """_bt_unlink_halfdead_page|||backend\access\nbtree\nbtpage.c|||2313|||/*
 * Second stage of page deletion.
 *
 * Unlinks a single page (in the subtree undergoing deletion) from its
 * siblings.  Also marks the page deleted.
 *
 * To get rid of the whole subtree, including the leaf page itself, call here
 * until the leaf page is deleted.  The original "top parent" established in
 * the first stage of deletion is deleted in the first call here, while the
 * leaf page is deleted in the last call here.  Note that the leaf page itself
 * is often the initial top parent page.
 *
 * Returns 'false' if the page could not be unlinked (shouldn't happen).  If
 * the right sibling of the current target page is empty, *rightsib_empty is
 * set to true, allowing caller to delete the target's right sibling page in
 * passing.  Note that *rightsib_empty is only actually used by caller when
 * target page is leafbuf, following last call here for leafbuf/the subtree
 * containing leafbuf.  (We always set *rightsib_empty for caller, just to be
 * consistent.)
 *
 * Must hold pin and lock on leafbuf at entry (read or write doesn't matter).
 * On success exit, we'll be holding pin and write lock.  On failure exit,
 * we'll release both pin and lock before returning (we define it that way
 * to avoid having to reacquire a lock we already released).
 */""",
  """_bt_lock_subtree_parent|||backend\access\nbtree\nbtpage.c|||2812|||/*
 * Establish how tall the to-be-deleted subtree will be during the first stage
 * of page deletion.
 *
 * Caller's child argument is the block number of the page caller wants to
 * delete (this is leafbuf's block number, except when we're called
 * recursively).  stack is a search stack leading to it.  Note that we will
 * update the stack entry(s) to reflect current downlink positions --- this is
 * similar to the corresponding point in page split handling.
 *
 * If "first stage" caller cannot go ahead with deleting _any_ pages, returns
 * false.  Returns true on success, in which case caller can use certain
 * details established here to perform the first stage of deletion.  This
 * function is the last point at which page deletion may be deemed unsafe
 * (barring index corruption, or unexpected concurrent page deletions).
 *
 * We write lock the parent of the root of the to-be-deleted subtree for
 * caller on success (i.e. we leave our lock on the *subtreeparent buffer for
 * caller).  Caller will have to remove a downlink from *subtreeparent.  We
 * also set a *subtreeparent offset number in *poffset, to indicate the
 * location of the pivot tuple that contains the relevant downlink.
 *
 * The root of the to-be-deleted subtree is called the "top parent".  Note
 * that the leafbuf page is often the final "top parent" page (you can think
 * of the leafbuf page as a degenerate single page subtree when that happens).
 * Caller should initialize *topparent to the target leafbuf page block number
 * (while *topparentrightsib should be set to leafbuf's right sibling block
 * number).  We will update *topparent (and *topparentrightsib) for caller
 * here, though only when it turns out that caller will delete at least one
 * internal page (i.e. only when caller needs to store a valid link to the top
 * parent block in the leafbuf page using BTreeTupleSetTopParent()).
 */""",
  """_bt_pendingfsm_init|||backend\access\nbtree\nbtpage.c|||2953|||/*
 * Initialize local memory state used by VACUUM for _bt_pendingfsm_finalize
 * optimization.
 *
 * Called at the start of a btvacuumscan().  Caller's cleanuponly argument
 * indicates if ongoing VACUUM has not (and will not) call btbulkdelete().
 *
 * We expect to allocate memory inside VACUUM's top-level memory context here.
 * The working buffer is subject to a limit based on work_mem.  Our strategy
 * when the array can no longer grow within the bounds of that limit is to
 * stop saving additional newly deleted pages, while proceeding as usual with
 * the pages that we can fit.
 */""",
  """_bt_pendingfsm_finalize|||backend\access\nbtree\nbtpage.c|||2994|||/*
 * Place any newly deleted pages (i.e. pages that _bt_pagedel() deleted during
 * the ongoing VACUUM operation) into the free space map -- though only when
 * it is actually safe to do so by now.
 *
 * Called at the end of a btvacuumscan(), just before free space map vacuuming
 * takes place.
 *
 * Frees memory allocated by _bt_pendingfsm_init(), if any.
 */""",
  """_bt_pendingfsm_add|||backend\access\nbtree\nbtpage.c|||3061|||/*
 * Maintain array of pages that were deleted during current btvacuumscan()
 * call, for use in _bt_pendingfsm_finalize()
 */""",
  """<clinit>|||backend\access\nbtree\nbtree.c|||67|||/*
 * BTParallelScanDescData contains btree specific shared information required
 * for parallel scan.
 */""",
  """bthandler|||backend\access\nbtree\nbtree.c|||100|||/*
 * Btree handler function: return IndexAmRoutine with access method parameters
 * and callbacks.
 */""",
  """btbuildempty|||backend\access\nbtree\nbtree.c|||158|||/*
 *	btbuildempty() -- build an empty btree index in the initialization fork
 */""",
  """btinsert|||backend\access\nbtree\nbtree.c|||181|||/*
 *	btinsert() -- insert an index tuple into a btree.
 *
 *		Descend the tree recursively, find the appropriate location for our
 *		new tuple, and put it there.
 */""",
  """btgettuple|||backend\access\nbtree\nbtree.c|||205|||/*
 *	btgettuple() -- Get the next tuple in the scan.
 */""",
  """btgetbitmap|||backend\access\nbtree\nbtree.c|||265|||/*
 * btgetbitmap() -- gets all matching tuples, and adds them to a bitmap
 */""",
  """btbeginscan|||backend\access\nbtree\nbtree.c|||311|||/*
 *	btbeginscan() -- start a scan on a btree index
 */""",
  """btrescan|||backend\access\nbtree\nbtree.c|||358|||/*
 *	btrescan() -- rescan an index relation
 */""",
  """btendscan|||backend\access\nbtree\nbtree.c|||416|||/*
 *	btendscan() -- close down a scan
 */""",
  """btmarkpos|||backend\access\nbtree\nbtree.c|||452|||/*
 *	btmarkpos() -- save current scan position
 */""",
  """btrestrpos|||backend\access\nbtree\nbtree.c|||478|||/*
 *	btrestrpos() -- restore scan to last saved position
 */""",
  """btestimateparallelscan|||backend\access\nbtree\nbtree.c|||536|||/*
 * btestimateparallelscan -- estimate storage for BTParallelScanDescData
 */""",
  """btinitparallelscan|||backend\access\nbtree\nbtree.c|||546|||/*
 * btinitparallelscan -- initialize BTParallelScanDesc for parallel btree scan
 */""",
  """btparallelrescan|||backend\access\nbtree\nbtree.c|||560|||/*
 *	btparallelrescan() -- reset parallel scan
 */""",
  """_bt_parallel_seize|||backend\access\nbtree\nbtree.c|||603|||/*
 * _bt_parallel_seize() -- Begin the process of advancing the scan to a new
 *		page.  Other scans must wait until we call _bt_parallel_release()
 *		or _bt_parallel_done().
 *
 * The return value is true if we successfully seized the scan and false
 * if we did not.  The latter case occurs when no pages remain, or when
 * another primitive index scan is scheduled that caller's backend cannot
 * start just yet (only backends that call from _bt_first are capable of
 * starting primitive index scans, which they indicate by passing first=true).
 *
 * If the return value is true, *pageno returns the next or current page
 * of the scan (depending on the scan direction).  An invalid block number
 * means the scan hasn't yet started, or that caller needs to start the next
 * primitive index scan (if it's the latter case we'll set so.needPrimScan).
 * The first time a participating process reaches the last page, it will return
 * true and set *pageno to P_NONE; after that, further attempts to seize the
 * scan will return false.
 *
 * Callers should ignore the value of pageno if the return value is false.
 */""",
  """_bt_parallel_release|||backend\access\nbtree\nbtree.c|||712|||/*
 * _bt_parallel_release() -- Complete the process of advancing the scan to a
 *		new page.  We now have the new value btps_scanPage; some other backend
 *		can now begin advancing the scan.
 *
 * Callers whose scan uses array keys must save their scan_page argument so
 * that it can be passed to _bt_parallel_primscan_schedule, should caller
 * determine that another primitive index scan is required.  If that happens,
 * scan_page won't be scanned by any backend (unless the next primitive index
 * scan lands on scan_page).
 */""",
  """_bt_parallel_done|||backend\access\nbtree\nbtree.c|||735|||/*
 * _bt_parallel_done() -- Mark the parallel scan as complete.
 *
 * When there are no pages left to scan, this function should be called to
 * notify other workers.  Otherwise, they might wait forever for the scan to
 * advance to the next page.
 */""",
  """_bt_parallel_primscan_schedule|||backend\access\nbtree\nbtree.c|||783|||/*
 * _bt_parallel_primscan_schedule() -- Schedule another primitive index scan.
 *
 * Caller passes the block number most recently passed to _bt_parallel_release
 * by its backend.  Caller successfully schedules the next primitive index scan
 * if the shared parallel state hasn't been seized since caller's backend last
 * advanced the scan.
 */""",
  """btbulkdelete|||backend\access\nbtree\nbtree.c|||820|||/*
 * Bulk deletion of all index entries pointing to a set of heap tuples.
 * The set of target tuples is specified via a callback routine that tells
 * whether any given heap tuple (identified by ItemPointer) is being deleted.
 *
 * Result: a palloc'd struct containing statistical info for VACUUM displays.
 */""",
  """btvacuumcleanup|||backend\access\nbtree\nbtree.c|||850|||/*
 * Post-VACUUM cleanup.
 *
 * Result: a palloc'd struct containing statistical info for VACUUM displays.
 */""",
  """btvacuumscan|||backend\access\nbtree\nbtree.c|||938|||/*
 * btvacuumscan --- scan the index for VACUUMing purposes
 *
 * This combines the functions of looking for leaf tuples that are deletable
 * according to the vacuum callback, looking for empty pages that can be
 * deleted, and looking for old deleted pages that can be recycled.  Both
 * btbulkdelete and btvacuumcleanup invoke this (the latter only if no
 * btbulkdelete call occurred and _bt_vacuum_needs_cleanup returned true).
 *
 * The caller is responsible for initially allocating/zeroing a stats struct
 * and for obtaining a vacuum cycle ID if necessary.
 */""",
  """btvacuumpage|||backend\access\nbtree\nbtree.c|||1072|||/*
 * btvacuumpage --- VACUUM one page
 *
 * This processes a single page for btvacuumscan().  In some cases we must
 * backtrack to re-examine and VACUUM pages that were the scanblkno during
 * a previous call here.  This is how we handle page splits (that happened
 * after our cycleid was acquired) whose right half page happened to reuse
 * a block that we might have processed at some point before it was
 * recycled (i.e. before the page split).
 */""",
  """btreevacuumposting|||backend\access\nbtree\nbtree.c|||1407|||/*
 * btreevacuumposting --- determine TIDs still needed in posting list
 *
 * Returns metadata describing how to build replacement tuple without the TIDs
 * that VACUUM needs to delete.  Returned value is NULL in the common case
 * where no changes are needed to caller's posting list tuple (we avoid
 * allocating memory here as an optimization).
 *
 * The number of TIDs that should remain in the posting list tuple is set for
 * caller in *nremaining.
 */""",
  """btcanreturn|||backend\access\nbtree\nbtree.c|||1456|||/*
 *	btcanreturn() -- Check whether btree indexes support index-only scans.
 *
 * btrees always do, so this is trivial.
 */""",
  """_bt_drop_lock_and_maybe_pin|||backend\access\nbtree\nbtsearch.c|||60|||/*
 *	_bt_drop_lock_and_maybe_pin()
 *
 * Unlock the buffer; and if it is safe to release the pin, do that, too.
 * This will prevent vacuum from stalling in a blocked state trying to read a
 * page when a cursor is sitting on it.
 *
 * See nbtree/README section on making concurrent TID recycling safe.
 */""",
  """_bt_search|||backend\access\nbtree\nbtsearch.c|||95|||/*
 *	_bt_search() -- Search the tree for a particular scankey,
 *		or more precisely for the first leaf page it could be on.
 *
 * The passed scankey is an insertion-type scankey (see nbtree/README),
 * but it can omit the rightmost column(s) of the index.
 *
 * Return value is a stack of parent-page pointers (i.e. there is no entry for
 * the leaf level/page).  *bufP is set to the address of the leaf-page buffer,
 * which is locked and pinned.  No locks are held on the parent pages,
 * however!
 *
 * The returned buffer is locked according to access parameter.  Additionally,
 * access = BT_WRITE will allow an empty root page to be created and returned.
 * When access = BT_READ, an empty index will result in *bufP being set to
 * InvalidBuffer.  Also, in BT_WRITE mode, any incomplete splits encountered
 * during the search will be finished.
 *
 * heaprel must be provided by callers that pass access = BT_WRITE, since we
 * might need to allocate a new root page for caller -- see _bt_allocbuf.
 */""",
  """_bt_moveright|||backend\access\nbtree\nbtsearch.c|||234|||/*
 *	_bt_moveright() -- move right in the btree if necessary.
 *
 * When we follow a pointer to reach a page, it is possible that
 * the page has changed in the meanwhile.  If this happens, we're
 * guaranteed that the page has "split right" -- that is, that any
 * data that appeared on the page originally is either on the page
 * or strictly to the right of it.
 *
 * This routine decides whether or not we need to move right in the
 * tree by examining the high key entry on the page.  If that entry is
 * strictly less than the scankey, or <= the scankey in the
 * key.nextkey=true case, then we followed the wrong link and we need
 * to move right.
 *
 * The passed insertion-type scankey can omit the rightmost column(s) of the
 * index. (see nbtree/README)
 *
 * When key.nextkey is false (the usual case), we are looking for the first
 * item >= key.  When key.nextkey is true, we are looking for the first item
 * strictly greater than key.
 *
 * If forupdate is true, we will attempt to finish any incomplete splits
 * that we encounter.  This is required when locking a target page for an
 * insertion, because we don't allow inserting on a page before the split is
 * completed.  'heaprel' and 'stack' are only used if forupdate is true.
 *
 * On entry, we have the buffer pinned and a lock of the type specified by
 * 'access'.  If we move right, we release the buffer and lock and acquire
 * the same on the right sibling.  Return value is the buffer we stop at.
 */""",
  """_bt_binsrch|||backend\access\nbtree\nbtsearch.c|||336|||/*
 *	_bt_binsrch() -- Do a binary search for a key on a particular page.
 *
 * On an internal (non-leaf) page, _bt_binsrch() returns the OffsetNumber
 * of the last key < given scankey, or last key <= given scankey if nextkey
 * is true.  (Since _bt_compare treats the first data key of such a page as
 * minus infinity, there will be at least one key < scankey, so the result
 * always points at one of the keys on the page.)
 *
 * On a leaf page, _bt_binsrch() returns the final result of the initial
 * positioning process that started with _bt_first's call to _bt_search.
 * We're returning a non-pivot tuple offset, so things are a little different.
 * It is possible that we'll return an offset that's either past the last
 * non-pivot slot, or (in the case of a backward scan) before the first slot.
 *
 * This procedure is not responsible for walking right, it just examines
 * the given page.  _bt_binsrch() has no lock or refcount side effects
 * on the buffer.
 */""",
  """_bt_binsrch_insert|||backend\access\nbtree\nbtsearch.c|||467|||/*
 *
 *	_bt_binsrch_insert() -- Cacheable, incremental leaf page binary search.
 *
 * Like _bt_binsrch(), but with support for caching the binary search
 * bounds.  Only used during insertion, and only on the leaf page that it
 * looks like caller will insert tuple on.  Exclusive-locked and pinned
 * leaf page is contained within insertstate.
 *
 * Caches the bounds fields in insertstate so that a subsequent call can
 * reuse the low and strict high bounds of original binary search.  Callers
 * that use these fields directly must be prepared for the case where low
 * and/or stricthigh are not on the same page (one or both exceed maxoff
 * for the page).  The case where there are no items on the page (high <
 * low) makes bounds invalid.
 *
 * Caller is responsible for invalidating bounds when it modifies the page
 * before calling here a second time, and for dealing with posting list
 * tuple matches (callers can use insertstate's postingoff field to
 * determine which existing heap TID will need to be replaced by a posting
 * list split).
 */""",
  """_bt_binsrch_posting|||backend\access\nbtree\nbtsearch.c|||595|||/*----------
 *	_bt_binsrch_posting() -- posting list binary search.
 *
 * Helper routine for _bt_binsrch_insert().
 *
 * Returns offset into posting list where caller's scantid belongs.
 *----------
 */""",
  """_bt_compare|||backend\access\nbtree\nbtsearch.c|||681|||/*----------
 *	_bt_compare() -- Compare insertion-type scankey to tuple on a page.
 *
 *	page/offnum: location of btree item to be compared to.
 *
 *		This routine returns:
 *			<0 if scankey < tuple at offnum;
 *			 0 if scankey == tuple at offnum;
 *			>0 if scankey > tuple at offnum.
 *
 * NULLs in the keys are treated as sortable values.  Therefore
 * "equality" does not necessarily mean that the item should be returned
 * to the caller as a matching key.  Similarly, an insertion scankey
 * with its scantid set is treated as equal to a posting tuple whose TID
 * range overlaps with their scantid.  There generally won't be a
 * matching TID in the posting tuple, which caller must handle
 * themselves (e.g., by splitting the posting list tuple).
 *
 * CRUCIAL NOTE: on a non-leaf page, the first data key is assumed to be
 * "minus infinity": this routine will always claim it is less than the
 * scankey.  The actual key value stored is explicitly truncated to 0
 * attributes (explicitly minus infinity) with version 3+ indexes, but
 * that isn't relied upon.  This allows us to implement the Lehman and
 * Yao convention that the first down-link pointer is before the first
 * key.  See backend/access/nbtree/README for details.
 *----------
 */""",
  """_bt_first|||backend\access\nbtree\nbtsearch.c|||875|||/*
 *	_bt_first() -- Find the first item in a scan.
 *
 *		We need to be clever about the direction of scan, the search
 *		conditions, and the tree ordering.  We find the first item (or,
 *		if backwards scan, the last item) in the tree that satisfies the
 *		qualifications in the scan key.  On success exit, the page containing
 *		the current index tuple is pinned but not locked, and data about
 *		the matching tuple(s) on the page has been loaded into so->currPos.
 *		scan->xs_heaptid is set to the heap TID of the current tuple, and if
 *		requested, scan->xs_itup points to a copy of the index tuple.
 *
 * If there are no matching items in the index, we return false, with no
 * pins or locks held.
 *
 * Note that scan->keyData[], and the so->keyData[] scankey built from it,
 * are both search-type scankeys (see nbtree/README for more about this).
 * Within this routine, we build a temporary insertion-type scankey to use
 * in locating the scan start position.
 */""",
  """_bt_next|||backend\access\nbtree\nbtsearch.c|||1495|||/*
 *	_bt_next() -- Get the next item in a scan.
 *
 *		On entry, so->currPos describes the current page, which may be pinned
 *		but is not locked, and so->currPos.itemIndex identifies which item was
 *		previously returned.
 *
 *		On successful exit, scan->xs_heaptid is set to the TID of the next
 *		heap tuple, and if requested, scan->xs_itup points to a copy of the
 *		index tuple.  so->currPos is updated as needed.
 *
 *		On failure exit (no more tuples), we release pin and set
 *		so->currPos.buf to InvalidBuffer.
 */""",
  """_bt_readpage|||backend\access\nbtree\nbtsearch.c|||1559|||/*
 *	_bt_readpage() -- Load data from current index page into so->currPos
 *
 * Caller must have pinned and read-locked so->currPos.buf; the buffer's state
 * is not changed here.  Also, currPos.moreLeft and moreRight must be valid;
 * they are updated as appropriate.  All other fields of so->currPos are
 * initialized from scratch here.
 *
 * We scan the current page starting at offnum and moving in the indicated
 * direction.  All items matching the scan keys are loaded into currPos.items.
 * moreLeft or moreRight (as appropriate) is cleared if _bt_checkkeys reports
 * that there can be no more matching tuples in the current scan direction
 * (could just be for the current primitive index scan when scan has arrays).
 *
 * _bt_first caller passes us an offnum returned by _bt_binsrch, which might
 * be an out of bounds offnum such as "maxoff + 1" in certain corner cases.
 * _bt_checkkeys will stop the scan as soon as an equality qual fails (when
 * its scan key was marked required), so _bt_first _must_ pass us an offnum
 * exactly at the beginning of where equal tuples are to be found.  When we're
 * passed an offnum past the end of the page, we might still manage to stop
 * the scan on this page by calling _bt_checkkeys against the high key.
 *
 * In the case of a parallel scan, caller must have called _bt_parallel_seize
 * prior to calling this function; this function will invoke
 * _bt_parallel_release before returning.
 *
 * Returns true if any matching items found on the page, false if none.
 */""",
  """_bt_saveitem|||backend\access\nbtree\nbtsearch.c|||1944|||/* Save an index item into so->currPos.items[itemIndex] */""",
  """_bt_setuppostingitems|||backend\access\nbtree\nbtsearch.c|||1974|||/*
 * Setup state to save TIDs/items from a single posting list tuple.
 *
 * Saves an index item into so->currPos.items[itemIndex] for TID that is
 * returned to scan first.  Second or subsequent TIDs for posting list should
 * be saved by calling _bt_savepostingitem().
 *
 * Returns an offset into tuple storage space that main tuple is stored at if
 * needed.
 */""",
  """_bt_savepostingitem|||backend\access\nbtree\nbtsearch.c|||2012|||/*
 * Save an index item into so->currPos.items[itemIndex] for current posting
 * tuple.
 *
 * Assumes that _bt_setuppostingitems() has already been called for current
 * posting list tuple.  Caller passes its return value as tupleOffset.
 */""",
  """_bt_steppage|||backend\access\nbtree\nbtsearch.c|||2040|||/*
 *	_bt_steppage() -- Step to next page containing valid data for scan
 *
 * On entry, if so->currPos.buf is valid the buffer is pinned but not locked;
 * if pinned, we'll drop the pin before moving to next page.  The buffer is
 * not locked on entry.
 *
 * For success on a scan using a non-MVCC snapshot we hold a pin, but not a
 * read lock, on that page.  If we do not hold the pin, we set so->currPos.buf
 * to InvalidBuffer.  We return true to indicate success.
 */""",
  """_bt_readnextpage|||backend\access\nbtree\nbtsearch.c|||2180|||/*
 *	_bt_readnextpage() -- Read next page containing valid data for scan
 *
 * On success exit, so->currPos is updated to contain data from the next
 * interesting page, and we return true.  Caller must release the lock (and
 * maybe the pin) on the buffer on success exit.
 *
 * If there are no more matching records in the given direction, we drop all
 * locks and pins, set so->currPos.buf to InvalidBuffer, and return false.
 */""",
  """_bt_parallel_readpage|||backend\access\nbtree\nbtsearch.c|||2346|||/*
 *	_bt_parallel_readpage() -- Read current page containing valid data for scan
 *
 * On success, release lock and maybe pin on buffer.  We return true to
 * indicate success.
 */""",
  """_bt_walk_left|||backend\access\nbtree\nbtsearch.c|||2377|||/*
 * _bt_walk_left() -- step left one page, if possible
 *
 * The given buffer must be pinned and read-locked.  This will be dropped
 * before stepping left.  On return, we have pin and read lock on the
 * returned page, instead.
 *
 * Returns InvalidBuffer if there is no page to the left (no lock is held
 * in that case).
 *
 * It is possible for the returned leaf page to be half-dead; caller must
 * check that condition and step left again when required.
 */""",
  """_bt_get_endpoint|||backend\access\nbtree\nbtsearch.c|||2491|||/*
 * _bt_get_endpoint() -- Find the first or last page on a given tree level
 *
 * If the index is empty, we will return InvalidBuffer; any other failure
 * condition causes ereport().  We will not return a dead page.
 *
 * The returned buffer is pinned and read-locked.
 */""",
  """_bt_endpoint|||backend\access\nbtree\nbtsearch.c|||2572|||/*
 *	_bt_endpoint() -- Find the first or last page in the index, and scan
 * from there to the first key satisfying all the quals.
 *
 * This is used by _bt_first() to set up a scan when we've determined
 * that the scan must start at the beginning or end of the index (for
 * a forward or backward scan respectively).  Exit conditions are the
 * same as for _bt_first().
 */""",
  """_bt_initialize_more_data|||backend\access\nbtree\nbtsearch.c|||2662|||/*
 * _bt_initialize_more_data() -- initialize moreLeft, moreRight and scan dir
 * from currPos
 */""",
  """btbuild|||backend\access\nbtree\nbtsort.c|||292|||/*
 *	btbuild() -- build a new btree index.
 */""",
  """_bt_spools_heapscan|||backend\access\nbtree\nbtsort.c|||362|||/*
 * Create and initialize one or two spool structures, and save them in caller's
 * buildstate argument.  May also fill-in fields within indexInfo used by index
 * builds.
 *
 * Scans the heap, possibly in parallel, filling spools with IndexTuples.  This
 * routine encapsulates all aspects of managing parallelism.  Caller need only
 * call _bt_end_parallel() in parallel case after it is done with spool/spool2.
 *
 * Returns the total number of heap tuples scanned.
 */"""
)
INFO:__main__:Extracted 0 methods with comments in this batch
WARNING:__main__:Empty batch at offset 1000 (count: 1)
INFO:__main__:Fetching batch: offset=1100, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=1100, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres342[0m: [32mList[0m[[32mString[0m] = List(
  """_bt_spooldestroy|||backend\access\nbtree\nbtsort.c|||514|||/*
 * clean up a spool structure and its substructures.
 */""",
  """_bt_spool|||backend\access\nbtree\nbtsort.c|||524|||/*
 * spool an index entry into the sort file.
 */""",
  """_bt_leafbuild|||backend\access\nbtree\nbtsort.c|||535|||/*
 * given a spool loaded by successive calls to _bt_spool,
 * create an entire btree.
 */""",
  """_bt_build_callback|||backend\access\nbtree\nbtsort.c|||576|||/*
 * Per-tuple callback for table_index_build_scan
 */""",
  """_bt_blnewpage|||backend\access\nbtree\nbtsort.c|||605|||/*
 * allocate workspace for a new, clean btree page, not linked to any siblings.
 */""",
  """_bt_blwritepage|||backend\access\nbtree\nbtsort.c|||634|||/*
 * emit a completed btree page, and release the working storage.
 */""",
  """_bt_pagestate|||backend\access\nbtree\nbtsort.c|||645|||/*
 * allocate and initialize a new BTPageState.  the returned structure
 * is suitable for immediate use by _bt_buildadd.
 */""",
  """_bt_slideleft|||backend\access\nbtree\nbtsort.c|||682|||/*
 * Slide the array of ItemIds from the page back one slot (from P_FIRSTKEY to
 * P_HIKEY, overwriting P_HIKEY).
 *
 * _bt_blnewpage() makes the P_HIKEY line pointer appear allocated, but the
 * rightmost page on its level is not supposed to get a high key.  Now that
 * it's clear that this page is a rightmost page, remove the unneeded empty
 * P_HIKEY line pointer space.
 */""",
  """_bt_sortaddtup|||backend\access\nbtree\nbtsort.c|||713|||/*
 * Add an item to a page being built.
 *
 * This is very similar to nbtinsert.c's _bt_pgaddtup(), but this variant
 * raises an error directly.
 *
 * Note that our nbtsort.c caller does not know yet if the page will be
 * rightmost.  Offset P_FIRSTKEY is always assumed to be the first data key by
 * caller.  Page that turns out to be the rightmost on its level is fixed by
 * calling _bt_slideleft().
 */""",
  """_bt_buildadd|||backend\access\nbtree\nbtsort.c|||783|||/*----------
 * Add an item to a disk page from the sort output (or add a posting list
 * item formed from the sort output).
 *
 * We must be careful to observe the page layout conventions of nbtsearch.c:
 * - rightmost pages start data items at P_HIKEY instead of at P_FIRSTKEY.
 * - on non-leaf pages, the key portion of the first item need not be
 *	 stored, we should store only the link.
 *
 * A leaf page being built looks like:
 *
 * +----------------+---------------------------------+
 * | PageHeaderData | linp0 linp1 linp2 ...           |
 * +-----------+----+---------------------------------+
 * | ... linpN |									  |
 * +-----------+--------------------------------------+
 * |	 ^ last										  |
 * |												  |
 * +-------------+------------------------------------+
 * |			 | itemN ...                          |
 * +-------------+------------------+-----------------+
 * |		  ... item3 item2 item1 | "special space" |
 * +--------------------------------+-----------------+
 *
 * Contrast this with the diagram in bufpage.h; note the mismatch
 * between linps and items.  This is because we reserve linp0 as a
 * placeholder for the pointer to the "high key" item; when we have
 * filled up the page, we will set linp0 to point to itemN and clear
 * linpN.  On the other hand, if we find this is the last (rightmost)
 * page, we leave the items alone and slide the linp array over.  If
 * the high key is to be truncated, offset 1 is deleted, and we insert
 * the truncated high key at offset 1.
 *
 * 'last' pointer indicates the last offset added to the page.
 *
 * 'truncextra' is the size of the posting list in itup, if any.  This
 * information is stashed for the next call here, when we may benefit
 * from considering the impact of truncating away the posting list on
 * the page before deciding to finish the page off.  Posting lists are
 * often relatively large, so it is worth going to the trouble of
 * accounting for the saving from truncating away the posting list of
 * the tuple that becomes the high key (that may be the only way to
 * get close to target free space on the page).  Note that this is
 * only used for the soft fillfactor-wise limit, not the critical hard
 * limit.
 *----------
 */""",
  """_bt_sort_dedup_finish_pending|||backend\access\nbtree\nbtsort.c|||1028|||/*
 * Finalize pending posting list tuple, and add it to the index.  Final tuple
 * is based on saved base tuple, and saved list of heap TIDs.
 *
 * This is almost like _bt_dedup_finish_pending(), but it adds a new tuple
 * using _bt_buildadd().
 */""",
  """_bt_uppershutdown|||backend\access\nbtree\nbtsort.c|||1062|||/*
 * Finish writing out the completed btree.
 */""",
  """_bt_load|||backend\access\nbtree\nbtsort.c|||1134|||/*
 * Read tuples in correct sort order from tuplesort, and load them into
 * btree leaves.
 */""",
  """_bt_begin_parallel|||backend\access\nbtree\nbtsort.c|||1395|||/*
 * Create parallel context, and launch workers for leader.
 *
 * buildstate argument should be initialized (with the exception of the
 * tuplesort state in spools, which may later be created based on shared
 * state initially set up here).
 *
 * isconcurrent indicates if operation is CREATE INDEX CONCURRENTLY.
 *
 * request is the target number of parallel worker processes to launch.
 *
 * Sets buildstate's BTLeader, which caller must use to shut down parallel
 * mode by passing it to _bt_end_parallel() at the very end of its index
 * build.  If not even a single worker process can be launched, this is
 * never set, and caller should proceed with a serial index build.
 */""",
  """_bt_end_parallel|||backend\access\nbtree\nbtsort.c|||1606|||/*
 * Shut down workers, destroy parallel context, and end parallel mode.
 */""",
  """_bt_parallel_estimate_shared|||backend\access\nbtree\nbtsort.c|||1632|||/*
 * Returns size of shared memory required to store state for a parallel
 * btree index build based on the snapshot its parallel scan will use.
 */""",
  """_bt_parallel_heapscan|||backend\access\nbtree\nbtsort.c|||1652|||/*
 * Within leader, wait for end of heap scan.
 *
 * When called, parallel heap scan started by _bt_begin_parallel() will
 * already be underway within worker processes (when leader participates
 * as a worker, we should end up here just as workers are finishing).
 *
 * Fills in fields needed for ambuild statistics, and lets caller set
 * field indicating that some worker encountered a broken HOT chain.
 *
 * Returns the total number of heap tuples scanned.
 */""",
  """_bt_leader_participate_as_worker|||backend\access\nbtree\nbtsort.c|||1686|||/*
 * Within leader, participate as a parallel worker.
 */""",
  """_bt_parallel_build_main|||backend\access\nbtree\nbtsort.c|||1739|||/*
 * Perform work within a launched parallel process.
 */""",
  """_bt_parallel_scan_and_sort|||backend\access\nbtree\nbtsort.c|||1861|||/*
 * Perform a worker's portion of a parallel sort.
 *
 * This generates a tuplesort for passed btspool, and a second tuplesort
 * state if a second btspool is need (i.e. for unique index builds).  All
 * other spool fields should already be set when this is called.
 *
 * sortmem is the amount of working memory to use within each worker,
 * expressed in KBs.
 *
 * When this returns, workers are done, and need only release resources.
 */""",
  """_bt_findsplitloc|||backend\access\nbtree\nbtsplitloc.c|||128|||/*
 *	_bt_findsplitloc() -- find an appropriate place to split a page.
 *
 * The main goal here is to equalize the free space that will be on each
 * split page, *after accounting for the inserted tuple*.  (If we fail to
 * account for it, we might find ourselves with too little room on the page
 * that it needs to go into!)
 *
 * If the page is the rightmost page on its level, we instead try to arrange
 * to leave the left split page fillfactor% full.  In this way, when we are
 * inserting successively increasing keys (consider sequences, timestamps,
 * etc) we will end up with a tree whose pages are about fillfactor% full,
 * instead of the 50% full result that we'd get without this special case.
 * This is the same as nbtsort.c produces for a newly-created tree.  Note
 * that leaf and nonleaf pages use different fillfactors.  Note also that
 * there are a number of further special cases where fillfactor is not
 * applied in the standard way.
 *
 * We are passed the intended insert position of the new tuple, expressed as
 * the offsetnumber of the tuple it must go in front of (this could be
 * maxoff+1 if the tuple is to go at the end).  The new tuple itself is also
 * passed, since it's needed to give some weight to how effective suffix
 * truncation will be.  The implementation picks the split point that
 * maximizes the effectiveness of suffix truncation from a small list of
 * alternative candidate split points that leave each side of the split with
 * about the same share of free space.  Suffix truncation is secondary to
 * equalizing free space, except in cases with large numbers of duplicates.
 * Note that it is always assumed that caller goes on to perform truncation,
 * even with pg_upgrade'd indexes where that isn't actually the case
 * (!heapkeyspace indexes).  See nbtree/README for more information about
 * suffix truncation.
 *
 * We return the index of the first existing tuple that should go on the
 * righthand page (which is called firstrightoff), plus a boolean
 * indicating whether the new tuple goes on the left or right page.  You
 * can think of the returned state as a point _between_ two adjacent data
 * items (lastleft and firstright data items) on an imaginary version of
 * origpage that already includes newitem.  The bool is necessary to
 * disambiguate the case where firstrightoff == newitemoff (i.e. it is
 * sometimes needed to determine if the firstright tuple for the split is
 * newitem rather than the tuple from origpage at offset firstrightoff).
 */""",
  """_bt_recsplitloc|||backend\access\nbtree\nbtsplitloc.c|||448|||/*
 * Subroutine to record a particular point between two tuples (possibly the
 * new item) on page (ie, combination of firstrightoff and newitemonleft
 * settings) in *state for later analysis.  This is also a convenient point to
 * check if the split is legal (if it isn't, it won't be recorded).
 *
 * firstrightoff is the offset of the first item on the original page that
 * goes to the right page, and firstrightofforigpagetuplesz is the size of
 * that tuple.  firstrightoff can be > max offset, which means that all the
 * old items go to the left page and only the new item goes to the right page.
 * We don't actually use firstrightofforigpagetuplesz in that case (actually,
 * we don't use it for _any_ split where the firstright tuple happens to be
 * newitem).
 *
 * olddataitemstoleft is the total size of all old items to the left of the
 * split point that is recorded here when legal.  Should not include
 * newitemsz, since that is handled here.
 */""",
  """_bt_deltasortsplits|||backend\access\nbtree\nbtsplitloc.c|||565|||/*
 * Subroutine to assign space deltas to materialized array of candidate split
 * points based on current fillfactor, and to sort array using that fillfactor
 */""",
  """_bt_splitcmp|||backend\access\nbtree\nbtsplitloc.c|||593|||/*
 * qsort-style comparator used by _bt_deltasortsplits()
 */""",
  """_bt_afternewitemoff|||backend\access\nbtree\nbtsplitloc.c|||629|||/*
 * Subroutine to determine whether or not a non-rightmost leaf page should be
 * split immediately after the would-be original page offset for the
 * new/incoming tuple (or should have leaf fillfactor applied when new item is
 * to the right on original page).  This is appropriate when there is a
 * pattern of localized monotonically increasing insertions into a composite
 * index, where leading attribute values form local groupings, and we
 * anticipate further insertions of the same/current grouping (new item's
 * grouping) in the near future.  This can be thought of as a variation on
 * applying leaf fillfactor during rightmost leaf page splits, since cases
 * that benefit will converge on packing leaf pages leaffillfactor% full over
 * time.
 *
 * We may leave extra free space remaining on the rightmost page of a "most
 * significant column" grouping of tuples if that grouping never ends up
 * having future insertions that use the free space.  That effect is
 * self-limiting; a future grouping that becomes the "nearest on the right"
 * grouping of the affected grouping usually puts the extra free space to good
 * use.
 *
 * Caller uses optimization when routine returns true, though the exact action
 * taken by caller varies.  Caller uses original leaf page fillfactor in
 * standard way rather than using the new item offset directly when *usemult
 * was also set to true here.  Otherwise, caller applies optimization by
 * locating the legal split point that makes the new tuple the lastleft tuple
 * for the split.
 */""",
  """_bt_adjacenthtid|||backend\access\nbtree\nbtsplitloc.c|||748|||/*
 * Subroutine for determining if two heap TIDS are "adjacent".
 *
 * Adjacent means that the high TID is very likely to have been inserted into
 * heap relation immediately after the low TID, probably during the current
 * transaction.
 */""",
  """_bt_bestsplitloc|||backend\access\nbtree\nbtsplitloc.c|||787|||/*
 * Subroutine to find the "best" split point among candidate split points.
 * The best split point is the split point with the lowest penalty among split
 * points that fall within current/final split interval.  Penalty is an
 * abstract score, with a definition that varies depending on whether we're
 * splitting a leaf page or an internal page.  See _bt_split_penalty() for
 * details.
 *
 * "perfectpenalty" is assumed to be the lowest possible penalty among
 * candidate split points.  This allows us to return early without wasting
 * cycles on calculating the first differing attribute for all candidate
 * splits when that clearly cannot improve our choice (or when we only want a
 * minimally distinguishing split point, and don't want to make the split any
 * more unbalanced than is necessary).
 *
 * We return the index of the first existing tuple that should go on the right
 * page, plus a boolean indicating if new item is on left of split point.
 */""",
  """_bt_defaultinterval|||backend\access\nbtree\nbtsplitloc.c|||875|||/*
 * Return a split interval to use for the default strategy.  This is a limit
 * on the number of candidate split points to give further consideration to.
 * Only a fraction of all candidate splits points (those located at the start
 * of the now-sorted splits array) fall within the split interval.  Split
 * interval is applied within _bt_bestsplitloc().
 *
 * Split interval represents an acceptable range of split points -- those that
 * have leftfree and rightfree values that are acceptably balanced.  The final
 * split point chosen is the split point with the lowest "penalty" among split
 * points in this split interval (unless we change our entire strategy, in
 * which case the interval also changes -- see _bt_strategy()).
 *
 * The "Prefix B-Trees" paper calls split interval sigma l for leaf splits,
 * and sigma b for internal ("branch") splits.  It's hard to provide a
 * theoretical justification for the size of the split interval, though it's
 * clear that a small split interval can make tuples on level L+1 much smaller
 * on average, without noticeably affecting space utilization on level L.
 * (Note that the way that we calculate split interval might need to change if
 * suffix truncation is taught to truncate tuples "within" the last
 * attribute/datum for data types like text, which is more or less how it is
 * assumed to work in the paper.)
 */""",
  """_bt_strategy|||backend\access\nbtree\nbtsplitloc.c|||933|||/*
 * Subroutine to decide whether split should use default strategy/initial
 * split interval, or whether it should finish splitting the page using
 * alternative strategies (this is only possible with leaf pages).
 *
 * Caller uses alternative strategy (or sticks with default strategy) based
 * on how *strategy is set here.  Return value is "perfect penalty", which is
 * passed to _bt_bestsplitloc() as a final constraint on how far caller is
 * willing to go to avoid appending a heap TID when using the many duplicates
 * strategy (it also saves _bt_bestsplitloc() useless cycles).
 */""",
  """_bt_interval_edges|||backend\access\nbtree\nbtsplitloc.c|||1051|||/*
 * Subroutine to locate leftmost and rightmost splits for current/default
 * split interval.  Note that it will be the same split iff there is only one
 * split in interval.
 */""",
  """_bt_split_penalty|||backend\access\nbtree\nbtsplitloc.c|||1130|||/*
 * Subroutine to find penalty for caller's candidate split point.
 *
 * On leaf pages, penalty is the attribute number that distinguishes each side
 * of a split.  It's the last attribute that needs to be included in new high
 * key for left page.  It can be greater than the number of key attributes in
 * cases where a heap TID will need to be appended during truncation.
 *
 * On internal pages, penalty is simply the size of the firstright tuple for
 * the split (including line pointer overhead).  This tuple will become the
 * new high key for the left page.
 */""",
  """_bt_split_lastleft|||backend\access\nbtree\nbtsplitloc.c|||1158|||/*
 * Subroutine to get a lastleft IndexTuple for a split point
 */""",
  """_bt_split_firstright|||backend\access\nbtree\nbtsplitloc.c|||1174|||/*
 * Subroutine to get a firstright IndexTuple for a split point
 */""",
  """_bt_mkscankey|||backend\access\nbtree\nbtutils.c|||128|||/*
 * _bt_mkscankey
 *		Build an insertion scan key that contains comparison data from itup
 *		as well as comparator routines appropriate to the key datatypes.
 *
 *		The result is intended for use with _bt_compare() and _bt_truncate().
 *		Callers that don't need to fill out the insertion scankey arguments
 *		(e.g. they use an ad-hoc comparison routine, or only need a scankey
 *		for _bt_truncate()) can pass a NULL index tuple.  The scankey will
 *		be initialized as if an "all truncated" pivot tuple was passed
 *		instead.
 *
 *		Note that we may occasionally have to share lock the metapage to
 *		determine whether or not the keys in the index are expected to be
 *		unique (i.e. if this is a "heapkeyspace" index).  We assume a
 *		heapkeyspace index when caller passes a NULL tuple, allowing index
 *		build callers to avoid accessing the non-existent metapage.  We
 *		also assume that the index is _not_ allequalimage when a NULL tuple
 *		is passed; CREATE INDEX callers call _bt_allequalimage() to set the
 *		field themselves.
 */""",
  """_bt_freestack|||backend\access\nbtree\nbtutils.c|||220|||/*
 * free a retracement stack made by _bt_search.
 */""",
  """_bt_preprocess_array_keys|||backend\access\nbtree\nbtutils.c|||268|||/*
 *	_bt_preprocess_array_keys() -- Preprocess SK_SEARCHARRAY scan keys
 *
 * If there are any SK_SEARCHARRAY scan keys, deconstruct the array(s) and
 * set up BTArrayKeyInfo info for each one that is an equality-type key.
 * Returns modified scan keys as input for further, standard preprocessing.
 *
 * Currently we perform two kinds of preprocessing to deal with redundancies.
 * For inequality array keys, it's sufficient to find the extreme element
 * value and replace the whole array with that scalar value.  This eliminates
 * all but one array element as redundant.  Similarly, we are capable of
 * "merging together" multiple equality array keys (from two or more input
 * scan keys) into a single output scan key containing only the intersecting
 * array elements.  This can eliminate many redundant array elements, as well
 * as eliminating whole array scan keys as redundant.  It can also allow us to
 * detect contradictory quals.
 *
 * It is convenient for _bt_preprocess_keys caller to have to deal with no
 * more than one equality strategy array scan key per index attribute.  We'll
 * always be able to set things up that way when complete opfamilies are used.
 * Eliminated array scan keys can be recognized as those that have had their
 * sk_strategy field set to InvalidStrategy here by us.  Caller should avoid
 * including these in the scan's so->keyData[] output array.
 *
 * We set the scan key references from the scan's BTArrayKeyInfo info array to
 * offsets into the temp modified input array returned to caller.  Scans that
 * have array keys should call _bt_preprocess_array_keys_final when standard
 * preprocessing steps are complete.  This will convert the scan key offset
 * references into references to the scan's so->keyData[] output scan keys.
 *
 * Note: the reason we need to return a temp scan key array, rather than just
 * scribbling on scan->keyData, is that callers are permitted to call btrescan
 * without supplying a new set of scankey data.
 */""",
  """_bt_preprocess_array_keys_final|||backend\access\nbtree\nbtutils.c|||550|||/*
 *	_bt_preprocess_array_keys_final() -- fix up array scan key references
 *
 * When _bt_preprocess_array_keys performed initial array preprocessing, it
 * set each array's array->scan_key to the array's arrayKeys[] entry offset
 * (that also work as references into the original scan->keyData[] array).
 * This function handles translation of the scan key references from the
 * BTArrayKeyInfo info array, from input scan key references (to the keys in
 * scan->keyData[]), into output references (to the keys in so->keyData[]).
 * Caller's keyDataMap[] array tells us how to perform this remapping.
 *
 * Also finalizes so->orderProcs[] for the scan.  Arrays already have an ORDER
 * proc, which might need to be repositioned to its so->keyData[]-wise offset
 * (very much like the remapping that we apply to array->scan_key references).
 * Non-array equality strategy scan keys (that survived preprocessing) don't
 * yet have an so->orderProcs[] entry, so we set one for them here.
 *
 * Also converts single-element array scan keys into equivalent non-array
 * equality scan keys, which decrements so->numArrayKeys.  It's possible that
 * this will leave this new btrescan without any arrays at all.  This isn't
 * necessary for correctness; it's just an optimization.  Non-array equality
 * scan keys are slightly faster than equivalent array scan keys at runtime.
 */""",
  """_bt_setup_array_cmp|||backend\access\nbtree\nbtutils.c|||711|||/*
 * _bt_setup_array_cmp() -- Set up array comparison functions
 *
 * Sets ORDER proc in caller's orderproc argument, which is used during binary
 * searches of arrays during the index scan.  Also sets a same-type ORDER proc
 * in caller's *sortprocp argument, which is used when sorting the array.
 *
 * Preprocessing calls here with all equality strategy scan keys (when scan
 * uses equality array keys), including those not associated with any array.
 * See _bt_advance_array_keys for an explanation of why it'll need to treat
 * simple scalar equality scan keys as degenerate single element arrays.
 *
 * Caller should pass an orderproc pointing to space that'll store the ORDER
 * proc for the scan, and a *sortprocp pointing to its own separate space.
 * When calling here for a non-array scan key, sortprocp arg should be NULL.
 *
 * In the common case where we don't need to deal with cross-type operators,
 * only one ORDER proc is actually required by caller.  We'll set *sortprocp
 * to point to the same memory that caller's orderproc continues to point to.
 * Otherwise, *sortprocp will continue to point to caller's own space.  Either
 * way, *sortprocp will point to a same-type ORDER proc (since that's the only
 * safe way to sort/deduplicate the array associated with caller's scan key).
 */""",
  """_bt_find_extreme_element|||backend\access\nbtree\nbtutils.c|||788|||/*
 * _bt_find_extreme_element() -- get least or greatest array element
 *
 * scan and skey identify the index column, whose opfamily determines the
 * comparison semantics.  strat should be BTLessStrategyNumber to get the
 * least element, or BTGreaterStrategyNumber to get the greatest.
 */""",
  """_bt_sort_array_elements|||backend\access\nbtree\nbtutils.c|||848|||/*
 * _bt_sort_array_elements() -- sort and de-dup array elements
 *
 * The array elements are sorted in-place, and the new number of elements
 * after duplicate removal is returned.
 *
 * skey identifies the index column whose opfamily determines the comparison
 * semantics, and sortproc is a corresponding ORDER proc.  If reverse is true,
 * we sort in descending order.
 */""",
  """_bt_merge_arrays|||backend\access\nbtree\nbtutils.c|||892|||/*
 * _bt_merge_arrays() -- merge next array's elements into an original array
 *
 * Called when preprocessing encounters a pair of array equality scan keys,
 * both against the same index attribute (during initial array preprocessing).
 * Merging reorganizes caller's original array (the left hand arg) in-place,
 * without ever copying elements from one array into the other. (Mixing the
 * elements together like this would be wrong, since they don't necessarily
 * use the same underlying element type, despite all the other similarities.)
 *
 * Both arrays must have already been sorted and deduplicated by calling
 * _bt_sort_array_elements.  sortproc is the same-type ORDER proc that was
 * just used to sort and deduplicate caller's "next" array.  We'll usually be
 * able to reuse that order PROC to merge the arrays together now.  If not,
 * then we'll perform a separate ORDER proc lookup.
 *
 * If the opfamily doesn't supply a complete set of cross-type ORDER procs we
 * may not be able to determine which elements are contradictory.  If we have
 * the required ORDER proc then we return true (and validly set *nelems_orig),
 * guaranteeing that at least the next array can be considered redundant.  We
 * return false if the required comparisons cannot not be made (caller must
 * keep both arrays when this happens).
 */""",
  """_bt_compare_array_scankey_args|||backend\access\nbtree\nbtutils.c|||975|||/*
 * Compare an array scan key to a scalar scan key, eliminating contradictory
 * array elements such that the scalar scan key becomes redundant.
 *
 * Array elements can be eliminated as contradictory when excluded by some
 * other operator on the same attribute.  For example, with an index scan qual
 * "WHERE a IN (1, 2, 3) AND a < 2", all array elements except the value "1"
 * are eliminated, and the < scan key is eliminated as redundant.  Cases where
 * every array element is eliminated by a redundant scalar scan key have an
 * unsatisfiable qual, which we handle by setting *qual_ok=false for caller.
 *
 * If the opfamily doesn't supply a complete set of cross-type ORDER procs we
 * may not be able to determine which elements are contradictory.  If we have
 * the required ORDER proc then we return true (and validly set *qual_ok),
 * guaranteeing that at least the scalar scan key can be considered redundant.
 * We return false if the comparison could not be made (caller must keep both
 * scan keys when this happens).
 */""",
  """_bt_compare_array_elements|||backend\access\nbtree\nbtutils.c|||1098|||/*
 * qsort_arg comparator for sorting array elements
 */""",
  """_bt_compare_array_skey|||backend\access\nbtree\nbtutils.c|||1130|||/*
 * _bt_compare_array_skey() -- apply array comparison function
 *
 * Compares caller's tuple attribute value to a scan key/array element.
 * Helper function used during binary searches of SK_SEARCHARRAY arrays.
 *
 *		This routine returns:
 *			<0 if tupdatum < arrdatum;
 *			 0 if tupdatum == arrdatum;
 *			>0 if tupdatum > arrdatum.
 *
 * This is essentially the same interface as _bt_compare: both functions
 * compare the value that they're searching for to a binary search pivot.
 * However, unlike _bt_compare, this function's "tuple argument" comes first,
 * while its "array/scankey argument" comes second.
*/""",
  """_bt_binsrch_array_skey|||backend\access\nbtree\nbtutils.c|||1200|||/*
 * _bt_binsrch_array_skey() -- Binary search for next matching array key
 *
 * Returns an index to the first array element >= caller's tupdatum argument.
 * This convention is more natural for forwards scan callers, but that can't
 * really matter to backwards scan callers.  Both callers require handling for
 * the case where the match we return is < tupdatum, and symmetric handling
 * for the case where our best match is > tupdatum.
 *
 * Also sets *set_elem_result to the result _bt_compare_array_skey returned
 * when we used it to compare the matching array element to tupdatum/tupnull.
 *
 * cur_elem_trig indicates if array advancement was triggered by this array's
 * scan key, and that the array is for a required scan key.  We can apply this
 * information to find the next matching array element in the current scan
 * direction using far fewer comparisons (fewer on average, compared to naive
 * binary search).  This scheme takes advantage of an important property of
 * required arrays: required arrays always advance in lockstep with the index
 * scan's progress through the index's key space.
 */""",
  """_bt_start_array_keys|||backend\access\nbtree\nbtutils.c|||1342|||/*
 * _bt_start_array_keys() -- Initialize array keys at start of a scan
 *
 * Set up the cur_elem counters and fill in the first sk_argument value for
 * each array scankey.
 */""",
  """_bt_advance_array_keys_increment|||backend\access\nbtree\nbtutils.c|||1380|||/*
 * _bt_advance_array_keys_increment() -- Advance to next set of array elements
 *
 * Advances the array keys by a single increment in the current scan
 * direction.  When there are multiple array keys this can roll over from the
 * lowest order array to higher order arrays.
 *
 * Returns true if there is another set of values to consider, false if not.
 * On true result, the scankeys are initialized with the next set of values.
 * On false result, the scankeys stay the same, and the array keys are not
 * advanced (every array remains at its final element for scan direction).
 */""",
  """_bt_rewind_nonrequired_arrays|||backend\access\nbtree\nbtutils.c|||1466|||/*
 * _bt_rewind_nonrequired_arrays() -- Rewind non-required arrays
 *
 * Called when _bt_advance_array_keys decides to start a new primitive index
 * scan on the basis of the current scan position being before the position
 * that _bt_first is capable of repositioning the scan to by applying an
 * inequality operator required in the opposite-to-scan direction only.
 *
 * Although equality strategy scan keys (for both arrays and non-arrays alike)
 * are either marked required in both directions or in neither direction,
 * there is a sense in which non-required arrays behave like required arrays.
 * With a qual such as "WHERE a IN (100, 200) AND b >= 3 AND c IN (5, 6, 7)",
 * the scan key on "c" is non-required, but nevertheless enables positioning
 * the scan at the first tuple >= "(100, 3, 5)" on the leaf level during the
 * first descent of the tree by _bt_first.  Later on, there could also be a
 * second descent, that places the scan right before tuples >= "(200, 3, 5)".
 * _bt_first must never be allowed to build an insertion scan key whose "c"
 * entry is set to a value other than 5, the "c" array's first element/value.
 * (Actually, it's the first in the current scan direction.  This example uses
 * a forward scan.)
 *
 * Calling here resets the array scan key elements for the scan's non-required
 * arrays.  This is strictly necessary for correctness in a subset of cases
 * involving "required in opposite direction"-triggered primitive index scans.
 * Not all callers are at risk of _bt_first using a non-required array like
 * this, but advancement always resets the arrays when another primitive scan
 * is scheduled, just to keep things simple.  Array advancement even makes
 * sure to reset non-required arrays during scans that have no inequalities.
 * (Advancement still won't call here when there are no inequalities, though
 * that's just because it's all handled indirectly instead.)
 *
 * Note: _bt_verify_arrays_bt_first is called by an assertion to enforce that
 * everybody got this right.
 */""",
  """_bt_tuple_before_array_skeys|||backend\access\nbtree\nbtutils.c|||1543|||/*
 * _bt_tuple_before_array_skeys() -- too early to advance required arrays?
 *
 * We always compare the tuple using the current array keys (which we assume
 * are already set in so->keyData[]).  readpagetup indicates if tuple is the
 * scan's current _bt_readpage-wise tuple.
 *
 * readpagetup callers must only call here when _bt_check_compare already set
 * continuescan=false.  We help these callers deal with _bt_check_compare's
 * inability to distinguishing between the < and > cases (it uses equality
 * operator scan keys, whereas we use 3-way ORDER procs).  These callers pass
 * a _bt_check_compare-set sktrig value that indicates which scan key
 * triggered the call (!readpagetup callers just pass us sktrig=0 instead).
 * This information allows us to avoid wastefully checking earlier scan keys
 * that were already deemed to have been satisfied inside _bt_check_compare.
 *
 * Returns false when caller's tuple is >= the current required equality scan
 * keys (or <=, in the case of backwards scans).  This happens to readpagetup
 * callers when the scan has reached the point of needing its array keys
 * advanced; caller will need to advance required and non-required arrays at
 * scan key offsets >= sktrig, plus scan keys < sktrig iff sktrig rolls over.
 * (When we return false to readpagetup callers, tuple can only be == current
 * required equality scan keys when caller's sktrig indicates that the arrays
 * need to be advanced due to an unsatisfied required inequality key trigger.)
 *
 * Returns true when caller passes a tuple that is < the current set of
 * equality keys for the most significant non-equal required scan key/column
 * (or > the keys, during backwards scans).  This happens to readpagetup
 * callers when tuple is still before the start of matches for the scan's
 * required equality strategy scan keys.  (sktrig can't have indicated that an
 * inequality strategy scan key wasn't satisfied in _bt_check_compare when we
 * return true.  In fact, we automatically return false when passed such an
 * inequality sktrig by readpagetup callers -- _bt_check_compare's initial
 * continuescan=false doesn't really need to be confirmed here by us.)
 *
 * !readpagetup callers optionally pass us *scanBehind, which tracks whether
 * any missing truncated attributes might have affected array advancement
 * (compared to what would happen if it was shown the first non-pivot tuple on
 * the page to the right of caller's finaltup/high key tuple instead).  It's
 * only possible that we'll set *scanBehind to true when caller passes us a
 * pivot tuple (with truncated -inf attributes) that we return false for.
 */""",
  """_bt_start_prim_scan|||backend\access\nbtree\nbtutils.c|||1667|||/*
 * _bt_start_prim_scan() -- start scheduled primitive index scan?
 *
 * Returns true if _bt_checkkeys scheduled another primitive index scan, just
 * as the last one ended.  Otherwise returns false, indicating that the array
 * keys are now fully exhausted.
 *
 * Only call here during scans with one or more equality type array scan keys,
 * after _bt_first or _bt_next return false.
 */""",
  """_bt_advance_array_keys|||backend\access\nbtree\nbtutils.c|||1788|||/*
 * _bt_advance_array_keys() -- Advance array elements using a tuple
 *
 * The scan always gets a new qual as a consequence of calling here (except
 * when we determine that the top-level scan has run out of matching tuples).
 * All later _bt_check_compare calls also use the same new qual that was first
 * used here (at least until the next call here advances the keys once again).
 * It's convenient to structure _bt_check_compare rechecks of caller's tuple
 * (using the new qual) as one the steps of advancing the scan's array keys,
 * so this function works as a wrapper around _bt_check_compare.
 *
 * Like _bt_check_compare, we'll set pstate.continuescan on behalf of the
 * caller, and return a boolean indicating if caller's tuple satisfies the
 * scan's new qual.  But unlike _bt_check_compare, we set so->needPrimScan
 * when we set continuescan=false, indicating if a new primitive index scan
 * has been scheduled (otherwise, the top-level scan has run out of tuples in
 * the current scan direction).
 *
 * Caller must use _bt_tuple_before_array_skeys to determine if the current
 * place in the scan is >= the current array keys _before_ calling here.
 * We're responsible for ensuring that caller's tuple is <= the newly advanced
 * required array keys once we return.  We try to find an exact match, but
 * failing that we'll advance the array keys to whatever set of array elements
 * comes next in the key space for the current scan direction.  Required array
 * keys "ratchet forwards" (or backwards).  They can only advance as the scan
 * itself advances through the index/key space.
 *
 * (The rules are the same for backwards scans, except that the operators are
 * flipped: just replace the precondition's >= operator with a <=, and the
 * postcondition's <= operator with a >=.  In other words, just swap the
 * precondition with the postcondition.)
 *
 * We also deal with "advancing" non-required arrays here.  Callers whose
 * sktrig scan key is non-required specify sktrig_required=false.  These calls
 * are the only exception to the general rule about always advancing the
 * required array keys (the scan may not even have a required array).  These
 * callers should just pass a NULL pstate (since there is never any question
 * of stopping the scan).  No call to _bt_tuple_before_array_skeys is required
 * ahead of these calls (it's already clear that any required scan keys must
 * be satisfied by caller's tuple).
 *
 * Note that we deal with non-array required equality strategy scan keys as
 * degenerate single element arrays here.  Obviously, they can never really
 * advance in the way that real arrays can, but they must still affect how we
 * advance real array scan keys (exactly like true array equality scan keys).
 * We have to keep around a 3-way ORDER proc for these (using the "=" operator
 * won't do), since in general whether the tuple is < or > _any_ unsatisfied
 * required equality key influences how the scan's real arrays must advance.
 *
 * Note also that we may sometimes need to advance the array keys when the
 * existing required array keys (and other required equality keys) are already
 * an exact match for every corresponding value from caller's tuple.  We must
 * do this for inequalities that _bt_check_compare set continuescan=false for.
 * They'll advance the array keys here, just like any other scan key that
 * _bt_check_compare stops on.  (This can even happen _after_ we advance the
 * array keys, in which case we'll advance the array keys a second time.  That
 * way _bt_checkkeys caller always has its required arrays advance to the
 * maximum possible extent that its tuple will allow.)
 */""",
  """_bt_preprocess_keys|||backend\access\nbtree\nbtutils.c|||2551|||/*
 *	_bt_preprocess_keys() -- Preprocess scan keys
 *
 * The given search-type keys (taken from scan->keyData[])
 * are copied to so->keyData[] with possible transformation.
 * scan->numberOfKeys is the number of input keys, so->numberOfKeys gets
 * the number of output keys (possibly less, never greater).
 *
 * The output keys are marked with additional sk_flags bits beyond the
 * system-standard bits supplied by the caller.  The DESC and NULLS_FIRST
 * indoption bits for the relevant index attribute are copied into the flags.
 * Also, for a DESC column, we commute (flip) all the sk_strategy numbers
 * so that the index sorts in the desired direction.
 *
 * One key purpose of this routine is to discover which scan keys must be
 * satisfied to continue the scan.  It also attempts to eliminate redundant
 * keys and detect contradictory keys.  (If the index opfamily provides
 * incomplete sets of cross-type operators, we may fail to detect redundant
 * or contradictory keys, but we can survive that.)
 *
 * The output keys must be sorted by index attribute.  Presently we expect
 * (but verify) that the input keys are already so sorted --- this is done
 * by match_clauses_to_index() in indxpath.c.  Some reordering of the keys
 * within each attribute may be done as a byproduct of the processing here.
 * That process must leave array scan keys (within an attribute) in the same
 * order as corresponding entries from the scan's BTArrayKeyInfo array info.
 *
 * The output keys are marked with flags SK_BT_REQFWD and/or SK_BT_REQBKWD
 * if they must be satisfied in order to continue the scan forward or backward
 * respectively.  _bt_checkkeys uses these flags.  For example, if the quals
 * are "x = 1 AND y < 4 AND z < 5", then _bt_checkkeys will reject a tuple
 * (1,2,7), but we must continue the scan in case there are tuples (1,3,z).
 * But once we reach tuples like (1,4,z) we can stop scanning because no
 * later tuples could match.  This is reflected by marking the x and y keys,
 * but not the z key, with SK_BT_REQFWD.  In general, the keys for leading
 * attributes with "=" keys are marked both SK_BT_REQFWD and SK_BT_REQBKWD.
 * For the first attribute without an "=" key, any "<" and "<=" keys are
 * marked SK_BT_REQFWD while any ">" and ">=" keys are marked SK_BT_REQBKWD.
 * This can be seen to be correct by considering the above example.  Note
 * in particular that if there are no keys for a given attribute, the keys for
 * subsequent attributes can never be required; for instance "WHERE y = 4"
 * requires a full-index scan.
 *
 * If possible, redundant keys are eliminated: we keep only the tightest
 * >/>= bound and the tightest </<= bound, and if there's an = key then
 * that's the only one returned.  (So, we return either a single = key,
 * or one or two boundary-condition keys for each attr.)  However, if we
 * cannot compare two keys for lack of a suitable cross-type operator,
 * we cannot eliminate either.  If there are two such keys of the same
 * operator strategy, the second one is just pushed into the output array
 * without further processing here.  We may also emit both >/>= or both
 * </<= keys if we can't compare them.  The logic about required keys still
 * works if we don't eliminate redundant keys.
 *
 * Note that one reason we need direction-sensitive required-key flags is
 * precisely that we may not be able to eliminate redundant keys.  Suppose
 * we have "x > 4::int AND x > 10::bigint", and we are unable to determine
 * which key is more restrictive for lack of a suitable cross-type operator.
 * _bt_first will arbitrarily pick one of the keys to do the initial
 * positioning with.  If it picks x > 4, then the x > 10 condition will fail
 * until we reach index entries > 10; but we can't stop the scan just because
 * x > 10 is failing.  On the other hand, if we are scanning backwards, then
 * failure of either key is indeed enough to stop the scan.  (In general, when
 * inequality keys are present, the initial-positioning code only promises to
 * position before the first possible match, not exactly at the first match,
 * for a forward scan; or after the last match for a backward scan.)
 *
 * As a byproduct of this work, we can detect contradictory quals such
 * as "x = 1 AND x > 2".  If we see that, we return so->qual_ok = false,
 * indicating the scan need not be run at all since no tuples can match.
 * (In this case we do not bother completing the output key array!)
 * Again, missing cross-type operators might cause us to fail to prove the
 * quals contradictory when they really are, but the scan will work correctly.
 *
 * Row comparison keys are currently also treated without any smarts:
 * we just transfer them into the preprocessed array without any
 * editorialization.  We can treat them the same as an ordinary inequality
 * comparison on the row's first index column, for the purposes of the logic
 * about required keys.
 *
 * Note: the reason we have to copy the preprocessed scan keys into private
 * storage is that we are modifying the array based on comparisons of the
 * key argument values, which could change on a rescan.  Therefore we can't
 * overwrite the source data.
 */""",
  """_bt_verify_arrays_bt_first|||backend\access\nbtree\nbtutils.c|||3005|||/*
 * Verify that the scan's qual state matches what we expect at the point that
 * _bt_start_prim_scan is about to start a just-scheduled new primitive scan.
 *
 * We enforce a rule against non-required array scan keys: they must start out
 * with whatever element is the first for the scan's current scan direction.
 * See _bt_rewind_nonrequired_arrays comments for an explanation.
 */""",
  """_bt_verify_keys_with_arraykeys|||backend\access\nbtree\nbtutils.c|||3043|||/*
 * Verify that the scan's "so->keyData[]" scan keys are in agreement with
 * its array key state
 */""",
  """_bt_compare_scankey_args|||backend\access\nbtree\nbtutils.c|||3121|||/*
 * Compare two scankey values using a specified operator.
 *
 * The test we want to perform is logically "leftarg op rightarg", where
 * leftarg and rightarg are the sk_argument values in those ScanKeys, and
 * the comparison operator is the one in the op ScanKey.  However, in
 * cross-data-type situations we may need to look up the correct operator in
 * the index's opfamily: it is the one having amopstrategy = op->sk_strategy
 * and amoplefttype/amoprighttype equal to the two argument datatypes.
 *
 * If the opfamily doesn't supply a complete set of cross-type operators we
 * may not be able to make the comparison.  If we can make the comparison
 * we store the operator result in *result and return true.  We return false
 * if the comparison could not be made.
 *
 * If either leftarg or rightarg are an array, we'll apply array-specific
 * rules to determine which array elements are redundant on behalf of caller.
 * It is up to our caller to save whichever of the two scan keys is the array,
 * and discard the non-array scan key (the non-array scan key is guaranteed to
 * be redundant with any complete opfamily).  Caller isn't expected to call
 * here with a pair of array scan keys provided we're dealing with a complete
 * opfamily (_bt_preprocess_array_keys will merge array keys together to make
 * sure of that).
 *
 * Note: we'll also shrink caller's array as needed to eliminate redundant
 * array elements.  One reason why caller should prefer to discard non-array
 * scan keys is so that we'll have the opportunity to shrink the array
 * multiple times, in multiple calls (for each of several other scan keys on
 * the same index attribute).
 *
 * Note: op always points at the same ScanKey as either leftarg or rightarg.
 * Since we don't scribble on the scankeys themselves, this aliasing should
 * cause no trouble.
 *
 * Note: this routine needs to be insensitive to any DESC option applied
 * to the index column.  For example, "x < 4" is a tighter constraint than
 * "x < 5" regardless of which way the index is sorted.
 */""",
  """_bt_fix_scankey_strategy|||backend\access\nbtree\nbtutils.c|||3328|||/*
 * Adjust a scankey's strategy and flags setting as needed for indoptions.
 *
 * We copy the appropriate indoption value into the scankey sk_flags
 * (shifting to avoid clobbering system-defined flag bits).  Also, if
 * the DESC option is set, commute (flip) the operator strategy number.
 *
 * A secondary purpose is to check for IS NULL/NOT NULL scankeys and set up
 * the strategy field correctly for them.
 *
 * Lastly, for ordinary scankeys (not IS NULL/NOT NULL), we check for a
 * NULL comparison value.  Since all btree operators are assumed strict,
 * a NULL means that the qual cannot be satisfied.  We return true if the
 * comparison value isn't NULL, or false if the scan should be abandoned.
 *
 * This function is applied to the *input* scankey structure; therefore
 * on a rescan we will be looking at already-processed scankeys.  Hence
 * we have to be careful not to re-commute the strategy if we already did it.
 * It's a bit ugly to modify the caller's copy of the scankey but in practice
 * there shouldn't be any problem, since the index's indoptions are certainly
 * not going to change while the scankey survives.
 */""",
  """_bt_mark_scankey_required|||backend\access\nbtree\nbtutils.c|||3437|||/*
 * Mark a scankey as "required to continue the scan".
 *
 * Depending on the operator type, the key may be required for both scan
 * directions or just one.  Also, if the key is a row comparison header,
 * we have to mark its first subsidiary ScanKey as required.  (Subsequent
 * subsidiary ScanKeys are normally for lower-order columns, and thus
 * cannot be required, since they're after the first non-equality scankey.)
 *
 * Note: when we set required-key flag bits in a subsidiary scankey, we are
 * scribbling on a data structure belonging to the index AM's caller, not on
 * our private copy.  This should be OK because the marking will not change
 * from scan to scan within a query, and so we'd just re-mark the same way
 * anyway on a rescan.  Something to keep an eye on though.
 */""",
  """_bt_checkkeys|||backend\access\nbtree\nbtutils.c|||3507|||/*
 * Test whether an indextuple satisfies all the scankey conditions.
 *
 * Return true if so, false if not.  If the tuple fails to pass the qual,
 * we also determine whether there's any need to continue the scan beyond
 * this tuple, and set pstate.continuescan accordingly.  See comments for
 * _bt_preprocess_keys(), above, about how this is done.
 *
 * Forward scan callers can pass a high key tuple in the hopes of having
 * us set *continuescan to false, and avoiding an unnecessary visit to
 * the page to the right.
 *
 * Advances the scan's array keys when necessary for arrayKeys=true callers.
 * Caller can avoid all array related side-effects when calling just to do a
 * page continuescan precheck -- pass arrayKeys=false for that.  Scans without
 * any arrays keys must always pass arrayKeys=false.
 *
 * Also stops and starts primitive index scans for arrayKeys=true callers.
 * Scans with array keys are required to set up page state that helps us with
 * this.  The page's finaltup tuple (the page high key for a forward scan, or
 * the page's first non-pivot tuple for a backward scan) must be set in
 * pstate.finaltup ahead of the first call here for the page (or possibly the
 * first call after an initial continuescan-setting page precheck call).  Set
 * this to NULL for rightmost page (or the leftmost page for backwards scans).
 *
 * scan: index scan descriptor (containing a search-type scankey)
 * pstate: page level input and output parameters
 * arrayKeys: should we advance the scan's array keys if necessary?
 * tuple: index tuple to test
 * tupnatts: number of attributes in tupnatts (high key may be truncated)
 */""",
  """_bt_check_compare|||backend\access\nbtree\nbtutils.c|||3681|||/*
 * Test whether an indextuple satisfies current scan condition.
 *
 * Return true if so, false if not.  If not, also sets *continuescan to false
 * when it's also not possible for any later tuples to pass the current qual
 * (with the scan's current set of array keys, in the current scan direction),
 * in addition to setting *ikey to the so->keyData[] subscript/offset for the
 * unsatisfied scan key (needed when caller must consider advancing the scan's
 * array keys).
 *
 * This is a subroutine for _bt_checkkeys.  We provisionally assume that
 * reaching the end of the current set of required keys (in particular the
 * current required array keys) ends the ongoing (primitive) index scan.
 * Callers without array keys should just end the scan right away when they
 * find that continuescan has been set to false here by us.  Things are more
 * complicated for callers with array keys.
 *
 * Callers with array keys must first consider advancing the arrays when
 * continuescan has been set to false here by us.  They must then consider if
 * it really does make sense to end the current (primitive) index scan, in
 * light of everything that is known at that point.  (In general when we set
 * continuescan=false for these callers it must be treated as provisional.)
 *
 * We deal with advancing unsatisfied non-required arrays directly, though.
 * This is safe, since by definition non-required keys can't end the scan.
 * This is just how we determine if non-required arrays are just unsatisfied
 * by the current array key, or if they're truly unsatisfied (that is, if
 * they're unsatisfied by every possible array key).
 *
 * Though we advance non-required array keys on our own, that shouldn't have
 * any lasting consequences for the scan.  By definition, non-required arrays
 * have no fixed relationship with the scan's progress.  (There are delicate
 * considerations for non-required arrays when the arrays need to be advanced
 * following our setting continuescan to false, but that doesn't concern us.)
 *
 * Pass advancenonrequired=false to avoid all array related side effects.
 * This allows _bt_advance_array_keys caller to avoid infinite recursion.
 */""",
  """_bt_check_rowcompare|||backend\access\nbtree\nbtutils.c|||3887|||/*
 * Test whether an indextuple satisfies a row-comparison scan condition.
 *
 * Return true if so, false if not.  If not, also clear *continuescan if
 * it's not possible for any future tuples in the current scan direction
 * to pass the qual.
 *
 * This is a subroutine for _bt_checkkeys/_bt_check_compare.
 */""",
  """_bt_checkkeys_look_ahead|||backend\access\nbtree\nbtutils.c|||4071|||/*
 * Determine if a scan with array keys should skip over uninteresting tuples.
 *
 * This is a subroutine for _bt_checkkeys.  Called when _bt_readpage's linear
 * search process (started after it finishes reading an initial group of
 * matching tuples, used to locate the start of the next group of tuples
 * matching the next set of required array keys) has already scanned an
 * excessive number of tuples whose key space is "between arrays".
 *
 * When we perform look ahead successfully, we'll sets pstate.skip, which
 * instructs _bt_readpage to skip ahead to that tuple next (could be past the
 * end of the scan's leaf page).  Pages where the optimization is effective
 * will generally still need to skip several times.  Each call here performs
 * only a single "look ahead" comparison of a later tuple, whose distance from
 * the current tuple's offset number is determined by applying heuristics.
 */""",
  """_bt_killitems|||backend\access\nbtree\nbtutils.c|||4170|||/*
 * _bt_killitems - set LP_DEAD state for items an indexscan caller has
 * told us were killed
 *
 * scan->opaque, referenced locally through so, contains information about the
 * current page and killed tuples thereon (generally, this should only be
 * called if so->numKilled > 0).
 *
 * Caller should not have a lock on the so->currPos page, but may hold a
 * buffer pin.  When we return, it still won't be locked.  It'll continue to
 * hold whatever pins were held before calling here.
 *
 * We match items by heap TID before assuming they are the right ones to
 * delete.  We cope with cases where items have moved right due to insertions.
 * If an item has moved off the current page due to a split, we'll fail to
 * find it and do nothing (this is not an error case --- we assume the item
 * will eventually get marked in a future indexscan).
 *
 * Note that if we hold a pin on the target page continuously from initially
 * reading the items until applying this function, VACUUM cannot have deleted
 * any items from the page, and so there is no need to search left from the
 * recorded offset.  (This observation also guarantees that the item is still
 * the right one to delete, which might otherwise be questionable since heap
 * TIDs can get recycled.)	This holds true even if the page has been modified
 * by inserts and page splits, so there is no need to consult the LSN.
 *
 * If the pin was released after reading the page, then we re-read it.  If it
 * has been modified since we read it (as determined by the LSN), we dare not
 * flag any entries because it is possible that the old entry was vacuumed
 * away and the TID was re-used by a completely different heap tuple.
 */""",
  """<clinit>|||backend\access\nbtree\nbtutils.c|||4373|||/*
 * The following routines manage a shared-memory area in which we track
 * assignment of "vacuum cycle IDs" to currently-active btree vacuuming
 * operations.  There is a single counter which increments each time we
 * start a vacuum to assign it a cycle ID.  Since multiple vacuums could
 * be active concurrently, we have to track the cycle ID for each active
 * vacuum; this requires at most MaxBackends entries (usually far fewer).
 * We assume at most one vacuum can be active for a given index.
 *
 * Access to the shared memory area is controlled by BtreeVacuumLock.
 * In principle we could use a separate lmgr locktag for each index,
 * but a single LWLock is much cheaper, and given the short time that
 * the lock is ever held, the concurrency hit should be minimal.
 */""",
  """_bt_vacuum_cycleid|||backend\access\nbtree\nbtutils.c|||4393|||/*
 * _bt_vacuum_cycleid --- get the active vacuum cycle ID for an index,
 *		or zero if there is no active VACUUM
 *
 * Note: for correct interlocking, the caller must already hold pin and
 * exclusive lock on each buffer it will store the cycle ID into.  This
 * ensures that even if a VACUUM starts immediately afterwards, it cannot
 * process those pages until the page split is complete.
 */""",
  """_bt_start_vacuum|||backend\access\nbtree\nbtutils.c|||4427|||/*
 * _bt_start_vacuum --- assign a cycle ID to a just-starting VACUUM operation
 *
 * Note: the caller must guarantee that it will eventually call
 * _bt_end_vacuum, else we'll permanently leak an array slot.  To ensure
 * that this happens even in elog(FATAL) scenarios, the appropriate coding
 * is not just a PG_TRY, but
 *		PG_ENSURE_ERROR_CLEANUP(_bt_end_vacuum_callback, PointerGetDatum(rel))
 */""",
  """_bt_end_vacuum|||backend\access\nbtree\nbtutils.c|||4484|||/*
 * _bt_end_vacuum --- mark a btree VACUUM operation as done
 *
 * Note: this is deliberately coded not to complain if no entry is found;
 * this allows the caller to put PG_TRY around the start_vacuum operation.
 */""",
  """_bt_end_vacuum_callback|||backend\access\nbtree\nbtutils.c|||4512|||/*
 * _bt_end_vacuum wrapped as an on_shmem_exit callback function
 */""",
  """BTreeShmemSize|||backend\access\nbtree\nbtutils.c|||4521|||/*
 * BTreeShmemSize --- report amount of shared memory space needed
 */""",
  """BTreeShmemInit|||backend\access\nbtree\nbtutils.c|||4534|||/*
 * BTreeShmemInit --- initialize this module's shared memory
 */""",
  """btproperty|||backend\access\nbtree\nbtutils.c|||4585|||/*
 *	btproperty() -- Check boolean properties of indexes.
 *
 * This is optional, but handling AMPROP_RETURNABLE here saves opening the rel
 * to call btcanreturn.
 */""",
  """btbuildphasename|||backend\access\nbtree\nbtutils.c|||4608|||/*
 *	btbuildphasename() -- Return name of index build phase.
 */""",
  """_bt_truncate|||backend\access\nbtree\nbtutils.c|||4656|||/*
 *	_bt_truncate() -- create tuple without unneeded suffix attributes.
 *
 * Returns truncated pivot index tuple allocated in caller's memory context,
 * with key attributes copied from caller's firstright argument.  If rel is
 * an INCLUDE index, non-key attributes will definitely be truncated away,
 * since they're not part of the key space.  More aggressive suffix
 * truncation can take place when it's clear that the returned tuple does not
 * need one or more suffix key attributes.  We only need to keep firstright
 * attributes up to and including the first non-lastleft-equal attribute.
 * Caller's insertion scankey is used to compare the tuples; the scankey's
 * argument values are not considered here.
 *
 * Note that returned tuple's t_tid offset will hold the number of attributes
 * present, so the original item pointer offset is not represented.  Caller
 * should only change truncated tuple's downlink.  Note also that truncated
 * key attributes are treated as containing "minus infinity" values by
 * _bt_compare().
 *
 * In the worst case (when a heap TID must be appended to distinguish lastleft
 * from firstright), the size of the returned tuple is the size of firstright
 * plus the size of an additional MAXALIGN()'d item pointer.  This guarantee
 * is important, since callers need to stay under the 1/3 of a page
 * restriction on tuple size.  If this routine is ever taught to truncate
 * within an attribute/datum, it will need to avoid returning an enlarged
 * tuple to caller when truncation + TOAST compression ends up enlarging the
 * final datum.
 */""",
  """_bt_keep_natts|||backend\access\nbtree\nbtutils.c|||4801|||/*
 * _bt_keep_natts - how many key attributes to keep when truncating.
 *
 * Caller provides two tuples that enclose a split point.  Caller's insertion
 * scankey is used to compare the tuples; the scankey's argument values are
 * not considered here.
 *
 * This can return a number of attributes that is one greater than the
 * number of key attributes for the index relation.  This indicates that the
 * caller must use a heap TID as a unique-ifier in new pivot tuple.
 */""",
  """_bt_keep_natts_fast|||backend\access\nbtree\nbtutils.c|||4875|||/*
 * _bt_keep_natts_fast - fast bitwise variant of _bt_keep_natts.
 *
 * This is exported so that a candidate split point can have its effect on
 * suffix truncation inexpensively evaluated ahead of time when finding a
 * split location.  A naive bitwise approach to datum comparisons is used to
 * save cycles.
 *
 * The approach taken here usually provides the same answer as _bt_keep_natts
 * will (for the same pair of tuples from a heapkeyspace index), since the
 * majority of btree opclasses can never indicate that two datums are equal
 * unless they're bitwise equal after detoasting.  When an index only has
 * "equal image" columns, routine is guaranteed to give the same result as
 * _bt_keep_natts would.
 *
 * Callers can rely on the fact that attributes considered equal here are
 * definitely also equal according to _bt_keep_natts, even when the index uses
 * an opclass or collation that is not "allequalimage"/deduplication-safe.
 * This weaker guarantee is good enough for nbtsplitloc.c caller, since false
 * negatives generally only have the effect of making leaf page splits use a
 * more balanced split point.
 */""",
  """_bt_check_natts|||backend\access\nbtree\nbtutils.c|||4922|||/*
 *  _bt_check_natts() -- Verify tuple has expected number of attributes.
 *
 * Returns value indicating if the expected number of attributes were found
 * for a particular offset on page.  This can be used as a general purpose
 * sanity check.
 *
 * Testing a tuple directly with BTreeTupleGetNAtts() should generally be
 * preferred to calling here.  That's usually more convenient, and is always
 * more explicit.  Call here instead when offnum's tuple may be a negative
 * infinity tuple that uses the pre-v11 on-disk representation, or when a low
 * context check is appropriate.  This routine is as strict as possible about
 * what is expected on each version of btree.
 */""",
  """_bt_check_third_page|||backend\access\nbtree\nbtutils.c|||5082|||/*
 *
 *  _bt_check_third_page() -- check whether tuple fits on a btree page at all.
 *
 * We actually need to be able to fit three items on every page, so restrict
 * any one item to 1/3 the per-page available space.  Note that itemsz should
 * not include the ItemId overhead.
 *
 * It might be useful to apply TOAST methods rather than throw an error here.
 * Using out of line storage would break assumptions made by suffix truncation
 * and by contrib/amcheck, though.
 */""",
  """_bt_allequalimage|||backend\access\nbtree\nbtutils.c|||5140|||/*
 * Are all attributes in rel "equality is image equality" attributes?
 *
 * We use each attribute's BTEQUALIMAGE_PROC opclass procedure.  If any
 * opclass either lacks a BTEQUALIMAGE_PROC procedure or returns false, we
 * return false; otherwise we return true.
 *
 * Returned boolean value is stored in index metapage during index builds.
 * Deduplication can only be used when we return true.
 */""",
  """btvalidate|||backend\access\nbtree\nbtvalidate.c|||40|||/*
 * Validator for a btree opclass.
 *
 * Some of the checks done here cover the whole opfamily, and therefore are
 * redundant when checking each opclass in a family.  But they don't run long
 * enough to be much of a problem, so we accept the duplication rather than
 * complicate the amvalidate API.
 */""",
  """btadjustmembers|||backend\access\nbtree\nbtvalidate.c|||292|||/*
 * Prechecking function for adding operators/functions to a btree opfamily.
 */""",
  """_bt_restore_page|||backend\access\nbtree\nbtxlog.c|||35|||/*
 * _bt_restore_page -- re-enter all the index tuples on a page
 *
 * The page is freshly init'd, and *from (length len) is a copy of what
 * had been its upper part (pd_upper to pd_special).  We assume that the
 * tuples had been added to the page in item-number order, and therefore
 * the one with highest item number appears first (lowest on the page).
 */""",
  """_bt_clear_incomplete_split|||backend\access\nbtree\nbtxlog.c|||138|||/*
 * _bt_clear_incomplete_split -- clear INCOMPLETE_SPLIT flag on a page
 *
 * This is a common subroutine of the redo functions of all the WAL record
 * types that can insert a downlink: insert, split, and newroot.
 */""",
  """btree_xlog_insert|||backend\access\nbtree\nbtxlog.c|||159|||/*
 * _bt_clear_incomplete_split -- clear INCOMPLETE_SPLIT flag on a page
 *
 * This is a common subroutine of the redo functions of all the WAL record
 * types that can insert a downlink: insert, split, and newroot.
 */""",
  """btree_xlog_reuse_page|||backend\access\nbtree\nbtxlog.c|||1002|||/*
 * In general VACUUM must defer recycling as a way of avoiding certain race
 * conditions.  Deleted pages contain a safexid value that is used by VACUUM
 * to determine whether or not it's safe to place a page that was deleted by
 * VACUUM earlier into the FSM now.  See nbtree/README.
 *
 * As far as any backend operating during original execution is concerned, the
 * FSM is a cache of recycle-safe pages; the mere presence of the page in the
 * FSM indicates that the page must already be safe to recycle (actually,
 * _bt_getbuf() verifies it's safe using BTPageIsRecyclable(), but that's just
 * because it would be unwise to completely trust the FSM, given its current
 * limitations).
 *
 * This isn't sufficient to prevent similar concurrent recycling race
 * conditions during Hot Standby, though.  For that we need to log a
 * xl_btree_reuse_page record at the point that a page is actually recycled
 * and reused for an entirely unrelated page inside _bt_split().  These
 * records include the same safexid value from the original deleted page,
 * stored in the record's snapshotConflictHorizon field.
 *
 * The GlobalVisCheckRemovableFullXid() test in BTPageIsRecyclable() is used
 * to determine if it's safe to recycle a page.  This mirrors our own test:
 * the PGPROC->xmin > limitXmin test inside GetConflictingVirtualXIDs().
 * Consequently, one XID value achieves the same exclusion effect on primary
 * and standby.
 */""",
  """btree_mask|||backend\access\nbtree\nbtxlog.c|||1090|||/*
 * Mask a btree page before performing consistency checks on it.
 */""",
  """brin_desc|||backend\access\rmgrdesc\brindesc.c|||19|||/*-------------------------------------------------------------------------
 *
 * brindesc.c
 *	  rmgr descriptor routines for BRIN indexes
 *
 * Portions Copyright (c) 1996-2024, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 *
 * IDENTIFICATION
 *	  src/backend/access/rmgrdesc/brindesc.c
 *
 *-------------------------------------------------------------------------
 */""",
  """clog_desc|||backend\access\rmgrdesc\clogdesc.c|||20|||/*-------------------------------------------------------------------------
 *
 * clogdesc.c
 *	  rmgr descriptor routines for access/transam/clog.c
 *
 * Portions Copyright (c) 1996-2024, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 *
 * IDENTIFICATION
 *	  src/backend/access/rmgrdesc/clogdesc.c
 *
 *-------------------------------------------------------------------------
 */""",
  """commit_ts_desc|||backend\access\rmgrdesc\committsdesc.c|||20|||/*-------------------------------------------------------------------------
 *
 * committsdesc.c
 *	  rmgr descriptor routines for access/transam/commit_ts.c
 *
 * Portions Copyright (c) 1996-2024, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 *
 * IDENTIFICATION
 *	  src/backend/access/rmgrdesc/committsdesc.c
 *
 *-------------------------------------------------------------------------
 */""",
  """dbase_desc|||backend\access\rmgrdesc\dbasedesc.c|||21|||/*-------------------------------------------------------------------------
 *
 * dbasedesc.c
 *	  rmgr descriptor routines for commands/dbcommands.c
 *
 * Portions Copyright (c) 1996-2024, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 *
 * IDENTIFICATION
 *	  src/backend/access/rmgrdesc/dbasedesc.c
 *
 *-------------------------------------------------------------------------
 */""",
  """generic_desc|||backend\access\rmgrdesc\genericdesc.c|||23|||/*
 * Description of generic xlog record: write page regions that this record
 * overrides.
 */""",
  """generic_identify|||backend\access\rmgrdesc\genericdesc.c|||51|||/*
 * Identification of generic xlog record: we don't distinguish any subtypes
 * inside generic xlog records.
 */""",
  """desc_recompress_leaf|||backend\access\rmgrdesc\gindesc.c|||20|||/*-------------------------------------------------------------------------
 *
 * gindesc.c
 *	  rmgr descriptor routines for access/transam/gin/ginxlog.c
 *
 * Portions Copyright (c) 1996-2024, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 *
 * IDENTIFICATION
 *	  src/backend/access/rmgrdesc/gindesc.c
 *
 *-------------------------------------------------------------------------
 */""",
  """out_gistxlogPageUpdate|||backend\access\rmgrdesc\gistdesc.c|||20|||/*-------------------------------------------------------------------------
 *
 * gistdesc.c
 *	  rmgr descriptor routines for access/gist/gistxlog.c
 *
 * Portions Copyright (c) 1996-2024, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 *
 * IDENTIFICATION
 *	  src/backend/access/rmgrdesc/gistdesc.c
 *
 *-------------------------------------------------------------------------
 */""",
  """out_gistxlogPageReuse|||backend\access\rmgrdesc\gistdesc.c|||25|||/*-------------------------------------------------------------------------
 *
 * gistdesc.c
 *	  rmgr descriptor routines for access/gist/gistxlog.c
 *
 * Portions Copyright (c) 1996-2024, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 *
 * IDENTIFICATION
 *	  src/backend/access/rmgrdesc/gistdesc.c
 *
 *-------------------------------------------------------------------------
 */""",
  """hash_desc|||backend\access\rmgrdesc\hashdesc.c|||19|||/*-------------------------------------------------------------------------
 *
 * hashdesc.c
 *	  rmgr descriptor routines for access/hash/hash.c
 *
 * Portions Copyright (c) 1996-2024, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 *
 * IDENTIFICATION
 *	  src/backend/access/rmgrdesc/hashdesc.c
 *
 *-------------------------------------------------------------------------
 */""",
  """infobits_desc|||backend\access\rmgrdesc\heapdesc.c|||24|||/*
 * NOTE: "keyname" argument cannot have trailing spaces or punctuation
 * characters
 */""",
  """truncate_flags_desc|||backend\access\rmgrdesc\heapdesc.c|||53|||/*
 * NOTE: "keyname" argument cannot have trailing spaces or punctuation
 * characters
 */""",
  """heap_xlog_deserialize_prune_and_freeze|||backend\access\rmgrdesc\heapdesc.c|||103|||/*
 * Given a MAXALIGNed buffer returned by XLogRecGetBlockData() and pointed to
 * by cursor and any xl_heap_prune flags, deserialize the arrays of
 * OffsetNumbers contained in an XLOG_HEAP2_PRUNE_* record.
 *
 * This is in heapdesc.c so it can be shared between heap2_redo and heap2_desc
 * code, the latter of which is used in frontend (pg_waldump) code.
 */""",
  """logicalmsg_desc|||backend\access\rmgrdesc\logicalmsgdesc.c|||18|||/*-------------------------------------------------------------------------
 *
 * logicalmsgdesc.c
 *	  rmgr descriptor routines for replication/logical/message.c
 *
 * Portions Copyright (c) 2015-2024, PostgreSQL Global Development Group
 *
 *
 * IDENTIFICATION
 *	  src/backend/access/rmgrdesc/logicalmsgdesc.c
 *
 *-------------------------------------------------------------------------
 */""",
  """out_member|||backend\access\rmgrdesc\mxactdesc.c|||19|||/*-------------------------------------------------------------------------
 *
 * mxactdesc.c
 *	  rmgr descriptor routines for access/transam/multixact.c
 *
 * Portions Copyright (c) 1996-2024, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 *
 * IDENTIFICATION
 *	  src/backend/access/rmgrdesc/mxactdesc.c
 *
 *-------------------------------------------------------------------------
 */""",
  """btree_desc|||backend\access\rmgrdesc\nbtdesc.c|||23|||/*-------------------------------------------------------------------------
 *
 * nbtdesc.c
 *	  rmgr descriptor routines for access/nbtree/nbtxlog.c
 *
 * Portions Copyright (c) 1996-2024, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 *
 * IDENTIFICATION
 *	  src/backend/access/rmgrdesc/nbtdesc.c
 *
 *-------------------------------------------------------------------------
 */"""
)
INFO:__main__:Extracted 9 methods with comments in this batch
INFO:__main__:Fetching batch: offset=1200, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=1200, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.33s
INFO:__main__:Extracted 19 methods with comments in this batch
INFO:__main__:Fetching batch: offset=1300, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=1300, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.32s
INFO:__main__:Extracted 20 methods with comments in this batch
INFO:__main__:Fetching batch: offset=1400, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=1400, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.34s
INFO:__main__:Extracted 8 methods with comments in this batch
INFO:__main__:Fetching batch: offset=1500, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=1500, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.30s
INFO:__main__:Extracted 13 methods with comments in this batch
INFO:__main__:Fetching batch: offset=1600, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=1600, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.32s
INFO:__main__:Extracted 2 methods with comments in this batch
INFO:__main__:Fetching batch: offset=1700, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=1700, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.25s
INFO:__main__:Extracted 6 methods with comments in this batch
INFO:__main__:Fetching batch: offset=1800, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=1800, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres349[0m: [32mList[0m[[32mString[0m] = List(
  """check_wal_consistency_checking|||backend\access\transam\xlog.c|||4626|||/*
 * GUC check_hook for wal_consistency_checking
 */""",
  """assign_wal_consistency_checking|||backend\access\transam\xlog.c|||4711|||/*
 * GUC assign_hook for wal_consistency_checking
 */""",
  """InitializeWalConsistencyChecking|||backend\access\transam\xlog.c|||4738|||/*
 * InitializeWalConsistencyChecking: run after loading custom resource managers
 *
 * If any unknown resource managers were specified in the
 * wal_consistency_checking GUC, processing was deferred.  Now that
 * shared_preload_libraries have been loaded, process wal_consistency_checking
 * again.
 */""",
  """show_archive_command|||backend\access\transam\xlog.c|||4764|||/*
 * GUC show_hook for archive_command
 */""",
  """show_in_hot_standby|||backend\access\transam\xlog.c|||4776|||/*
 * GUC show_hook for in_hot_standby
 */""",
  """LocalProcessControlFile|||backend\access\transam\xlog.c|||4800|||/*
 * Read the control file, set respective GUCs.
 *
 * This is to be called during startup, including a crash recovery cycle,
 * unless in bootstrap mode, where no control file yet exists.  As there's no
 * usable shared memory yet (its sizing can depend on the contents of the
 * control file!), first store the contents in local memory. XLOGShmemInit()
 * will then copy it to shared memory later.
 *
 * reset just controls whether previous contents are to be expected (in the
 * reset case, there's a dangling pointer into old shared memory), or not.
 */""",
  """GetActiveWalLevelOnStandby|||backend\access\transam\xlog.c|||4813|||/*
 * Get the wal_level from the control file. For a standby, this value should be
 * considered as its active wal_level, because it may be different from what
 * was originally configured on standby.
 */""",
  """XLOGShmemSize|||backend\access\transam\xlog.c|||4822|||/*
 * Initialization of shared memory for XLOG
 */""",
  """BootStrapXLOG|||backend\access\transam\xlog.c|||4987|||/*
 * This func must be called ONCE on system install.  It creates pg_control
 * and the initial XLOG segment.
 */""",
  """XLogInitNewTimeline|||backend\access\transam\xlog.c|||5168|||/*
 * Initialize the first WAL segment on new timeline.
 */""",
  """CleanupAfterArchiveRecovery|||backend\access\transam\xlog.c|||5243|||/*
 * Perform cleanup actions at the conclusion of archive recovery.
 */""",
  """CheckRequiredParameterValues|||backend\access\transam\xlog.c|||5339|||/*
 * Check to see if required parameters are set high enough on this server
 * for various aspects of recovery operation.
 *
 * Note that all the parameters which this function tests need to be
 * listed in Administrator's Overview section in high-availability.sgml.
 * If you change them, don't forget to update the list.
 */""",
  """StartupXLOG|||backend\access\transam\xlog.c|||5383|||/*
 * This must be called ONCE during postmaster or standalone-backend startup
 */""",
  """SwitchIntoArchiveRecovery|||backend\access\transam\xlog.c|||6187|||/*
 * Callback from PerformWalRecovery(), called when we switch from crash
 * recovery to archive recovery mode.  Updates the control file accordingly.
 */""",
  """ReachedEndOfBackup|||backend\access\transam\xlog.c|||6225|||/*
 * Callback from PerformWalRecovery(), called when we reach the end of backup.
 * Updates the control file accordingly.
 */""",
  """PerformRecoveryXLogAction|||backend\access\transam\xlog.c|||6262|||/*
 * Perform whatever XLOG actions are necessary at end of REDO.
 *
 * The goal here is to make sure that we'll be able to recover properly if
 * we crash again. If we choose to write a checkpoint, we'll write a shutdown
 * checkpoint rather than an on-line one. This is not particularly critical,
 * but since we may be assigning a new TLI, using a shutdown checkpoint allows
 * us to have the rule that TLI only changes in shutdown checkpoints, which
 * allows some extra error checking in xlog_redo.
 */""",
  """RecoveryInProgress|||backend\access\transam\xlog.c|||6312|||/*
 * Is the system still in recovery?
 *
 * Unlike testing InRecovery, this works in any process that's connected to
 * shared memory.
 */""",
  """GetRecoveryState|||backend\access\transam\xlog.c|||6348|||/*
 * Returns current recovery state from shared memory.
 *
 * This returned state is kept consistent with the contents of the control
 * file.  See details about the possible values of RecoveryState in xlog.h.
 */""",
  """XLogInsertAllowed|||backend\access\transam\xlog.c|||6367|||/*
 * Is this process allowed to insert new WAL records?
 *
 * Ordinarily this is essentially equivalent to !RecoveryInProgress().
 * But we also have provisions for forcing the result "true" or "false"
 * within specific processes regardless of the global state.
 */""",
  """LocalSetXLogInsertAllowed|||backend\access\transam\xlog.c|||6400|||/*
 * Make XLogInsertAllowed() return true in the current process only.
 *
 * Note: it is allowed to switch LocalXLogInsertAllowed back to -1 later,
 * and even call LocalSetXLogInsertAllowed() again after that.
 *
 * Returns the previous value of LocalXLogInsertAllowed.
 */""",
  """GetRedoRecPtr|||backend\access\transam\xlog.c|||6415|||/*
 * Return the current Redo pointer from shared memory.
 *
 * As a side-effect, the local RedoRecPtr copy is updated.
 */""",
  """GetFullPageWriteInfo|||backend\access\transam\xlog.c|||6445|||/*
 * Return information needed to decide whether a modified block needs a
 * full-page image to be included in the WAL record.
 *
 * The returned values are cached copies from backend-private memory, and
 * possibly out-of-date or, indeed, uninitialized, in which case they will
 * be InvalidXLogRecPtr and false, respectively.  XLogInsertRecord will
 * re-check them against up-to-date values, while holding the WAL insert lock.
 */""",
  """GetInsertRecPtr|||backend\access\transam\xlog.c|||6460|||/*
 * GetInsertRecPtr -- Returns the current insert position.
 *
 * NOTE: The value *actually* returned is the position of the last full
 * xlog page. It lags behind the real insert position by at most 1 page.
 * For that, we don't need to scan through WAL insertion locks, and an
 * approximation is enough for the current usage of this function.
 */""",
  """GetFlushRecPtr|||backend\access\transam\xlog.c|||6477|||/*
 * GetFlushRecPtr -- Returns the current flush position, ie, the last WAL
 * position known to be fsync'd to disk. This should only be used on a
 * system that is known not to be in recovery.
 */""",
  """GetWALInsertionTimeLine|||backend\access\transam\xlog.c|||6498|||/*
 * GetWALInsertionTimeLine -- Returns the current timeline of a system that
 * is not in recovery.
 */""",
  """GetWALInsertionTimeLineIfSet|||backend\access\transam\xlog.c|||6514|||/*
 * GetWALInsertionTimeLineIfSet -- If the system is not in recovery, returns
 * the WAL insertion timeline; else, returns 0. Wherever possible, use
 * GetWALInsertionTimeLine() instead, since it's cheaper. Note that this
 * function decides recovery has ended as soon as the insert TLI is set, which
 * happens before we set XLogCtl->SharedRecoveryState to RECOVERY_STATE_DONE.
 */""",
  """GetLastImportantRecPtr|||backend\access\transam\xlog.c|||6534|||/*
 * GetLastImportantRecPtr -- Returns the LSN of the last important record
 * inserted. All records not explicitly marked as unimportant are considered
 * important.
 *
 * The LSN is determined by computing the maximum of
 * WALInsertLocks[i].lastImportantAt.
 */""",
  """GetLastSegSwitchData|||backend\access\transam\xlog.c|||6563|||/*
 * Get the time and LSN of the last xlog segment switch
 */""",
  """ShutdownXLOG|||backend\access\transam\xlog.c|||6580|||/*
 * This must be called ONCE during postmaster or standalone-backend shutdown
 */""",
  """LogCheckpointStart|||backend\access\transam\xlog.c|||6627|||/*
 * Log start of a checkpoint.
 */""",
  """LogCheckpointEnd|||backend\access\transam\xlog.c|||6659|||/*
 * Log end of a checkpoint.
 */""",
  """UpdateCheckPointDistanceEstimate|||backend\access\transam\xlog.c|||6762|||/*
 * Update the estimate of distance between checkpoints.
 *
 * The estimate is used to calculate the number of WAL segments to keep
 * preallocated, see XLOGfileslop().
 */""",
  """update_checkpoint_display|||backend\access\transam\xlog.c|||6800|||/*
 * Update the ps display for a process running a checkpoint.  Note that
 * this routine should not do any allocations so as it can be called
 * from a critical section.
 */""",
  """CreateCheckPoint|||backend\access\transam\xlog.c|||6862|||/*
 * Perform a checkpoint --- either during shutdown, or on-the-fly
 *
 * flags is a bitwise OR of the following:
 *	CHECKPOINT_IS_SHUTDOWN: checkpoint is for database shutdown.
 *	CHECKPOINT_END_OF_RECOVERY: checkpoint is for end of WAL recovery.
 *	CHECKPOINT_IMMEDIATE: finish the checkpoint ASAP,
 *		ignoring checkpoint_completion_target parameter.
 *	CHECKPOINT_FORCE: force a checkpoint even if no XLOG activity has occurred
 *		since the last one (implied by CHECKPOINT_IS_SHUTDOWN or
 *		CHECKPOINT_END_OF_RECOVERY).
 *	CHECKPOINT_FLUSH_ALL: also flush buffers of unlogged tables.
 *
 * Note: flags contains other bits, of interest here only for logging purposes.
 * In particular note that this routine is synchronous and does not pay
 * attention to CHECKPOINT_WAIT.
 *
 * If !shutdown then we are writing an online checkpoint. An XLOG_CHECKPOINT_REDO
 * record is inserted into WAL at the logical location of the checkpoint, before
 * flushing anything to disk, and when the checkpoint is eventually completed,
 * and it is from this point that WAL replay will begin in the case of a recovery
 * from this checkpoint. Once everything is written to disk, an
 * XLOG_CHECKPOINT_ONLINE record is written to complete the checkpoint, and
 * points back to the earlier XLOG_CHECKPOINT_REDO record. This mechanism allows
 * other write-ahead log records to be written while the checkpoint is in
 * progress, but we must be very careful about order of operations. This function
 * may take many minutes to execute on a busy system.
 *
 * On the other hand, when shutdown is true, concurrent insertion into the
 * write-ahead log is impossible, so there is no need for two separate records.
 * In this case, we only insert an XLOG_CHECKPOINT_SHUTDOWN record, and it's
 * both the record marking the completion of the checkpoint and the location
 * from which WAL replay would begin if needed.
 */""",
  """CreateEndOfRecoveryRecord|||backend\access\transam\xlog.c|||7368|||/*
 * Mark the end of recovery in WAL though without running a full checkpoint.
 * We can expect that a restartpoint is likely to be in progress as we
 * do this, though we are unwilling to wait for it to complete.
 *
 * CreateRestartPoint() allows for the case where recovery may end before
 * the restartpoint completes so there is no concern of concurrent behaviour.
 */""",
  """CreateOverwriteContrecordRecord|||backend\access\transam\xlog.c|||7433|||/*
 * Write an OVERWRITE_CONTRECORD message.
 *
 * When on WAL replay we expect a continuation record at the start of a page
 * that is not there, recovery ends and WAL writing resumes at that point.
 * But it's wrong to resume writing new WAL back at the start of the record
 * that was broken, because downstream consumers of that WAL (physical
 * replicas) are not prepared to "rewind".  So the first action after
 * finishing replay of all valid WAL must be to write a record of this type
 * at the point where the contrecord was missing; to support xlogreader
 * detecting the special case, XLP_FIRST_IS_OVERWRITE_CONTRECORD is also added
 * to the page header where the record occurs.  xlogreader has an ad-hoc
 * mechanism to report metadata about the broken record, which is what we
 * use here.
 *
 * At replay time, XLP_FIRST_IS_OVERWRITE_CONTRECORD instructs xlogreader to
 * skip the record it was reading, and pass back the LSN of the skipped
 * record, so that its caller can verify (on "replay" of that record) that the
 * XLOG_OVERWRITE_CONTRECORD matches what was effectively overwritten.
 *
 * 'aborted_lsn' is the beginning position of the record that was incomplete.
 * It is included in the WAL record.  'pagePtr' and 'newTLI' point to the
 * beginning of the XLOG page where the record is to be inserted.  They must
 * match the current WAL insert position, they're passed here just so that we
 * can verify that.
 */""",
  """CheckPointGuts|||backend\access\transam\xlog.c|||7503|||/*
 * Flush all data in shared memory to disk, and fsync
 *
 * This is the common code shared between regular checkpoints and
 * recovery restartpoints.
 */""",
  """RecoveryRestartPoint|||backend\access\transam\xlog.c|||7543|||/*
 * Save a checkpoint for recovery restart if appropriate
 *
 * This function is called each time a checkpoint record is read from XLOG.
 * It must determine whether the checkpoint represents a safe restartpoint or
 * not.  If so, the checkpoint record is stashed in shared memory so that
 * CreateRestartPoint can consult it.  (Note that the latter function is
 * executed by the checkpointer, while this one will be executed by the
 * startup process.)
 */""",
  """CreateRestartPoint|||backend\access\transam\xlog.c|||7584|||/*
 * Establish a restartpoint if possible.
 *
 * This is similar to CreateCheckPoint, but is used during WAL recovery
 * to establish a point from which recovery can roll forward without
 * replaying the entire recovery log.
 *
 * Returns true if a new restartpoint was established. We can only establish
 * a restartpoint if we have replayed a safe checkpoint record since last
 * restartpoint.
 */""",
  """GetWALAvailability|||backend\access\transam\xlog.c|||7880|||/*
 * Report availability of WAL for the given target LSN
 *		(typically a slot's restart_lsn)
 *
 * Returns one of the following enum values:
 *
 * * WALAVAIL_RESERVED means targetLSN is available and it is in the range of
 *   max_wal_size.
 *
 * * WALAVAIL_EXTENDED means it is still available by preserving extra
 *   segments beyond max_wal_size. If max_slot_wal_keep_size is smaller
 *   than max_wal_size, this state is not returned.
 *
 * * WALAVAIL_UNRESERVED means it is being lost and the next checkpoint will
 *   remove reserved segments. The walsender using this slot may return to the
 *   above.
 *
 * * WALAVAIL_REMOVED means it has been removed. A replication stream on
 *   a slot with this LSN cannot continue.  (Any associated walsender
 *   processes should have been terminated already.)
 *
 * * WALAVAIL_INVALID_LSN means the slot hasn't been set to reserve WAL.
 */""",
  """KeepLogSeg|||backend\access\transam\xlog.c|||7966|||/*
 * Retreat *logSegNo to the last segment that we need to retain because of
 * either wal_keep_size or replication slots.
 *
 * This is calculated by subtracting wal_keep_size from the given xlog
 * location, recptr and by making sure that that result is below the
 * requirement of replication slots.  For the latter criterion we do consider
 * the effects of max_slot_wal_keep_size: reserve at most that much space back
 * from recptr.
 *
 * Note about replication slots: if this function calculates a value
 * that's further ahead than what slots need reserved, then affected
 * slots need to be invalidated and this function invoked again.
 * XXX it might be a good idea to rewrite this function so that
 * invalidation is optionally done here, instead.
 */""",
  """XLogPutNextOid|||backend\access\transam\xlog.c|||8038|||/*
 * Write a NEXTOID log record
 */""",
  """RequestXLogSwitch|||backend\access\transam\xlog.c|||8075|||/*
 * Write an XLOG SWITCH record.
 *
 * Here we just blindly issue an XLogInsert request for the record.
 * All the magic happens inside XLogInsert.
 *
 * The return value is either the end+1 address of the switch record,
 * or the end+1 address of the prior segment if we did not need to
 * write a switch record because we are already at segment start.
 */""",
  """XLogRestorePoint|||backend\access\transam\xlog.c|||8093|||/*
 * Write a RESTORE POINT record
 */""",
  """XLogReportParameters|||backend\access\transam\xlog.c|||8118|||/*
 * Check if any of the GUC parameters that are critical for hot standby
 * have changed, and update the value in pg_control file if necessary.
 */""",
  """UpdateFullPageWrites|||backend\access\transam\xlog.c|||8181|||/*
 * Update full_page_writes in shared memory, and write an
 * XLOG_FPW_CHANGE record if necessary.
 *
 * Note: this function assumes there is no other process running
 * concurrently that could update it.
 */""",
  """xlog_redo|||backend\access\transam\xlog.c|||8250|||/*
 * XLOG resource manager's routines
 *
 * Definitions of info values are in include/catalog/pg_control.h, though
 * not all record types are related to control file updates.
 *
 * NOTE: Some XLOG record types that are directly related to WAL recovery
 * are handled in xlogrecovery_redo().
 */""",
  """get_sync_bit|||backend\access\transam\xlog.c|||8608|||/*
 * Return the extra open flags used for opening a file, depending on the
 * value of the GUCs wal_sync_method, fsync and debug_io_direct.
 */""",
  """assign_wal_sync_method|||backend\access\transam\xlog.c|||8656|||/*
 * GUC support
 */""",
  """issue_xlog_fsync|||backend\access\transam\xlog.c|||8698|||/*
 * Issue appropriate kind of fsync (if any) for an XLOG output file.
 *
 * 'fd' is a file descriptor for the XLOG file to be fsync'd.
 * 'segno' is for error reporting purposes.
 */""",
  """do_pg_backup_start|||backend\access\transam\xlog.c|||8807|||/*
 * do_pg_backup_start is the workhorse of the user-visible pg_backup_start()
 * function. It creates the necessary starting checkpoint and constructs the
 * backup state and tablespace map.
 *
 * Input parameters are "state" (the backup state), "fast" (if true, we do
 * the checkpoint in immediate mode to make it faster), and "tablespaces"
 * (if non-NULL, indicates a list of tablespaceinfo structs describing the
 * cluster's tablespaces.).
 *
 * The tablespace map contents are appended to passed-in parameter
 * tablespace_map and the caller is responsible for including it in the backup
 * archive as 'tablespace_map'. The tablespace_map file is required mainly for
 * tar format in windows as native windows utilities are not able to create
 * symlinks while extracting files from tar. However for consistency and
 * platform-independence, we do it the same way everywhere.
 *
 * It fills in "state" with the information required for the backup, such
 * as the minimum WAL location that must be present to restore from this
 * backup (starttli) and the corresponding timeline ID (starttli).
 *
 * Every successfully started backup must be stopped by calling
 * do_pg_backup_stop() or do_pg_abort_backup(). There can be many
 * backups active at the same time.
 *
 * It is the responsibility of the caller of this function to verify the
 * permissions of the calling user!
 */""",
  """get_backup_status|||backend\access\transam\xlog.c|||9116|||/*
 * Utility routine to fetch the session-level status of a backup running.
 */""",
  """do_pg_backup_stop|||backend\access\transam\xlog.c|||9135|||/*
 * do_pg_backup_stop
 *
 * Utility function called at the end of an online backup.  It creates history
 * file (if required), resets sessionBackupState and so on.  It can optionally
 * wait for WAL segments to be archived.
 *
 * "state" is filled with the information necessary to restore from this
 * backup with its stop LSN (stoppoint), its timeline ID (stoptli), etc.
 *
 * It is the responsibility of the caller of this function to verify the
 * permissions of the calling user!
 */""",
  """do_pg_abort_backup|||backend\access\transam\xlog.c|||9409|||/*
 * do_pg_abort_backup: abort a running backup
 *
 * This does just the most basic steps of do_pg_backup_stop(), by taking the
 * system out of backup mode, thus making it a lot more safe to call from
 * an error handler.
 *
 * 'arg' indicates that it's being called during backup setup; so
 * sessionBackupState has not been modified yet, but runningBackups has
 * already been incremented.  When it's false, then it's invoked as a
 * before_shmem_exit handler, and therefore we must not change state
 * unless sessionBackupState indicates that a backup is actually running.
 *
 * NB: This gets used as a PG_ENSURE_ERROR_CLEANUP callback and
 * before_shmem_exit handler, hence the odd-looking signature.
 */""",
  """register_persistent_abort_backup_handler|||backend\access\transam\xlog.c|||9436|||/*
 * Register a handler that will warn about unterminated backups at end of
 * session, unless this has already been done.
 */""",
  """GetXLogInsertRecPtr|||backend\access\transam\xlog.c|||9450|||/*
 * Get latest WAL insert pointer
 */""",
  """GetXLogWriteRecPtr|||backend\access\transam\xlog.c|||9466|||/*
 * Get latest WAL write pointer
 */""",
  """GetOldestRestartPoint|||backend\access\transam\xlog.c|||9478|||/*
 * Returns the redo pointer of the last checkpoint or restartpoint. This is
 * the oldest point in WAL that we still need, if we have to restart recovery.
 */""",
  """XLogShutdownWalRcv|||backend\access\transam\xlog.c|||9488|||/* Thin wrapper around ShutdownWalRcv(). */""",
  """SetInstallXLogFileSegmentActive|||backend\access\transam\xlog.c|||9499|||/* Enable WAL file recycling and preallocation. */""",
  """IsInstallXLogFileSegmentActive|||backend\access\transam\xlog.c|||9507|||/* Enable WAL file recycling and preallocation. */""",
  """SetWalWriterSleeping|||backend\access\transam\xlog.c|||9522|||/*
 * Update the WalWriterSleeping flag.
 */""",
  """RestoreArchivedFile|||backend\access\transam\xlogarchive.c|||53|||/*
 * Attempt to retrieve the specified file from off-line archival storage.
 * If successful, fill "path" with its complete path (note that this will be
 * a temp file name that doesn't follow the normal naming convention), and
 * return true.
 *
 * If not successful, fill "path" with the name of the normal on-line file
 * (which may or may not actually exist, but we'll try to use it), and return
 * false.
 *
 * For fixed-size files, the caller may pass the expected size as an
 * additional crosscheck on successful recovery.  If the file size is not
 * known, set expectedSize = 0.
 *
 * When 'cleanupEnabled' is false, refrain from deleting any old WAL segments
 * in the archive. This is used when fetching the initial checkpoint record,
 * when we are not yet sure how far back we need the WAL.
 */""",
  """ExecuteRecoveryCommand|||backend\access\transam\xlogarchive.c|||294|||/*
 * Attempt to execute an external shell command during recovery.
 *
 * 'command' is the shell command to be executed, 'commandName' is a
 * human-readable name describing the command emitted in the logs. If
 * 'failOnSignal' is true and the command is killed by a signal, a FATAL
 * error is thrown. Otherwise a WARNING is emitted.
 *
 * This is currently used for recovery_end_command and archive_cleanup_command.
 */""",
  """KeepFileRestoredFromArchive|||backend\access\transam\xlogarchive.c|||357|||/*
 * A file was restored from the archive under a temporary filename (path),
 * and now we want to keep it. Rename it under the permanent filename in
 * pg_wal (xlogfname), replacing any existing file with the same name.
 */""",
  """XLogArchiveNotify|||backend\access\transam\xlogarchive.c|||443|||/*
 * XLogArchiveNotify
 *
 * Create an archive notification file
 *
 * The name of the notification file is the message that will be picked up
 * by the archiver, e.g. we write 0000000100000001000000C6.ready
 * and the archiver then knows to archive XLOGDIR/0000000100000001000000C6,
 * then when complete, rename it to 0000000100000001000000C6.done
 */""",
  """XLogArchiveNotifySeg|||backend\access\transam\xlogarchive.c|||491|||/*
 * Convenience routine to notify using segment number representation of filename
 */""",
  """XLogArchiveForceDone|||backend\access\transam\xlogarchive.c|||509|||/*
 * XLogArchiveForceDone
 *
 * Emit notification forcibly that an XLOG segment file has been successfully
 * archived, by creating <XLOG>.done regardless of whether <XLOG>.ready
 * exists or not.
 */""",
  """XLogArchiveCheckDone|||backend\access\transam\xlogarchive.c|||564|||/*
 * XLogArchiveCheckDone
 *
 * This is called when we are ready to delete or recycle an old XLOG segment
 * file or backup history file.  If it is okay to delete it then return true.
 * If it is not time to delete it, make sure a .ready file exists, and return
 * false.
 *
 * If <XLOG>.done exists, then return true; else if <XLOG>.ready exists,
 * then return false; else create <XLOG>.ready and return false.
 *
 * The reason we do things this way is so that if the original attempt to
 * create <XLOG>.ready fails, we'll retry during subsequent checkpoints.
 */""",
  """XLogArchiveIsBusy|||backend\access\transam\xlogarchive.c|||618|||/*
 * XLogArchiveIsBusy
 *
 * Check to see if an XLOG segment file is still unarchived.
 * This is almost but not quite the inverse of XLogArchiveCheckDone: in
 * the first place we aren't chartered to recreate the .ready file, and
 * in the second place we should consider that if the file is already gone
 * then it's not busy.  (This check is needed to handle the race condition
 * that a checkpoint already deleted the no-longer-needed file.)
 */""",
  """XLogArchiveIsReadyOrDone|||backend\access\transam\xlogarchive.c|||663|||/*
 * XLogArchiveIsReadyOrDone
 *
 * Check to see if an XLOG segment file has a .ready or .done file.
 * This is similar to XLogArchiveIsBusy(), but returns true if the file
 * is already archived or is about to be archived.
 *
 * This is currently only used at recovery.  During normal operation this
 * would be racy: the file might get removed or marked with .ready as we're
 * checking it, or immediately after we return.
 */""",
  """XLogArchiveIsReady|||backend\access\transam\xlogarchive.c|||693|||/*
 * XLogArchiveIsReady
 *
 * Check to see if an XLOG segment file has an archive notification (.ready)
 * file.
 */""",
  """XLogArchiveCleanup|||backend\access\transam\xlogarchive.c|||711|||/*
 * XLogArchiveCleanup
 *
 * Cleanup archive notification file(s) for a particular xlog segment
 */""",
  """build_backup_content|||backend\access\transam\xlogbackup.c|||28|||/*
 * Build contents for backup_label or backup history file.
 *
 * When ishistoryfile is true, it creates the contents for a backup history
 * file, otherwise it creates contents for a backup_label file.
 *
 * Returns the result generated as a palloc'd string.
 */""",
  """pg_backup_start|||backend\access\transam\xlogfuncs.c|||55|||/*
 * pg_backup_start: set up for taking an on-line backup dump
 *
 * Essentially what this does is to create the contents required for the
 * backup_label file and the tablespace map.
 *
 * Permission checking for this function is managed through the normal
 * GRANT system.
 */""",
  """pg_backup_stop|||backend\access\transam\xlogfuncs.c|||122|||/*
 * pg_backup_stop: finish taking an on-line backup.
 *
 * The first parameter (variable 'waitforarchive'), which is optional,
 * allows the user to choose if they want to wait for the WAL to be archived
 * or if we should just return as soon as the WAL record is written.
 *
 * This function stops an in-progress backup, creates backup_label contents and
 * it returns the backup stop LSN, backup_label and tablespace_map contents.
 *
 * The backup_label contains the user-supplied label string (typically this
 * would be used to tell where the backup dump will be stored), the starting
 * time, starting WAL location for the dump and so on.  It is the caller's
 * responsibility to write the backup_label and tablespace_map files in the
 * data folder that will be restored from this backup.
 *
 * Permission checking for this function is managed through the normal
 * GRANT system.
 */""",
  """pg_switch_wal|||backend\access\transam\xlogfuncs.c|||175|||/*
 * pg_switch_wal: switch to next xlog file
 *
 * Permission checking for this function is managed through the normal
 * GRANT system.
 */""",
  """pg_log_standby_snapshot|||backend\access\transam\xlogfuncs.c|||200|||/*
 * pg_log_standby_snapshot: call LogStandbySnapshot()
 *
 * Permission checking for this function is managed through the normal
 * GRANT system.
 */""",
  """pg_create_restore_point|||backend\access\transam\xlogfuncs.c|||231|||/*
 * pg_create_restore_point: a named point for restore
 *
 * Permission checking for this function is managed through the normal
 * GRANT system.
 */""",
  """pg_current_wal_lsn|||backend\access\transam\xlogfuncs.c|||272|||/*
 * Report the current WAL write location (same format as pg_backup_start etc)
 *
 * This is useful for determining how much of WAL is visible to an external
 * archiving process.  Note that the data before this point is written out
 * to the kernel, but is not necessarily synced to disk.
 */""",
  """pg_current_wal_insert_lsn|||backend\access\transam\xlogfuncs.c|||293|||/*
 * Report the current WAL insert location (same format as pg_backup_start etc)
 *
 * This function is mostly for debugging purposes.
 */""",
  """pg_current_wal_flush_lsn|||backend\access\transam\xlogfuncs.c|||314|||/*
 * Report the current WAL flush location (same format as pg_backup_start etc)
 *
 * This function is mostly for debugging purposes.
 */""",
  """pg_last_wal_receive_lsn|||backend\access\transam\xlogfuncs.c|||336|||/*
 * Report the last WAL receive location (same format as pg_backup_start etc)
 *
 * This is useful for determining how much of WAL is guaranteed to be received
 * and synced to disk by walreceiver.
 */""",
  """pg_last_wal_replay_lsn|||backend\access\transam\xlogfuncs.c|||355|||/*
 * Report the last WAL replay location (same format as pg_backup_start etc)
 *
 * This is useful for determining how much of WAL is visible to read-only
 * connections during recovery.
 */""",
  """pg_walfile_name_offset|||backend\access\transam\xlogfuncs.c|||372|||/*
 * Compute an xlog file name and decimal byte offset given a WAL location,
 * such as is returned by pg_backup_stop() or pg_switch_wal().
 */""",
  """pg_walfile_name|||backend\access\transam\xlogfuncs.c|||436|||/*
 * Compute an xlog file name given a WAL location,
 * such as is returned by pg_backup_stop() or pg_switch_wal().
 */""",
  """pg_split_walfile_name|||backend\access\transam\xlogfuncs.c|||461|||/*
 * Extract the sequence number and the timeline ID from given a WAL file
 * name.
 */""",
  """pg_wal_replay_pause|||backend\access\transam\xlogfuncs.c|||516|||/*
 * pg_wal_replay_pause - Request to pause recovery
 *
 * Permission checking for this function is managed through the normal
 * GRANT system.
 */""",
  """pg_wal_replay_resume|||backend\access\transam\xlogfuncs.c|||546|||/*
 * pg_wal_replay_resume - resume recovery now
 *
 * Permission checking for this function is managed through the normal
 * GRANT system.
 */""",
  """pg_is_wal_replay_paused|||backend\access\transam\xlogfuncs.c|||570|||/*
 * pg_is_wal_replay_paused
 */""",
  """pg_get_wal_replay_pause_state|||backend\access\transam\xlogfuncs.c|||591|||/*
 * pg_get_wal_replay_pause_state - Returns the recovery pause state.
 *
 * Returned values:
 *
 * 'not paused' - if pause is not requested
 * 'pause requested' - if pause is requested but recovery is not yet paused
 * 'paused' - if recovery is paused
 */""",
  """pg_last_xact_replay_timestamp|||backend\access\transam\xlogfuncs.c|||626|||/*
 * Returns timestamp of latest processed commit/abort record.
 *
 * When the server has been started normally without recovery the function
 * returns NULL.
 */""",
  """pg_is_in_recovery|||backend\access\transam\xlogfuncs.c|||641|||/*
 * Returns bool with current recovery mode, a global state.
 */""",
  """pg_wal_lsn_diff|||backend\access\transam\xlogfuncs.c|||650|||/*
 * Compute the difference in bytes between two WAL locations.
 */""",
  """pg_promote|||backend\access\transam\xlogfuncs.c|||668|||/*
 * Promotes a standby server.
 *
 * A result of "true" means that promotion has been completed if "wait" is
 * "true", or initiated if "wait" is false.
 */""",
  """<clinit>|||backend\access\transam\xloginsert.c|||68|||/*
 * For each block reference registered with XLogRegisterBuffer, we fill in
 * a registered_buffer struct.
 */""",
  """XLogBeginInsert|||backend\access\transam\xloginsert.c|||148|||/*
 * Begin constructing a WAL record. This must be called before the
 * XLogRegister* functions and XLogInsert().
 */""",
  """XLogEnsureRecordSpace|||backend\access\transam\xloginsert.c|||174|||/*
 * Ensure that there are enough buffer and data slots in the working area,
 * for subsequent XLogRegisterBuffer, XLogRegisterData and XLogRegisterBufData
 * calls.
 *
 * There is always space for a small number of buffers and data chunks, enough
 * for most record types. This function is for the exceptional cases that need
 * more.
 */""",
  """XLogResetInsertion|||backend\access\transam\xloginsert.c|||221|||/*
 * Reset WAL record construction buffers.
 */""",
  """XLogRegisterBuffer|||backend\access\transam\xloginsert.c|||241|||/*
 * Register a reference to a buffer with the WAL record being constructed.
 * This must be called for every page that the WAL-logged operation modifies.
 */"""
)
INFO:__main__:Extracted 18 methods with comments in this batch
INFO:__main__:Fetching batch: offset=1900, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=1900, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres350[0m: [32mList[0m[[32mString[0m] = List(
  """XLogRegisterBlock|||backend\access\transam\xloginsert.c|||308|||/*
 * Like XLogRegisterBuffer, but for registering a block that's not in the
 * shared buffer pool (i.e. when you don't have a Buffer for it).
 */""",
  """XLogRegisterData|||backend\access\transam\xloginsert.c|||363|||/*
 * Add data to the WAL record that's being constructed.
 *
 * The data is appended to the "main chunk", available at replay with
 * XLogRecGetData().
 */""",
  """XLogRegisterBufData|||backend\access\transam\xloginsert.c|||404|||/*
 * Add buffer-specific data to the WAL record that's being constructed.
 *
 * Block_id must reference a block previously registered with
 * XLogRegisterBuffer(). If this is called more than once for the same
 * block_id, the data is appended.
 *
 * The maximum amount of data that can be registered per block is 65535
 * bytes. That should be plenty; if you need more than BLCKSZ bytes to
 * reconstruct the changes to the page, you might as well just log a full
 * copy of it. (the "main data" that's not associated with a block is not
 * limited)
 */""",
  """XLogSetRecordFlags|||backend\access\transam\xloginsert.c|||455|||/*
 * Set insert status flags for the upcoming WAL record.
 *
 * The flags that can be used here are:
 * - XLOG_INCLUDE_ORIGIN, to determine if the replication origin should be
 *	 included in the record.
 * - XLOG_MARK_UNIMPORTANT, to signal that the record is not important for
 *	 durability, which allows to avoid triggering WAL archiving and other
 *	 background activity.
 */""",
  """XLogInsert|||backend\access\transam\xloginsert.c|||473|||/*
 * Insert an XLOG record having the specified RMID and info bytes, with the
 * body of the record being the data and buffer references registered earlier
 * with XLogRegister* calls.
 *
 * Returns XLOG pointer to end of record (beginning of next record).
 * This can be used as LSN for data pages affected by the logged action.
 * (LSN is the XLOG point up to which the XLOG must be flushed to disk
 * before the data page can be written out.  This implements the basic
 * WAL rule "write the log before the data".)
 */""",
  """XLogRecordAssemble|||backend\access\transam\xloginsert.c|||547|||/*
 * Assemble a WAL record from the registered data and buffers into an
 * XLogRecData chain, ready for insertion with XLogInsertRecord().
 *
 * The record header fields are filled in, except for the xl_prev field. The
 * calculated CRC does not include the record header yet.
 *
 * If there are any registered buffers, and a full-page image was not taken
 * of all of them, *fpw_lsn is set to the lowest LSN among such pages. This
 * signals that the assembled record is only good for insertion on the
 * assumption that the RedoRecPtr and doPageWrites values were up-to-date.
 *
 * *topxid_included is set if the topmost transaction ID is logged with the
 * current subtransaction.
 */""",
  """XLogCompressBackupBlock|||backend\access\transam\xloginsert.c|||943|||/*
 * Create a compressed version of a backup block image.
 *
 * Returns false if compression fails (i.e., compressed result is actually
 * bigger than original). Otherwise, returns true and sets 'dlen' to
 * the length of compressed block image.
 */""",
  """XLogCheckBufferNeedsBackup|||backend\access\transam\xloginsert.c|||1026|||/*
 * Determine whether the buffer referenced has to be backed up.
 *
 * Since we don't yet have the insert lock, fullPageWrites and runningBackups
 * (which forces full-page writes) could change later, so the result should
 * be used for optimization purposes only.
 */""",
  """XLogSaveBufferForHint|||backend\access\transam\xloginsert.c|||1064|||/*
 * Write a backup block if needed when we are setting a hint. Note that
 * this may be called for a variety of page types, not just heaps.
 *
 * Callable while holding just share lock on the buffer content.
 *
 * We can't use the plain backup block mechanism since that relies on the
 * Buffer being exclusively locked. Since some modifications (setting LSN, hint
 * bits) are allowed in a sharelocked buffer that can lead to wal checksum
 * failures. So instead we copy the page and insert the copied data as normal
 * record data.
 *
 * We only need to do something if page has not yet been full page written in
 * this checkpoint round. The LSN of the inserted wal record is returned if we
 * had to write, InvalidXLogRecPtr otherwise.
 *
 * It is possible that multiple concurrent backends could attempt to write WAL
 * records. In that case, multiple copies of the same block would be recorded
 * in separate WAL records by different backends, though that is still OK from
 * a correctness perspective.
 */""",
  """log_newpage|||backend\access\transam\xloginsert.c|||1142|||/*
 * Write a WAL record containing a full image of a page. Caller is responsible
 * for writing the page to disk after calling this routine.
 *
 * Note: If you're using this function, you should be building pages in private
 * memory and writing them directly to smgr.  If you're using buffers, call
 * log_newpage_buffer instead.
 *
 * If the page follows the standard page layout, with a PageHeader and unused
 * space between pd_lower and pd_upper, set 'page_std' to true. That allows
 * the unused space to be left out from the WAL record, making it smaller.
 */""",
  """log_newpages|||backend\access\transam\xloginsert.c|||1174|||/*
 * Like log_newpage(), but allows logging multiple pages in one operation.
 * It is more efficient than calling log_newpage() for each page separately,
 * because we can write multiple pages in a single WAL record.
 */""",
  """log_newpage_buffer|||backend\access\transam\xloginsert.c|||1236|||/*
 * Write a WAL record containing a full image of a page.
 *
 * Caller should initialize the buffer and mark it dirty before calling this
 * function.  This function will set the page LSN.
 *
 * If the page follows the standard page layout, with a PageHeader and unused
 * space between pd_lower and pd_upper, set 'page_std' to true. That allows
 * the unused space to be left out from the WAL record, making it smaller.
 */""",
  """log_newpage_range|||backend\access\transam\xloginsert.c|||1269|||/*
 * WAL-log a range of blocks in a relation.
 *
 * An image of all pages with block numbers 'startblk' <= X < 'endblk' is
 * written to the WAL. If the range is large, this is done in multiple WAL
 * records.
 *
 * If all page follows the standard page layout, with a PageHeader and unused
 * space between pd_lower and pd_upper, set 'page_std' to true. That allows
 * the unused space to be left out from the WAL records, making them smaller.
 *
 * NOTE: This function acquires exclusive-locks on the pages. Typically, this
 * is used on a newly-built relation, and the caller is holding a
 * AccessExclusiveLock on it, so no other backend can be accessing it at the
 * same time. If that's not the case, you must ensure that this does not
 * cause a deadlock through some other means.
 */""",
  """InitXLogInsert|||backend\access\transam\xloginsert.c|||1347|||/*
 * Allocate working buffers needed for WAL record construction.
 */""",
  """<clinit>|||backend\access\transam\xlogprefetcher.c|||103|||/*
 * A simple circular queue of LSNs, using to control the number of
 * (potentially) inflight IOs.  This stands in for a later more general IO
 * control mechanism, which is why it has the apparently unnecessary
 * indirection through a function pointer.
 */""",
  """<clinit>|||backend\access\transam\xlogprefetcher.c|||124|||/*
 * A prefetcher.  This is a mechanism that wraps an XLogReader, prefetching
 * blocks that will be soon be referenced, to try to avoid IO stalls.
 */""",
  """lrq_alloc|||backend\access\transam\xlogprefetcher.c|||201|||/*
 * Counters exposed in shared memory for pg_stat_recovery_prefetch.
 */""",
  """lrq_prefetch|||backend\access\transam\xlogprefetcher.c|||244|||	/* Try to start as many IOs as we can within our limits. */""",
  """lrq_complete_lsn|||backend\access\transam\xlogprefetcher.c|||271|||	/*
	 * We know that LSNs before 'lsn' have been replayed, so we can now assume
	 * that any IOs that were started before then have finished.
	 */""",
  """XLogPrefetchResetStats|||backend\access\transam\xlogprefetcher.c|||302|||/*
 * Reset all counters to zero.
 */""",
  """XLogPrefetchShmemInit|||backend\access\transam\xlogprefetcher.c|||314|||/*
 * Reset all counters to zero.
 */""",
  """XLogPrefetchReconfigure|||backend\access\transam\xlogprefetcher.c|||339|||/*
 * Called when any GUC is changed that affects prefetching.
 */""",
  """XLogPrefetchIncrement|||backend\access\transam\xlogprefetcher.c|||350|||/*
 * Increment a counter in shared memory.  This is equivalent to *counter++ on a
 * plain uint64 without any memory barrier or locking, except on platforms
 * where readers can't read uint64 without possibly observing a torn value.
 */""",
  """XLogPrefetcherAllocate|||backend\access\transam\xlogprefetcher.c|||361|||/*
 * Create a prefetcher that is ready to begin prefetching blocks referenced by
 * WAL records.
 */""",
  """XLogPrefetcherFree|||backend\access\transam\xlogprefetcher.c|||391|||/*
 * Destroy a prefetcher and release all resources.
 */""",
  """XLogPrefetcherGetReader|||backend\access\transam\xlogprefetcher.c|||402|||/*
 * Provide access to the reader.
 */""",
  """XLogPrefetcherComputeStats|||backend\access\transam\xlogprefetcher.c|||411|||/*
 * Update the statistics visible in the pg_stat_recovery_prefetch view.
 */""",
  """XLogPrefetcherNextBlock|||backend\access\transam\xlogprefetcher.c|||460|||/*
 * A callback that examines the next block reference in the WAL, and possibly
 * starts an IO so that a later read will be fast.
 *
 * Returns LRQ_NEXT_AGAIN if no more WAL data is available yet.
 *
 * Returns LRQ_NEXT_IO if the next block reference is for a main fork block
 * that isn't in the buffer pool, and the kernel has been asked to start
 * reading it to make a future read system call faster. An LSN is written to
 * *lsn, and the I/O will be considered to have completed once that LSN is
 * replayed.
 *
 * Returns LRQ_NEXT_NO_IO if we examined the next block reference and found
 * that it was already in the buffer pool, or we decided for various reasons
 * not to prefetch.
 */""",
  """pg_stat_get_recovery_prefetch|||backend\access\transam\xlogprefetcher.c|||825|||/*
 * Expose statistics about recovery prefetching.
 */""",
  """XLogPrefetcherAddFilter|||backend\access\transam\xlogprefetcher.c|||857|||/*
 * Don't prefetch any blocks >= 'blockno' from a given 'rlocator', until 'lsn'
 * has been replayed.
 */""",
  """XLogPrefetcherCompleteFilters|||backend\access\transam\xlogprefetcher.c|||895|||/*
 * Have we replayed any records that caused us to begin filtering a block
 * range?  That means that relations should have been created, extended or
 * dropped as required, so we can stop filtering out accesses to a given
 * relfilenumber.
 */""",
  """XLogPrefetcherIsFiltered|||backend\access\transam\xlogprefetcher.c|||915|||/*
 * Check if a given block should be skipped due to a filter.
 */""",
  """XLogPrefetcherBeginRead|||backend\access\transam\xlogprefetcher.c|||963|||/*
 * A wrapper for XLogBeginRead() that also resets the prefetcher.
 */""",
  """XLogPrefetcherReadRecord|||backend\access\transam\xlogprefetcher.c|||982|||/*
 * A wrapper for XLogReadRecord() that provides the same interface, but also
 * tries to initiate I/O for blocks referenced in future WAL records.
 */""",
  """assign_recovery_prefetch|||backend\access\transam\xlogprefetcher.c|||1096|||	/* Reconfigure prefetching, because a setting it depends on changed. */""",
  """report_invalid_record|||backend\access\transam\xlogreader.c|||70|||/*
 * Construct a string in state->errormsg_buf explaining what's wrong with
 * the current record being read.
 */""",
  """XLogReaderSetDecodeBuffer|||backend\access\transam\xlogreader.c|||89|||/*
 * Set the size of the decoding buffer.  A pointer to a caller supplied memory
 * region may also be passed in, in which case non-oversized records will be
 * decoded there.
 */""",
  """XLogReaderAllocate|||backend\access\transam\xlogreader.c|||105|||/*
 * Allocate and initialize a new XLogReader.
 *
 * Returns NULL if the xlogreader couldn't be allocated.
 */""",
  """allocate_recordbuf|||backend\access\transam\xlogreader.c|||189|||/*
 * Allocate readRecordBuf to fit a record of at least the given length.
 *
 * readRecordBufSize is set to the new buffer size.
 *
 * To avoid useless small increases, round its size to a multiple of
 * XLOG_BLCKSZ, and make sure it's at least 5*Max(BLCKSZ, XLOG_BLCKSZ) to start
 * with.  (That is enough for all "normal" records, but very large commit or
 * abort records might need more space.)
 *
 * Note: This routine should *never* be called for xl_tot_len until the header
 * of the record has been fully validated.
 */""",
  """WALOpenSegmentInit|||backend\access\transam\xlogreader.c|||206|||/*
 * Initialize the passed segment structs.
 */""",
  """XLogBeginRead|||backend\access\transam\xlogreader.c|||230|||/*
 * Begin reading WAL at 'RecPtr'.
 *
 * 'RecPtr' should point to the beginning of a valid WAL record.  Pointing at
 * the beginning of a page is also OK, if there is a new record right after
 * the page header, i.e. not a continuation.
 *
 * This does not make any attempt to read the WAL yet, and hence cannot fail.
 * If the starting address is not correct, the first call to XLogReadRecord()
 * will error out.
 */""",
  """XLogReleasePreviousRecord|||backend\access\transam\xlogreader.c|||248|||/*
 * Release the last record that was returned by XLogNextRecord(), if any, to
 * free up space.  Returns the LSN past the end of the record.
 */""",
  """XLogNextRecord|||backend\access\transam\xlogreader.c|||324|||/*
 * Attempt to read an XLOG record.
 *
 * XLogBeginRead() or XLogFindNextRecord() and then XLogReadAhead() must be
 * called before the first call to XLogNextRecord().  This functions returns
 * records and errors that were put into an internal queue by XLogReadAhead().
 *
 * On success, a record is returned.
 *
 * The returned record (or *errormsg) points to an internal buffer that's
 * valid until the next call to XLogNextRecord.
 */""",
  """XLogReadRecord|||backend\access\transam\xlogreader.c|||388|||/*
 * Attempt to read an XLOG record.
 *
 * XLogBeginRead() or XLogFindNextRecord() must be called before the first call
 * to XLogReadRecord().
 *
 * If the page_read callback fails to read the requested data, NULL is
 * returned.  The callback is expected to have reported the error; errormsg
 * is set to NULL.
 *
 * If the reading fails for some other reason, NULL is also returned, and
 * *errormsg is set to a string with details of the failure.
 *
 * The returned pointer (or *errormsg) points to an internal buffer that's
 * valid until the next call to XLogReadRecord.
 */""",
  """XLogReadRecordAlloc|||backend\access\transam\xlogreader.c|||437|||/*
 * Allocate space for a decoded record.  The only member of the returned
 * object that is initialized is the 'oversized' flag, indicating that the
 * decoded record wouldn't fit in the decode buffer and must eventually be
 * freed explicitly.
 *
 * The caller is responsible for adjusting decode_buffer_tail with the real
 * size after successfully decoding a record into this space.  This way, if
 * decoding fails, then there is nothing to undo unless the 'oversized' flag
 * was set and pfree() must be called.
 *
 * Return NULL if there is no space in the decode buffer and allow_oversized
 * is false, or if memory allocation fails for an oversized buffer.
 */""",
  """XLogReadAhead|||backend\access\transam\xlogreader.c|||975|||/*
 * Try to decode the next available record, and return it.  The record will
 * also be returned to XLogNextRecord(), which must be called to 'consume'
 * each record.
 *
 * If nonblocking is true, may return NULL due to lack of data or WAL decoding
 * space.
 */""",
  """ReadPageInternal|||backend\access\transam\xlogreader.c|||1009|||/*
 * Read a single xlog page including at least [pageptr, reqLen] of valid data
 * via the page_read() callback.
 *
 * Returns XLREAD_FAIL if the required page cannot be read for some
 * reason; errormsg_buf is set in that case (unless the error occurs in the
 * page_read callback).
 *
 * Returns XLREAD_WOULDBLOCK if the requested data can't be read without
 * waiting.  This can be returned only if the installed page_read callback
 * respects the state->nonblocking flag, and cannot read the requested data
 * immediately.
 *
 * We fetch the page from a reader-local cache if we know we have the required
 * data and if there hasn't been any error since caching the data.
 */""",
  """XLogReaderInvalReadState|||backend\access\transam\xlogreader.c|||1122|||/*
 * Invalidate the xlogreader's read state to force a re-read.
 */""",
  """ValidXLogRecordHeader|||backend\access\transam\xlogreader.c|||1136|||/*
 * Validate an XLOG record header.
 *
 * This is just a convenience subroutine to avoid duplicated code in
 * XLogReadRecord.  It's not intended for use from anywhere else.
 */""",
  """ValidXLogRecord|||backend\access\transam\xlogreader.c|||1202|||/*
 * CRC-check an XLOG record.  We do not believe the contents of an XLOG
 * record (other than to the minimal extent of computing the amount of
 * data to read in) until we've checked the CRCs.
 *
 * We assume all of the record (that is, xl_tot_len bytes) has been read
 * into memory at *record.  Also, ValidXLogRecordHeader() has accepted the
 * record's header, which means in particular that xl_tot_len is at least
 * SizeOfXLogRecord.
 */""",
  """XLogReaderValidatePageHeader|||backend\access\transam\xlogreader.c|||1233|||/*
 * Validate a page header.
 *
 * Check if 'phdr' is valid as the header of the XLog page at position
 * 'recptr'.
 */""",
  """XLogReaderResetError|||backend\access\transam\xlogreader.c|||1374|||/*
 * Forget about an error produced by XLogReaderValidatePageHeader().
 */""",
  """XLogFindNextRecord|||backend\access\transam\xlogreader.c|||1392|||/*
 * Find the first record with an lsn >= RecPtr.
 *
 * This is different from XLogBeginRead() in that RecPtr doesn't need to point
 * to a valid record boundary.  Useful for checking whether RecPtr is a valid
 * xlog address for reading, and to find the first valid address after some
 * address when dumping records for debugging purposes.
 *
 * This positions the reader, like XLogBeginRead(), so that the next call to
 * XLogReadRecord() will read the next valid record.
 */""",
  """WALRead|||backend\access\transam\xlogreader.c|||1512|||/*
 * Helper function to ease writing of XLogReaderRoutine->page_read callbacks.
 * If this function is used, caller must supply a segment_open callback in
 * 'state', as that is used here.
 *
 * Read 'count' bytes into 'buf', starting at location 'startptr', from WAL
 * fetched from timeline 'tli'.
 *
 * Returns true if succeeded, false if an error occurs, in which case
 * 'errinfo' receives error details.
 */""",
  """ResetDecoder|||backend\access\transam\xlogreader.c|||1604|||/*
 * Private function to reset the state, forgetting all decoded records, if we
 * are asked to move to a new read position.
 */""",
  """DecodeXLogRecordRequiredSpace|||backend\access\transam\xlogreader.c|||1638|||/*
 * Compute the maximum possible amount of padding that could be required to
 * decode a record, given xl_tot_len from the record's header.  This is the
 * amount of output buffer space that we need to decode a record, though we
 * might not finish up using it all.
 *
 * This computation is pessimistic and assumes the maximum possible number of
 * blocks, due to lack of better information.
 */""",
  """DecodeXLogRecord|||backend\access\transam\xlogreader.c|||1671|||/*
 * Decode a record.  "decoded" must point to a MAXALIGNed memory area that has
 * space for at least DecodeXLogRecordRequiredSpace(record) bytes.  On
 * success, decoded->size contains the actual space occupied by the decoded
 * record, which may turn out to be less.
 *
 * Only decoded->oversized member must be initialized already, and will not be
 * modified.  Other members will be initialized as required.
 *
 * On error, a human-readable error message is returned in *errormsg, and
 * the return value is false.
 */""",
  """XLogRecGetBlockTag|||backend\access\transam\xlogreader.c|||1980|||/*
 * Returns information about the block that a block reference refers to.
 *
 * This is like XLogRecGetBlockTagExtended, except that the block reference
 * must exist and there's no access to prefetch_buffer.
 */""",
  """XLogRecGetBlockTagExtended|||backend\access\transam\xlogreader.c|||2006|||/*
 * Returns information about the block that a block reference refers to,
 * optionally including the buffer that the block may already be in.
 *
 * If the WAL record contains a block reference with the given ID, *rlocator,
 * *forknum, *blknum and *prefetch_buffer are filled in (if not NULL), and
 * returns true.  Otherwise returns false.
 */""",
  """XLogRecGetBlockData|||backend\access\transam\xlogreader.c|||2034|||/*
 * Returns the data associated with a block reference, or NULL if there is
 * no data (e.g. because a full-page image was taken instead). The returned
 * pointer points to a MAXALIGNed buffer.
 */""",
  """RestoreBlockImage|||backend\access\transam\xlogreader.c|||2065|||/*
 * Restore a full-page image from a backup block attached to an XLOG record.
 *
 * Returns true if a full-page image is restored, and false on failure with
 * an error to be consumed by the caller.
 */""",
  """XLogRecGetFullXid|||backend\access\transam\xlogreader.c|||2176|||/*
 * Extract the FullTransactionId from a WAL record.
 */""",
  """<clinit>|||backend\access\transam\xlogrecovery.c|||209|||/*
 * Codes indicating where we got a WAL file from during recovery, or where
 * to attempt to get one.
 */""",
  """XLogRecoveryShmemSize|||backend\access\transam\xlogrecovery.c|||446|||/*
 * Initialization of shared memory for WAL recovery
 */""",
  """XLogRecoveryShmemInit|||backend\access\transam\xlogrecovery.c|||457|||/*
 * Initialization of shared memory for WAL recovery
 */""",
  """EnableStandbyMode|||backend\access\transam\xlogrecovery.c|||477|||/*
 * A thin wrapper to enable StandbyMode and do other preparatory work as
 * needed.
 */""",
  """InitWalRecovery|||backend\access\transam\xlogrecovery.c|||511|||/*
 * Prepare the system for WAL recovery, if needed.
 *
 * This is called by StartupXLOG() which coordinates the server startup
 * sequence.  This function analyzes the control file and the backup label
 * file, if any, and figures out whether we need to perform crash recovery or
 * archive recovery, and how far we need to replay the WAL to reach a
 * consistent state.
 *
 * This doesn't yet change the on-disk state, except for creating the symlinks
 * from table space map file if any, and for fetching WAL files needed to find
 * the checkpoint record.  On entry, the caller has already read the control
 * file into memory, and passes it as argument.  This function updates it to
 * reflect the recovery state, and the caller is expected to write it back to
 * disk does after initializing other subsystems, but before calling
 * PerformWalRecovery().
 *
 * This initializes some global variables like ArchiveRecoveryRequested, and
 * StandbyModeRequested and InRecovery.
 */""",
  """readRecoverySignalFile|||backend\access\transam\xlogrecovery.c|||1026|||/*
 * See if there are any recovery signal files and if so, set state for
 * recovery.
 *
 * See if there is a recovery command file (recovery.conf), and if so
 * throw an ERROR since as of PG12 we no longer recognize that.
 */""",
  """read_backup_label|||backend\access\transam\xlogrecovery.c|||1207|||/*
 * read_backup_label: check to see if a backup_label file is present
 *
 * If we see a backup_label during recovery, we assume that we are recovering
 * from a backup dump file, and we therefore roll forward from the checkpoint
 * identified by the label file, NOT what pg_control says.  This avoids the
 * problem that pg_control might have been archived one or more checkpoints
 * later than the start of the dump, and so if we rely on it as the start
 * point, we will fail to restore a consistent database state.
 *
 * Returns true if a backup_label was found (and fills the checkpoint
 * location and TLI into *checkPointLoc and *backupLabelTLI, respectively);
 * returns false if not. If this backup_label came from a streamed backup,
 * *backupEndRequired is set to true. If this backup_label was created during
 * recovery, *backupFromStandby is set to true.
 *
 * Also sets the global variables RedoStartLSN and RedoStartTLI with the LSN
 * and TLI read from the backup file.
 */""",
  """read_tablespace_map|||backend\access\transam\xlogrecovery.c|||1353|||/*
 * read_tablespace_map: check to see if a tablespace_map file is present
 *
 * If we see a tablespace_map file during recovery, we assume that we are
 * recovering from a backup dump file, and we therefore need to create symlinks
 * as per the information present in tablespace_map file.
 *
 * Returns true if a tablespace_map file was found (and fills *tablespaces
 * with a tablespaceinfo struct for each tablespace listed in the file);
 * returns false if not.
 */""",
  """FinishWalRecovery|||backend\access\transam\xlogrecovery.c|||1457|||/*
 * Finish WAL recovery.
 *
 * This does not close the 'xlogreader' yet, because in some cases the caller
 * still wants to re-read the last checkpoint record by calling
 * ReadCheckpointRecord().
 *
 * Returns the position of the last valid or applied record, after which new
 * WAL should be appended, information about why recovery was ended, and some
 * other things. See the EndOfWalRecoveryInfo struct for details.
 */""",
  """ShutdownWalRecovery|||backend\access\transam\xlogrecovery.c|||1607|||/*
 * Clean up the WAL reader and leftovers from restoring WAL from archive
 */""",
  """PerformWalRecovery|||backend\access\transam\xlogrecovery.c|||1651|||/*
 * Perform WAL recovery.
 *
 * If the system was shut down cleanly, this is never called.
 */""",
  """ApplyWalRecord|||backend\access\transam\xlogrecovery.c|||1907|||/*
 * Subroutine of PerformWalRecovery, to apply one WAL record.
 */""",
  """xlogrecovery_redo|||backend\access\transam\xlogrecovery.c|||2071|||/*
 * Some XLOG RM record types that are directly related to WAL recovery are
 * handled here rather than in the xlog_redo()
 */""",
  """CheckTablespaceDirectory|||backend\access\transam\xlogrecovery.c|||2142|||/*
 * Verify that, in non-test mode, ./pg_tblspc doesn't contain any real
 * directories.
 *
 * Replay of database creation XLOG records for databases that were later
 * dropped can create fake directories in pg_tblspc.  By the time consistency
 * is reached these directories should have been removed; here we verify
 * that this did indeed happen.  This is to be called at the point where
 * consistent state is reached.
 *
 * allow_in_place_tablespaces turns the PANIC into a WARNING, which is
 * useful for testing purposes, and also allows for an escape hatch in case
 * things go south.
 */""",
  """CheckRecoveryConsistency|||backend\access\transam\xlogrecovery.c|||2174|||/*
 * Checks if recovery has reached a consistent state. When consistency is
 * reached and we have a valid starting standby snapshot, tell postmaster
 * that it can start accepting read-only connections.
 */""",
  """rm_redo_error_callback|||backend\access\transam\xlogrecovery.c|||2274|||/*
 * Error context callback for errors occurring during rm_redo().
 */""",
  """xlog_outdesc|||backend\access\transam\xlogrecovery.c|||2296|||/*
 * Returns a string describing an XLogRecord, consisting of its identity
 * optionally followed by a colon, a space, and a further description.
 */""",
  """xlog_outrec|||backend\access\transam\xlogrecovery.c|||2317|||/*
 * Returns a string describing an XLogRecord, consisting of its identity
 * optionally followed by a colon, a space, and a further description.
 */""",
  """xlog_block_info|||backend\access\transam\xlogrecovery.c|||2335|||/*
 * Returns a string giving information about all the blocks in an
 * XLogRecord.
 */""",
  """checkTimeLineSwitch|||backend\access\transam\xlogrecovery.c|||2376|||/*
 * Check that it's OK to switch to new timeline during recovery.
 *
 * 'lsn' is the address of the shutdown checkpoint record we're about to
 * replay. (Currently, timeline can only change at a shutdown checkpoint).
 */""",
  """getRecordTimestamp|||backend\access\transam\xlogrecovery.c|||2425|||/*
 * Extract timestamp from WAL record.
 *
 * If the record contains a timestamp, returns true, and saves the timestamp
 * in *recordXtime. If the record type has no timestamp, returns false.
 * Currently, only transaction commit/abort records and restore points contain
 * timestamps.
 */""",
  """verifyBackupPageConsistency|||backend\access\transam\xlogrecovery.c|||2460|||/*
 * Checks whether the current buffer page and backup page stored in the
 * WAL record are consistent or not. Before comparing the two pages, a
 * masking can be applied to the pages to ignore certain areas like hint bits,
 * unused space between pd_lower and pd_upper among other things. This
 * function should be called once WAL replay has been completed for a
 * given record.
 */""",
  """recoveryStopsBefore|||backend\access\transam\xlogrecovery.c|||2572|||/*
 * For point-in-time recovery, this function decides whether we want to
 * stop applying the XLOG before the current record.
 *
 * Returns true if we are stopping, false otherwise. If stopping, some
 * information is saved in recoveryStopXid et al for use in annotating the
 * new timeline's history file.
 */""",
  """recoveryStopsAfter|||backend\access\transam\xlogrecovery.c|||2725|||/*
 * Same as recoveryStopsBefore, but called after applying the record.
 *
 * We also track the timestamp of the latest applied COMMIT/ABORT
 * record in XLogRecoveryCtl->recoveryLastXTime.
 */""",
  """getRecoveryStopReason|||backend\access\transam\xlogrecovery.c|||2885|||/*
 * Create a comment for the history file to explain why and where
 * timeline changed.
 */""",
  """recoveryPausesHere|||backend\access\transam\xlogrecovery.c|||2924|||/*
 * Wait until shared recoveryPauseState is set to RECOVERY_NOT_PAUSED.
 *
 * endOfRecovery is true if the recovery target is reached and
 * the paused state starts at the end of recovery because of
 * recovery_target_action=pause, and false otherwise.
 */""",
  """recoveryApplyDelay|||backend\access\transam\xlogrecovery.c|||2981|||/*
 * When recovery_min_apply_delay is set, we wait long enough to make sure
 * certain record types are applied at least that interval behind the primary.
 *
 * Returns true if we waited.
 *
 * Note that the delay is calculated between the WAL record log time and
 * the current time on standby. We would prefer to keep track of when this
 * standby received each WAL record, which would allow a more consistent
 * approach and one not affected by time synchronisation issues, but that
 * is significantly more effort and complexity for little actual gain in
 * usability.
 */""",
  """GetRecoveryPauseState|||backend\access\transam\xlogrecovery.c|||3069|||/*
 * Get the current state of the recovery pause request.
 */""",
  """SetRecoveryPause|||backend\access\transam\xlogrecovery.c|||3089|||/*
 * Set the recovery pause state.
 *
 * If recovery pause is requested then sets the recovery pause state to
 * 'pause requested' if it is not already 'paused'.  Otherwise, sets it
 * to 'not paused' to resume the recovery.  The recovery pause will be
 * confirmed by the ConfirmRecoveryPaused.
 */""",
  """ConfirmRecoveryPaused|||backend\access\transam\xlogrecovery.c|||3109|||/*
 * Confirm the recovery pause by setting the recovery pause state to
 * RECOVERY_PAUSED.
 */""",
  """ReadRecord|||backend\access\transam\xlogrecovery.c|||3130|||/*
 * Attempt to read the next XLOG record.
 *
 * Before first call, the reader needs to be positioned to the first record
 * by calling XLogPrefetcherBeginRead().
 *
 * If no valid record is available, returns NULL, or fails if emode is PANIC.
 * (emode must be either PANIC, LOG). In standby mode, retries until a valid
 * record is available.
 */""",
  """XLogPageRead|||backend\access\transam\xlogrecovery.c|||3297|||/*
 * Read the XLOG page containing targetPagePtr into readBuf (if not read
 * already).  Returns number of bytes read, if the page is read successfully,
 * or XLREAD_FAIL in case of errors.  When errors occur, they are ereport'ed,
 * but only if they have not been previously reported.
 *
 * See XLogReaderRoutine.page_read for more details.
 *
 * While prefetching, xlogreader->nonblocking may be set.  In that case,
 * returns XLREAD_WOULDBLOCK if we'd otherwise have to wait for more WAL.
 *
 * This is responsible for restoring files from archive as needed, as well
 * as for waiting for the requested WAL record to arrive in standby mode.
 *
 * xlogreader->private_data->emode specifies the log level used for reporting
 * "file not found" or "end of WAL" situations in archive recovery, or in
 * standby mode when promotion is triggered. If set to WARNING or below,
 * XLogPageRead() returns XLREAD_FAIL in those situations, on higher log
 * levels the ereport() won't return.
 *
 * In standby mode, if after a successful return of XLogPageRead() the
 * caller finds the record it's interested in to be broken, it should
 * ereport the error with the level determined by
 * emode_for_corrupt_record(), and then set lastSourceFailed
 * and call XLogPageRead() again with the same arguments. This lets
 * XLogPageRead() to try fetching the record from another source, or to
 * sleep and retry.
 */""",
  """WaitForWALToBecomeAvailable|||backend\access\transam\xlogrecovery.c|||3541|||/*
 * Open the WAL segment containing WAL location 'RecPtr'.
 *
 * The segment can be fetched via restore_command, or via walreceiver having
 * streamed the record, or it can already be present in pg_wal. Checking
 * pg_wal is mainly for crash recovery, but it will be polled in standby mode
 * too, in case someone copies a new segment directly to pg_wal. That is not
 * documented or recommended, though.
 *
 * If 'fetching_ckpt' is true, we're fetching a checkpoint record, and should
 * prepare to read WAL starting from RedoStartLSN after this.
 *
 * 'RecPtr' might not point to the beginning of the record we're interested
 * in, it might also point to the page or segment header. In that case,
 * 'tliRecPtr' is the position of the WAL record we're interested in. It is
 * used to decide which timeline to stream the requested WAL from.
 *
 * 'replayLSN' is the current replay LSN, so that if we scan for new
 * timelines, we can reject a switch to a timeline that branched off before
 * this point.
 *
 * If the record is not immediately available, the function returns false
 * if we're not in standby mode. In standby mode, waits for it to become
 * available.
 *
 * When the requested record becomes available, the function opens the file
 * containing it (if not open already), and returns XLREAD_SUCCESS. When end
 * of standby mode is triggered by the user, and there is no more WAL
 * available, returns XLREAD_FAIL.
 *
 * If nonblocking is true, then give up immediately if we can't satisfy the
 * request, returning XLREAD_WOULDBLOCK instead of waiting.
 */""",
  """emode_for_corrupt_record|||backend\access\transam\xlogrecovery.c|||4030|||/*
 * Determine what log level should be used to report a corrupt WAL record
 * in the current WAL page, previously read by XLogPageRead().
 *
 * 'emode' is the error mode that would be used to report a file-not-found
 * or legitimate end-of-WAL situation.   Generally, we use it as-is, but if
 * we're retrying the exact same record that we've tried previously, only
 * complain the first time to keep the noise down.  However, we only do when
 * reading from pg_wal, because we don't expect any invalid records in archive
 * or in records streamed from the primary. Files in the archive should be complete,
 * and we should never hit the end of WAL because we stop and wait for more WAL
 * to arrive before replaying it.
 *
 * NOTE: This function remembers the RecPtr value it was last called with,
 * to suppress repeated messages about the same record. Only call this when
 * you are about to ereport(), or you might cause a later message to be
 * erroneously suppressed.
 */""",
  """ReadCheckpointRecord|||backend\access\transam\xlogrecovery.c|||4049|||/*
 * Subroutine to try to fetch and validate a prior checkpoint record.
 */""",
  """rescanLatestTimeLine|||backend\access\transam\xlogrecovery.c|||4104|||/*
 * Scan for new timelines that might have appeared in the archive since we
 * started recovery.
 *
 * If there are any, the function changes recovery target TLI to the latest
 * one and returns 'true'.
 */""",
  """XLogFileRead|||backend\access\transam\xlogrecovery.c|||4191|||/*
 * Open a logfile segment for reading (during recovery).
 *
 * If source == XLOG_FROM_ARCHIVE, the segment is retrieved from archive.
 * Otherwise, it's assumed to be already available in pg_wal.
 */""",
  """XLogFileReadAnyTLI|||backend\access\transam\xlogrecovery.c|||4273|||/*
 * Open a logfile segment for reading (during recovery).
 *
 * This version searches for the segment with any TLI listed in expectedTLEs.
 */"""
)
INFO:__main__:Extracted 1 methods with comments in this batch
INFO:__main__:Fetching batch: offset=2000, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=2000, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres351[0m: [32mList[0m[[32mString[0m] = List(
  """StartupRequestWalReceiverRestart|||backend\access\transam\xlogrecovery.c|||4375|||/*
 * Set flag to signal the walreceiver to restart.  (The startup process calls
 * this on noticing a relevant configuration change.)
 */""",
  """PromoteIsTriggered|||backend\access\transam\xlogrecovery.c|||4394|||/*
 * Has a standby promotion already been triggered?
 *
 * Unlike CheckForStandbyTrigger(), this works in any process
 * that's connected to shared memory.
 */""",
  """SetPromoteIsTriggered|||backend\access\transam\xlogrecovery.c|||4412|||/*
 * Has a standby promotion already been triggered?
 *
 * Unlike CheckForStandbyTrigger(), this works in any process
 * that's connected to shared memory.
 */""",
  """CheckForStandbyTrigger|||backend\access\transam\xlogrecovery.c|||4433|||/*
 * Check whether a promote request has arrived.
 */""",
  """RemovePromoteSignalFiles|||backend\access\transam\xlogrecovery.c|||4454|||/*
 * Remove the files signaling a standby promotion request.
 */""",
  """CheckPromoteSignal|||backend\access\transam\xlogrecovery.c|||4463|||/*
 * Check to see if a promote request has arrived.
 */""",
  """WakeupRecovery|||backend\access\transam\xlogrecovery.c|||4478|||/*
 * Wake up startup process to replay newly arrived WAL, or to notice that
 * failover has been requested.
 */""",
  """XLogRequestWalReceiverReply|||backend\access\transam\xlogrecovery.c|||4487|||/*
 * Schedule a walreceiver wakeup in the main recovery loop.
 */""",
  """HotStandbyActive|||backend\access\transam\xlogrecovery.c|||4502|||/*
 * Is HotStandby active yet? This is only important in special backends
 * since normal backends won't ever be able to connect until this returns
 * true. Postmaster knows this by way of signal, not via shared memory.
 *
 * Unlike testing standbyState, this works in any process that's connected to
 * shared memory.  (And note that standbyState alone doesn't tell the truth
 * anyway.)
 */""",
  """HotStandbyActiveInReplay|||backend\access\transam\xlogrecovery.c|||4527|||/*
 * Like HotStandbyActive(), but to be used only in WAL replay code,
 * where we don't need to ask any other process what the state is.
 */""",
  """GetXLogReplayRecPtr|||backend\access\transam\xlogrecovery.c|||4539|||/*
 * Get latest redo apply position.
 *
 * Exported to allow WALReceiver to read the pointer directly.
 */""",
  """GetCurrentReplayRecPtr|||backend\access\transam\xlogrecovery.c|||4562|||/*
 * Get position of last applied, or the record being applied.
 *
 * This is different from GetXLogReplayRecPtr() in that if a WAL
 * record is currently being applied, this includes that record.
 */""",
  """SetLatestXTime|||backend\access\transam\xlogrecovery.c|||4585|||/*
 * Save timestamp of latest processed commit/abort record.
 *
 * We keep this in XLogRecoveryCtl, not a simple static variable, so that it can be
 * seen by processes other than the startup process.  Note in particular
 * that CreateRestartPoint is executed in the checkpointer.
 */""",
  """GetLatestXTime|||backend\access\transam\xlogrecovery.c|||4596|||/*
 * Fetch timestamp of latest processed commit/abort record.
 */""",
  """SetCurrentChunkStartTime|||backend\access\transam\xlogrecovery.c|||4614|||/*
 * Save timestamp of the next chunk of WAL records to apply.
 *
 * We keep this in XLogRecoveryCtl, not a simple static variable, so that it can be
 * seen by all backends.
 */""",
  """GetCurrentChunkReplayStartTime|||backend\access\transam\xlogrecovery.c|||4626|||/*
 * Fetch timestamp of latest processed commit/abort record.
 * Startup process maintains an accurate local copy in XLogReceiptTime
 */""",
  """GetXLogReceiptTime|||backend\access\transam\xlogrecovery.c|||4642|||/*
 * Returns time of receipt of current chunk of XLOG data, as well as
 * whether it was received from streaming replication or from archives.
 */""",
  """RecoveryRequiresIntParameter|||backend\access\transam\xlogrecovery.c|||4659|||/*
 * Note that text field supplied is a parameter name and does not require
 * translation
 */""",
  """check_primary_slot_name|||backend\access\transam\xlogrecovery.c|||4740|||/*
 * GUC check_hook for primary_slot_name
 */""",
  """error_multiple_recovery_targets|||backend\access\transam\xlogrecovery.c|||4768|||/*
 * Recovery target settings: Only one of the several recovery_target* settings
 * may be set.  Setting a second one results in an error.  The global variable
 * recoveryTarget tracks which kind of recovery target was chosen.  Other
 * variables store the actual target value (for example a string or a xid).
 * The assign functions of the parameters check whether a competing parameter
 * was already set.  But we want to allow setting the same parameter multiple
 * times.  We also want to allow unsetting a parameter and setting a different
 * one, so we unset recoveryTarget when the parameter is set to an empty
 * string.
 *
 * XXX this code is broken by design.  Throwing an error from a GUC assign
 * hook breaks fundamental assumptions of guc.c.  So long as all the variables
 * for which this can happen are PGC_POSTMASTER, the consequences are limited,
 * since we'd just abort postmaster startup anyway.  Nonetheless it's likely
 * that we have odd behaviors such as unexpected GUC ordering dependencies.
 */""",
  """check_recovery_target|||backend\access\transam\xlogrecovery.c|||4781|||/*
 * GUC check_hook for recovery_target
 */""",
  """assign_recovery_target|||backend\access\transam\xlogrecovery.c|||4795|||/*
 * GUC assign_hook for recovery_target
 */""",
  """check_recovery_target_lsn|||backend\access\transam\xlogrecovery.c|||4811|||/*
 * GUC check_hook for recovery_target_lsn
 */""",
  """assign_recovery_target_lsn|||backend\access\transam\xlogrecovery.c|||4834|||/*
 * GUC assign_hook for recovery_target_lsn
 */""",
  """check_recovery_target_name|||backend\access\transam\xlogrecovery.c|||4853|||/*
 * GUC check_hook for recovery_target_name
 */""",
  """assign_recovery_target_name|||backend\access\transam\xlogrecovery.c|||4869|||/*
 * GUC assign_hook for recovery_target_name
 */""",
  """check_recovery_target_time|||backend\access\transam\xlogrecovery.c|||4894|||/*
 * GUC check_hook for recovery_target_time
 *
 * The interpretation of the recovery_target_time string can depend on the
 * time zone setting, so we need to wait until after all GUC processing is
 * done before we can do the final parsing of the string.  This check function
 * only does a parsing pass to catch syntax errors, but we store the string
 * and parse it again when we need to use it.
 */""",
  """assign_recovery_target_time|||backend\access\transam\xlogrecovery.c|||4949|||/*
 * GUC assign_hook for recovery_target_time
 */""",
  """check_recovery_target_timeline|||backend\access\transam\xlogrecovery.c|||4965|||/*
 * GUC check_hook for recovery_target_timeline
 */""",
  """assign_recovery_target_timeline|||backend\access\transam\xlogrecovery.c|||4998|||/*
 * GUC assign_hook for recovery_target_timeline
 */""",
  """check_recovery_target_xid|||backend\access\transam\xlogrecovery.c|||5011|||/*
 * GUC check_hook for recovery_target_xid
 */""",
  """assign_recovery_target_xid|||backend\access\transam\xlogrecovery.c|||5034|||/*
 * GUC assign_hook for recovery_target_xid
 */""",
  """XLogRecGetLen|||backend\access\transam\xlogstats.c|||21|||/*
 * Calculate the size of a record, split into !FPI and FPI parts.
 */""",
  """XLogRecStoreStats|||backend\access\transam\xlogstats.c|||53|||/*
 * Store per-rmgr and per-record statistics for a given record.
 */""",
  """report_invalid_page|||backend\access\transam\xlogutils.c|||85|||/* Report a reference to an invalid page */""",
  """log_invalid_page|||backend\access\transam\xlogutils.c|||101|||/* Log a reference to an invalid page */""",
  """forget_invalid_pages|||backend\access\transam\xlogutils.c|||165|||/* Forget any invalid pages >= minblkno, because they've been dropped */""",
  """forget_invalid_pages_db|||backend\access\transam\xlogutils.c|||201|||/* Forget any invalid pages in a whole database */""",
  """XLogHaveInvalidPages|||backend\access\transam\xlogutils.c|||234|||/* Are there any unresolved references to invalid pages? */""",
  """XLogCheckInvalidPages|||backend\access\transam\xlogutils.c|||244|||/* Complain about any remaining invalid-page entries */""",
  """XLogReadBufferForRedo|||backend\access\transam\xlogutils.c|||313|||/*
 * XLogReadBufferForRedo
 *		Read a page during XLOG replay
 *
 * Reads a block referenced by a WAL record into shared buffer cache, and
 * determines what needs to be done to redo the changes to it.  If the WAL
 * record includes a full-page image of the page, it is restored.
 *
 * 'record.EndRecPtr' is compared to the page's LSN to determine if the record
 * has already been replayed.  'block_id' is the ID number the block was
 * registered with, when the WAL record was created.
 *
 * Returns one of the following:
 *
 *	BLK_NEEDS_REDO	- changes from the WAL record need to be applied
 *	BLK_DONE		- block doesn't need replaying
 *	BLK_RESTORED	- block was restored from a full-page image included in
 *					  the record
 *	BLK_NOTFOUND	- block was not found (because it was truncated away by
 *					  an operation later in the WAL stream)
 *
 * On return, the buffer is locked in exclusive-mode, and returned in *buf.
 * Note that the buffer is locked and returned even if it doesn't need
 * replaying.  (Getting the buffer lock is not really necessary during
 * single-process crash recovery, but some subroutines such as MarkBufferDirty
 * will complain if we don't have the lock.  In hot standby mode it's
 * definitely necessary.)
 *
 * Note: when a backup block is available in XLOG with the BKPIMAGE_APPLY flag
 * set, we restore it, even if the page in the database appears newer.  This
 * is to protect ourselves against database pages that were partially or
 * incorrectly written during a crash.  We assume that the XLOG data must be
 * good because it has passed a CRC check, while the database page might not
 * be.  This will force us to replay all subsequent modifications of the page
 * that appear in XLOG, rather than possibly ignoring them as already
 * applied, but that's not a huge drawback.
 */""",
  """XLogInitBufferForRedo|||backend\access\transam\xlogutils.c|||325|||/*
 * Pin and lock a buffer referenced by a WAL record, for the purpose of
 * re-initializing it.
 */""",
  """XLogReadBufferForRedoExtended|||backend\access\transam\xlogutils.c|||350|||/*
 * XLogReadBufferForRedoExtended
 *		Like XLogReadBufferForRedo, but with extra options.
 *
 * In RBM_ZERO_* modes, if the page doesn't exist, the relation is extended
 * with all-zeroes pages up to the referenced block number.  In
 * RBM_ZERO_AND_LOCK and RBM_ZERO_AND_CLEANUP_LOCK modes, the return value
 * is always BLK_NEEDS_REDO.
 *
 * (The RBM_ZERO_AND_CLEANUP_LOCK mode is redundant with the get_cleanup_lock
 * parameter. Do not use an inconsistent combination!)
 *
 * If 'get_cleanup_lock' is true, a "cleanup lock" is acquired on the buffer
 * using LockBufferForCleanup(), instead of a regular exclusive lock.
 */""",
  """XLogReadBufferExtended|||backend\access\transam\xlogutils.c|||470|||/*
 * XLogReadBufferExtended
 *		Read a page during XLOG replay
 *
 * This is functionally comparable to ReadBufferExtended. There's some
 * differences in the behavior wrt. the "mode" argument:
 *
 * In RBM_NORMAL mode, if the page doesn't exist, or contains all-zeroes, we
 * return InvalidBuffer. In this case the caller should silently skip the
 * update on this page. (In this situation, we expect that the page was later
 * dropped or truncated. If we don't see evidence of that later in the WAL
 * sequence, we'll complain at the end of WAL replay.)
 *
 * In RBM_ZERO_* modes, if the page doesn't exist, the relation is extended
 * with all-zeroes pages up to the given block number.
 *
 * In RBM_NORMAL_NO_LOG mode, we return InvalidBuffer if the page doesn't
 * exist, and we don't check for all-zeroes.  Thus, no log entry is made
 * to imply that the page should be dropped or truncated later.
 *
 * Optionally, recent_buffer can be used to provide a hint about the location
 * of the page in the buffer pool; it does not have to be correct, but avoids
 * a buffer mapping table probe if it is.
 *
 * NB: A redo function should normally not call this directly. To get a page
 * to modify, use XLogReadBufferForRedoExtended instead. It is important that
 * all pages modified by a WAL record are registered in the WAL records, or
 * they will be invisible to tools that need to know which pages are modified.
 */""",
  """CreateFakeRelcacheEntry|||backend\access\transam\xlogutils.c|||581|||/*
 * Create a fake relation cache entry for a physical relation
 *
 * It's often convenient to use the same functions in XLOG replay as in the
 * main codepath, but those functions typically work with a relcache entry.
 * We don't have a working relation cache during XLOG replay, but this
 * function can be used to create a fake relcache entry instead. Only the
 * fields related to physical storage, like rd_rel, are initialized, so the
 * fake entry is only usable in low-level operations like ReadBuffer().
 *
 * This is also used for syncing WAL-skipped files.
 *
 * Caller must free the returned entry with FreeFakeRelcacheEntry().
 */""",
  """FreeFakeRelcacheEntry|||backend\access\transam\xlogutils.c|||628|||/*
 * Free a fake relation cache entry.
 */""",
  """XLogDropRelation|||backend\access\transam\xlogutils.c|||640|||/*
 * Drop a relation during XLOG replay
 *
 * This is called when the relation is about to be deleted; we need to remove
 * any open "invalid-page" records for the relation.
 */""",
  """XLogDropDatabase|||backend\access\transam\xlogutils.c|||651|||/*
 * Drop a whole database during XLOG replay
 *
 * As above, but for DROP DATABASE instead of dropping a single rel
 */""",
  """XLogTruncateRelation|||backend\access\transam\xlogutils.c|||670|||/*
 * Truncate a relation during XLOG replay
 *
 * We need to clean up any open "invalid-page" records for the dropped pages.
 */""",
  """XLogReadDetermineTimeline|||backend\access\transam\xlogutils.c|||717|||/*
 * Determine which timeline to read an xlog page from and set the
 * XLogReaderState's currTLI to that timeline ID.
 *
 * We care about timelines in xlogreader when we might be reading xlog
 * generated prior to a promotion, either if we're currently a standby in
 * recovery or if we're a promoted primary reading xlogs generated by the old
 * primary before our promotion.
 *
 * wantPage must be set to the start address of the page to read and
 * wantLength to the amount of the page that will be read, up to
 * XLOG_BLCKSZ. If the amount to be read isn't known, pass XLOG_BLCKSZ.
 *
 * The currTLI argument should be the system-wide current timeline.
 * Note that this may be different from state->currTLI, which is the timeline
 * from which the caller is currently reading previous xlog records.
 *
 * We switch to an xlog segment from the new timeline eagerly when on a
 * historical timeline, as soon as we reach the start of the xlog segment
 * containing the timeline switch.  The server copied the segment to the new
 * timeline so all the data up to the switch point is the same, but there's no
 * guarantee the old segment will still exist. It may have been deleted or
 * renamed with a .partial suffix so we can't necessarily keep reading from
 * the old TLI even though tliSwitchPoint says it's OK.
 *
 * We can't just check the timeline when we read a page on a different segment
 * to the last page. We could've received a timeline switch from a cascading
 * upstream, so the current segment ends abruptly (possibly getting renamed to
 * .partial) and we have to switch to a new one.  Even in the middle of reading
 * a page we could have to dump the cached page and switch to a new TLI.
 *
 * Because of this, callers MAY NOT assume that currTLI is the timeline that
 * will be in a page's xlp_tli; the page may begin on an older timeline or we
 * might be reading from historical timeline data on a segment that's been
 * copied to a new timeline.
 *
 * The caller must also make sure it doesn't read past the current replay
 * position (using GetXLogReplayRecPtr) if executing in recovery, so it
 * doesn't fail to notice that the current timeline became historical.
 */""",
  """wal_segment_open|||backend\access\transam\xlogutils.c|||816|||/* XLogReaderRoutine->segment_open callback for local pg_wal files */""",
  """wal_segment_close|||backend\access\transam\xlogutils.c|||841|||/* stock XLogReaderRoutine->segment_close callback */""",
  """read_local_xlog_page|||backend\access\transam\xlogutils.c|||860|||/*
 * XLogReaderRoutine->page_read callback for reading local xlog files
 *
 * Public because it would likely be very helpful for someone writing another
 * output method outside walsender, e.g. in a bgworker.
 *
 * TODO: The walsender has its own version of this, but it relies on the
 * walsender's latch being set whenever WAL is flushed. No such infrastructure
 * exists for normal backends, so we have to do a check/sleep/repeat style of
 * loop for now.
 */""",
  """read_local_xlog_page_no_wait|||backend\access\transam\xlogutils.c|||872|||/*
 * Same as read_local_xlog_page except that it doesn't wait for future WAL
 * to be available.
 */""",
  """read_local_xlog_page_guts|||backend\access\transam\xlogutils.c|||884|||/*
 * Implementation of read_local_xlog_page and its no wait version.
 */""",
  """WALReadRaiseError|||backend\access\transam\xlogutils.c|||1019|||/*
 * Backend-specific convenience code to handle read errors encountered by
 * WALRead().
 */""",
  """IsManifestEnabled|||backend\backup\backup_manifest.c|||32|||/*
 * Does the user want a backup manifest?
 *
 * It's simplest to always have a manifest_info object, so that we don't need
 * checks for NULL pointers in too many places. However, if the user doesn't
 * want a manifest, we set manifest->buffile to NULL.
 */""",
  """InitializeBackupManifest|||backend\backup\backup_manifest.c|||55|||/*
 * Initialize state so that we can construct a backup manifest.
 *
 * NB: Although the checksum type for the data files is configurable, the
 * checksum for the manifest itself always uses SHA-256. See comments in
 * SendBackupManifest.
 */""",
  """FreeBackupManifest|||backend\backup\backup_manifest.c|||90|||/*
 * Free resources assigned to a backup manifest constructed.
 */""",
  """AddFileToBackupManifest|||backend\backup\backup_manifest.c|||100|||/*
 * Add an entry to the backup manifest for a file.
 */""",
  """AddWALInfoToBackupManifest|||backend\backup\backup_manifest.c|||211|||/*
 * Add information about the WAL that will need to be replayed when restoring
 * this backup to the manifest.
 */""",
  """SendBackupManifest|||backend\backup\backup_manifest.c|||315|||/*
 * Finalize the backup manifest, and send it to the client.
 */""",
  """AppendStringToManifest|||backend\backup\backup_manifest.c|||382|||/*
 * Append a cstring to the manifest.
 */""",
  """perform_base_backup|||backend\backup\basebackup.c|||233|||/*
 * Actually do a base backup for the specified tablespaces.
 *
 * This is split out mainly to avoid complaints about "variable might be
 * clobbered by longjmp" from stupider versions of gcc.
 */""",
  """compareWalFileNames|||backend\backup\basebackup.c|||683|||/*
 * list_sort comparison function, to compare log/seg portion of WAL segment
 * filenames, ignoring the timeline portion.
 */""",
  """parse_basebackup_options|||backend\backup\basebackup.c|||695|||/*
 * Parse the base backup options passed down by the parser
 */""",
  """SendBaseBackup|||backend\backup\basebackup.c|||987|||/*
 * SendBaseBackup() - send a complete base backup.
 *
 * The function will put the system into backup mode like pg_backup_start()
 * does, so that the backup is consistent even though we read directly from
 * the filesystem, bypassing the buffer cache.
 */""",
  """sendFileWithContent|||backend\backup\basebackup.c|||1072|||/*
 * Inject a file with given name and content in the output tar stream.
 *
 * "len" can optionally be set to an arbitrary length of data sent.  If set
 * to -1, the content sent is treated as a string with strlen() as length.
 */""",
  """sendTablespace|||backend\backup\basebackup.c|||1133|||/*
 * Include the tablespace directory pointed to by 'path' in the output tar
 * stream.  If 'sizeonly' is true, we just calculate a total length and return
 * it, without actually sending anything.
 *
 * Only used to send auxiliary tablespaces, not PGDATA.
 */""",
  """sendDir|||backend\backup\basebackup.c|||1186|||/*
 * Include all files from the given directory in the output tar stream. If
 * 'sizeonly' is true, we just calculate a total length and return it, without
 * actually sending anything.
 *
 * Omit any directory in the tablespaces list, to avoid backing up
 * tablespaces twice when they were created inside PGDATA.
 *
 * If sendtblspclinks is true, we need to include symlink
 * information in the tar file. If not, we can skip that
 * as it will be sent separately in the tablespace_map file.
 */""",
  """sendFile|||backend\backup\basebackup.c|||1571|||/*
 * Given the member, write the TAR header & send the file.
 *
 * If 'missing_ok' is true, will not throw an error if the file is not found.
 *
 * If dboid is anything other than InvalidOid then any checksum failures
 * detected will get reported to the cumulative stats system.
 *
 * If the file is to be sent incrementally, then num_incremental_blocks
 * should be the number of blocks to be sent, and incremental_blocks
 * an array of block numbers relative to the start of the current segment.
 * If the whole file is to be sent, then incremental_blocks should be NULL,
 * and num_incremental_blocks can have any value, as it will be ignored.
 *
 * Returns true if the file was successfully sent, false if 'missing_ok',
 * and the file did not exist.
 */""",
  """read_file_data_into_buffer|||backend\backup\basebackup.c|||1846|||/*
 * Read some more data from the file into the bbsink's buffer, verifying
 * checksums as required.
 *
 * 'offset' is the file offset from which we should begin to read, and
 * 'length' is the amount of data that should be read. The actual amount
 * of data read will be less than the requested amount if the bbsink's
 * buffer isn't big enough to hold it all, or if the underlying file has
 * been truncated. The return value is the number of bytes actually read.
 *
 * 'blkno' is the block number of the first page in the bbsink's buffer
 * relative to the start of the relation.
 *
 * 'verify_checksum' indicates whether we should try to verify checksums
 * for the blocks we read. If we do this, we'll update *checksum_failures
 * and issue warnings as appropriate.
 */""",
  """push_to_sink|||backend\backup\basebackup.c|||1949|||/*
 * Push data into a bbsink.
 *
 * It's better, when possible, to read data directly into the bbsink's buffer,
 * rather than using this function to copy it into the buffer; this function is
 * for cases where that approach is not practical.
 *
 * bytes_done should point to a count of the number of bytes that are
 * currently used in the bbsink's buffer. Upon return, the bytes identified by
 * data and length will have been copied into the bbsink's buffer, flushing
 * as required, and *bytes_done will have been updated accordingly. If the
 * buffer was flushed, the previous contents will also have been fed to
 * checksum_ctx.
 *
 * Note that after one or more calls to this function it is the caller's
 * responsibility to perform any required final flush.
 */""",
  """verify_page_checksum|||backend\backup\basebackup.c|||1990|||/*
 * Try to verify the checksum for the provided page, if it seems appropriate
 * to do so.
 *
 * Returns true if verification succeeds or if we decide not to check it,
 * and false if verification fails. When return false, it also sets
 * *expected_checksum to the computed value.
 */""",
  """_tarWritePadding|||backend\backup\basebackup.c|||2070|||/*
 * Pad with zero bytes out to a multiple of TAR_BLOCK_SIZE.
 */""",
  """convert_link_to_directory|||backend\backup\basebackup.c|||2093|||/*
 * If the entry in statbuf is a link, then adjust statbuf to make it look like a
 * directory, so that it will be written that way.
 */""",
  """basebackup_read_file|||backend\backup\basebackup.c|||2110|||/*
 * Read some data from a file, setting a wait event and reporting any error
 * encountered.
 *
 * If partial_read_ok is false, also report an error if the number of bytes
 * read is not equal to the number of bytes requested.
 *
 * Returns the number of bytes read.
 */""",
  """bbsink_copystream_new|||backend\backup\basebackup_copy.c|||107|||/*
 * Create a new 'copystream' bbsink.
 */""",
  """bbsink_copystream_begin_backup|||backend\backup\basebackup_copy.c|||125|||/*
 * Send start-of-backup wire protocol messages.
 */""",
  """bbsink_copystream_begin_archive|||backend\backup\basebackup_copy.c|||164|||/*
 * Send a CopyData message announcing the beginning of a new archive.
 */""",
  """bbsink_copystream_archive_contents|||backend\backup\basebackup_copy.c|||182|||/*
 * Send a CopyData message containing a chunk of archive content.
 */""",
  """bbsink_copystream_end_archive|||backend\backup\basebackup_copy.c|||240|||/*
 * We don't need to explicitly signal the end of the archive; the client
 * will figure out that we've reached the end when we begin the next one,
 * or begin the manifest, or end the COPY stream. However, this seems like
 * a good time to force out a progress report. One reason for that is that
 * if this is the last archive, and we don't force a progress report now,
 * the client will never be told that we sent all the bytes.
 */""",
  """bbsink_copystream_begin_manifest|||backend\backup\basebackup_copy.c|||259|||/*
 * Send a CopyData message announcing the beginning of the backup manifest.
 */""",
  """bbsink_copystream_manifest_contents|||backend\backup\basebackup_copy.c|||272|||/*
 * Each chunk of manifest data is sent using a CopyData message.
 */""",
  """bbsink_copystream_end_manifest|||backend\backup\basebackup_copy.c|||287|||/*
 * We don't need an explicit terminator for the backup manifest.
 */""",
  """bbsink_copystream_end_backup|||backend\backup\basebackup_copy.c|||296|||/*
 * Send end-of-backup wire protocol messages.
 */""",
  """bbsink_copystream_cleanup|||backend\backup\basebackup_copy.c|||307|||/*
 * Cleanup.
 */""",
  """SendCopyOutResponse|||backend\backup\basebackup_copy.c|||316|||/*
 * Send a CopyOutResponse message.
 */""",
  """SendCopyDone|||backend\backup\basebackup_copy.c|||330|||/*
 * Send a CopyDone message.
 */""",
  """SendXlogRecPtrResult|||backend\backup\basebackup_copy.c|||340|||/*
 * Send a single resultset containing just a single
 * XLogRecPtr record (in text format)
 */""",
  """SendTablespaceList|||backend\backup\basebackup_copy.c|||377|||/*
 * Send a result set via libpq describing the tablespace list.
 */""",
  """bbsink_gzip_new|||backend\backup\basebackup_gzip.c|||61|||/*
 * Create a new basebackup sink that performs gzip compression.
 */""",
  """bbsink_gzip_begin_backup|||backend\backup\basebackup_gzip.c|||93|||/*
 * Begin backup.
 */""",
  """bbsink_gzip_begin_archive|||backend\backup\basebackup_gzip.c|||113|||/*
 * Prepare to compress the next archive.
 */""",
  """bbsink_gzip_archive_contents|||backend\backup\basebackup_gzip.c|||166|||/*
 * Compress the input data to the output buffer until we run out of input
 * data. Each time the output buffer fills up, invoke the archive_contents()
 * method for then next sink.
 *
 * Note that since we're compressing the input, it may very commonly happen
 * that we consume all the input data without filling the output buffer. In
 * that case, the compressed representation of the current input data won't
 * actually be sent to the next bbsink until a later call to this function,
 * or perhaps even not until bbsink_gzip_end_archive() is invoked.
 */""",
  """bbsink_gzip_end_archive|||backend\backup\basebackup_gzip.c|||224|||/*
 * There might be some data inside zlib's internal buffers; we need to get
 * that flushed out and forwarded to the successor sink as archive content.
 *
 * Then we can end processing for this archive.
 */""",
  """bbsink_gzip_manifest_contents|||backend\backup\basebackup_gzip.c|||277|||/*
 * Manifest contents are not compressed, but we do need to copy them into
 * the successor sink's buffer, because we have our own.
 */""",
  """gzip_palloc|||backend\backup\basebackup_gzip.c|||288|||/*
 * Wrapper function to adjust the signature of palloc to match what libz
 * expects.
 */""",
  """gzip_pfree|||backend\backup\basebackup_gzip.c|||298|||/*
 * Wrapper function to adjust the signature of pfree to match what libz
 * expects.
 */""",
  """CreateIncrementalBackupInfo|||backend\backup\basebackup_incremental.c|||153|||/*
 * Create a new object for storing information extracted from the manifest
 * supplied when creating an incremental backup.
 */"""
)
INFO:__main__:Extracted 42 methods with comments in this batch
INFO:__main__:Fetching batch: offset=2100, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=2100, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres352[0m: [32mList[0m[[32mString[0m] = List(
  """AppendIncrementalManifestData|||backend\backup\basebackup_incremental.c|||195|||/*
 * Before taking an incremental backup, the caller must supply the backup
 * manifest from a prior backup. Each chunk of manifest data received
 * from the client should be passed to this function.
 */""",
  """FinalizeIncrementalManifest|||backend\backup\basebackup_incremental.c|||228|||/*
 * Finalize an IncrementalBackupInfo object after all manifest data has
 * been supplied via calls to AppendIncrementalManifestData.
 */""",
  """PrepareForIncrementalBackup|||backend\backup\basebackup_incremental.c|||264|||/*
 * Prepare to take an incremental backup.
 *
 * Before this function is called, AppendIncrementalManifestData and
 * FinalizeIncrementalManifest should have already been called to pass all
 * the manifest data to this object.
 *
 * This function performs sanity checks on the data extracted from the
 * manifest and figures out for which WAL ranges we need summaries, and
 * whether those summaries are available. Then, it reads and combines the
 * data from those summary files. It also updates the backup_state with the
 * reference TLI and LSN for the prior backup.
 */""",
  """GetIncrementalFilePath|||backend\backup\basebackup_incremental.c|||626|||/*
 * Get the pathname that should be used when a file is sent incrementally.
 *
 * The result is a palloc'd string.
 */""",
  """GetFileBackupMethod|||backend\backup\basebackup_incremental.c|||666|||/*
 * How should we back up a particular file as part of an incremental backup?
 *
 * If the return value is BACK_UP_FILE_FULLY, caller should back up the whole
 * file just as if this were not an incremental backup.  The contents of the
 * relative_block_numbers array are unspecified in this case.
 *
 * If the return value is BACK_UP_FILE_INCREMENTALLY, caller should include
 * an incremental file in the backup instead of the entire file. On return,
 * *num_blocks_required will be set to the number of blocks that need to be
 * sent, and the actual block numbers will have been stored in
 * relative_block_numbers, which should be an array of at least RELSEG_SIZE.
 * In addition, *truncation_block_length will be set to the value that should
 * be included in the incremental file.
 */""",
  """GetIncrementalHeaderSize|||backend\backup\basebackup_incremental.c|||870|||/*
 * Compute the size for a header of an incremental file containing a given
 * number of blocks. The header is rounded to a multiple of BLCKSZ, but
 * only if the file will store some block data.
 */""",
  """GetIncrementalFileSize|||backend\backup\basebackup_incremental.c|||898|||/*
 * Compute the size for an incremental file containing a given number of blocks.
 */""",
  """hash_string_pointer|||backend\backup\basebackup_incremental.c|||920|||/*
 * Helper function for filemap hash table.
 */""",
  """manifest_process_version|||backend\backup\basebackup_incremental.c|||931|||/*
 * This callback to validate the manifest version for incremental backup.
 */""",
  """manifest_process_system_identifier|||backend\backup\basebackup_incremental.c|||945|||/*
 * This callback to validate the manifest system identifier against the current
 * database server.
 */""",
  """manifest_process_file|||backend\backup\basebackup_incremental.c|||967|||/*
 * This callback is invoked for each file mentioned in the backup manifest.
 *
 * We store the path to each file and the size of each file for sanity-checking
 * purposes. For further details, see comments for IncrementalBackupInfo.
 */""",
  """manifest_process_wal_range|||backend\backup\basebackup_incremental.c|||994|||/*
 * This callback is invoked for each WAL range mentioned in the backup
 * manifest.
 *
 * We're just interested in learning the oldest LSN and the corresponding TLI
 * that appear in any WAL range.
 */""",
  """manifest_report_error|||backend\backup\basebackup_incremental.c|||1012|||/*
 * This callback is invoked if an error occurs while parsing the backup
 * manifest.
 */""",
  """compare_block_numbers|||backend\backup\basebackup_incremental.c|||1039|||/*
 * Quicksort comparator for block numbers.
 */""",
  """bbsink_lz4_new|||backend\backup\basebackup_lz4.c|||61|||/*
 * Create a new basebackup sink that performs lz4 compression.
 */""",
  """bbsink_lz4_begin_backup|||backend\backup\basebackup_lz4.c|||92|||/*
 * Begin backup.
 */""",
  """bbsink_lz4_begin_archive|||backend\backup\basebackup_lz4.c|||131|||/*
 * Prepare to compress the next archive.
 */""",
  """bbsink_lz4_archive_contents|||backend\backup\basebackup_lz4.c|||179|||/*
 * Compress the input data to the output buffer until we run out of input
 * data. Each time the output buffer falls below the compression bound for
 * the input buffer, invoke the archive_contents() method for then next sink.
 *
 * Note that since we're compressing the input, it may very commonly happen
 * that we consume all the input data without filling the output buffer. In
 * that case, the compressed representation of the current input data won't
 * actually be sent to the next bbsink until a later call to this function,
 * or perhaps even not until bbsink_lz4_end_archive() is invoked.
 */""",
  """bbsink_lz4_end_archive|||backend\backup\basebackup_lz4.c|||227|||/*
 * There might be some data inside lz4's internal buffers; we need to get
 * that flushed out and also finalize the lz4 frame and then get that forwarded
 * to the successor sink as archive content.
 *
 * Then we can end processing for this archive.
 */""",
  """bbsink_lz4_manifest_contents|||backend\backup\basebackup_lz4.c|||273|||/*
 * Manifest contents are not compressed, but we do need to copy them into
 * the successor sink's buffer, because we have our own.
 */""",
  """bbsink_lz4_cleanup|||backend\backup\basebackup_lz4.c|||284|||/*
 * In case the backup fails, make sure we free the compression context by
 * calling LZ4F_freeCompressionContext() if needed to avoid memory leak.
 */""",
  """bbsink_progress_new|||backend\backup\basebackup_progress.c|||58|||/*
 * Create a new basebackup sink that performs progress tracking functions and
 * forwards data to a successor sink.
 */""",
  """bbsink_progress_begin_backup|||backend\backup\basebackup_progress.c|||83|||/*
 * Progress reporting at start of backup.
 */""",
  """bbsink_progress_end_archive|||backend\backup\basebackup_progress.c|||113|||/*
 * End-of archive progress reporting.
 */""",
  """bbsink_progress_archive_contents|||backend\backup\basebackup_progress.c|||149|||/*
 * Handle progress tracking for new archive contents.
 *
 * Increment the counter for the amount of data already streamed
 * by the given number of bytes, and update the progress report for
 * pg_stat_progress_basebackup.
 */""",
  """basebackup_progress_wait_checkpoint|||backend\backup\basebackup_progress.c|||185|||/*
 * Advertise that we are waiting for the start-of-backup checkpoint.
 */""",
  """basebackup_progress_estimate_backup_size|||backend\backup\basebackup_progress.c|||195|||/*
 * Advertise that we are estimating the backup size.
 */""",
  """basebackup_progress_wait_wal_archive|||backend\backup\basebackup_progress.c|||205|||/*
 * Advertise that we are waiting for WAL archiving at end-of-backup.
 */""",
  """basebackup_progress_transfer_wal|||backend\backup\basebackup_progress.c|||228|||/*
 * Advertise that we are transferring WAL files into the final archive.
 */""",
  """basebackup_progress_done|||backend\backup\basebackup_progress.c|||238|||/*
 * Advertise that we are no longer performing a backup.
 */""",
  """bbsink_server_new|||backend\backup\basebackup_server.c|||59|||/*
 * Create a new 'server' bbsink.
 */""",
  """bbsink_server_begin_archive|||backend\backup\basebackup_server.c|||133|||/*
 * Open the correct output file for this archive.
 */""",
  """bbsink_server_archive_contents|||backend\backup\basebackup_server.c|||159|||/*
 * Write the data to the output file.
 */""",
  """bbsink_server_end_archive|||backend\backup\basebackup_server.c|||193|||/*
 * fsync and close the current output file.
 */""",
  """bbsink_server_begin_manifest|||backend\backup\basebackup_server.c|||227|||/*
 * Open the output file to which we will write the manifest.
 *
 * Just like pg_basebackup, we write the manifest first under a temporary
 * name and then rename it into place after fsync. That way, if the manifest
 * is there and under the correct name, the user can be sure that the backup
 * completed.
 */""",
  """bbsink_server_manifest_contents|||backend\backup\basebackup_server.c|||252|||/*
 * Each chunk of manifest data is sent using a CopyData message.
 */""",
  """bbsink_server_end_manifest|||backend\backup\basebackup_server.c|||286|||/*
 * fsync the backup manifest, close the file, and then rename it into place.
 */""",
  """bbsink_forward_begin_backup|||backend\backup\basebackup_sink.c|||23|||/*
 * Forward begin_backup callback.
 *
 * Only use this implementation if you want the bbsink you're implementing to
 * share a buffer with the successor bbsink.
 */""",
  """bbsink_forward_begin_archive|||backend\backup\basebackup_sink.c|||36|||/*
 * Forward begin_archive callback.
 */""",
  """bbsink_forward_archive_contents|||backend\backup\basebackup_sink.c|||53|||/*
 * Forward archive_contents callback.
 *
 * Code that wants to use this should initialize its own bbs_buffer and
 * bbs_buffer_length fields to the values from the successor sink. In cases
 * where the buffer isn't shared, the data needs to be copied before forwarding
 * the callback. We don't do try to do that here, because there's really no
 * reason to have separately allocated buffers containing the same identical
 * data.
 */""",
  """bbsink_forward_end_archive|||backend\backup\basebackup_sink.c|||65|||/*
 * Forward end_archive callback.
 */""",
  """bbsink_forward_begin_manifest|||backend\backup\basebackup_sink.c|||75|||/*
 * Forward begin_manifest callback.
 */""",
  """bbsink_forward_manifest_contents|||backend\backup\basebackup_sink.c|||88|||/*
 * Forward manifest_contents callback.
 *
 * As with the archive_contents callback, it's expected that the buffer is
 * shared.
 */""",
  """bbsink_forward_end_manifest|||backend\backup\basebackup_sink.c|||100|||/*
 * Forward end_manifest callback.
 */""",
  """bbsink_forward_end_backup|||backend\backup\basebackup_sink.c|||110|||/*
 * Forward end_backup callback.
 */""",
  """bbsink_forward_cleanup|||backend\backup\basebackup_sink.c|||120|||/*
 * Forward cleanup callback.
 */""",
  """BaseBackupAddTarget|||backend\backup\basebackup_target.c|||60|||/*
 * Add a new base backup target type.
 *
 * This is intended for use by server extensions.
 */""",
  """BaseBackupGetTargetHandle|||backend\backup\basebackup_target.c|||116|||/*
 * Look up a base backup target and validate the target_detail.
 *
 * Extensions that define new backup targets will probably define a new
 * type of bbsink to match. Validation of the target_detail can be performed
 * either in the check_detail routine called here, or in the bbsink
 * constructor, which will be called from BaseBackupGetSink. It's mostly
 * a matter of taste, but the check_detail function runs somewhat earlier.
 */""",
  """BaseBackupGetSink|||backend\backup\basebackup_target.c|||162|||/*
 * Construct a bbsink that will implement the backup target.
 *
 * The get_sink function does all the real work, so all we have to do here
 * is call it with the correct arguments. Whatever the check_detail function
 * returned is here passed through to the get_sink function. This lets those
 * two functions communicate with each other, if they wish. If not, the
 * check_detail function can simply return the target_detail and let the
 * get_sink function take it from there.
 */""",
  """initialize_target_list|||backend\backup\basebackup_target.c|||171|||/*
 * Load predefined target types into BaseBackupTargetTypeList.
 */""",
  """blackhole_get_sink|||backend\backup\basebackup_target.c|||193|||/*
 * Normally, a get_sink function should construct and return a new bbsink that
 * implements the backup target, but the 'blackhole' target just throws the
 * data away. We could implement that by adding a bbsink that does nothing
 * but forward, but it's even cheaper to implement that by not adding a bbsink
 * at all.
 */""",
  """server_get_sink|||backend\backup\basebackup_target.c|||202|||/*
 * Create a bbsink implementing a server-side backup.
 */""",
  """reject_target_detail|||backend\backup\basebackup_target.c|||212|||/*
 * Implement target-detail checking for a target that does not accept a
 * detail.
 */""",
  """server_check_detail|||backend\backup\basebackup_target.c|||231|||/*
 * Implement target-detail checking for a server-side backup.
 *
 * target_detail should be the name of the directory to which the backup
 * should be written, but we don't check that here. Rather, that check,
 * as well as the necessary permissions checking, happens in bbsink_server_new.
 */""",
  """bbsink_throttle_new|||backend\backup\basebackup_throttle.c|||67|||/*
 * Create a new basebackup sink that performs throttling and forwards data
 * to a successor sink.
 */""",
  """bbsink_throttle_begin_backup|||backend\backup\basebackup_throttle.c|||95|||/*
 * There's no real work to do here, but we need to record the current time so
 * that it can be used for future calculations.
 */""",
  """bbsink_throttle_archive_contents|||backend\backup\basebackup_throttle.c|||109|||/*
 * First throttle, and then pass archive contents to next sink.
 */""",
  """bbsink_throttle_manifest_contents|||backend\backup\basebackup_throttle.c|||120|||/*
 * First throttle, and then pass manifest contents to next sink.
 */""",
  """throttle|||backend\backup\basebackup_throttle.c|||133|||/*
 * Increment the network transfer counter by the given number of bytes,
 * and sleep if necessary to comply with the requested network transfer
 * rate.
 */""",
  """bbsink_zstd_new|||backend\backup\basebackup_zstd.c|||60|||/*
 * Create a new basebackup sink that performs zstd compression.
 */""",
  """bbsink_zstd_begin_backup|||backend\backup\basebackup_zstd.c|||87|||/*
 * Begin backup.
 */""",
  """bbsink_zstd_begin_archive|||backend\backup\basebackup_zstd.c|||157|||/*
 * Prepare to compress the next archive.
 */""",
  """bbsink_zstd_archive_contents|||backend\backup\basebackup_zstd.c|||192|||/*
 * Compress the input data to the output buffer until we run out of input
 * data. Each time the output buffer falls below the compression bound for
 * the input buffer, invoke the archive_contents() method for the next sink.
 *
 * Note that since we're compressing the input, it may very commonly happen
 * that we consume all the input data without filling the output buffer. In
 * that case, the compressed representation of the current input data won't
 * actually be sent to the next bbsink until a later call to this function,
 * or perhaps even not until bbsink_zstd_end_archive() is invoked.
 */""",
  """bbsink_zstd_end_archive|||backend\backup\basebackup_zstd.c|||234|||/*
 * There might be some data inside zstd's internal buffers; we need to get that
 * flushed out, also end the zstd frame and then get that forwarded to the
 * successor sink as archive content.
 *
 * Then we can end processing for this archive.
 */""",
  """bbsink_zstd_end_backup|||backend\backup\basebackup_zstd.c|||281|||/*
 * Free the resources and context.
 */""",
  """bbsink_zstd_manifest_contents|||backend\backup\basebackup_zstd.c|||301|||/*
 * Manifest contents are not compressed, but we do need to copy them into
 * the successor sink's buffer, because we have our own.
 */""",
  """bbsink_zstd_cleanup|||backend\backup\basebackup_zstd.c|||312|||/*
 * In case the backup fails, make sure we free any compression context that
 * got allocated, so that we don't leak memory.
 */""",
  """GetWalSummaries|||backend\backup\walsummary.c|||42|||/*
 * Get a list of WAL summaries.
 *
 * If tli != 0, only WAL summaries with the indicated TLI will be included.
 *
 * If start_lsn != InvalidXLogRecPtr, only summaries that end after the
 * indicated LSN will be included.
 *
 * If end_lsn != InvalidXLogRecPtr, only summaries that start before the
 * indicated LSN will be included.
 *
 * The intent is that you can call GetWalSummaries(tli, start_lsn, end_lsn)
 * to get all WAL summaries on the indicated timeline that overlap the
 * specified LSN range.
 */""",
  """FilterWalSummaries|||backend\backup\walsummary.c|||99|||/*
 * Build a new list of WAL summaries based on an existing list, but filtering
 * out summaries that don't match the search parameters.
 *
 * If tli != 0, only WAL summaries with the indicated TLI will be included.
 *
 * If start_lsn != InvalidXLogRecPtr, only summaries that end after the
 * indicated LSN will be included.
 *
 * If end_lsn != InvalidXLogRecPtr, only summaries that start before the
 * indicated LSN will be included.
 */""",
  """WalSummariesAreComplete|||backend\backup\walsummary.c|||137|||/*
 * Check whether the supplied list of WalSummaryFile objects covers the
 * whole range of LSNs from start_lsn to end_lsn. This function ignores
 * timelines, so the caller should probably filter using the appropriate
 * timeline before calling this.
 *
 * If the whole range of LSNs is covered, returns true, otherwise false.
 * If false is returned, *missing_lsn is set either to InvalidXLogRecPtr
 * if there are no WAL summary files in the input list, or to the first LSN
 * in the range that is not covered by a WAL summary file in the input list.
 */""",
  """OpenWalSummaryFile|||backend\backup\walsummary.c|||204|||/*
 * Open a WAL summary file.
 *
 * This will throw an error in case of trouble. As an exception, if
 * missing_ok = true and the trouble is specifically that the file does
 * not exist, it will not throw an error and will return a value less than 0.
 */""",
  """RemoveWalSummaryIfOlderThan|||backend\backup\walsummary.c|||229|||/*
 * Remove a WAL summary file if the last modification time precedes the
 * cutoff time.
 */""",
  """IsWalSummaryFilename|||backend\backup\walsummary.c|||262|||/*
 * Test whether a filename looks like a WAL summary file.
 */""",
  """ReadWalSummary|||backend\backup\walsummary.c|||272|||/*
 * Data read callback for use with CreateBlockRefTableReader.
 */""",
  """WriteWalSummary|||backend\backup\walsummary.c|||293|||/*
 * Data write callback for use with WriteBlockRefTable.
 */""",
  """ReportWalSummaryError|||backend\backup\walsummary.c|||321|||/*
 * Error-reporting callback for use with CreateBlockRefTableReader.
 */""",
  """ListComparatorForWalSummaryFiles|||backend\backup\walsummary.c|||346|||/*
 * Comparator to sort a List of WalSummaryFile objects by start_lsn.
 */""",
  """pg_available_wal_summaries|||backend\backup\walsummaryfuncs.c|||31|||/*
 * List the WAL summary files available in pg_wal/summaries.
 */""",
  """pg_wal_summary_contents|||backend\backup\walsummaryfuncs.c|||68|||/*
 * List the contents of a WAL summary file identified by TLI, start LSN,
 * and end LSN.
 */""",
  """pg_get_wal_summarizer_state|||backend\backup\walsummaryfuncs.c|||176|||/*
 * Returns information about the state of the WAL summarizer process.
 */""",
  """<clinit>|||backend\bootstrap\bootstrap.c|||74|||/*
 * Basic information associated with each type.  This is used before
 * pg_type is filled, so it has to cover the datatypes used as column types
 * in the core "bootstrapped" catalogs.
 *
 *		XXX several of these input/output functions do catalog scans
 *			(e.g., F_REGPROCIN scans pg_proc).  this obviously creates some
 *			order dependencies in the catalog creation process.
 */""",
  """CheckerModeMain|||backend\bootstrap\bootstrap.c|||180|||/*
 * In shared memory checker mode, all we really want to do is create shared
 * memory and semaphores (just to prove we can do it with the current GUC
 * settings).  Since, in fact, that was already done by
 * CreateSharedMemoryAndSemaphores(), we have nothing more to do here.
 */""",
  """BootstrapModeMain|||backend\bootstrap\bootstrap.c|||198|||/*
 *	 The main entry point for running the backend in bootstrap mode
 *
 *	 The bootstrap mode is used to initialize the template database.
 *	 The bootstrap backend doesn't speak SQL, but instead expects
 *	 commands in a special bootstrap language.
 *
 *	 When check_only is true, startup is done only far enough to verify that
 *	 the current configuration, particularly the passed in options pertaining
 *	 to shared memory sizing, options work (or at least do not cause an error
 *	 up to shared memory creation).
 */""",
  """bootstrap_signals|||backend\bootstrap\bootstrap.c|||380|||/*
 * Set up signal handling for a bootstrap process
 */""",
  """boot_openrel|||backend\bootstrap\bootstrap.c|||407|||/* ----------------
 *		boot_openrel
 *
 * Execute BKI OPEN command.
 * ----------------
 */""",
  """closerel|||backend\bootstrap\bootstrap.c|||452|||/* ----------------
 *		closerel
 * ----------------
 */""",
  """DefineAttr|||backend\bootstrap\bootstrap.c|||489|||/* ----------------
 * DEFINEATTR()
 *
 * define a <field,type> pair
 * if there are n fields in a relation to be created, this routine
 * will be called n times
 * ----------------
 */""",
  """InsertOneTuple|||backend\bootstrap\bootstrap.c|||597|||/* ----------------
 *		InsertOneTuple
 *
 * If objectid is not zero, it is a specific OID to assign to the tuple.
 * Otherwise, an OID will be assigned (if necessary) by heap_insert.
 * ----------------
 */""",
  """InsertOneValue|||backend\bootstrap\bootstrap.c|||625|||/* ----------------
 *		InsertOneValue
 * ----------------
 */""",
  """InsertOneNull|||backend\bootstrap\bootstrap.c|||663|||/* ----------------
 *		InsertOneNull
 * ----------------
 */""",
  """cleanup|||backend\bootstrap\bootstrap.c|||681|||/* ----------------
 *		cleanup
 * ----------------
 */""",
  """populate_typ_list|||backend\bootstrap\bootstrap.c|||694|||/* ----------------
 *		populate_typ_list
 *
 * Load the Typ list by reading pg_type.
 * ----------------
 */""",
  """gettype|||backend\bootstrap\bootstrap.c|||734|||/* ----------------
 *		gettype
 *
 * NB: this is really ugly; it will return an integer index into TypInfo[],
 * and not an OID at all, until the first reference to a type not known in
 * TypInfo[].  At that point it will read and cache pg_type in Typ,
 * and subsequently return a real OID (and set the global pointer Ap to
 * point at the found row in Typ).  So caller must check whether Typ is
 * still NIL to determine what the return value is!
 * ----------------
 */""",
  """boot_get_type_io_data|||backend\bootstrap\bootstrap.c|||805|||/* ----------------
 *		boot_get_type_io_data
 *
 * Obtain type I/O information at bootstrap time.  This intentionally has
 * almost the same API as lsyscache.c's get_type_io_data, except that
 * we only support obtaining the typinput and typoutput routines, not
 * the binary I/O routines.  It is exported so that array_in and array_out
 * can be made to work during early bootstrap.
 * ----------------
 */""",
  """AllocateAttribute|||backend\bootstrap\bootstrap.c|||882|||/* ----------------
 *		AllocateAttribute
 *
 * Note: bootstrap never sets any per-column ACLs, so we only need
 * ATTRIBUTE_FIXED_PART_SIZE space per attribute.
 * ----------------
 */""",
  """index_register|||backend\bootstrap\bootstrap.c|||900|||/*
 *	index_register() -- record an index that has been set up for building
 *						later.
 *
 *		At bootstrap time, we define a bunch of indexes on system catalogs.
 *		We postpone actually building the indexes until just before we're
 *		finished with initialization, however.  This is because the indexes
 *		themselves have catalog entries, and those have to be included in the
 *		indexes on those catalogs.  Doing it in two phases is the simplest
 *		way of making sure the indexes have the right contents at the end.
 */""",
  """build_indices|||backend\bootstrap\bootstrap.c|||950|||/*
 * build_indices -- fill in all the indexes registered earlier
 */""",
  """merge_acl_with_grant|||backend\catalog\aclchk.c|||181|||/*
 * If is_grant is true, adds the given privileges for the list of
 * grantees to the existing old_acl.  If is_grant is false, the
 * privileges for the given grantees are removed from old_acl.
 *
 * NB: the original old_acl is pfree'd.
 */""",
  """restrict_and_check_grant|||backend\catalog\aclchk.c|||240|||/*
 * Restrict the privileges to what we can actually grant, and emit
 * the standards-mandated warning and error messages.
 */""",
  """ExecuteGrantStmt|||backend\catalog\aclchk.c|||391|||/*
 * Called to execute the utility commands GRANT and REVOKE
 */"""
)
INFO:__main__:Extracted 80 methods with comments in this batch
INFO:__main__:Fetching batch: offset=2200, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=2200, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres353[0m: [32mList[0m[[32mString[0m] = List(
  """ExecGrantStmt_oids|||backend\catalog\aclchk.c|||601|||/*
 * ExecGrantStmt_oids
 *
 * Internal entry point for granting and revoking privileges.
 */""",
  """objectNamesToOids|||backend\catalog\aclchk.c|||668|||/*
 * objectNamesToOids
 *
 * Turn a list of object names of a given type into an Oid list.
 *
 * XXX: This function doesn't take any sort of locks on the objects whose
 * names it looks up.  In the face of concurrent DDL, we might easily latch
 * onto an old version of an object, causing the GRANT or REVOKE statement
 * to fail.
 */""",
  """objectsInSchemaToOids|||backend\catalog\aclchk.c|||848|||/*
 * objectsInSchemaToOids
 *
 * Find all objects of a given type in specified schemas, and make a list
 * of their Oids.  We check USAGE privilege on the schemas, but there is
 * no privilege checking on the individual objects here.
 */""",
  """getRelationsInNamespace|||backend\catalog\aclchk.c|||937|||/*
 * getRelationsInNamespace
 *
 * Return Oid list of relations in given namespace filtered by relation kind
 */""",
  """ExecAlterDefaultPrivilegesStmt|||backend\catalog\aclchk.c|||975|||/*
 * ALTER DEFAULT PRIVILEGES statement
 */""",
  """SetDefaultACLsInSchemas|||backend\catalog\aclchk.c|||1160|||/*
 * Process ALTER DEFAULT PRIVILEGES for a list of target schemas
 *
 * All fields of *iacls except nspid were filled already
 */""",
  """SetDefaultACL|||backend\catalog\aclchk.c|||1202|||/*
 * Create or update a pg_default_acl entry
 */""",
  """RemoveRoleFromObjectACL|||backend\catalog\aclchk.c|||1465|||/*
 * RemoveRoleFromObjectACL
 *
 * Used by shdepDropOwned to remove mentions of a role in ACLs.
 *
 * Notice that this doesn't accept an objsubid parameter, which is a bit bogus
 * since the pg_shdepend record that caused us to call it certainly had one.
 * If, for example, pg_shdepend records the existence of a permission on
 * mytable.mycol, this function will effectively issue a REVOKE ALL ON TABLE
 * mytable.  That gets the job done because (per SQL spec) such a REVOKE also
 * revokes per-column permissions.  We could not recreate a situation where
 * the role has table-level but not column-level permissions; but it's okay
 * (for now anyway) because this is only used when we're dropping the role
 * and so all its permissions everywhere must go away.  At worst it's a bit
 * inefficient if the role has column permissions on several columns of the
 * same table.
 */""",
  """expand_col_privileges|||backend\catalog\aclchk.c|||1600|||/*
 * expand_col_privileges
 *
 * OR the specified privilege(s) into per-column array entries for each
 * specified attribute.  The per-column array is indexed starting at
 * FirstLowInvalidHeapAttributeNumber, up to relation's last attribute.
 */""",
  """expand_all_col_privileges|||backend\catalog\aclchk.c|||1633|||/*
 * expand_all_col_privileges
 *
 * OR the specified privilege(s) into per-column array entries for each valid
 * attribute of a relation.  The per-column array is indexed starting at
 * FirstLowInvalidHeapAttributeNumber, up to relation's last attribute.
 */""",
  """ExecGrant_Attribute|||backend\catalog\aclchk.c|||1679|||/*
 *	This processes attributes, but expects to be called from
 *	ExecGrant_Relation, not directly from ExecuteGrantStmt.
 */""",
  """ExecGrant_Relation|||backend\catalog\aclchk.c|||1824|||/*
 *	This processes both sequences and non-sequences.
 */""",
  """aclcheck_error|||backend\catalog\aclchk.c|||2704|||/*
 * Standardized reporting of aclcheck permissions failures.
 *
 * Note: we do not double-quote the %s's below, because many callers
 * supply strings that might be already quoted.
 */""",
  """aclcheck_error_type|||backend\catalog\aclchk.c|||3023|||/*
 * Special common handling for types: use element type instead of array type,
 * and format nicely
 */""",
  """pg_aclmask|||backend\catalog\aclchk.c|||3035|||/*
 * Relay for the various pg_*_mask routines depending on object kind
 */""",
  """object_aclmask|||backend\catalog\aclchk.c|||3100|||/*
 * Generic routine for examining a user's privileges for an object
 */""",
  """object_aclmask_ext|||backend\catalog\aclchk.c|||3111|||/*
 * Generic routine for examining a user's privileges for an object,
 * with is_missing
 */""",
  """pg_attribute_aclmask|||backend\catalog\aclchk.c|||3203|||/*
 * Routine for examining a user's privileges for a column
 *
 * Note: this considers only privileges granted specifically on the column.
 * It is caller's responsibility to take relation-level privileges into account
 * as appropriate.  (For the same reason, we have no special case for
 * superuser-ness here.)
 */""",
  """pg_attribute_aclmask_ext|||backend\catalog\aclchk.c|||3214|||/*
 * Routine for examining a user's privileges for a column, with is_missing
 */""",
  """pg_class_aclmask|||backend\catalog\aclchk.c|||3328|||/*
 * Exported routine for examining a user's privileges for a table
 */""",
  """pg_class_aclmask_ext|||backend\catalog\aclchk.c|||3338|||/*
 * Routine for examining a user's privileges for a table, with is_missing
 */""",
  """pg_parameter_aclmask|||backend\catalog\aclchk.c|||3468|||/*
 * Routine for examining a user's privileges for a configuration
 * parameter (GUC), identified by GUC name.
 */""",
  """pg_parameter_acl_aclmask|||backend\catalog\aclchk.c|||3532|||/*
 * Routine for examining a user's privileges for a configuration
 * parameter (GUC), identified by the OID of its pg_parameter_acl entry.
 */""",
  """pg_largeobject_aclmask_snapshot|||backend\catalog\aclchk.c|||3591|||/*
 * Routine for examining a user's privileges for a largeobject
 *
 * When a large object is opened for reading, it is opened relative to the
 * caller's snapshot, but when it is opened for writing, a current
 * MVCC snapshot will be used.  See doc/src/sgml/lobj.sgml.  This function
 * takes a snapshot argument so that the permissions check can be made
 * relative to the same snapshot that will be used to read the underlying
 * data.  The caller will actually pass NULL for an instantaneous MVCC
 * snapshot, since all we do with the snapshot argument is pass it through
 * to systable_beginscan().
 */""",
  """pg_namespace_aclmask_ext|||backend\catalog\aclchk.c|||3664|||/*
 * Routine for examining a user's privileges for a namespace, with is_missing
 */""",
  """pg_type_aclmask_ext|||backend\catalog\aclchk.c|||3766|||/*
 * Routine for examining a user's privileges for a type, with is_missing
 */""",
  """object_aclcheck|||backend\catalog\aclchk.c|||3892|||/*
 * Exported generic routine for checking a user's access privileges to an object
 */""",
  """object_aclcheck_ext|||backend\catalog\aclchk.c|||3902|||/*
 * Exported generic routine for checking a user's access privileges to an
 * object, with is_missing
 */""",
  """pg_attribute_aclcheck|||backend\catalog\aclchk.c|||3924|||/*
 * Exported routine for checking a user's access privileges to a column
 *
 * Returns ACLCHECK_OK if the user has any of the privileges identified by
 * 'mode'; otherwise returns a suitable error code (in practice, always
 * ACLCHECK_NO_PRIV).
 *
 * As with pg_attribute_aclmask, only privileges granted directly on the
 * column are considered here.
 */""",
  """pg_attribute_aclcheck_ext|||backend\catalog\aclchk.c|||3936|||/*
 * Exported routine for checking a user's access privileges to a column,
 * with is_missing
 */""",
  """pg_attribute_aclcheck_all|||backend\catalog\aclchk.c|||3966|||/*
 * Exported routine for checking a user's access privileges to any/all columns
 *
 * If 'how' is ACLMASK_ANY, then returns ACLCHECK_OK if user has any of the
 * privileges identified by 'mode' on any non-dropped column in the relation;
 * otherwise returns a suitable error code (in practice, always
 * ACLCHECK_NO_PRIV).
 *
 * If 'how' is ACLMASK_ALL, then returns ACLCHECK_OK if user has any of the
 * privileges identified by 'mode' on each non-dropped column in the relation
 * (and there must be at least one such column); otherwise returns a suitable
 * error code (in practice, always ACLCHECK_NO_PRIV).
 *
 * As with pg_attribute_aclmask, only privileges granted directly on the
 * column(s) are considered here.
 *
 * Note: system columns are not considered here; there are cases where that
 * might be appropriate but there are also cases where it wouldn't.
 */""",
  """pg_attribute_aclcheck_all_ext|||backend\catalog\aclchk.c|||3977|||/*
 * Exported routine for checking a user's access privileges to any/all columns,
 * with is_missing
 */""",
  """pg_class_aclcheck|||backend\catalog\aclchk.c|||4095|||/*
 * Exported routine for checking a user's access privileges to a table
 *
 * Returns ACLCHECK_OK if the user has any of the privileges identified by
 * 'mode'; otherwise returns a suitable error code (in practice, always
 * ACLCHECK_NO_PRIV).
 */""",
  """pg_class_aclcheck_ext|||backend\catalog\aclchk.c|||4105|||/*
 * Exported routine for checking a user's access privileges to a table,
 * with is_missing
 */""",
  """pg_parameter_aclcheck|||backend\catalog\aclchk.c|||4120|||/*
 * Exported routine for checking a user's access privileges to a configuration
 * parameter (GUC), identified by GUC name.
 */""",
  """pg_largeobject_aclcheck_snapshot|||backend\catalog\aclchk.c|||4132|||/*
 * Exported routine for checking a user's access privileges to a largeobject
 */""",
  """object_ownercheck|||backend\catalog\aclchk.c|||4146|||/*
 * Generic ownership check for an object
 */""",
  """has_createrole_privilege|||backend\catalog\aclchk.c|||4227|||/*
 * Check whether specified role has CREATEROLE privilege (or is a superuser)
 *
 * Note: roles do not have owners per se; instead we use this test in
 * places where an ownership-like permissions test is needed for a role.
 * Be sure to apply it to the role trying to do the operation, not the
 * role being operated on!	Also note that this generally should not be
 * considered enough privilege if the target role is a superuser.
 * (We don't handle that consideration here because we want to give a
 * separate error message for such cases, so the caller has to deal with it.)
 */""",
  """has_bypassrls_privilege|||backend\catalog\aclchk.c|||4246|||/*
 * Check whether specified role has CREATEROLE privilege (or is a superuser)
 *
 * Note: roles do not have owners per se; instead we use this test in
 * places where an ownership-like permissions test is needed for a role.
 * Be sure to apply it to the role trying to do the operation, not the
 * role being operated on!	Also note that this generally should not be
 * considered enough privilege if the target role is a superuser.
 * (We don't handle that consideration here because we want to give a
 * separate error message for such cases, so the caller has to deal with it.)
 */""",
  """get_default_acl_internal|||backend\catalog\aclchk.c|||4270|||/*
 * Fetch pg_default_acl entry for given role, namespace and object type
 * (object type must be given in pg_default_acl's encoding).
 * Returns NULL if no such entry.
 */""",
  """get_user_default_acl|||backend\catalog\aclchk.c|||4305|||/*
 * Get default permissions for newly created object within given schema
 *
 * Returns NULL if built-in system defaults should be used.
 *
 * If the result is not NULL, caller must call recordDependencyOnNewAcl
 * once the OID of the new object is known.
 */""",
  """recordDependencyOnNewAcl|||backend\catalog\aclchk.c|||4381|||/*
 * Record dependencies on roles mentioned in a new object's ACL.
 */""",
  """recordExtObjInitPriv|||backend\catalog\aclchk.c|||4408|||/*
 * Record initial privileges for the top-level object passed in.
 *
 * For the object passed in, this will record its ACL (if any) and the ACLs of
 * any sub-objects (eg: columns) into pg_init_privs.
 */""",
  """removeExtObjInitPriv|||backend\catalog\aclchk.c|||4572|||/*
 * For the object passed in, remove its ACL and the ACLs of any object subIds
 * from pg_init_privs (via recordExtensionInitPrivWorker()).
 */""",
  """recordExtensionInitPriv|||backend\catalog\aclchk.c|||4655|||/*
 * Record initial ACL for an extension object
 *
 * Can be called at any time, we check if 'creating_extension' is set and, if
 * not, exit immediately.
 *
 * Pass in the object OID, the OID of the class (the OID of the table which
 * the object is defined in) and the 'sub' id of the object (objsubid), if
 * any.  If there is no 'sub' id (they are currently only used for columns of
 * tables) then pass in '0'.  Finally, pass in the complete ACL to store.
 *
 * If an ACL already exists for this object/sub-object then we will replace
 * it with what is passed in.
 *
 * Passing in NULL for 'new_acl' will result in the entry for the object being
 * removed, if one is found.
 */""",
  """recordExtensionInitPrivWorker|||backend\catalog\aclchk.c|||4684|||/*
 * Record initial ACL for an extension object, worker.
 *
 * This will perform a wholesale replacement of the entire ACL for the object
 * passed in, therefore be sure to pass in the complete new ACL to use.
 *
 * Generally speaking, do *not* use this function directly but instead use
 * recordExtensionInitPriv(), which checks if 'creating_extension' is set.
 * This function does *not* check if 'creating_extension' is set as it is also
 * used when an object is added to or removed from an extension via ALTER
 * EXTENSION ... ADD/DROP.
 */""",
  """ReplaceRoleInInitPriv|||backend\catalog\aclchk.c|||4812|||/*
 * ReplaceRoleInInitPriv
 *
 * Used by shdepReassignOwned to replace mentions of a role in pg_init_privs.
 */""",
  """RemoveRoleFromInitPriv|||backend\catalog\aclchk.c|||4921|||/*
 * RemoveRoleFromInitPriv
 *
 * Used by shdepDropOwned to remove mentions of a role in pg_init_privs.
 */""",
  """IsSystemRelation|||backend\catalog\catalog.c|||72|||/*
 * IsSystemRelation
 *		True iff the relation is either a system catalog or a toast table.
 *		See IsCatalogRelation for the exact definition of a system catalog.
 *
 *		We treat toast tables of user relations as "system relations" for
 *		protection purposes, e.g. you can't change their schemas without
 *		special permissions.  Therefore, most uses of this function are
 *		checking whether allow_system_table_mods restrictions apply.
 *		For other purposes, consider whether you shouldn't be using
 *		IsCatalogRelation instead.
 *
 *		This function does not perform any catalog accesses.
 *		Some callers rely on that!
 */""",
  """IsSystemClass|||backend\catalog\catalog.c|||84|||/*
 * IsSystemClass
 *		Like the above, but takes a Form_pg_class as argument.
 *		Used when we do not want to open the relation and have to
 *		search pg_class directly.
 */""",
  """IsCatalogRelation|||backend\catalog\catalog.c|||102|||/*
 * IsCatalogRelation
 *		True iff the relation is a system catalog.
 *
 *		By a system catalog, we mean one that is created during the bootstrap
 *		phase of initdb.  That includes not just the catalogs per se, but
 *		also their indexes, and TOAST tables and indexes if any.
 *
 *		This function does not perform any catalog accesses.
 *		Some callers rely on that!
 */""",
  """IsCatalogRelationOid|||backend\catalog\catalog.c|||119|||/*
 * IsCatalogRelationOid
 *		True iff the relation identified by this OID is a system catalog.
 *
 *		By a system catalog, we mean one that is created during the bootstrap
 *		phase of initdb.  That includes not just the catalogs per se, but
 *		also their indexes, and TOAST tables and indexes if any.
 *
 *		This function does not perform any catalog accesses.
 *		Some callers rely on that!
 */""",
  """IsInplaceUpdateRelation|||backend\catalog\catalog.c|||151|||/*
 * IsInplaceUpdateRelation
 *		True iff core code performs inplace updates on the relation.
 *
 *		This is used for assertions and for making the executor follow the
 *		locking protocol described at README.tuplock section "Locking to write
 *		inplace-updated tables".  Extensions may inplace-update other heap
 *		tables, but concurrent SQL UPDATE on the same table may overwrite
 *		those modifications.
 *
 *		The executor can assume these are not partitions or partitioned and
 *		have no triggers.
 */""",
  """IsInplaceUpdateOid|||backend\catalog\catalog.c|||161|||/*
 * IsInplaceUpdateOid
 *		Like the above, but takes an OID as argument.
 */""",
  """IsToastRelation|||backend\catalog\catalog.c|||174|||/*
 * IsToastRelation
 *		True iff relation is a TOAST support relation (or index).
 *
 *		Does not perform any catalog accesses.
 */""",
  """IsToastClass|||backend\catalog\catalog.c|||194|||/*
 * IsToastClass
 *		Like the above, but takes a Form_pg_class as argument.
 *		Used when we do not want to open the relation and have to
 *		search pg_class directly.
 */""",
  """IsCatalogNamespace|||backend\catalog\catalog.c|||211|||/*
 * IsCatalogNamespace
 *		True iff namespace is pg_catalog.
 *
 *		Does not perform any catalog accesses.
 *
 * NOTE: the reason this isn't a macro is to avoid having to include
 * catalog/pg_namespace.h in a lot of places.
 */""",
  """IsToastNamespace|||backend\catalog\catalog.c|||229|||/*
 * IsToastNamespace
 *		True iff namespace is pg_toast or my temporary-toast-table namespace.
 *
 *		Does not perform any catalog accesses.
 *
 * Note: this will return false for temporary-toast-table namespaces belonging
 * to other backends.  Those are treated the same as other backends' regular
 * temp table namespaces, and access is prevented where appropriate.
 * If you need to check for those, you may be able to use isAnyTempNamespace,
 * but beware that that does involve a catalog access.
 */""",
  """IsReservedName|||backend\catalog\catalog.c|||246|||/*
 * IsReservedName
 *		True iff name starts with the pg_ prefix.
 *
 *		For some classes of objects, the prefix pg_ is reserved for
 *		system objects only.  As of 8.0, this was only true for
 *		schema and tablespace names.  With 9.6, this is also true
 *		for roles.
 */""",
  """IsSharedRelation|||backend\catalog\catalog.c|||272|||/*
 * IsSharedRelation
 *		Given the OID of a relation, determine whether it's supposed to be
 *		shared across an entire database cluster.
 *
 * In older releases, this had to be hard-wired so that we could compute the
 * locktag for a relation and lock it before examining its catalog entry.
 * Since we now have MVCC catalog access, the race conditions that made that
 * a hard requirement are gone, so we could look at relaxing this restriction.
 * However, if we scanned the pg_class entry to find relisshared, and only
 * then locked the relation, pg_class could get updated in the meantime,
 * forcing us to scan the relation again, which would definitely be complex
 * and might have undesirable performance consequences.  Fortunately, the set
 * of shared relations is fairly static, so a hand-maintained list of their
 * OIDs isn't completely impractical.
 */""",
  """IsPinnedObject|||backend\catalog\catalog.c|||342|||/*
 * IsPinnedObject
 *		Given the class + OID identity of a database object, report whether
 *		it is "pinned", that is not droppable because the system requires it.
 *
 * We used to represent this explicitly in pg_depend, but that proved to be
 * an undesirable amount of overhead, so now we rely on an OID range test.
 */""",
  """GetNewOidWithIndex|||backend\catalog\catalog.c|||420|||/*
 * GetNewOidWithIndex
 *		Generate a new OID that is unique within the system relation.
 *
 * Since the OID is not immediately inserted into the table, there is a
 * race condition here; but a problem could occur only if someone else
 * managed to cycle through 2^32 OIDs and generate the same OID before we
 * finish inserting our row.  This seems unlikely to be a problem.  Note
 * that if we had to *commit* the row to end the race condition, the risk
 * would be rather higher; therefore we use SnapshotAny in the test, so that
 * we will see uncommitted rows.  (We used to use SnapshotDirty, but that has
 * the disadvantage that it ignores recently-deleted rows, creating a risk
 * of transient conflicts for as long as our own MVCC snapshots think a
 * recently-deleted row is live.  The risk is far higher when selecting TOAST
 * OIDs, because SnapshotToast considers dead rows as active indefinitely.)
 *
 * Note that we are effectively assuming that the table has a relatively small
 * number of entries (much less than 2^32) and there aren't very long runs of
 * consecutive existing OIDs.  This is a mostly reasonable assumption for
 * system catalogs.
 *
 * Caller must have a suitable lock on the relation.
 */""",
  """GetNewRelFileNumber|||backend\catalog\catalog.c|||529|||/*
 * GetNewRelFileNumber
 *		Generate a new relfilenumber that is unique within the
 *		database of the given tablespace.
 *
 * If the relfilenumber will also be used as the relation's OID, pass the
 * opened pg_class catalog, and this routine will guarantee that the result
 * is also an unused OID within pg_class.  If the result is to be used only
 * as a relfilenumber for an existing relation, pass NULL for pg_class.
 *
 * As with GetNewOidWithIndex(), there is some theoretical risk of a race
 * condition, but it doesn't seem worth worrying about.
 *
 * Note: we don't support using this in bootstrap mode.  All relations
 * created by bootstrap have preassigned OIDs, so there's no need.
 */""",
  """pg_nextoid|||backend\catalog\catalog.c|||615|||/*
 * SQL callable interface for GetNewOidWithIndex().  Outside of initdb's
 * direct insertions into catalog tables, and recovering from corruption, this
 * should rarely be needed.
 *
 * Function is intentionally not documented in the user facing docs.
 */""",
  """pg_stop_making_pinned_objects|||backend\catalog\catalog.c|||694|||/*
 * SQL callable interface for StopGeneratingPinnedObjectIds().
 *
 * This is only to be used by initdb, so it's intentionally not documented in
 * the user facing docs.
 */""",
  """deleteObjectsInList|||backend\catalog\dependency.c|||184|||/*
 * Go through the objects given running the final actions on them, and execute
 * the actual deletion.
 */""",
  """performDeletion|||backend\catalog\dependency.c|||272|||/*
 * performDeletion: attempt to drop the specified object.  If CASCADE
 * behavior is specified, also drop any dependent objects (recursively).
 * If RESTRICT behavior is specified, error out if there are any dependent
 * objects, except for those that should be implicitly dropped anyway
 * according to the dependency type.
 *
 * This is the outer control routine for all forms of DROP that drop objects
 * that can participate in dependencies.  Note that performMultipleDeletions
 * is a variant on the same theme; if you change anything here you'll likely
 * need to fix that too.
 *
 * Bits in the flags argument can include:
 *
 * PERFORM_DELETION_INTERNAL: indicates that the drop operation is not the
 * direct result of a user-initiated action.  For example, when a temporary
 * schema is cleaned out so that a new backend can use it, or when a column
 * default is dropped as an intermediate step while adding a new one, that's
 * an internal operation.  On the other hand, when we drop something because
 * the user issued a DROP statement against it, that's not internal. Currently
 * this suppresses calling event triggers and making some permissions checks.
 *
 * PERFORM_DELETION_CONCURRENTLY: perform the drop concurrently.  This does
 * not currently work for anything except dropping indexes; don't set it for
 * other object types or you may get strange results.
 *
 * PERFORM_DELETION_QUIETLY: reduce message level from NOTICE to DEBUG2.
 *
 * PERFORM_DELETION_SKIP_ORIGINAL: do not delete the specified object(s),
 * but only what depends on it/them.
 *
 * PERFORM_DELETION_SKIP_EXTENSIONS: do not delete extensions, even when
 * deleting objects that are part of an extension.  This should generally
 * be used only when dropping temporary objects.
 *
 * PERFORM_DELETION_CONCURRENT_LOCK: perform the drop normally but with a lock
 * as if it were concurrent.  This is used by REINDEX CONCURRENTLY.
 *
 */""",
  """performMultipleDeletions|||backend\catalog\dependency.c|||331|||/*
 * performMultipleDeletions: Similar to performDeletion, but act on multiple
 * objects at once.
 *
 * The main difference from issuing multiple performDeletion calls is that the
 * list of objects that would be implicitly dropped, for each object to be
 * dropped, is the union of the implicit-object list for all objects.  This
 * makes each check be more relaxed.
 */""",
  """findDependentObjects|||backend\catalog\dependency.c|||431|||/*
 * findDependentObjects - find all objects that depend on 'object'
 *
 * For every object that depends on the starting object, acquire a deletion
 * lock on the object, add it to targetObjects (if not already there),
 * and recursively find objects that depend on it.  An object's dependencies
 * will be placed into targetObjects before the object itself; this means
 * that the finished list's order represents a safe deletion order.
 *
 * The caller must already have a deletion lock on 'object' itself,
 * but must not have added it to targetObjects.  (Note: there are corner
 * cases where we won't add the object either, and will also release the
 * caller-taken lock.  This is a bit ugly, but the API is set up this way
 * to allow easy rechecking of an object's liveness after we lock it.  See
 * notes within the function.)
 *
 * When dropping a whole object (subId = 0), we find dependencies for
 * its sub-objects too.
 *
 *	object: the object to add to targetObjects and find dependencies on
 *	objflags: flags to be ORed into the object's targetObjects entry
 *	flags: PERFORM_DELETION_xxx flags for the deletion operation as a whole
 *	stack: list of objects being visited in current recursion; topmost item
 *			is the object that we recursed from (NULL for external callers)
 *	targetObjects: list of objects that are scheduled to be deleted
 *	pendingObjects: list of other objects slated for destruction, but
 *			not necessarily in targetObjects yet (can be NULL if none)
 *	*depRel: already opened pg_depend relation
 *
 * Note: objflags describes the reason for visiting this particular object
 * at this time, and is not passed down when recursing.  The flags argument
 * is passed down, since it describes what we're doing overall.
 */""",
  """reportDependentObjects|||backend\catalog\dependency.c|||979|||/*
 * reportDependentObjects - report about dependencies, and fail if RESTRICT
 *
 * Tell the user about dependent objects that we are going to delete
 * (or would need to delete, but are prevented by RESTRICT mode);
 * then error out if there are any and it's not CASCADE mode.
 *
 *	targetObjects: list of objects that are scheduled to be deleted
 *	behavior: RESTRICT or CASCADE
 *	flags: other flags for the deletion operation
 *	origObject: base object of deletion, or NULL if not available
 *		(the latter case occurs in DROP OWNED)
 */""",
  """DropObjectById|||backend\catalog\dependency.c|||1188|||/*
 * Drop an object by OID.  Works for most catalogs, if no special processing
 * is needed.
 */""",
  """deleteOneObject|||backend\catalog\dependency.c|||1245|||/*
 * deleteOneObject: delete a single object for performDeletion.
 *
 * *depRel is the already-open pg_depend relation.
 */""",
  """doDeletion|||backend\catalog\dependency.c|||1351|||/*
 * doDeletion: actually delete a single object
 */""",
  """AcquireDeletionLock|||backend\catalog\dependency.c|||1495|||/*
 * AcquireDeletionLock - acquire a suitable lock for deleting an object
 *
 * Accepts the same flags as performDeletion (though currently only
 * PERFORM_DELETION_CONCURRENTLY does anything).
 *
 * We use LockRelation for relations, and otherwise LockSharedObject or
 * LockDatabaseObject as appropriate for the object type.
 */""",
  """ReleaseDeletionLock|||backend\catalog\dependency.c|||1527|||/*
 * ReleaseDeletionLock - release an object deletion lock
 *
 * Companion to AcquireDeletionLock.
 */""",
  """recordDependencyOnExpr|||backend\catalog\dependency.c|||1552|||/*
 * recordDependencyOnExpr - find expression dependencies
 *
 * This is used to find the dependencies of rules, constraint expressions,
 * etc.
 *
 * Given an expression or query in node-tree form, find all the objects
 * it refers to (tables, columns, operators, functions, etc).  Record
 * a dependency of the specified type from the given depender object
 * to each object mentioned in the expression.
 *
 * rtable is the rangetable to be used to interpret Vars with varlevelsup=0.
 * It can be NIL if no such variables are expected.
 */""",
  """recordDependencyOnSingleRelExpr|||backend\catalog\dependency.c|||1595|||/*
 * recordDependencyOnSingleRelExpr - find expression dependencies
 *
 * As above, but only one relation is expected to be referenced (with
 * varno = 1 and varlevelsup = 0).  Pass the relation OID instead of a
 * range table.  An additional frammish is that dependencies on that
 * relation's component columns will be marked with 'self_behavior',
 * whereas 'behavior' is used for everything else; also, if 'reverse_self'
 * is true, those dependencies are reversed so that the columns are made
 * to depend on the table not vice versa.
 *
 * NOTE: the caller should ensure that a whole-table dependency on the
 * specified relation is created separately, if one is needed.  In particular,
 * a whole-row Var "relation.*" will not cause this routine to emit any
 * dependency item.  This is appropriate behavior for subexpressions of an
 * ordinary query, so other cases need to cope as necessary.
 */""",
  """find_expr_references_walker|||backend\catalog\dependency.c|||1697|||/*
 * Recursively search an expression tree for object references.
 *
 * Note: in many cases we do not need to create dependencies on the datatypes
 * involved in an expression, because we'll have an indirect dependency via
 * some other object.  For instance Var nodes depend on a column which depends
 * on the datatype, and OpExpr nodes depend on the operator which depends on
 * the datatype.  However we do need a type dependency if there is no such
 * indirect dependency, as for example in Const and CoerceToDomain nodes.
 *
 * Similarly, we don't need to create dependencies on collations except where
 * the collation is being freshly introduced to the expression.
 */""",
  """process_function_rte_ref|||backend\catalog\dependency.c|||2320|||/*
 * find_expr_references_walker subroutine: handle a Var reference
 * to an RTE_FUNCTION RTE
 */""",
  """eliminate_duplicate_dependencies|||backend\catalog\dependency.c|||2382|||/*
 * Given an array of dependency references, eliminate any duplicates.
 */""",
  """object_address_comparator|||backend\catalog\dependency.c|||2442|||/*
 * qsort comparator for ObjectAddress items
 */""",
  """new_object_addresses|||backend\catalog\dependency.c|||2486|||/*
 * Routines for handling an expansible array of ObjectAddress items.
 *
 * new_object_addresses: create a new ObjectAddresses array.
 */""",
  """add_object_address|||backend\catalog\dependency.c|||2505|||/*
 * Add an entry to an ObjectAddresses array.
 */""",
  """add_exact_object_address|||backend\catalog\dependency.c|||2532|||/*
 * Add an entry to an ObjectAddresses array.
 *
 * As above, but specify entry exactly.
 */""",
  """add_exact_object_address_extra|||backend\catalog\dependency.c|||2557|||/*
 * Add an entry to an ObjectAddresses array.
 *
 * As above, but specify entry exactly and provide some "extra" data too.
 */""",
  """object_address_present|||backend\catalog\dependency.c|||2592|||/*
 * Test whether an object is present in an ObjectAddresses array.
 *
 * We return "true" if object is a subobject of something in the array, too.
 */""",
  """object_address_present_add_flags|||backend\catalog\dependency.c|||2618|||/*
 * As above, except that if the object is present then also OR the given
 * flags into its associated extra data (which must exist).
 */""",
  """stack_address_present_add_flags|||backend\catalog\dependency.c|||2691|||/*
 * Similar to above, except we search an ObjectAddressStack.
 */""",
  """record_object_address_dependencies|||backend\catalog\dependency.c|||2741|||/*
 * Record multiple dependencies from an ObjectAddresses array, after first
 * removing any duplicates.
 */""",
  """sort_object_addresses|||backend\catalog\dependency.c|||2760|||/*
 * Sort the items in an ObjectAddresses array.
 *
 * The major sort key is OID-descending, so that newer objects will be listed
 * first in most cases.  This is primarily useful for ensuring stable outputs
 * from regression tests; it's not recommended if the order of the objects is
 * determined by user input, such as the order of targets in a DROP command.
 */""",
  """free_object_addresses|||backend\catalog\dependency.c|||2772|||/*
 * Clean up when done with an ObjectAddresses array.
 */""",
  """DeleteInitPrivs|||backend\catalog\dependency.c|||2784|||/*
 * delete initial ACL for extension objects
 */""",
  """SystemAttributeDefinition|||backend\catalog\heap.c|||240|||/*
 * This function returns a Form_pg_attribute pointer for a system attribute.
 * Note that we elog if the presented attno is invalid, which would only
 * happen if there's a problem upstream.
 */""",
  """SystemAttributeByName|||backend\catalog\heap.c|||252|||/*
 * If the given name is a system attribute name, return a Form_pg_attribute
 * pointer for a prototype definition.  If not, return NULL.
 */""",
  """heap_create|||backend\catalog\heap.c|||289|||/* ----------------------------------------------------------------
 *		heap_create		- Create an uncataloged heap relation
 *
 *		Note API change: the caller must now always provide the OID
 *		to use for the relation.  The relfilenumber may be (and in
 *		the simplest cases is) left unspecified.
 *
 *		create_storage indicates whether or not to create the storage.
 *		However, even if create_storage is true, no storage will be
 *		created if the relkind is one that doesn't have storage.
 *
 *		rel->rd_rel is initialized by RelationBuildLocalRelation,
 *		and is mostly zeroes at return.
 * ----------------------------------------------------------------
 */""",
  """CheckAttributeNamesTypes|||backend\catalog\heap.c|||456|||/* --------------------------------
 *		CheckAttributeNamesTypes
 *
 *		this is used to make certain the tuple descriptor contains a
 *		valid set of attribute names and datatypes.  a problem simply
 *		generates ereport(ERROR) which aborts the current transaction.
 *
 *		relkind is the relkind of the relation to be created.
 *		flags controls which datatypes are allowed, cf CheckAttributeType.
 * --------------------------------
 */""",
  """CheckAttributeType|||backend\catalog\heap.c|||548|||/* --------------------------------
 *		CheckAttributeType
 *
 *		Verify that the proposed datatype of an attribute is legal.
 *		This is needed mainly because there are types (and pseudo-types)
 *		in the catalogs that we do not support as elements of real tuples.
 *		We also check some other properties required of a table column.
 *
 * If the attribute is being proposed for addition to an existing table or
 * composite type, pass a one-element list of the rowtype OID as
 * containing_rowtypes.  When checking a to-be-created rowtype, it's
 * sufficient to pass NIL, because there could not be any recursive reference
 * to a not-yet-existing rowtype.
 *
 * flags is a bitmask controlling which datatypes we allow.  For the most
 * part, pseudo-types are disallowed as attribute types, but there are some
 * exceptions: ANYARRAYOID, RECORDOID, and RECORDARRAYOID can be allowed
 * in some cases.  (This works because values of those type classes are
 * self-identifying to some extent.  However, RECORDOID and RECORDARRAYOID
 * are reliably identifiable only within a session, since the identity info
 * may use a typmod that is only locally assigned.  The caller is expected
 * to know whether these cases are safe.)
 *
 * flags can also control the phrasing of the error messages.  If
 * CHKATYPE_IS_PARTKEY is specified, "attname" should be a partition key
 * column number as text, not a real column name.
 * --------------------------------
 */""",
  """InsertPgAttributeTuples|||backend\catalog\heap.c|||702|||/*
 * InsertPgAttributeTuples
 *		Construct and insert a set of tuples in pg_attribute.
 *
 * Caller has already opened and locked pg_attribute.  tupdesc contains the
 * attributes to insert.  attcacheoff is always initialized to -1.
 * tupdesc_extra supplies the values for certain variable-length/nullable
 * pg_attribute fields and must contain the same number of elements as tupdesc
 * or be NULL.  The other variable-length fields of pg_attribute are always
 * initialized to null values.
 *
 * indstate is the index state for CatalogTupleInsertWithInfo.  It can be
 * passed as NULL, in which case we'll fetch the necessary info.  (Don't do
 * this when inserting multiple attributes, because it's a tad more
 * expensive.)
 *
 * new_rel_oid is the relation OID assigned to the attributes inserted.
 * If set to InvalidOid, the relation OID from tupdesc is used instead.
 */""",
  """AddNewAttributeTuples|||backend\catalog\heap.c|||820|||/* --------------------------------
 *		AddNewAttributeTuples
 *
 *		this registers the new relation's schema by adding
 *		tuples to pg_attribute.
 * --------------------------------
 */""",
  """InsertPgClassTuple|||backend\catalog\heap.c|||895|||/* --------------------------------
 *		InsertPgClassTuple
 *
 *		Construct and insert a new tuple in pg_class.
 *
 * Caller has already opened and locked pg_class.
 * Tuple data is taken from new_rel_desc->rd_rel, except for the
 * variable-width fields which are not present in a cached reldesc.
 * relacl and reloptions are passed in Datum form (to avoid having
 * to reference the data types in heap.h).  Pass (Datum) 0 to set them
 * to NULL.
 * --------------------------------
 */"""
)
INFO:__main__:Extracted 48 methods with comments in this batch
INFO:__main__:Fetching batch: offset=2300, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=2300, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres354[0m: [32mList[0m[[32mString[0m] = List(
  """AddNewRelationTuple|||backend\catalog\heap.c|||968|||/* --------------------------------
 *		AddNewRelationTuple
 *
 *		this registers the new relation in the catalogs by
 *		adding a tuple to pg_class.
 * --------------------------------
 */""",
  """AddNewRelationType|||backend\catalog\heap.c|||1026|||/* --------------------------------
 *		AddNewRelationType -
 *
 *		define a composite type corresponding to the new relation
 * --------------------------------
 */""",
  """heap_create_with_catalog|||backend\catalog\heap.c|||1104|||/* --------------------------------
 *		heap_create_with_catalog
 *
 *		creates a new cataloged relation.  see comments above.
 *
 * Arguments:
 *	relname: name to give to new rel
 *	relnamespace: OID of namespace it goes in
 *	reltablespace: OID of tablespace it goes in
 *	relid: OID to assign to new rel, or InvalidOid to select a new OID
 *	reltypeid: OID to assign to rel's rowtype, or InvalidOid to select one
 *	reloftypeid: if a typed table, OID of underlying type; else InvalidOid
 *	ownerid: OID of new rel's owner
 *	accessmtd: OID of new rel's access method
 *	tupdesc: tuple descriptor (source of column definitions)
 *	cooked_constraints: list of precooked check constraints and defaults
 *	relkind: relkind for new rel
 *	relpersistence: rel's persistence status (permanent, temp, or unlogged)
 *	shared_relation: true if it's to be a shared relation
 *	mapped_relation: true if the relation will use the relfilenumber map
 *	oncommit: ON COMMIT marking (only relevant if it's a temp table)
 *	reloptions: reloptions in Datum form, or (Datum) 0 if none
 *	use_user_acl: true if should look for user-defined default permissions;
 *		if false, relacl is always set NULL
 *	allow_system_table_mods: true to allow creation in system namespaces
 *	is_internal: is this a system-generated catalog?
 *
 * Output parameters:
 *	typaddress: if not null, gets the object address of the new pg_type entry
 *	(this must be null if the relkind is one that doesn't get a pg_type entry)
 *
 * Returns the OID of the new relation
 * --------------------------------
 */""",
  """RelationRemoveInheritance|||backend\catalog\heap.c|||1525|||/*
 *		RelationRemoveInheritance
 *
 * Formerly, this routine checked for child relations and aborted the
 * deletion if any were found.  Now we rely on the dependency mechanism
 * to check for or delete child relations.  By the time we get here,
 * there are no children and we need only remove any pg_inherits rows
 * linking this relation to its parent(s).
 */""",
  """DeleteRelationTuple|||backend\catalog\heap.c|||1558|||/*
 *		DeleteRelationTuple
 *
 * Remove pg_class row for the given relid.
 *
 * Note: this is shared by relation deletion and index deletion.  It's
 * not intended for use anyplace else.
 */""",
  """DeleteAttributeTuples|||backend\catalog\heap.c|||1587|||/*
 *		DeleteAttributeTuples
 *
 * Remove pg_attribute rows for the given relid.
 *
 * Note: this is shared by relation deletion and index deletion.  It's
 * not intended for use anyplace else.
 */""",
  """DeleteSystemAttributeTuples|||backend\catalog\heap.c|||1624|||/*
 *		DeleteSystemAttributeTuples
 *
 * Remove pg_attribute rows for system columns of the given relid.
 *
 * Note: this is only used when converting a table to a view.  Views don't
 * have system columns, so we should remove them from pg_attribute.
 */""",
  """RemoveAttributeById|||backend\catalog\heap.c|||1665|||/*
 *		RemoveAttributeById
 *
 * This is the guts of ALTER TABLE DROP COLUMN: actually mark the attribute
 * deleted in pg_attribute.  We also remove pg_statistic entries for it.
 * (Everything else needed, such as getting rid of any pg_attrdef entry,
 * is handled by dependency.c.)
 */""",
  """heap_drop_with_catalog|||backend\catalog\heap.c|||1766|||/*
 * heap_drop_with_catalog	- removes specified relation from catalogs
 *
 * Note that this routine is not responsible for dropping objects that are
 * linked to the pg_class entry via dependencies (for example, indexes and
 * constraints).  Those are deleted by the dependency-tracing logic in
 * dependency.c before control gets here.  In general, therefore, this routine
 * should never be called directly; go through performDeletion() instead.
 */""",
  """RelationClearMissing|||backend\catalog\heap.c|||1946|||/*
 * RelationClearMissing
 *
 * Set atthasmissing and attmissingval to false/null for all attributes
 * where they are currently set. This can be safely and usefully done if
 * the table is rewritten (e.g. by VACUUM FULL or CLUSTER) where we know there
 * are no rows left with less than a full complement of attributes.
 *
 * The caller must have an AccessExclusive lock on the relation.
 */""",
  """StoreAttrMissingVal|||backend\catalog\heap.c|||2012|||/*
 * StoreAttrMissingVal
 *
 * Set the missing value of a single attribute.
 */""",
  """SetAttrMissing|||backend\catalog\heap.c|||2068|||/*
 * SetAttrMissing
 *
 * Set the missing value of a single attribute. This should only be used by
 * binary upgrade. Takes an AccessExclusive lock on the relation owning the
 * attribute.
 */""",
  """StoreRelCheck|||backend\catalog\heap.c|||2129|||/*
 * Store a check-constraint expression for the given relation.
 *
 * Caller is responsible for updating the count of constraints
 * in the pg_class entry for the relation.
 *
 * The OID of the new constraint is returned.
 */""",
  """StoreConstraints|||backend\catalog\heap.c|||2239|||/*
 * Store defaults and constraints (passed as a list of CookedConstraint).
 *
 * Each CookedConstraint struct is modified to store the new catalog tuple OID.
 *
 * NOTE: only pre-cooked expressions will be passed this way, which is to
 * say expressions inherited from an existing relation.  Newly parsed
 * expressions can be added later, by direct calls to StoreAttrDefault
 * and StoreRelCheck (see AddRelationNewConstraints()).
 */""",
  """AddRelationNewConstraints|||backend\catalog\heap.c|||2313|||/*
 * AddRelationNewConstraints
 *
 * Add new column default expressions and/or constraint check expressions
 * to an existing relation.  This is defined to do both for efficiency in
 * DefineRelation, but of course you can do just one or the other by passing
 * empty lists.
 *
 * rel: relation to be modified
 * newColDefaults: list of RawColumnDefault structures
 * newConstraints: list of Constraint nodes
 * allow_merge: true if check constraints may be merged with existing ones
 * is_local: true if definition is local, false if it's inherited
 * is_internal: true if result of some internal process, not a user request
 * queryString: used during expression transformation of default values and
 *		cooked CHECK constraints
 *
 * All entries in newColDefaults will be processed.  Entries in newConstraints
 * will be processed only if they are CONSTR_CHECK type.
 *
 * Returns a list of CookedConstraint nodes that shows the cooked form of
 * the default and constraint expressions added to the relation.
 *
 * NB: caller should have opened rel with some self-conflicting lock mode,
 * and should hold that lock till end of transaction; for normal cases that'll
 * be AccessExclusiveLock, but if caller knows that the constraint is already
 * enforced by some other means, it can be ShareUpdateExclusiveLock.  Also, we
 * assume the caller has done a CommandCounterIncrement if necessary to make
 * the relation's catalog tuples visible.
 */""",
  """MergeWithExistingConstraint|||backend\catalog\heap.c|||2556|||/*
 * Check for a pre-existing check constraint that conflicts with a proposed
 * new one, and either adjust its conislocal/coninhcount settings or throw
 * error as needed.
 *
 * Returns true if merged (constraint is a duplicate), or false if it's
 * got a so-far-unique name, or throws error if conflict.
 *
 * XXX See MergeConstraintsIntoExisting too if you change this code.
 */""",
  """SetRelationNumChecks|||backend\catalog\heap.c|||2711|||/*
 * Update the count of constraints in the relation's pg_class tuple.
 *
 * Caller had better hold exclusive lock on the relation.
 *
 * An important side effect is that a SI update message will be sent out for
 * the pg_class tuple, which will force other backends to rebuild their
 * relcache entries for the rel.  Also, this backend will rebuild its
 * own relcache entry at the next CommandCounterIncrement.
 */""",
  """check_nested_generated_walker|||backend\catalog\heap.c|||2745|||/*
 * Check for references to generated columns
 */""",
  """cookDefault|||backend\catalog\heap.c|||2805|||/*
 * Take a raw default and convert it to a cooked format ready for
 * storage.
 *
 * Parse state should be set up to recognize any vars that might appear
 * in the expression.  (Even though we plan to reject vars, it's more
 * user-friendly to give the correct error message than "unknown var".)
 *
 * If atttypid is not InvalidOid, coerce the expression to the specified
 * type (and typmod atttypmod).   attname is only needed in this case:
 * it is used in the error message, if any.
 */""",
  """cookConstraint|||backend\catalog\heap.c|||2882|||/*
 * Take a raw CHECK constraint expression and convert it to a cooked format
 * ready for storage.
 *
 * Parse state must be set up to recognize any vars that might appear
 * in the expression.
 */""",
  """CopyStatistics|||backend\catalog\heap.c|||2920|||/*
 * CopyStatistics --- copy entries in pg_statistic from one rel to another
 */""",
  """RemoveStatistics|||backend\catalog\heap.c|||2973|||/*
 * RemoveStatistics --- remove entries in pg_statistic for a rel or column
 *
 * If attnum is zero, remove all entries for rel; else remove only the one(s)
 * for that column.
 */""",
  """RelationTruncateIndexes|||backend\catalog\heap.c|||3020|||/*
 * RelationTruncateIndexes - truncate all indexes associated
 * with the heap relation to zero tuples.
 *
 * The routine will truncate and then reconstruct the indexes on
 * the specified relation.  Caller must hold exclusive lock on rel.
 */""",
  """heap_truncate|||backend\catalog\heap.c|||3068|||/*
 *	 heap_truncate
 *
 *	 This routine deletes all data within all the specified relations.
 *
 * This is not transaction-safe!  There is another, transaction-safe
 * implementation in commands/tablecmds.c.  We now use this only for
 * ON COMMIT truncation of temporary tables, where it doesn't matter.
 */""",
  """heap_truncate_one_rel|||backend\catalog\heap.c|||3109|||/*
 *	 heap_truncate_one_rel
 *
 *	 This routine deletes all data within the specified relation.
 *
 * This is not transaction-safe, because the truncation is done immediately
 * and cannot be rolled back later.  Caller is responsible for having
 * checked permissions etc, and must have obtained AccessExclusiveLock.
 */""",
  """heap_truncate_check_FKs|||backend\catalog\heap.c|||3153|||/*
 * heap_truncate_check_FKs
 *		Check for foreign keys referencing a list of relations that
 *		are to be truncated, and raise error if there are any
 *
 * We disallow such FKs (except self-referential ones) since the whole point
 * of TRUNCATE is to not scan the individual rows to be thrown away.
 *
 * This is split out so it can be shared by both implementations of truncate.
 * Caller should already hold a suitable lock on the relations.
 *
 * tempTables is only used to select an appropriate error message.
 */""",
  """heap_truncate_find_FKs|||backend\catalog\heap.c|||3248|||/*
 * heap_truncate_find_FKs
 *		Find relations having foreign keys referencing any of the given rels
 *
 * Input and result are both lists of relation OIDs.  The result contains
 * no duplicates, does *not* include any rels that were already in the input
 * list, and is sorted in OID order.  (The last property is enforced mainly
 * to guarantee consistent behavior in the regression tests; we don't want
 * behavior to change depending on chance locations of rows in pg_constraint.)
 *
 * Note: caller should already have appropriate lock on all rels mentioned
 * in relationIds.  Since adding or dropping an FK requires exclusive lock
 * on both rels, this ensures that the answer will be stable.
 */""",
  """StorePartitionKey|||backend\catalog\heap.c|||3375|||/*
 * StorePartitionKey
 *		Store information about the partition key rel into the catalog
 */""",
  """RemovePartitionKeyByRelId|||backend\catalog\heap.c|||3500|||/*
 *	RemovePartitionKeyByRelId
 *		Remove pg_partitioned_table entry for a relation
 */""",
  """StorePartitionBound|||backend\catalog\heap.c|||3531|||/*
 * StorePartitionBound
 *		Update pg_class tuple of rel to store the partition bound and set
 *		relispartition to true
 *
 * If this is the default partition, also update the default partition OID in
 * pg_partitioned_table.
 *
 * Also, invalidate the parent's relcache, so that the next rebuild will load
 * the new partition's info into its partition descriptor.  If there is a
 * default partition, we must invalidate its relcache entry as well.
 */""",
  """<clinit>|||backend\catalog\index.c|||92|||/*
 * Pointer-free representation of variables used when reindexing system
 * catalogs; we use this to propagate those values to parallel workers.
 */""",
  """relationHasPrimaryKey|||backend\catalog\index.c|||146|||/*
 * relationHasPrimaryKey
 *		See whether an existing relation has a primary key.
 *
 * Caller must have suitable lock on the relation.
 *
 * Note: we intentionally do not check indisvalid here; that's because this
 * is used to enforce the rule that there can be only one indisprimary index,
 * and we want that to be true even if said index is invalid.
 */""",
  """index_check_primary_key|||backend\catalog\index.c|||200|||/*
 * index_check_primary_key
 *		Apply special checks needed before creating a PRIMARY KEY index
 *
 * This processing used to be in DefineIndex(), but has been split out
 * so that it can be applied during ALTER TABLE ADD PRIMARY KEY USING INDEX.
 *
 * We check for a pre-existing primary key, and that all columns of the index
 * are simple column references (not expressions), and that all those
 * columns are marked NOT NULL.  If not, fail.
 *
 * We used to automatically change unmarked columns to NOT NULL here by doing
 * our own local ALTER TABLE command.  But that doesn't work well if we're
 * executing one subcommand of an ALTER TABLE: the operations may not get
 * performed in the right order overall.  Now we expect that the parser
 * inserted any required ALTER TABLE SET NOT NULL operations before trying
 * to create a primary-key index.
 *
 * Caller had better have at least ShareLock on the table, else the not-null
 * checking isn't trustworthy.
 */""",
  """ConstructTupleDescriptor|||backend\catalog\index.c|||279|||/*
 *		ConstructTupleDescriptor
 *
 * Build an index tuple descriptor for a new index
 */""",
  """InitializeAttributeOids|||backend\catalog\index.c|||491|||/* ----------------------------------------------------------------
 *		InitializeAttributeOids
 * ----------------------------------------------------------------
 */""",
  """AppendAttributeTuples|||backend\catalog\index.c|||509|||/* ----------------------------------------------------------------
 *		AppendAttributeTuples
 * ----------------------------------------------------------------
 */""",
  """UpdateIndexRelation|||backend\catalog\index.c|||560|||/* ----------------------------------------------------------------
 *		UpdateIndexRelation
 *
 * Construct and insert a new entry in the pg_index catalog
 * ----------------------------------------------------------------
 */""",
  """index_create|||backend\catalog\index.c|||723|||/*
 * index_create
 *
 * heapRelation: table to build index on (suitably locked by caller)
 * indexRelationName: what it say
 * indexRelationId: normally, pass InvalidOid to let this routine
 *		generate an OID for the index.  During bootstrap this may be
 *		nonzero to specify a preselected OID.
 * parentIndexRelid: if creating an index partition, the OID of the
 *		parent index; otherwise InvalidOid.
 * parentConstraintId: if creating a constraint on a partition, the OID
 *		of the constraint in the parent; otherwise InvalidOid.
 * relFileNumber: normally, pass InvalidRelFileNumber to get new storage.
 *		May be nonzero to attach an existing valid build.
 * indexInfo: same info executor uses to insert into the index
 * indexColNames: column names to use for index (List of char *)
 * accessMethodId: OID of index AM to use
 * tableSpaceId: OID of tablespace to use
 * collationIds: array of collation OIDs, one per index column
 * opclassIds: array of index opclass OIDs, one per index column
 * coloptions: array of per-index-column indoption settings
 * reloptions: AM-specific options
 * flags: bitmask that can include any combination of these bits:
 *		INDEX_CREATE_IS_PRIMARY
 *			the index is a primary key
 *		INDEX_CREATE_ADD_CONSTRAINT:
 *			invoke index_constraint_create also
 *		INDEX_CREATE_SKIP_BUILD:
 *			skip the index_build() step for the moment; caller must do it
 *			later (typically via reindex_index())
 *		INDEX_CREATE_CONCURRENT:
 *			do not lock the table against writers.  The index will be
 *			marked "invalid" and the caller must take additional steps
 *			to fix it up.
 *		INDEX_CREATE_IF_NOT_EXISTS:
 *			do not throw an error if a relation with the same name
 *			already exists.
 *		INDEX_CREATE_PARTITIONED:
 *			create a partitioned index (table must be partitioned)
 * constr_flags: flags passed to index_constraint_create
 *		(only if INDEX_CREATE_ADD_CONSTRAINT is set)
 * allow_system_table_mods: allow table to be a system catalog
 * is_internal: if true, post creation hook for new index
 * constraintId: if not NULL, receives OID of created constraint
 *
 * Returns the OID of the created index.
 */""",
  """index_concurrently_create_copy|||backend\catalog\index.c|||1297|||/*
 * index_concurrently_create_copy
 *
 * Create concurrently an index based on the definition of the one provided by
 * caller.  The index is inserted into catalogs and needs to be built later
 * on.  This is called during concurrent reindex processing.
 *
 * "tablespaceOid" is the tablespace to use for this index.
 */""",
  """index_concurrently_build|||backend\catalog\index.c|||1481|||/*
 * index_concurrently_build
 *
 * Build index for a concurrent operation.  Low-level locks are taken when
 * this operation is performed to prevent only schema changes, but they need
 * to be kept until the end of the transaction performing this operation.
 * 'indexOid' refers to an index relation OID already created as part of
 * previous processing, and 'heapOid' refers to its parent heap relation.
 */""",
  """index_concurrently_swap|||backend\catalog\index.c|||1548|||/*
 * index_concurrently_swap
 *
 * Swap name, dependencies, and constraints of the old index over to the new
 * index, while marking the old index as invalid and the new as valid.
 */""",
  """index_concurrently_set_dead|||backend\catalog\index.c|||1819|||/*
 * index_concurrently_set_dead
 *
 * Perform the last invalidation stage of DROP INDEX CONCURRENTLY or REINDEX
 * CONCURRENTLY before actually dropping the index.  After calling this
 * function, the index is seen by all the backends as dead.  Low-level locks
 * taken here are kept until the end of the transaction calling this function.
 */""",
  """index_constraint_create|||backend\catalog\index.c|||1880|||/*
 * index_constraint_create
 *
 * Set up a constraint associated with an index.  Return the new constraint's
 * address.
 *
 * heapRelation: table owning the index (must be suitably locked by caller)
 * indexRelationId: OID of the index
 * parentConstraintId: if constraint is on a partition, the OID of the
 *		constraint in the parent.
 * indexInfo: same info executor uses to insert into the index
 * constraintName: what it say (generally, should match name of index)
 * constraintType: one of CONSTRAINT_PRIMARY, CONSTRAINT_UNIQUE, or
 *		CONSTRAINT_EXCLUSION
 * flags: bitmask that can include any combination of these bits:
 *		INDEX_CONSTR_CREATE_MARK_AS_PRIMARY: index is a PRIMARY KEY
 *		INDEX_CONSTR_CREATE_DEFERRABLE: constraint is DEFERRABLE
 *		INDEX_CONSTR_CREATE_INIT_DEFERRED: constraint is INITIALLY DEFERRED
 *		INDEX_CONSTR_CREATE_UPDATE_INDEX: update the pg_index row
 *		INDEX_CONSTR_CREATE_REMOVE_OLD_DEPS: remove existing dependencies
 *			of index on table's columns
 * allow_system_table_mods: allow table to be a system catalog
 * is_internal: index is constructed due to internal process
 */""",
  """index_drop|||backend\catalog\index.c|||2113|||/*
 *		index_drop
 *
 * NOTE: this routine should now only be called through performDeletion(),
 * else associated dependencies won't be cleaned up.
 *
 * If concurrent is true, do a DROP INDEX CONCURRENTLY.  If concurrent is
 * false but concurrent_lock_mode is true, then do a normal DROP INDEX but
 * take a lock for CONCURRENTLY processing.  That is used as part of REINDEX
 * CONCURRENTLY.
 */""",
  """BuildIndexInfo|||backend\catalog\index.c|||2403|||/* ----------------
 *		BuildIndexInfo
 *			Construct an IndexInfo record for an open index
 *
 * IndexInfo stores the information about the index that's needed by
 * FormIndexDatum, which is used for both index_build() and later insertion
 * of individual index tuples.  Normally we build an IndexInfo for an index
 * just once per command, and then use it for (potentially) many tuples.
 * ----------------
 */""",
  """BuildDummyIndexInfo|||backend\catalog\index.c|||2462|||/* ----------------
 *		BuildDummyIndexInfo
 *			Construct a dummy IndexInfo record for an open index
 *
 * This differs from the real BuildIndexInfo in that it will never run any
 * user-defined code that might exist in index expressions or predicates.
 * Instead of the real index expressions, we return null constants that have
 * the right types/typmods/collations.  Predicates and exclusion clauses are
 * just ignored.  This is sufficient for the purpose of truncating an index,
 * since we will not need to actually evaluate the expressions or predicates;
 * the only thing that's likely to be done with the data is construction of
 * a tupdesc describing the index's rowtype.
 * ----------------
 */""",
  """CompareIndexInfo|||backend\catalog\index.c|||2510|||/*
 * CompareIndexInfo
 *		Return whether the properties of two indexes (in different tables)
 *		indicate that they have the "same" definitions.
 *
 * Note: passing collations and opfamilies separately is a kludge.  Adding
 * them to IndexInfo may result in better coding here and elsewhere.
 *
 * Use build_attrmap_by_name(index2, index1) to build the attmap.
 */""",
  """BuildSpeculativeIndexInfo|||backend\catalog\index.c|||2641|||/* ----------------
 *		BuildSpeculativeIndexInfo
 *			Add extra state to IndexInfo record
 *
 * For unique indexes, we usually don't want to add info to the IndexInfo for
 * checking uniqueness, since the B-Tree AM handles that directly.  However,
 * in the case of speculative insertion, additional support is required.
 *
 * Do this processing here rather than in BuildIndexInfo() to not incur the
 * overhead in the common non-speculative cases.
 * ----------------
 */""",
  """FormIndexDatum|||backend\catalog\index.c|||2701|||/* ----------------
 *		FormIndexDatum
 *			Construct values[] and isnull[] arrays for a new index tuple.
 *
 *	indexInfo		Info about the index
 *	slot			Heap tuple for which we must prepare an index entry
 *	estate			executor state for evaluating any index expressions
 *	values			Array of index Datums (output area)
 *	isnull			Array of is-null indicators (output area)
 *
 * When there are no index expressions, estate may be NULL.  Otherwise it
 * must be supplied, *and* the ecxt_scantuple slot of its per-tuple expr
 * context must point to the heap tuple passed in.
 *
 * Notice we don't actually call index_form_tuple() here; we just prepare
 * its input arrays values[] and isnull[].  This is because the index AM
 * may wish to alter the data before storage.
 * ----------------
 */""",
  """index_update_stats|||backend\catalog\index.c|||2780|||/*
 * index_update_stats --- update pg_class entry after CREATE INDEX or REINDEX
 *
 * This routine updates the pg_class row of either an index or its parent
 * relation after CREATE INDEX or REINDEX.  Its rather bizarre API is designed
 * to ensure we can do all the necessary work in just one update.
 *
 * hasindex: set relhasindex to this value
 * reltuples: if >= 0, set reltuples to this value; else no change
 *
 * If reltuples >= 0, relpages and relallvisible are also updated (using
 * RelationGetNumberOfBlocks() and visibilitymap_count()).
 *
 * NOTE: an important side-effect of this operation is that an SI invalidation
 * message is sent out to all backends --- including me --- causing relcache
 * entries to be flushed or updated with the new data.  This must happen even
 * if we find that no change is needed in the pg_class row.  When updating
 * a heap entry, this ensures that other backends find out about the new
 * index.  When updating an index, it's important because some index AMs
 * expect a relcache flush to occur after REINDEX.
 */""",
  """index_build|||backend\catalog\index.c|||2939|||/*
 * index_build - invoke access-method-specific index build procedure
 *
 * On entry, the index's catalog entries are valid, and its physical disk
 * file has been created but is empty.  We call the AM-specific build
 * procedure to fill in the index contents.  We then update the pg_class
 * entries of the index and heap relation as needed, using statistics
 * returned by ambuild as well as data passed by the caller.
 *
 * isreindex indicates we are recreating a previously-existing index.
 * parallel indicates if parallelism may be useful.
 *
 * Note: before Postgres 8.2, the passed-in heap and index Relations
 * were automatically closed by this routine.  This is no longer the case.
 * The caller opened 'em, and the caller should close 'em.
 */""",
  """IndexCheckExclusion|||backend\catalog\index.c|||3132|||/*
 * IndexCheckExclusion - verify that a new exclusion constraint is satisfied
 *
 * When creating an exclusion constraint, we first build the index normally
 * and then rescan the heap to check for conflicts.  We assume that we only
 * need to validate tuples that are live according to an up-to-date snapshot,
 * and that these were correctly indexed even in the presence of broken HOT
 * chains.  This should be OK since we are holding at least ShareLock on the
 * table, meaning there can be no uncommitted updates from other transactions.
 * (Note: that wouldn't necessarily work for system catalogs, since many
 * operations release write lock early on the system catalogs.)
 */""",
  """validate_index|||backend\catalog\index.c|||3288|||/*
 * validate_index - support code for concurrent index builds
 *
 * We do a concurrent index build by first inserting the catalog entry for the
 * index via index_create(), marking it not indisready and not indisvalid.
 * Then we commit our transaction and start a new one, then we wait for all
 * transactions that could have been modifying the table to terminate.  Now
 * we know that any subsequently-started transactions will see the index and
 * honor its constraints on HOT updates; so while existing HOT-chains might
 * be broken with respect to the index, no currently live tuple will have an
 * incompatible HOT update done to it.  We now build the index normally via
 * index_build(), while holding a weak lock that allows concurrent
 * insert/update/delete.  Also, we index only tuples that are valid
 * as of the start of the scan (see table_index_build_scan), whereas a normal
 * build takes care to include recently-dead tuples.  This is OK because
 * we won't mark the index valid until all transactions that might be able
 * to see those tuples are gone.  The reason for doing that is to avoid
 * bogus unique-index failures due to concurrent UPDATEs (we might see
 * different versions of the same row as being valid when we pass over them,
 * if we used HeapTupleSatisfiesVacuum).  This leaves us with an index that
 * does not contain any tuples added to the table while we built the index.
 *
 * Next, we mark the index "indisready" (but still not "indisvalid") and
 * commit the second transaction and start a third.  Again we wait for all
 * transactions that could have been modifying the table to terminate.  Now
 * we know that any subsequently-started transactions will see the index and
 * insert their new tuples into it.  We then take a new reference snapshot
 * which is passed to validate_index().  Any tuples that are valid according
 * to this snap, but are not in the index, must be added to the index.
 * (Any tuples committed live after the snap will be inserted into the
 * index by their originating transaction.  Any tuples committed dead before
 * the snap need not be indexed, because we will wait out all transactions
 * that might care about them before we mark the index valid.)
 *
 * validate_index() works by first gathering all the TIDs currently in the
 * index, using a bulkdelete callback that just stores the TIDs and doesn't
 * ever say "delete it".  (This should be faster than a plain indexscan;
 * also, not all index AMs support full-index indexscan.)  Then we sort the
 * TIDs, and finally scan the table doing a "merge join" against the TID list
 * to see which tuples are missing from the index.  Thus we will ensure that
 * all tuples valid according to the reference snapshot are in the index.
 *
 * Building a unique index this way is tricky: we might try to insert a
 * tuple that is already dead or is in process of being deleted, and we
 * mustn't have a uniqueness failure against an updated version of the same
 * row.  We could try to check the tuple to see if it's already dead and tell
 * index_insert() not to do the uniqueness check, but that still leaves us
 * with a race condition against an in-progress update.  To handle that,
 * we expect the index AM to recheck liveness of the to-be-inserted tuple
 * before it declares a uniqueness error.
 *
 * After completing validate_index(), we wait until all transactions that
 * were alive at the time of the reference snapshot are gone; this is
 * necessary to be sure there are none left with a transaction snapshot
 * older than the reference (and hence possibly able to see tuples we did
 * not index).  Then we mark the index "indisvalid" and commit.  Subsequent
 * transactions will be able to use it for queries.
 *
 * Doing two full table scans is a brute-force strategy.  We could try to be
 * cleverer, eg storing new tuples in a special area of the table (perhaps
 * making the table append-only by setting use_fsm).  However that would
 * add yet more locking issues.
 */""",
  """validate_index_callback|||backend\catalog\index.c|||3421|||/*
 * validate_index_callback - bulkdelete callback to collect the index TIDs
 */""",
  """index_set_state_flags|||backend\catalog\index.c|||3441|||/*
 * index_set_state_flags - adjust pg_index state flags
 *
 * This is used during CREATE/DROP INDEX CONCURRENTLY to adjust the pg_index
 * flags that denote the index's state.
 *
 * Note that CatalogTupleUpdate() sends a cache invalidation message for the
 * tuple, so other sessions will hear about the update as soon as we commit.
 */""",
  """IndexGetRelation|||backend\catalog\index.c|||3521|||/*
 * IndexGetRelation: given an index's relation OID, get the OID of the
 * relation it is an index on.  Uses the system cache.
 */""",
  """reindex_index|||backend\catalog\index.c|||3546|||/*
 * reindex_index - This routine is used to recreate a single index
 */""",
  """reindex_relation|||backend\catalog\index.c|||3886|||/*
 * reindex_relation - This routine is used to recreate all indexes
 * of a relation (and optionally its toast relation too, if any).
 *
 * "flags" is a bitmask that can include any combination of these bits:
 *
 * REINDEX_REL_PROCESS_TOAST: if true, process the toast table too (if any).
 *
 * REINDEX_REL_SUPPRESS_INDEX_USE: if true, the relation was just completely
 * rebuilt by an operation such as VACUUM FULL or CLUSTER, and therefore its
 * indexes are inconsistent with it.  This makes things tricky if the relation
 * is a system catalog that we might consult during the reindexing.  To deal
 * with that case, we mark all of the indexes as pending rebuild so that they
 * won't be trusted until rebuilt.  The caller is required to call us *without*
 * having made the rebuilt table visible by doing CommandCounterIncrement;
 * we'll do CCI after having collected the index list.  (This way we can still
 * use catalog indexes while collecting the list.)
 *
 * REINDEX_REL_CHECK_CONSTRAINTS: if true, recheck unique and exclusion
 * constraint conditions, else don't.  To avoid deadlocks, VACUUM FULL or
 * CLUSTER on a system catalog must omit this flag.  REINDEX should be used to
 * rebuild an index if constraint inconsistency is suspected.  For optimal
 * performance, other callers should include the flag only after transforming
 * the data in a manner that risks a change in constraint validity.
 *
 * REINDEX_REL_FORCE_INDEXES_UNLOGGED: if true, set the persistence of the
 * rebuilt indexes to unlogged.
 *
 * REINDEX_REL_FORCE_INDEXES_PERMANENT: if true, set the persistence of the
 * rebuilt indexes to permanent.
 *
 * Returns true if any indexes were rebuilt (including toast table's index
 * when relevant).  Note that a CommandCounterIncrement will occur after each
 * index rebuild.
 */""",
  """ReindexIsProcessingHeap|||backend\catalog\index.c|||4057|||/*
 * ReindexIsProcessingHeap
 *		True if heap specified by OID is currently being reindexed.
 */""",
  """ReindexIsCurrentlyProcessingIndex|||backend\catalog\index.c|||4067|||/*
 * ReindexIsCurrentlyProcessingIndex
 *		True if index specified by OID is currently being reindexed.
 */""",
  """ReindexIsProcessingIndex|||backend\catalog\index.c|||4078|||/*
 * ReindexIsProcessingIndex
 *		True if index specified by OID is currently being reindexed,
 *		or should be treated as invalid because it is awaiting reindex.
 */""",
  """SetReindexProcessing|||backend\catalog\index.c|||4089|||/*
 * SetReindexProcessing
 *		Set flag that specified heap/index are being reindexed.
 */""",
  """ResetReindexProcessing|||backend\catalog\index.c|||4108|||/*
 * ResetReindexProcessing
 *		Unset reindexing status.
 */""",
  """SetReindexPending|||backend\catalog\index.c|||4122|||/*
 * SetReindexPending
 *		Mark the given indexes as pending reindex.
 *
 * NB: we assume that the current memory context stays valid throughout.
 */""",
  """RemoveReindexPending|||backend\catalog\index.c|||4138|||/*
 * RemoveReindexPending
 *		Remove the given index from the pending list.
 */""",
  """ResetReindexState|||backend\catalog\index.c|||4151|||/*
 * ResetReindexState
 *		Clear all reindexing state during (sub)transaction abort.
 */""",
  """EstimateReindexStateSpace|||backend\catalog\index.c|||4180|||/*
 * EstimateReindexStateSpace
 *		Estimate space needed to pass reindex state to parallel workers.
 */""",
  """SerializeReindexState|||backend\catalog\index.c|||4191|||/*
 * SerializeReindexState
 *		Serialize reindex state for parallel workers.
 */""",
  """RestoreReindexState|||backend\catalog\index.c|||4209|||/*
 * RestoreReindexState
 *		Restore reindex state in a parallel worker.
 */""",
  """CatalogOpenIndexes|||backend\catalog\indexing.c|||42|||/*
 * CatalogOpenIndexes - open the indexes on a system catalog.
 *
 * When inserting or updating tuples in a system catalog, call this
 * to prepare to update the indexes for the catalog.
 *
 * In the current implementation, we share code for opening/closing the
 * indexes with execUtils.c.  But we do not use ExecInsertIndexTuples,
 * because we don't want to create an EState.  This implies that we
 * do not support partial or expressional indexes on system catalogs,
 * nor can we support generalized exclusion constraints.
 * This could be fixed with localized changes here if we wanted to pay
 * the extra overhead of building an EState.
 */""",
  """CatalogCloseIndexes|||backend\catalog\indexing.c|||60|||/*
 * CatalogCloseIndexes - clean up resources allocated by CatalogOpenIndexes
 */""",
  """CatalogIndexInsert|||backend\catalog\indexing.c|||74|||/*
 * CatalogIndexInsert - insert index entries for one catalog tuple
 *
 * This should be called for each inserted or updated catalog tuple.
 *
 * This is effectively a cut-down version of ExecInsertIndexTuples.
 */""",
  """CatalogTupleCheckConstraints|||backend\catalog\indexing.c|||194|||	/*
	 * Currently, the only constraints implemented for system catalogs are
	 * attnotnull constraints.
	 */""",
  """CatalogTupleInsert|||backend\catalog\indexing.c|||232|||/*
 * CatalogTupleInsert - do heap and indexing work for a new catalog tuple
 *
 * Insert the tuple data in "tup" into the specified catalog relation.
 *
 * This is a convenience routine for the common case of inserting a single
 * tuple in a system catalog; it inserts a new heap tuple, keeping indexes
 * current.  Avoid using it for multiple tuples, since opening the indexes
 * and building the index info structures is moderately expensive.
 * (Use CatalogTupleInsertWithInfo in such cases.)
 */""",
  """CatalogTupleInsertWithInfo|||backend\catalog\indexing.c|||255|||/*
 * CatalogTupleInsertWithInfo - as above, but with caller-supplied index info
 *
 * This should be used when it's important to amortize CatalogOpenIndexes/
 * CatalogCloseIndexes work across multiple insertions.  At some point we
 * might cache the CatalogIndexState data somewhere (perhaps in the relcache)
 * so that callers needn't trouble over this ... but we don't do so today.
 */""",
  """CatalogTuplesMultiInsertWithInfo|||backend\catalog\indexing.c|||272|||/*
 * CatalogTuplesMultiInsertWithInfo - as above, but for multiple tuples
 *
 * Insert multiple tuples into the given catalog relation at once, with an
 * amortized cost of CatalogOpenIndexes.
 */""",
  """CatalogTupleUpdate|||backend\catalog\indexing.c|||312|||/*
 * CatalogTupleUpdate - do heap and indexing work for updating a catalog tuple
 *
 * Update the tuple identified by "otid", replacing it with the data in "tup".
 *
 * This is a convenience routine for the common case of updating a single
 * tuple in a system catalog; it updates one heap tuple, keeping indexes
 * current.  Avoid using it for multiple tuples, since opening the indexes
 * and building the index info structures is moderately expensive.
 * (Use CatalogTupleUpdateWithInfo in such cases.)
 */""",
  """CatalogTupleUpdateWithInfo|||backend\catalog\indexing.c|||336|||/*
 * CatalogTupleUpdateWithInfo - as above, but with caller-supplied index info
 *
 * This should be used when it's important to amortize CatalogOpenIndexes/
 * CatalogCloseIndexes work across multiple updates.  At some point we
 * might cache the CatalogIndexState data somewhere (perhaps in the relcache)
 * so that callers needn't trouble over this ... but we don't do so today.
 */""",
  """CatalogTupleDelete|||backend\catalog\indexing.c|||364|||/*
 * CatalogTupleDelete - do heap and indexing work for deleting a catalog tuple
 *
 * Delete the tuple identified by "tid" in the specified catalog.
 *
 * With Postgres heaps, there is no index work to do at deletion time;
 * cleanup will be done later by VACUUM.  However, callers of this function
 * shouldn't have to know that; we'd like a uniform abstraction for all
 * catalog tuple changes.  Hence, provide this currently-trivial wrapper.
 *
 * The abstraction is a bit leaky in that we don't provide an optimized
 * CatalogTupleDeleteWithInfo version, because there is currently nothing to
 * optimize.  If we ever need that, rather than touching a lot of call sites,
 * it might be better to do something about caching CatalogIndexState.
 */""",
  """spcachekey_hash|||backend\catalog\namespace.c|||253|||/*
 * Recomputing the namespace path can be costly when done frequently, such as
 * when a function has search_path set in proconfig. Add a search path cache
 * that can be used by recomputeNamespacePath().
 *
 * The cache is also used to remember already-validated strings in
 * check_search_path() to avoid the need to call SplitIdentifierString()
 * repeatedly.
 *
 * The search path cache is based on a wrapper around a simplehash hash table
 * (nsphash, defined below). The spcache wrapper deals with OOM while trying
 * to initialize a key, optimizes repeated lookups of the same key, and also
 * offers a more convenient API.
 */""",
  """spcache_init|||backend\catalog\namespace.c|||305|||/*
 * Create or reset search_path cache as necessary.
 */""",
  """spcache_lookup|||backend\catalog\namespace.c|||343|||/*
 * Look up entry in search path cache without inserting. Returns NULL if not
 * present.
 */""",
  """spcache_insert|||backend\catalog\namespace.c|||373|||/*
 * Look up or insert entry in search path cache.
 *
 * Initialize key safely, so that OOM does not leave an entry without a valid
 * key. Caller must ensure that non-key contents are properly initialized.
 */""",
  """RangeVarGetRelidExtended|||backend\catalog\namespace.c|||440|||/*
 * RangeVarGetRelidExtended
 *		Given a RangeVar describing an existing relation,
 *		select the proper namespace and look up the relation OID.
 *
 * If the schema or relation is not found, return InvalidOid if flags contains
 * RVR_MISSING_OK, otherwise raise an error.
 *
 * If flags contains RVR_NOWAIT, throw an error if we'd have to wait for a
 * lock.
 *
 * If flags contains RVR_SKIP_LOCKED, return InvalidOid if we'd have to wait
 * for a lock.
 *
 * flags cannot contain both RVR_NOWAIT and RVR_SKIP_LOCKED.
 *
 * Note that if RVR_MISSING_OK and RVR_SKIP_LOCKED are both specified, a
 * return value of InvalidOid could either mean the relation is missing or it
 * could not be locked.
 *
 * Callback allows caller to check permissions or acquire additional locks
 * prior to grabbing the relation lock.
 */""",
  """RangeVarGetCreationNamespace|||backend\catalog\namespace.c|||653|||/*
 * RangeVarGetCreationNamespace
 *		Given a RangeVar describing a to-be-created relation,
 *		choose which namespace to create it in.
 *
 * Note: calling this may result in a CommandCounterIncrement operation.
 * That will happen on the first request for a temp table in any particular
 * backend run; we will need to either create or clean out the temp schema.
 */""",
  """RangeVarGetAndCheckCreationNamespace|||backend\catalog\namespace.c|||738|||/*
 * RangeVarGetAndCheckCreationNamespace
 *
 * This function returns the OID of the namespace in which a new relation
 * with a given name should be created.  If the user does not have CREATE
 * permission on the target namespace, this function will instead signal
 * an ERROR.
 *
 * If non-NULL, *existing_relation_id is set to the OID of any existing relation
 * with the same name which already exists in that namespace, or to InvalidOid
 * if no such relation exists.
 *
 * If lockmode != NoLock, the specified lock mode is acquired on the existing
 * relation, if any, provided that the current user owns the target relation.
 * However, if lockmode != NoLock and the user does not own the target
 * relation, we throw an ERROR, as we must not try to lock relations the
 * user does not have permissions on.
 *
 * As a side effect, this function acquires AccessShareLock on the target
 * namespace.  Without this, the namespace could be dropped before our
 * transaction commits, leaving behind relations with relnamespace pointing
 * to a no-longer-existent namespace.
 *
 * As a further side-effect, if the selected namespace is a temporary namespace,
 * we mark the RangeVar as RELPERSISTENCE_TEMP.
 */""",
  """RangeVarAdjustRelationPersistence|||backend\catalog\namespace.c|||845|||/*
 * Adjust the relpersistence for an about-to-be-created relation based on the
 * creation namespace, and throw an error for invalid combinations.
 */""",
  """RelnameGetRelid|||backend\catalog\namespace.c|||884|||/*
 * RelnameGetRelid
 *		Try to resolve an unqualified relation name.
 *		Returns OID if relation found in search path, else InvalidOid.
 */""",
  """RelationIsVisible|||backend\catalog\namespace.c|||912|||/*
 * RelationIsVisible
 *		Determine whether a relation (identified by OID) is visible in the
 *		current search path.  Visible means "would be found by searching
 *		for the unqualified relation name".
 */""",
  """RelationIsVisibleExt|||backend\catalog\namespace.c|||924|||/*
 * RelationIsVisibleExt
 *		As above, but if the relation isn't found and is_missing is not NULL,
 *		then set *is_missing = true and return false instead of throwing
 *		an error.  (Caller must initialize *is_missing = false.)
 */""",
  """TypenameGetTypid|||backend\catalog\namespace.c|||994|||/*
 * TypenameGetTypid
 *		Wrapper for binary compatibility.
 */""",
  """TypenameGetTypidExtended|||backend\catalog\namespace.c|||1007|||/*
 * TypenameGetTypidExtended
 *		Try to resolve an unqualified datatype name.
 *		Returns OID if type found in search path, else InvalidOid.
 *
 * This is essentially the same as RelnameGetRelid.
 */""",
  """TypeIsVisible|||backend\catalog\namespace.c|||1039|||/*
 * TypeIsVisible
 *		Determine whether a type (identified by OID) is visible in the
 *		current search path.  Visible means "would be found by searching
 *		for the unqualified type name".
 */""",
  """TypeIsVisibleExt|||backend\catalog\namespace.c|||1051|||/*
 * TypeIsVisibleExt
 *		As above, but if the type isn't found and is_missing is not NULL,
 *		then set *is_missing = true and return false instead of throwing
 *		an error.  (Caller must initialize *is_missing = false.)
 */""",
  """FuncnameGetCandidates|||backend\catalog\namespace.c|||1191|||/*
 * FuncnameGetCandidates
 *		Given a possibly-qualified function name and argument count,
 *		retrieve a list of the possible matches.
 *
 * If nargs is -1, we return all functions matching the given name,
 * regardless of argument count.  (argnames must be NIL, and expand_variadic
 * and expand_defaults must be false, in this case.)
 *
 * If argnames isn't NIL, we are considering a named- or mixed-notation call,
 * and only functions having all the listed argument names will be returned.
 * (We assume that length(argnames) <= nargs and all the passed-in names are
 * distinct.)  The returned structs will include an argnumbers array showing
 * the actual argument index for each logical argument position.
 *
 * If expand_variadic is true, then variadic functions having the same number
 * or fewer arguments will be retrieved, with the variadic argument and any
 * additional argument positions filled with the variadic element type.
 * nvargs in the returned struct is set to the number of such arguments.
 * If expand_variadic is false, variadic arguments are not treated specially,
 * and the returned nvargs will always be zero.
 *
 * If expand_defaults is true, functions that could match after insertion of
 * default argument values will also be retrieved.  In this case the returned
 * structs could have nargs > passed-in nargs, and ndargs is set to the number
 * of additional args (which can be retrieved from the function's
 * proargdefaults entry).
 *
 * If include_out_arguments is true, then OUT-mode arguments are considered to
 * be included in the argument list.  Their types are included in the returned
 * arrays, and argnumbers are indexes in proallargtypes not proargtypes.
 * We also set nominalnargs to be the length of proallargtypes not proargtypes.
 * Otherwise OUT-mode arguments are ignored.
 *
 * It is not possible for nvargs and ndargs to both be nonzero in the same
 * list entry, since default insertion allows matches to functions with more
 * than nargs arguments while the variadic transformation requires the same
 * number or less.
 *
 * When argnames isn't NIL, the returned args[] type arrays are not ordered
 * according to the functions' declarations, but rather according to the call:
 * first any positional arguments, then the named arguments, then defaulted
 * arguments (if needed and allowed by expand_defaults).  The argnumbers[]
 * array can be used to map this back to the catalog information.
 * argnumbers[k] is set to the proargtypes or proallargtypes index of the
 * k'th call argument.
 *
 * We search a single namespace if the function name is qualified, else
 * all namespaces in the search path.  In the multiple-namespace case,
 * we arrange for entries in earlier namespaces to mask identical entries in
 * later namespaces.
 *
 * When expanding variadics, we arrange for non-variadic functions to mask
 * variadic ones if the expanded argument list is the same.  It is still
 * possible for there to be conflicts between different variadic functions,
 * however.
 *
 * It is guaranteed that the return list will never contain multiple entries
 * with identical argument lists.  When expand_defaults is true, the entries
 * could have more than nargs positions, but we still guarantee that they are
 * distinct in the first nargs positions.  However, if argnames isn't NIL or
 * either expand_variadic or expand_defaults is true, there might be multiple
 * candidate functions that expand to identical argument lists.  Rather than
 * throw error here, we report such situations by returning a single entry
 * with oid = 0 that represents a set of such conflicting candidates.
 * The caller might end up discarding such an entry anyway, but if it selects
 * such an entry it should react as though the call were ambiguous.
 *
 * If missing_ok is true, an empty list (NULL) is returned if the name was
 * schema-qualified with a schema that does not exist.  Likewise if no
 * candidate is found for other reasons.
 */""",
  """MatchNamedCall|||backend\catalog\namespace.c|||1584|||/*
 * MatchNamedCall
 *		Given a pg_proc heap tuple and a call's list of argument names,
 *		check whether the function could match the call.
 *
 * The call could match if all supplied argument names are accepted by
 * the function, in positions after the last positional argument, and there
 * are defaults for all unsupplied arguments.
 *
 * If include_out_arguments is true, we are treating OUT arguments as
 * included in the argument list.  pronargs is the number of arguments
 * we're considering (the length of either proargtypes or proallargtypes).
 *
 * The number of positional arguments is nargs - list_length(argnames).
 * Note caller has already done basic checks on argument count.
 *
 * On match, return true and fill *argnumbers with a palloc'd array showing
 * the mapping from call argument positions to actual function argument
 * numbers.  Defaulted arguments are included in this map, at positions
 * after the last supplied argument.
 */""",
  """FunctionIsVisible|||backend\catalog\namespace.c|||1695|||/*
 * FunctionIsVisible
 *		Determine whether a function (identified by OID) is visible in the
 *		current search path.  Visible means "would be found by searching
 *		for the unqualified function name with exact argument matches".
 */""",
  """FunctionIsVisibleExt|||backend\catalog\namespace.c|||1707|||/*
 * FunctionIsVisibleExt
 *		As above, but if the function isn't found and is_missing is not NULL,
 *		then set *is_missing = true and return false instead of throwing
 *		an error.  (Caller must initialize *is_missing = false.)
 */""",
  """OpernameGetOprid|||backend\catalog\namespace.c|||1784|||/*
 * OpernameGetOprid
 *		Given a possibly-qualified operator name and exact input datatypes,
 *		look up the operator.  Returns InvalidOid if not found.
 *
 * Pass oprleft = InvalidOid for a prefix op.
 *
 * If the operator name is not schema-qualified, it is sought in the current
 * namespace search path.  If the name is schema-qualified and the given
 * schema does not exist, InvalidOid is returned.
 */""",
  """OpernameGetCandidates|||backend\catalog\namespace.c|||1887|||/*
 * OpernameGetCandidates
 *		Given a possibly-qualified operator name and operator kind,
 *		retrieve a list of the possible matches.
 *
 * If oprkind is '\0', we return all operators matching the given name,
 * regardless of arguments.
 *
 * We search a single namespace if the operator name is qualified, else
 * all namespaces in the search path.  The return list will never contain
 * multiple entries with identical argument lists --- in the multiple-
 * namespace case, we arrange for entries in earlier namespaces to mask
 * identical entries in later namespaces.
 *
 * The returned items always have two args[] entries --- the first will be
 * InvalidOid for a prefix oprkind.  nargs is always 2, too.
 */"""
)
INFO:__main__:Extracted 18 methods with comments in this batch
INFO:__main__:Fetching batch: offset=2400, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=2400, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres355[0m: [32mList[0m[[32mString[0m] = List(
  """OperatorIsVisible|||backend\catalog\namespace.c|||2048|||/*
 * OperatorIsVisible
 *		Determine whether an operator (identified by OID) is visible in the
 *		current search path.  Visible means "would be found by searching
 *		for the unqualified operator name with exact argument matches".
 */""",
  """OperatorIsVisibleExt|||backend\catalog\namespace.c|||2060|||/*
 * OperatorIsVisibleExt
 *		As above, but if the operator isn't found and is_missing is not NULL,
 *		then set *is_missing = true and return false instead of throwing
 *		an error.  (Caller must initialize *is_missing = false.)
 */""",
  """OpclassnameGetOpcid|||backend\catalog\namespace.c|||2120|||/*
 * OpclassnameGetOpcid
 *		Try to resolve an unqualified index opclass name.
 *		Returns OID if opclass found in search path, else InvalidOid.
 *
 * This is essentially the same as TypenameGetTypid, but we have to have
 * an extra argument for the index AM OID.
 */""",
  """OpclassIsVisible|||backend\catalog\namespace.c|||2153|||/*
 * OpclassIsVisible
 *		Determine whether an opclass (identified by OID) is visible in the
 *		current search path.  Visible means "would be found by searching
 *		for the unqualified opclass name".
 */""",
  """OpclassIsVisibleExt|||backend\catalog\namespace.c|||2165|||/*
 * OpclassIsVisibleExt
 *		As above, but if the opclass isn't found and is_missing is not NULL,
 *		then set *is_missing = true and return false instead of throwing
 *		an error.  (Caller must initialize *is_missing = false.)
 */""",
  """OpfamilynameGetOpfid|||backend\catalog\namespace.c|||2222|||/*
 * OpfamilynameGetOpfid
 *		Try to resolve an unqualified index opfamily name.
 *		Returns OID if opfamily found in search path, else InvalidOid.
 *
 * This is essentially the same as TypenameGetTypid, but we have to have
 * an extra argument for the index AM OID.
 */""",
  """OpfamilyIsVisible|||backend\catalog\namespace.c|||2255|||/*
 * OpfamilyIsVisible
 *		Determine whether an opfamily (identified by OID) is visible in the
 *		current search path.  Visible means "would be found by searching
 *		for the unqualified opfamily name".
 */""",
  """OpfamilyIsVisibleExt|||backend\catalog\namespace.c|||2267|||/*
 * OpfamilyIsVisibleExt
 *		As above, but if the opfamily isn't found and is_missing is not NULL,
 *		then set *is_missing = true and return false instead of throwing
 *		an error.  (Caller must initialize *is_missing = false.)
 */""",
  """lookup_collation|||backend\catalog\namespace.c|||2321|||/*
 * lookup_collation
 *		If there's a collation of the given name/namespace, and it works
 *		with the given encoding, return its OID.  Else return InvalidOid.
 */""",
  """CollationGetCollid|||backend\catalog\namespace.c|||2372|||/*
 * CollationGetCollid
 *		Try to resolve an unqualified collation name.
 *		Returns OID if collation found in search path, else InvalidOid.
 *
 * Note that this will only find collations that work with the current
 * database's encoding.
 */""",
  """CollationIsVisible|||backend\catalog\namespace.c|||2406|||/*
 * CollationIsVisible
 *		Determine whether a collation (identified by OID) is visible in the
 *		current search path.  Visible means "would be found by searching
 *		for the unqualified collation name".
 *
 * Note that only collations that work with the current database's encoding
 * will be considered visible.
 */""",
  """CollationIsVisibleExt|||backend\catalog\namespace.c|||2418|||/*
 * CollationIsVisibleExt
 *		As above, but if the collation isn't found and is_missing is not NULL,
 *		then set *is_missing = true and return false instead of throwing
 *		an error.  (Caller must initialize *is_missing = false.)
 */""",
  """ConversionGetConid|||backend\catalog\namespace.c|||2476|||/*
 * ConversionGetConid
 *		Try to resolve an unqualified conversion name.
 *		Returns OID if conversion found in search path, else InvalidOid.
 *
 * This is essentially the same as RelnameGetRelid.
 */""",
  """ConversionIsVisible|||backend\catalog\namespace.c|||2508|||/*
 * ConversionIsVisible
 *		Determine whether a conversion (identified by OID) is visible in the
 *		current search path.  Visible means "would be found by searching
 *		for the unqualified conversion name".
 */""",
  """ConversionIsVisibleExt|||backend\catalog\namespace.c|||2520|||/*
 * ConversionIsVisibleExt
 *		As above, but if the conversion isn't found and is_missing is not NULL,
 *		then set *is_missing = true and return false instead of throwing
 *		an error.  (Caller must initialize *is_missing = false.)
 */""",
  """get_statistics_object_oid|||backend\catalog\namespace.c|||2574|||/*
 * get_statistics_object_oid - find a statistics object by possibly qualified name
 *
 * If not found, returns InvalidOid if missing_ok, else throws error
 */""",
  """StatisticsObjIsVisible|||backend\catalog\namespace.c|||2631|||/*
 * StatisticsObjIsVisible
 *		Determine whether a statistics object (identified by OID) is visible in
 *		the current search path.  Visible means "would be found by searching
 *		for the unqualified statistics object name".
 */""",
  """StatisticsObjIsVisibleExt|||backend\catalog\namespace.c|||2643|||/*
 * StatisticsObjIsVisibleExt
 *		As above, but if the statistics object isn't found and is_missing is
 *		not NULL, then set *is_missing = true and return false instead of
 *		throwing an error.  (Caller must initialize *is_missing = false.)
 */""",
  """get_ts_parser_oid|||backend\catalog\namespace.c|||2715|||/*
 * get_ts_parser_oid - find a TS parser by possibly qualified name
 *
 * If not found, returns InvalidOid if missing_ok, else throws error
 */""",
  """TSParserIsVisible|||backend\catalog\namespace.c|||2773|||/*
 * TSParserIsVisible
 *		Determine whether a parser (identified by OID) is visible in the
 *		current search path.  Visible means "would be found by searching
 *		for the unqualified parser name".
 */""",
  """TSParserIsVisibleExt|||backend\catalog\namespace.c|||2785|||/*
 * TSParserIsVisibleExt
 *		As above, but if the parser isn't found and is_missing is not NULL,
 *		then set *is_missing = true and return false instead of throwing
 *		an error.  (Caller must initialize *is_missing = false.)
 */""",
  """get_ts_dict_oid|||backend\catalog\namespace.c|||2860|||/*
 * get_ts_dict_oid - find a TS dictionary by possibly qualified name
 *
 * If not found, returns InvalidOid if missing_ok, else throws error
 */""",
  """TSDictionaryIsVisible|||backend\catalog\namespace.c|||2918|||/*
 * TSDictionaryIsVisible
 *		Determine whether a dictionary (identified by OID) is visible in the
 *		current search path.  Visible means "would be found by searching
 *		for the unqualified dictionary name".
 */""",
  """TSDictionaryIsVisibleExt|||backend\catalog\namespace.c|||2930|||/*
 * TSDictionaryIsVisibleExt
 *		As above, but if the dictionary isn't found and is_missing is not NULL,
 *		then set *is_missing = true and return false instead of throwing
 *		an error.  (Caller must initialize *is_missing = false.)
 */""",
  """get_ts_template_oid|||backend\catalog\namespace.c|||3006|||/*
 * get_ts_template_oid - find a TS template by possibly qualified name
 *
 * If not found, returns InvalidOid if missing_ok, else throws error
 */""",
  """TSTemplateIsVisible|||backend\catalog\namespace.c|||3064|||/*
 * TSTemplateIsVisible
 *		Determine whether a template (identified by OID) is visible in the
 *		current search path.  Visible means "would be found by searching
 *		for the unqualified template name".
 */""",
  """TSTemplateIsVisibleExt|||backend\catalog\namespace.c|||3076|||/*
 * TSTemplateIsVisibleExt
 *		As above, but if the template isn't found and is_missing is not NULL,
 *		then set *is_missing = true and return false instead of throwing
 *		an error.  (Caller must initialize *is_missing = false.)
 */""",
  """get_ts_config_oid|||backend\catalog\namespace.c|||3151|||/*
 * get_ts_config_oid - find a TS config by possibly qualified name
 *
 * If not found, returns InvalidOid if missing_ok, else throws error
 */""",
  """TSConfigIsVisible|||backend\catalog\namespace.c|||3209|||/*
 * TSConfigIsVisible
 *		Determine whether a text search configuration (identified by OID)
 *		is visible in the current search path.  Visible means "would be found
 *		by searching for the unqualified text search configuration name".
 */""",
  """TSConfigIsVisibleExt|||backend\catalog\namespace.c|||3221|||/*
 * TSConfigIsVisibleExt
 *		As above, but if the configuration isn't found and is_missing is not
 *		NULL, then set *is_missing = true and return false instead of throwing
 *		an error.  (Caller must initialize *is_missing = false.)
 */""",
  """DeconstructQualifiedName|||backend\catalog\namespace.c|||3300|||/*
 * DeconstructQualifiedName
 *		Given a possibly-qualified name expressed as a list of String nodes,
 *		extract the schema name and object name.
 *
 * *nspname_p is set to NULL if there is no explicit schema name.
 */""",
  """LookupNamespaceNoError|||backend\catalog\namespace.c|||3354|||/*
 * LookupNamespaceNoError
 *		Look up a schema name.
 *
 * Returns the namespace OID, or InvalidOid if not found.
 *
 * Note this does NOT perform any permissions check --- callers are
 * responsible for being sure that an appropriate check is made.
 * In the majority of cases LookupExplicitNamespace is preferable.
 */""",
  """LookupExplicitNamespace|||backend\catalog\namespace.c|||3384|||/*
 * LookupExplicitNamespace
 *		Process an explicitly-specified schema name: look up the schema
 *		and verify we have USAGE (lookup) rights in it.
 *
 * Returns the namespace OID
 */""",
  """LookupCreationNamespace|||backend\catalog\namespace.c|||3427|||/*
 * LookupCreationNamespace
 *		Look up the schema and verify we have CREATE rights on it.
 *
 * This is just like LookupExplicitNamespace except for the different
 * permission check, and that we are willing to create pg_temp if needed.
 *
 * Note: calling this may result in a CommandCounterIncrement operation,
 * if we have to create or clean out the temp namespace.
 */""",
  """CheckSetNamespace|||backend\catalog\namespace.c|||3458|||/*
 * Common checks on switching namespaces.
 *
 * We complain if either the old or new namespaces is a temporary schema
 * (or temporary toast schema), or if either the old or new namespaces is the
 * TOAST schema.
 */""",
  """QualifiedNameGetCreationNamespace|||backend\catalog\namespace.c|||3486|||/*
 * QualifiedNameGetCreationNamespace
 *		Given a possibly-qualified name for an object (in List-of-Strings
 *		format), determine what namespace the object should be created in.
 *		Also extract and return the object name (last component of list).
 *
 * Note: this does not apply any permissions check.  Callers must check
 * for CREATE rights on the selected namespace when appropriate.
 *
 * Note: calling this may result in a CommandCounterIncrement operation,
 * if we have to create or clean out the temp namespace.
 */""",
  """get_namespace_oid|||backend\catalog\namespace.c|||3534|||/*
 * get_namespace_oid - given a namespace name, look up the OID
 *
 * If missing_ok is false, throw an error if namespace name not found.  If
 * true, just return InvalidOid.
 */""",
  """makeRangeVarFromNameList|||backend\catalog\namespace.c|||3553|||/*
 * makeRangeVarFromNameList
 *		Utility routine to convert a qualified-name list into RangeVar form.
 */""",
  """NameListToString|||backend\catalog\namespace.c|||3593|||/*
 * NameListToString
 *		Utility routine to convert a qualified-name list into a string.
 *
 * This is used primarily to form error messages, and so we do not quote
 * the list elements, for the sake of legibility.
 *
 * In most scenarios the list elements should always be String values,
 * but we also allow A_Star for the convenience of ColumnRef processing.
 */""",
  """NameListToQuotedString|||backend\catalog\namespace.c|||3627|||/*
 * NameListToQuotedString
 *		Utility routine to convert a qualified-name list into a string.
 *
 * Same as above except that names will be double-quoted where necessary,
 * so the string could be re-parsed (eg, by textToQualifiedNameList).
 */""",
  """isTempNamespace|||backend\catalog\namespace.c|||3648|||/*
 * isTempNamespace - is the given namespace my temporary-table namespace?
 */""",
  """isTempToastNamespace|||backend\catalog\namespace.c|||3660|||/*
 * isTempToastNamespace - is the given namespace my temporary-toast-table
 *		namespace?
 */""",
  """isTempOrTempToastNamespace|||backend\catalog\namespace.c|||3672|||/*
 * isTempOrTempToastNamespace - is the given namespace my temporary-table
 *		namespace or my temporary-toast-table namespace?
 */""",
  """isAnyTempNamespace|||backend\catalog\namespace.c|||3686|||/*
 * isAnyTempNamespace - is the given namespace a temporary-table namespace
 * (either my own, or another backend's)?  Temporary-toast-table namespaces
 * are included, too.
 */""",
  """isOtherTempNamespace|||backend\catalog\namespace.c|||3709|||/*
 * isOtherTempNamespace - is the given namespace some other backend's
 * temporary-table namespace (including temporary-toast-table namespaces)?
 *
 * Note: for most purposes in the C code, this function is obsolete.  Use
 * RELATION_IS_OTHER_TEMP() instead to detect non-local temp relations.
 */""",
  """checkTempNamespaceStatus|||backend\catalog\namespace.c|||3728|||/*
 * checkTempNamespaceStatus - is the given namespace owned and actively used
 * by a backend?
 *
 * Note: this can be used while scanning relations in pg_class to detect
 * orphaned temporary tables or namespaces with a backend connected to a
 * given database.  The result may be out of date quickly, so the caller
 * must be careful how to handle this information.
 */""",
  """GetTempNamespaceProcNumber|||backend\catalog\namespace.c|||3765|||/*
 * GetTempNamespaceProcNumber - if the given namespace is a temporary-table
 * namespace (either my own, or another backend's), return the proc number
 * that owns it.  Temporary-toast-table namespaces are included, too.
 * If it isn't a temp namespace, return INVALID_PROC_NUMBER.
 */""",
  """GetTempToastNamespace|||backend\catalog\namespace.c|||3790|||/*
 * GetTempToastNamespace - get the OID of my temporary-toast-table namespace,
 * which must already be assigned.  (This is only used when creating a toast
 * table for a temp table, so we must have already done InitTempTableNamespace)
 */""",
  """GetTempNamespaceState|||backend\catalog\namespace.c|||3804|||/*
 * GetTempNamespaceState - fetch status of session's temporary namespace
 *
 * This is used for conveying state to a parallel worker, and is not meant
 * for general-purpose access.
 */""",
  """SetTempNamespaceState|||backend\catalog\namespace.c|||3820|||/*
 * SetTempNamespaceState - set status of session's temporary namespace
 *
 * This is used for conveying state to a parallel worker, and is not meant for
 * general-purpose access.  By transferring these namespace OIDs to workers,
 * we ensure they will have the same notion of the search path as their leader
 * does.
 */""",
  """GetSearchPathMatcher|||backend\catalog\namespace.c|||3851|||/*
 * GetSearchPathMatcher - fetch current search path definition.
 *
 * The result structure is allocated in the specified memory context
 * (which might or might not be equal to CurrentMemoryContext); but any
 * junk created by revalidation calculations will be in CurrentMemoryContext.
 */""",
  """CopySearchPathMatcher|||backend\catalog\namespace.c|||3888|||/*
 * CopySearchPathMatcher - copy the specified SearchPathMatcher.
 *
 * The result structure is allocated in CurrentMemoryContext.
 */""",
  """SearchPathMatchesCurrentEnvironment|||backend\catalog\namespace.c|||3910|||/*
 * SearchPathMatchesCurrentEnvironment - does path match current environment?
 *
 * This is tested over and over in some common code paths, and in the typical
 * scenario where the active search path seldom changes, it'll always succeed.
 * We make that case fast by keeping a generation counter that is advanced
 * whenever the active search path changes.
 */""",
  """get_collation_oid|||backend\catalog\namespace.c|||3970|||/*
 * get_collation_oid - find a collation by possibly qualified name
 *
 * Note that this will only find collations that work with the current
 * database's encoding.
 */""",
  """get_conversion_oid|||backend\catalog\namespace.c|||4024|||/*
 * get_conversion_oid - find a conversion by possibly qualified name
 */""",
  """FindDefaultConversionProc|||backend\catalog\namespace.c|||4079|||/*
 * FindDefaultConversionProc - find default encoding conversion proc
 */""",
  """preprocessNamespacePath|||backend\catalog\namespace.c|||4106|||/*
 * Look up namespace IDs and perform ACL checks. Return newly-allocated list.
 */""",
  """finalNamespacePath|||backend\catalog\namespace.c|||4197|||/*
 * Remove duplicates, run namespace search hooks, and prepend
 * implicitly-searched namespaces. Return newly-allocated list.
 *
 * If an object_access_hook is present, this must always be recalculated. It
 * may seem that duplicate elimination is not dependent on the result of the
 * hook, but if a hook returns different results on different calls for the
 * same namespace ID, then it could affect the order in which that namespace
 * appears in the final list.
 */""",
  """cachedNamespacePath|||backend\catalog\namespace.c|||4243|||/*
 * Retrieve search path information from the cache; or if not there, fill
 * it. The returned entry is valid only until the next call to this function.
 */""",
  """recomputeNamespacePath|||backend\catalog\namespace.c|||4298|||/*
 * recomputeNamespacePath - recompute path derived variables if needed.
 */""",
  """AccessTempTableNamespace|||backend\catalog\namespace.c|||4361|||/*
 * AccessTempTableNamespace
 *		Provide access to a temporary namespace, potentially creating it
 *		if not present yet.  This routine registers if the namespace gets
 *		in use in this transaction.  'force' can be set to true to allow
 *		the caller to enforce the creation of the temporary namespace for
 *		use in this backend, which happens if its creation is pending.
 */""",
  """InitTempTableNamespace|||backend\catalog\namespace.c|||4389|||/*
 * InitTempTableNamespace
 *		Initialize temp table namespace on first use in a particular backend
 */""",
  """AtEOXact_Namespace|||backend\catalog\namespace.c|||4511|||/*
 * End-of-transaction cleanup for namespaces.
 */""",
  """AtEOSubXact_Namespace|||backend\catalog\namespace.c|||4557|||/*
 * AtEOSubXact_Namespace
 *
 * At subtransaction commit, propagate the temp-namespace-creation
 * flag to the parent subtransaction.
 *
 * At subtransaction abort, forget the flag if we set it up.
 */""",
  """RemoveTempRelations|||backend\catalog\namespace.c|||4597|||/*
 * Remove all relations in the specified temp namespace.
 *
 * This is called at backend shutdown (if we made any temp relations).
 * It is also called when we begin using a pre-existing temp namespace,
 * in order to clean out any relations that might have been created by
 * a crashed backend.
 */""",
  """RemoveTempRelationsCallback|||backend\catalog\namespace.c|||4623|||/*
 * Callback to remove temp relations at backend exit.
 */""",
  """ResetTempTableNamespace|||backend\catalog\namespace.c|||4643|||/*
 * Remove all temp tables from the temporary namespace.
 */""",
  """check_search_path|||backend\catalog\namespace.c|||4656|||/* check_hook: validate new search_path value */""",
  """assign_search_path|||backend\catalog\namespace.c|||4712|||/* assign_hook: do extra actions as needed */""",
  """InitializeSearchPath|||backend\catalog\namespace.c|||4735|||/*
 * InitializeSearchPath: initialize module during InitPostgres.
 *
 * This is called after we are up enough to be able to do catalog lookups.
 */""",
  """InvalidationCallback|||backend\catalog\namespace.c|||4795|||/*
 * InvalidationCallback
 *		Syscache inval callback function
 */""",
  """fetch_search_path|||backend\catalog\namespace.c|||4818|||/*
 * Fetch the active search path. The return value is a palloc'ed list
 * of OIDs; the caller is responsible for freeing this storage as
 * appropriate.
 *
 * The returned list includes the implicitly-prepended namespaces only if
 * includeImplicit is true.
 *
 * Note: calling this may result in a CommandCounterIncrement operation,
 * if we have to create or clean out the temp namespace.
 */""",
  """fetch_search_path_array|||backend\catalog\namespace.c|||4858|||/*
 * Fetch the active search path into a caller-allocated array of OIDs.
 * Returns the number of path entries.  (If this is more than sarray_len,
 * then the data didn't fit and is not all stored.)
 *
 * The returned list always includes the implicitly-prepended namespaces,
 * but never includes the temp namespace.  (This is suitable for existing
 * users, which would want to ignore the temp namespace anyway.)  This
 * definition allows us to not worry about initializing the temp namespace.
 */""",
  """pg_table_is_visible|||backend\catalog\namespace.c|||4893|||/*
 * Export the FooIsVisible functions as SQL-callable functions.
 *
 * Note: as of Postgres 8.4, these will silently return NULL if called on
 * a nonexistent object OID, rather than failing.  This is to avoid race
 * condition errors when a query that's scanning a catalog using an MVCC
 * snapshot uses one of these functions.  The underlying IsVisible functions
 * always use an up-to-date snapshot and so might see the object as already
 * gone when it's still visible to the transaction snapshot.
 */""",
  """pg_type_is_visible|||backend\catalog\namespace.c|||4907|||/*
 * Export the FooIsVisible functions as SQL-callable functions.
 *
 * Note: as of Postgres 8.4, these will silently return NULL if called on
 * a nonexistent object OID, rather than failing.  This is to avoid race
 * condition errors when a query that's scanning a catalog using an MVCC
 * snapshot uses one of these functions.  The underlying IsVisible functions
 * always use an up-to-date snapshot and so might see the object as already
 * gone when it's still visible to the transaction snapshot.
 */""",
  """RunObjectPostCreateHook|||backend\catalog\objectaccess.c|||31|||/*
 * RunObjectPostCreateHook
 *
 * OAT_POST_CREATE object ID based event hook entrypoint
 */""",
  """RunObjectDropHook|||backend\catalog\objectaccess.c|||53|||/*
 * RunObjectDropHook
 *
 * OAT_DROP object ID based event hook entrypoint
 */""",
  """RunObjectTruncateHook|||backend\catalog\objectaccess.c|||75|||/*
 * RunObjectTruncateHook
 *
 * OAT_TRUNCATE object ID based event hook entrypoint
 */""",
  """RunObjectPostAlterHook|||backend\catalog\objectaccess.c|||91|||/*
 * RunObjectPostAlterHook
 *
 * OAT_POST_ALTER object ID based event hook entrypoint
 */""",
  """RunNamespaceSearchHook|||backend\catalog\objectaccess.c|||114|||/*
 * RunNamespaceSearchHook
 *
 * OAT_NAMESPACE_SEARCH object ID based event hook entrypoint
 */""",
  """RunFunctionExecuteHook|||backend\catalog\objectaccess.c|||138|||/*
 * RunFunctionExecuteHook
 *
 * OAT_FUNCTION_EXECUTE object ID based event hook entrypoint
 */""",
  """RunObjectPostCreateHookStr|||backend\catalog\objectaccess.c|||157|||/*
 * RunObjectPostCreateHookStr
 *
 * OAT_POST_CREATE object name based event hook entrypoint
 */""",
  """RunObjectDropHookStr|||backend\catalog\objectaccess.c|||179|||/*
 * RunObjectDropHookStr
 *
 * OAT_DROP object name based event hook entrypoint
 */""",
  """RunObjectTruncateHookStr|||backend\catalog\objectaccess.c|||201|||/*
 * RunObjectTruncateHookStr
 *
 * OAT_TRUNCATE object name based event hook entrypoint
 */""",
  """RunObjectPostAlterHookStr|||backend\catalog\objectaccess.c|||217|||/*
 * RunObjectPostAlterHookStr
 *
 * OAT_POST_ALTER object name based event hook entrypoint
 */""",
  """RunNamespaceSearchHookStr|||backend\catalog\objectaccess.c|||240|||/*
 * RunNamespaceSearchHookStr
 *
 * OAT_NAMESPACE_SEARCH object name based event hook entrypoint
 */""",
  """RunFunctionExecuteHookStr|||backend\catalog\objectaccess.c|||264|||/*
 * RunFunctionExecuteHookStr
 *
 * OAT_FUNCTION_EXECUTE object name based event hook entrypoint
 */""",
  """get_object_address|||backend\catalog\objectaddress.c|||921|||/*
 * Translate an object name and arguments (as passed by the parser) to an
 * ObjectAddress.
 *
 * The returned object will be locked using the specified lockmode.  If a
 * sub-object is looked up, the parent object will be locked instead.
 *
 * If the object is a relation or a child object of a relation (e.g. an
 * attribute or constraint), the relation is also opened and *relp receives
 * the open relcache entry pointer; otherwise, *relp is set to NULL.  This
 * is a bit grotty but it makes life simpler, since the caller will
 * typically need the relcache entry too.  Caller must close the relcache
 * entry when done with it.  The relation is locked with the specified lockmode
 * if the target object is the relation itself or an attribute, but for other
 * child objects, only AccessShareLock is acquired on the relation.
 *
 * If the object is not found, an error is thrown, unless missing_ok is
 * true.  In this case, no lock is acquired, relp is set to NULL, and the
 * returned address has objectId set to InvalidOid.
 *
 * We don't currently provide a function to release the locks acquired here;
 * typically, the lock must be held until commit to guard against a concurrent
 * drop operation.
 *
 * Note: If the object is not found, we don't give any indication of the
 * reason.  (It might have been a missing schema if the name was qualified, or
 * a nonexistent type name in case of a cast, function or operator; etc).
 * Currently there is only one caller that might be interested in such info, so
 * we don't spend much effort here.  If more callers start to care, it might be
 * better to add some support for that in this function.
 */""",
  """get_object_address_rv|||backend\catalog\objectaddress.c|||1219|||/*
 * Return an ObjectAddress based on a RangeVar and an object name. The
 * name of the relation identified by the RangeVar is prepended to the
 * (possibly empty) list passed in as object. This is useful to find
 * the ObjectAddress of objects that depend on a relation. All other
 * considerations are exactly as for get_object_address above.
 */""",
  """get_object_address_unqualified|||backend\catalog\objectaddress.c|||1241|||/*
 * Find an ObjectAddress for a type of object that is identified by an
 * unqualified name.
 */""",
  """get_relation_by_qualified_name|||backend\catalog\objectaddress.c|||1332|||/*
 * Locate a relation by qualified name.
 */""",
  """get_object_address_relobject|||backend\catalog\objectaddress.c|||1414|||/*
 * Find object address for an object that is attached to a relation.
 *
 * Note that we take only an AccessShareLock on the relation.  We need not
 * pass down the LOCKMODE from get_object_address(), because that is the lock
 * mode for the object itself, not the relation to which it is attached.
 */""",
  """get_object_address_attribute|||backend\catalog\objectaddress.c|||1493|||/*
 * Find the ObjectAddress for an attribute.
 */""",
  """get_object_address_attrdef|||backend\catalog\objectaddress.c|||1544|||/*
 * Find the ObjectAddress for an attribute's default value.
 */""",
  """get_object_address_type|||backend\catalog\objectaddress.c|||1602|||/*
 * Find the ObjectAddress for a type or domain
 */""",
  """get_object_address_opcf|||backend\catalog\objectaddress.c|||1641|||/*
 * Find the ObjectAddress for an opclass or opfamily.
 */""",
  """get_object_address_opf_member|||backend\catalog\objectaddress.c|||1679|||/*
 * Find the ObjectAddress for an opclass/opfamily member.
 *
 * (The returned address corresponds to a pg_amop/pg_amproc object).
 */""",
  """get_object_address_usermapping|||backend\catalog\objectaddress.c|||1791|||/*
 * Find the ObjectAddress for a user mapping.
 */""",
  """get_object_address_publication_rel|||backend\catalog\objectaddress.c|||1862|||/*
 * Find the ObjectAddress for a publication relation.  The first element of
 * the object parameter is the relation name, the second is the
 * publication name.
 */""",
  """get_object_address_publication_schema|||backend\catalog\objectaddress.c|||1915|||/*
 * Find the ObjectAddress for a publication schema. The first element of the
 * object parameter is the schema name, the second is the publication name.
 */"""
)
INFO:__main__:Extracted 0 methods with comments in this batch
WARNING:__main__:Empty batch at offset 2400 (count: 1)
INFO:__main__:Fetching batch: offset=2500, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=2500, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres356[0m: [32mList[0m[[32mString[0m] = List(
  """get_object_address_defacl|||backend\catalog\objectaddress.c|||1957|||/*
 * Find the ObjectAddress for a default ACL.
 */""",
  """textarray_to_strvaluelist|||backend\catalog\objectaddress.c|||2073|||/*
 * Convert an array of TEXT into a List of string Values, as emitted by the
 * parser, which is what get_object_address uses as input.
 */""",
  """pg_get_object_address|||backend\catalog\objectaddress.c|||2099|||/*
 * SQL-callable version of get_object_address
 */""",
  """check_object_ownership|||backend\catalog\objectaddress.c|||2381|||/*
 * Check ownership of an object previously identified by get_object_address.
 */""",
  """get_object_namespace|||backend\catalog\objectaddress.c|||2563|||/*
 * get_object_namespace
 *
 * Find the schema containing the specified object.  For non-schema objects,
 * this function returns InvalidOid.
 */""",
  """read_objtype_from_string|||backend\catalog\objectaddress.c|||2599|||/*
 * Return ObjectType for the given object type as given by
 * getObjectTypeDescription; if no valid ObjectType code exists, but it's a
 * possible output type from getObjectTypeDescription, return -1.
 * Otherwise, an error is thrown.
 */""",
  """get_object_class_descr|||backend\catalog\objectaddress.c|||2619|||/*
 * Interfaces to reference fields of ObjectPropertyType
 */""",
  """get_object_oid_index|||backend\catalog\objectaddress.c|||2627|||/*
 * Interfaces to reference fields of ObjectPropertyType
 */""",
  """get_object_catcache_oid|||backend\catalog\objectaddress.c|||2635|||/*
 * Interfaces to reference fields of ObjectPropertyType
 */""",
  """get_object_catcache_name|||backend\catalog\objectaddress.c|||2643|||/*
 * Interfaces to reference fields of ObjectPropertyType
 */""",
  """get_object_type|||backend\catalog\objectaddress.c|||2698|||/*
 * get_object_type
 *
 * Return the object type associated with a given object.  This routine
 * is primarily used to determine the object type to mention in ACL check
 * error messages, so it's desirable for it to avoid failing.
 */""",
  """get_object_namensp_unique|||backend\catalog\objectaddress.c|||2716|||/*
 * get_object_type
 *
 * Return the object type associated with a given object.  This routine
 * is primarily used to determine the object type to mention in ACL check
 * error messages, so it's desirable for it to avoid failing.
 */""",
  """is_objectclass_supported|||backend\catalog\objectaddress.c|||2728|||/*
 * Return whether we have useful data for the given object class in the
 * ObjectProperty table.
 */""",
  """get_object_property_data|||backend\catalog\objectaddress.c|||2745|||/*
 * Find ObjectProperty structure by class_id.
 */""",
  """get_catalog_object_by_oid|||backend\catalog\objectaddress.c|||2780|||/*
 * Return a copy of the tuple for the object with the given object OID, from
 * the given catalog (which must have been opened by the caller and suitably
 * locked).  NULL is returned if the OID is not found.
 *
 * We try a syscache first, if available.
 */""",
  """get_catalog_object_by_oid_extended|||backend\catalog\objectaddress.c|||2793|||/*
 * Same as get_catalog_object_by_oid(), but with an additional "locktup"
 * argument controlling whether to acquire a LOCKTAG_TUPLE at mode
 * InplaceUpdateTupleLock.  See README.tuplock section "Locking to write
 * inplace-updated tables".
 */""",
  """getPublicationSchemaInfo|||backend\catalog\objectaddress.c|||2854|||/*
 * getPublicationSchemaInfo
 *
 * Get publication name and schema name from the object address into pubname and
 * nspname. Both pubname and nspname are palloc'd strings which will be freed by
 * the caller.
 */""",
  """getObjectDescription|||backend\catalog\objectaddress.c|||2902|||/*
 * getObjectDescription: build an object description for messages
 *
 * The result is a palloc'd string.  NULL is returned for an undefined
 * object if missing_ok is true, else an error is generated.
 */""",
  """getObjectDescriptionOids|||backend\catalog\objectaddress.c|||4070|||/*
 * getObjectDescriptionOids: as above, except the object is specified by Oids
 */""",
  """getRelationDescription|||backend\catalog\objectaddress.c|||4087|||/*
 * subroutine for getObjectDescription: describe a relation
 *
 * The result is appended to "buffer".
 */""",
  """getOpFamilyDescription|||backend\catalog\objectaddress.c|||4162|||/*
 * subroutine for getObjectDescription: describe an operator family
 */""",
  """pg_describe_object|||backend\catalog\objectaddress.c|||4204|||/*
 * SQL-level callable version of getObjectDescription
 */""",
  """pg_identify_object|||backend\catalog\objectaddress.c|||4232|||/*
 * SQL-level callable function to obtain object type + identity
 */""",
  """pg_identify_object_as_address|||backend\catalog\objectaddress.c|||4349|||/*
 * SQL-level callable function to obtain object type + identity
 */""",
  """getObjectTypeDescription|||backend\catalog\objectaddress.c|||4412|||/*
 * Return a palloc'ed string that describes the type of object that the
 * passed address is for.
 *
 * Keep ObjectTypeMap in sync with this.
 */""",
  """getRelationTypeDescription|||backend\catalog\objectaddress.c|||4602|||/*
 * subroutine for getObjectTypeDescription: describe a relation type
 */""",
  """getConstraintTypeDescription|||backend\catalog\objectaddress.c|||4665|||/*
 * subroutine for getObjectTypeDescription: describe a constraint type
 */""",
  """getProcedureTypeDescription|||backend\catalog\objectaddress.c|||4702|||/*
 * subroutine for getObjectTypeDescription: describe a procedure type
 */""",
  """getObjectIdentity|||backend\catalog\objectaddress.c|||4739|||/*
 * Obtain a given object's identity, as a palloc'ed string.
 *
 * This is for machine consumption, so it's not translated.  All elements are
 * schema-qualified when appropriate.  Returns NULL if the object could not
 * be found.
 */""",
  """getObjectIdentityParts|||backend\catalog\objectaddress.c|||4754|||/*
 * As above, but more detailed.
 *
 * There are two sets of return values: the identity itself as a palloc'd
 * string is returned.  objname and objargs, if not NULL, are output parameters
 * that receive lists of C-strings that are useful to give back to
 * get_object_address() to reconstruct the ObjectAddress.  Returns NULL if
 * the object could not be found.
 */""",
  """getRelationIdentity|||backend\catalog\objectaddress.c|||6008|||/*
 * Append the relation identity (quoted qualified name) to the given
 * StringInfo.
 */""",
  """strlist_to_textarray|||backend\catalog\objectaddress.c|||6042|||/*
 * Auxiliary function to build a TEXT array out of a list of C-strings.
 */""",
  """get_relkind_objtype|||backend\catalog\objectaddress.c|||6097|||/*
 * get_relkind_objtype
 *
 * Return the object type for the relkind given by the caller.
 *
 * If an unexpected relkind is passed, we say OBJECT_TABLE rather than
 * failing.  That's because this is mostly used for generating error messages
 * for failed ACL checks on relations, and we'd rather produce a generic
 * message saying "table" than fail entirely.
 */""",
  """get_partition_parent|||backend\catalog\partition.c|||52|||/*
 * get_partition_parent
 *		Obtain direct parent of given relation
 *
 * Returns inheritance parent of a partition by scanning pg_inherits
 *
 * If the partition is in the process of being detached, an error is thrown,
 * unless even_if_detached is passed as true.
 *
 * Note: Because this function assumes that the relation whose OID is passed
 * as an argument will have precisely one parent, it should only be called
 * when it is known that the relation is a partition.
 */""",
  """get_partition_parent_worker|||backend\catalog\partition.c|||84|||/*
 * get_partition_parent_worker
 *		Scan the pg_inherits relation to return the OID of the parent of the
 *		given relation
 *
 * If the partition is being detached, *detach_pending is set true (but the
 * original parent is still returned.)
 */""",
  """get_partition_ancestors|||backend\catalog\partition.c|||133|||/*
 * get_partition_ancestors
 *		Obtain ancestors of given relation
 *
 * Returns a list of ancestors of the given relation.  The list is ordered:
 * The first element is the immediate parent and the last one is the topmost
 * parent in the partition hierarchy.
 *
 * Note: Because this function assumes that the relation whose OID is passed
 * as an argument and each ancestor will have precisely one parent, it should
 * only be called when it is known that the relation is a partition.
 */""",
  """get_partition_ancestors_worker|||backend\catalog\partition.c|||152|||/*
 * get_partition_ancestors_worker
 *		recursive worker for get_partition_ancestors
 */""",
  """index_get_partition|||backend\catalog\partition.c|||175|||/*
 * index_get_partition
 *		Return the OID of index of the given partition that is a child
 *		of the given index, or InvalidOid if there isn't one.
 */""",
  """map_partition_varattnos|||backend\catalog\partition.c|||221|||/*
 * map_partition_varattnos - maps varattnos of all Vars in 'expr' (that have
 * varno 'fromrel_varno') from the attnums of 'from_rel' to the attnums of
 * 'to_rel', each of which may be either a leaf partition or a partitioned
 * table, but both of which must be from the same partitioning hierarchy.
 *
 * We need this because even though all of the same column names must be
 * present in all relations in the hierarchy, and they must also have the
 * same types, the attnums may be different.
 *
 * Note: this will work on any node tree, so really the argument and result
 * should be declared "Node *".  But a substantial majority of the callers
 * are working on Lists, so it's less messy to do the casts internally.
 */""",
  """has_partition_attrs|||backend\catalog\partition.c|||254|||/*
 * Checks if any of the 'attnums' is a partition key attribute for rel
 *
 * Sets *used_in_expr if any of the 'attnums' is found to be referenced in some
 * partition key expression.  It's possible for a column to be both used
 * directly and as part of an expression; if that happens, *used_in_expr may
 * end up as either true or false.  That's OK for current uses of this
 * function, because *used_in_expr is only used to tailor the error message
 * text.
 */""",
  """get_default_partition_oid|||backend\catalog\partition.c|||314|||/*
 * get_default_partition_oid
 *
 * Given a relation OID, return the OID of the default partition, if one
 * exists.  Use get_default_oid_from_partdesc where possible, for
 * efficiency.
 */""",
  """update_default_partition_oid|||backend\catalog\partition.c|||339|||/*
 * update_default_partition_oid
 *
 * Update pg_partitioned_table.partdefid with a new default partition OID.
 */""",
  """get_proposed_default_constraint|||backend\catalog\partition.c|||369|||/*
 * get_proposed_default_constraint
 *
 * This function returns the negation of new_part_constraints, which
 * would be an integral part of the default partition constraints after
 * addition of the partition to which the new_part_constraints belongs.
 */""",
  """AggregateCreate|||backend\catalog\pg_aggregate.c|||45|||/*
 * AggregateCreate
 */""",
  """lookup_agg_function|||backend\catalog\pg_aggregate.c|||825|||/*
 * lookup_agg_function
 * common code for finding aggregate support functions
 *
 * fnName: possibly-schema-qualified function name
 * nargs, input_types: expected function argument types
 * variadicArgType: type of variadic argument if any, else InvalidOid
 *
 * Returns OID of function, and stores its return type into *rettype
 *
 * NB: must not scribble on input_types[], as we may re-use those
 */""",
  """StoreAttrDefault|||backend\catalog\pg_attrdef.c|||45|||/*
 * Store a default expression for column attnum of relation rel.
 *
 * Returns the OID of the new pg_attrdef tuple.
 *
 * add_column_mode must be true if we are storing the default for a new
 * attribute, and false if it's for an already existing attribute. The reason
 * for this is that the missing value must never be updated after it is set,
 * which can only be when a column is added to the table. Otherwise we would
 * in effect be changing existing tuples.
 */""",
  """RemoveAttrDefault|||backend\catalog\pg_attrdef.c|||218|||/*
 *		RemoveAttrDefault
 *
 * If the specified relation/attribute has a default, remove it.
 * (If no default, raise error if complain is true, else return quietly.)
 */""",
  """RemoveAttrDefaultById|||backend\catalog\pg_attrdef.c|||273|||/*
 *		RemoveAttrDefaultById
 *
 * Remove a pg_attrdef entry specified by OID.  This is the guts of
 * attribute-default removal.  Note it should be called via performDeletion,
 * not directly.
 */""",
  """GetAttrDefaultOid|||backend\catalog\pg_attrdef.c|||344|||/*
 * Get the pg_attrdef OID of the default expression for a column
 * identified by relation OID and column number.
 *
 * Returns InvalidOid if there is no such pg_attrdef entry.
 */""",
  """GetAttrDefaultColumnAddress|||backend\catalog\pg_attrdef.c|||386|||/*
 * Given a pg_attrdef OID, return the relation OID and column number of
 * the owning column (represented as an ObjectAddress for convenience).
 *
 * Returns InvalidObjectAddress if there is no such pg_attrdef entry.
 */""",
  """CastCreate|||backend\catalog\pg_cast.c|||48|||/*
 * ----------------------------------------------------------------
 *		CastCreate
 *
 * Forms and inserts catalog tuples for a new cast being created.
 * Caller must have already checked privileges, and done consistency
 * checks on the given datatypes and cast function (if applicable).
 *
 * Since we allow binary coercibility of the datatypes to the cast
 * function's input and result, there could be one or two WITHOUT FUNCTION
 * casts that this one depends on.  We don't record that explicitly
 * in pg_cast, but we still need to make dependencies on those casts.
 *
 * 'behavior' indicates the types of the dependencies that the new
 * cast will have on its input and output types, the cast function,
 * and the other casts if any.
 * ----------------------------------------------------------------
 */""",
  """errdetail_relkind_not_supported|||backend\catalog\pg_class.c|||23|||/*
 * Issue an errdetail() informing that the relkind is not supported for this
 * operation.
 */""",
  """CollationCreate|||backend\catalog\pg_collation.c|||41|||/*
 * CollationCreate
 *
 * Add a new tuple to pg_collation.
 *
 * if_not_exists: if true, don't fail on duplicate name, just print a notice
 * and return InvalidOid.
 * quiet: if true, don't fail on duplicate name, just silently return
 * InvalidOid (overrides if_not_exists).
 */""",
  """CreateConstraintEntry|||backend\catalog\pg_constraint.c|||47|||/*
 * CreateConstraintEntry
 *	Create a constraint table entry.
 *
 * Subsidiary records (such as triggers or indexes to implement the
 * constraint) are *not* created here.  But we do make dependency links
 * from the constraint to the things it depends on.
 *
 * The new constraint's OID is returned.
 */""",
  """ConstraintNameIsUsed|||backend\catalog\pg_constraint.c|||398|||/*
 * Test whether given name is currently used as a constraint name
 * for the given object (relation or domain).
 *
 * This is used to decide whether to accept a user-specified constraint name.
 * It is deliberately not the same test as ChooseConstraintName uses to decide
 * whether an auto-generated name is OK: here, we will allow it unless there
 * is an identical constraint name in use *on the same object*.
 *
 * NB: Caller should hold exclusive lock on the given object, else
 * this test can be fooled by concurrent additions.
 */""",
  """ConstraintNameExists|||backend\catalog\pg_constraint.c|||443|||/*
 * Does any constraint of the given name exist in the given namespace?
 *
 * This is used for code that wants to match ChooseConstraintName's rule
 * that we should avoid autogenerating duplicate constraint names within a
 * namespace.
 */""",
  """ChooseConstraintName|||backend\catalog\pg_constraint.c|||497|||/*
 * Select a nonconflicting name for a new constraint.
 *
 * The objective here is to choose a name that is unique within the
 * specified namespace.  Postgres does not require this, but the SQL
 * spec does, and some apps depend on it.  Therefore we avoid choosing
 * default names that so conflict.
 *
 * name1, name2, and label are used the same way as for makeObjectName(),
 * except that the label can't be NULL; digits will be appended to the label
 * if needed to create a name that is unique within the specified namespace.
 *
 * 'others' can be a list of string names already chosen within the current
 * command (but not yet reflected into the catalogs); we will not choose
 * a duplicate of one of these either.
 *
 * Note: it is theoretically possible to get a collision anyway, if someone
 * else chooses the same name concurrently.  This is fairly unlikely to be
 * a problem in practice, especially if one is holding an exclusive lock on
 * the relation identified by name1.
 *
 * Returns a palloc'd string.
 */""",
  """findDomainNotNullConstraint|||backend\catalog\pg_constraint.c|||568|||/*
 * Find and return the pg_constraint tuple that implements a validated
 * not-null constraint for the given domain.
 */""",
  """RemoveConstraintById|||backend\catalog\pg_constraint.c|||611|||/*
 * Delete a single constraint record.
 */""",
  """RenameConstraintById|||backend\catalog\pg_constraint.c|||702|||/*
 * RenameConstraintById
 *		Rename a constraint.
 *
 * Note: this isn't intended to be a user-exposed function; it doesn't check
 * permissions etc.  Currently this is only invoked when renaming an index
 * that is associated with a constraint, but it's made a little more general
 * than that with the expectation of someday having ALTER TABLE RENAME
 * CONSTRAINT.
 */""",
  """AlterConstraintNamespaces|||backend\catalog\pg_constraint.c|||754|||/*
 * AlterConstraintNamespaces
 *		Find any constraints belonging to the specified object,
 *		and move them to the specified new namespace.
 *
 * isType indicates whether the owning object is a type or a relation.
 */""",
  """ConstraintSetParentConstraint|||backend\catalog\pg_constraint.c|||823|||/*
 * ConstraintSetParentConstraint
 *		Set a partition's constraint as child of its parent constraint,
 *		or remove the linkage if parentConstrId is InvalidOid.
 *
 * This updates the constraint's pg_constraint row to show it as inherited, and
 * adds PARTITION dependencies to prevent the constraint from being deleted
 * on its own.  Alternatively, reverse that.
 */""",
  """get_relation_constraint_oid|||backend\catalog\pg_constraint.c|||896|||/*
 * get_relation_constraint_oid
 *		Find a constraint on the specified relation with the specified name.
 *		Returns constraint's OID.
 */""",
  """get_relation_constraint_attnos|||backend\catalog\pg_constraint.c|||953|||/*
 * get_relation_constraint_attnos
 *		Find a constraint on the specified relation with the specified name
 *		and return the constrained columns.
 *
 * Returns a Bitmapset of the column attnos of the constrained columns, with
 * attnos being offset by FirstLowInvalidHeapAttributeNumber so that system
 * columns can be represented.
 *
 * *constraintOid is set to the OID of the constraint, or InvalidOid on
 * failure.
 */""",
  """get_relation_idx_constraint_oid|||backend\catalog\pg_constraint.c|||1042|||/*
 * Return the OID of the constraint enforced by the given index in the
 * given relation; or InvalidOid if no such index is cataloged.
 *
 * Much like get_constraint_index, this function is concerned only with the
 * one constraint that "owns" the given index.  Therefore, constraints of
 * types other than unique, primary-key, and exclusion are ignored.
 */""",
  """get_domain_constraint_oid|||backend\catalog\pg_constraint.c|||1089|||/*
 * get_domain_constraint_oid
 *		Find a constraint on the specified domain with the specified name.
 *		Returns constraint's OID.
 */""",
  """get_primary_key_attnos|||backend\catalog\pg_constraint.c|||1148|||/*
 * get_primary_key_attnos
 *		Identify the columns in a relation's primary key, if any.
 *
 * Returns a Bitmapset of the column attnos of the primary key's columns,
 * with attnos being offset by FirstLowInvalidHeapAttributeNumber so that
 * system columns can be represented.
 *
 * If there is no primary key, return NULL.  We also return NULL if the pkey
 * constraint is deferrable and deferrableOk is false.
 *
 * *constraintOid is set to the OID of the pkey constraint, or InvalidOid
 * on failure.
 */""",
  """DeconstructFkConstraintRow|||backend\catalog\pg_constraint.c|||1234|||/*
 * Extract data from the pg_constraint tuple of a foreign-key constraint.
 *
 * All arguments save the first are output arguments.  All output arguments
 * other than numfks, conkey and confkey can be passed as NULL if caller
 * doesn't need them.
 */""",
  """check_functional_grouping|||backend\catalog\pg_constraint.c|||1366|||/*
 * Determine whether a relation can be proven functionally dependent on
 * a set of grouping columns.  If so, return true and add the pg_constraint
 * OIDs of the constraints needed for the proof to the *constraintDeps list.
 *
 * grouping_columns is a list of grouping expressions, in which columns of
 * the rel of interest are Vars with the indicated varno/varlevelsup.
 *
 * Currently we only check to see if the rel has a primary key that is a
 * subset of the grouping_columns.  We could also use plain unique constraints
 * if all their columns are known not null, but there's a problem: we need
 * to be able to represent the not-null-ness as part of the constraints added
 * to *constraintDeps.  FIXME whenever not-null constraints get represented
 * in pg_constraint.
 */""",
  """ConversionCreate|||backend\catalog\pg_conversion.c|||37|||/*
 * ConversionCreate
 *
 * Add a new tuple to pg_conversion.
 */""",
  """FindDefaultConversion|||backend\catalog\pg_conversion.c|||151|||/*
 * FindDefaultConversion
 *
 * Find "default" conversion proc by for_encoding and to_encoding in the
 * given namespace.
 *
 * If found, returns the procedure's oid, otherwise InvalidOid.  Note that
 * you get the procedure's OID not the conversion's OID!
 */""",
  """AlterSetting|||backend\catalog\pg_db_role_setting.c|||23|||/*
 * pg_db_role_setting.c
 *		Routines to support manipulation of the pg_db_role_setting relation
 *
 * Portions Copyright (c) 1996-2024, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 * IDENTIFICATION
 *		src/backend/catalog/pg_db_role_setting.c
 */""",
  """DropSetting|||backend\catalog\pg_db_role_setting.c|||169|||/*
 * Drop some settings from the catalog.  These can be for a particular
 * database, or for a particular role.  (It is of course possible to do both
 * too, but it doesn't make sense for current uses.)
 */""",
  """ApplySetting|||backend\catalog\pg_db_role_setting.c|||219|||/*
 * Scan pg_db_role_setting looking for applicable settings, and load them on
 * the current process.
 *
 * relsetting is pg_db_role_setting, already opened and locked.
 *
 * Note: we only consider setting for the exact databaseid/roleid combination.
 * This probably needs to be called more than once, with InvalidOid passed as
 * databaseid/roleid.
 */""",
  """recordDependencyOn|||backend\catalog\pg_depend.c|||45|||/*
 * Record a dependency between 2 objects via their respective objectAddress.
 * The first argument is the dependent object, the second the one it
 * references.
 *
 * This simply creates an entry in pg_depend, without any other processing.
 */""",
  """recordMultipleDependencies|||backend\catalog\pg_depend.c|||57|||/*
 * Record multiple dependencies (of the same kind) for a single dependent
 * object.  This has a little less overhead than recording each separately.
 */""",
  """recordDependencyOnCurrentExtension|||backend\catalog\pg_depend.c|||193|||/*
 * If we are executing a CREATE EXTENSION operation, mark the given object
 * as being a member of the extension, or check that it already is one.
 * Otherwise, do nothing.
 *
 * This must be called during creation of any user-definable object type
 * that could be a member of an extension.
 *
 * isReplace must be true if the object already existed, and false if it is
 * newly created.  In the former case we insist that it already be a member
 * of the current extension.  In the latter case we can skip checking whether
 * it is already a member of any extension.
 *
 * Note: isReplace = true is typically used when updating an object in
 * CREATE OR REPLACE and similar commands.  We used to allow the target
 * object to not already be an extension member, instead silently absorbing
 * it into the current extension.  However, this was both error-prone
 * (extensions might accidentally overwrite free-standing objects) and
 * a security hazard (since the object would retain its previous ownership).
 */""",
  """checkMembershipInCurrentExtension|||backend\catalog\pg_depend.c|||258|||/*
 * If we are executing a CREATE EXTENSION operation, check that the given
 * object is a member of the extension, and throw an error if it isn't.
 * Otherwise, do nothing.
 *
 * This must be called whenever a CREATE IF NOT EXISTS operation (for an
 * object type that can be an extension member) has found that an object of
 * the desired name already exists.  It is insecure for an extension to use
 * IF NOT EXISTS except when the conflicting object is already an extension
 * member; otherwise a hostile user could substitute an object with arbitrary
 * properties.
 */""",
  """deleteDependencyRecordsFor|||backend\catalog\pg_depend.c|||301|||/*
 * deleteDependencyRecordsFor -- delete all records with given depender
 * classId/objectId.  Returns the number of records deleted.
 *
 * This is used when redefining an existing object.  Links leading to the
 * object do not change, and links leading from it will be recreated
 * (possibly with some differences from before).
 *
 * If skipExtensionDeps is true, we do not delete any dependencies that
 * show that the given object is a member of an extension.  This avoids
 * needing a lot of extra logic to fetch and recreate that dependency.
 */""",
  """deleteDependencyRecordsForClass|||backend\catalog\pg_depend.c|||351|||/*
 * deleteDependencyRecordsForClass -- delete all records with given depender
 * classId/objectId, dependee classId, and deptype.
 * Returns the number of records deleted.
 *
 * This is a variant of deleteDependencyRecordsFor, useful when revoking
 * an object property that is expressed by a dependency record (such as
 * extension membership).
 */""",
  """deleteDependencyRecordsForSpecific|||backend\catalog\pg_depend.c|||398|||/*
 * deleteDependencyRecordsForSpecific -- delete all records with given depender
 * classId/objectId, dependee classId/objectId, of the given deptype.
 * Returns the number of records deleted.
 */""",
  """changeDependencyFor|||backend\catalog\pg_depend.c|||457|||/*
 * Adjust dependency record(s) to point to a different object of the same type
 *
 * classId/objectId specify the referencing object.
 * refClassId/oldRefObjectId specify the old referenced object.
 * newRefObjectId is the new referenced object (must be of class refClassId).
 *
 * Note the lack of objsubid parameters.  If there are subobject references
 * they will all be readjusted.  Also, there is an expectation that we are
 * dealing with NORMAL dependencies: if we have to replace an (implicit)
 * dependency on a pinned object with an explicit dependency on an unpinned
 * one, the new one will be NORMAL.
 *
 * Returns the number of records updated -- zero indicates a problem.
 */""",
  """changeDependenciesOf|||backend\catalog\pg_depend.c|||565|||/*
 * Adjust all dependency records to come from a different object of the same type
 *
 * classId/oldObjectId specify the old referencing object.
 * newObjectId is the new referencing object (must be of class classId).
 *
 * Returns the number of records updated.
 */""",
  """changeDependenciesOn|||backend\catalog\pg_depend.c|||621|||/*
 * Adjust all dependency records to point to a different object of the same type
 *
 * refClassId/oldRefObjectId specify the old referenced object.
 * newRefObjectId is the new referenced object (must be of class refClassId).
 *
 * Returns the number of records updated.
 */""",
  """isObjectPinned|||backend\catalog\pg_depend.c|||709|||/*
 * isObjectPinned()
 *
 * Test if an object is required for basic database functionality.
 *
 * The passed subId, if any, is ignored; we assume that only whole objects
 * are pinned (and that this implies pinning their components).
 */""",
  """getExtensionOfObject|||backend\catalog\pg_depend.c|||732|||/*
 * Find the extension containing the specified object, if any
 *
 * Returns the OID of the extension, or InvalidOid if the object does not
 * belong to any extension.
 *
 * Extension membership is marked by an EXTENSION dependency from the object
 * to the extension.  Note that the result will be indeterminate if pg_depend
 * contains links from this object to more than one extension ... but that
 * should never happen.
 */""",
  """getAutoExtensionsOfObject|||backend\catalog\pg_depend.c|||778|||/*
 * Return (possibly NIL) list of extensions that the given object depends on
 * in DEPENDENCY_AUTO_EXTENSION mode.
 */""",
  """sequenceIsOwned|||backend\catalog\pg_depend.c|||828|||/*
 * Detect whether a sequence is marked as "owned" by a column
 *
 * An ownership marker is an AUTO or INTERNAL dependency from the sequence to the
 * column.  If we find one, store the identity of the owning column
 * into *tableId and *colId and return true; else return false.
 *
 * Note: if there's more than one such pg_depend entry then you get
 * a random one of them returned into the out parameters.  This should
 * not happen, though.
 */""",
  """getOwnedSequences_internal|||backend\catalog\pg_depend.c|||877|||/*
 * Collect a list of OIDs of all sequences owned by the specified relation,
 * and column if specified.  If deptype is not zero, then only find sequences
 * with the specified dependency type.
 */""",
  """getOwnedSequences|||backend\catalog\pg_depend.c|||936|||/*
 * Collect a list of OIDs of all sequences owned (identity or serial) by the
 * specified relation.
 */""",
  """getIdentitySequence|||backend\catalog\pg_depend.c|||945|||/*
 * Get owned identity sequence, error if not exactly one.
 */""",
  """get_index_constraint|||backend\catalog\pg_depend.c|||988|||/*
 * get_index_constraint
 *		Given the OID of an index, return the OID of the owning unique,
 *		primary-key, or exclusion constraint, or InvalidOid if there
 *		is no owning constraint.
 */""",
  """get_index_ref_constraints|||backend\catalog\pg_depend.c|||1044|||/*
 * get_index_ref_constraints
 *		Given the OID of an index, return the OID of all foreign key
 *		constraints which reference the index.
 */""",
  """EnumValuesCreate|||backend\catalog\pg_enum.c|||83|||/*
 * EnumValuesCreate
 *		Create an entry in pg_enum for each of the supplied enum values.
 *
 * vals is a list of String values.
 *
 * We assume that this is called only by CREATE TYPE AS ENUM, and that it
 * will be called even if the vals list is empty.  So we can enter the
 * enum type's OID into uncommitted_enum_types here, rather than needing
 * another entry point to do it.
 */""",
  """EnumValuesDelete|||backend\catalog\pg_enum.c|||223|||/*
 * EnumValuesDelete
 *		Remove all the pg_enum entries for the specified enum type.
 */""",
  """init_uncommitted_enum_types|||backend\catalog\pg_enum.c|||254|||/*
 * Initialize the uncommitted enum types table for this transaction.
 */""",
  """init_uncommitted_enum_values|||backend\catalog\pg_enum.c|||271|||/*
 * Initialize the uncommitted enum values table for this transaction.
 */""",
  """AddEnumLabel|||backend\catalog\pg_enum.c|||291|||/*
 * AddEnumLabel
 *		Add a new label to the enum set. By default it goes at
 *		the end, but the user can choose to place it before or
 *		after any existing set member.
 */""",
  """RenameEnumLabel|||backend\catalog\pg_enum.c|||606|||/*
 * RenameEnumLabel
 *		Rename a label in an enum set.
 */""",
  """EnumTypeUncommitted|||backend\catalog\pg_enum.c|||689|||/*
 * Test if the given type OID is in the table of uncommitted enum types.
 */"""
)
INFO:__main__:Extracted 15 methods with comments in this batch
INFO:__main__:Fetching batch: offset=2600, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=2600, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres357[0m: [32mList[0m[[32mString[0m] = List(
  """EnumUncommitted|||backend\catalog\pg_enum.c|||707|||/*
 * Test if the given enum value is in the table of uncommitted enum values.
 */""",
  """AtEOXact_Enum|||backend\catalog\pg_enum.c|||725|||/*
 * Clean up enum stuff after end of top-level transaction.
 */""",
  """RenumberEnumType|||backend\catalog\pg_enum.c|||760|||/*
 * RenumberEnumType
 *		Renumber existing enum elements to have sort positions 1..n.
 *
 * We avoid doing this unless absolutely necessary; in most installations
 * it will never happen.  The reason is that updating existing pg_enum
 * entries creates hazards for other backends that are concurrently reading
 * pg_enum.  Although system catalog scans now use MVCC semantics, the
 * syscache machinery might read different pg_enum entries under different
 * snapshots, so some other backend might get confused about the proper
 * ordering if a concurrent renumbering occurs.
 *
 * We therefore make the following choices:
 *
 * 1. Any code that is interested in the enumsortorder values MUST read
 * all the relevant pg_enum entries with a single MVCC snapshot, or else
 * acquire lock on the enum type to prevent concurrent execution of
 * AddEnumLabel().
 *
 * 2. Code that is not examining enumsortorder can use a syscache
 * (for example, enum_in and enum_out do so).
 */""",
  """sort_order_cmp|||backend\catalog\pg_enum.c|||796|||/* qsort comparison function for tuples by sort order */""",
  """EstimateUncommittedEnumsSpace|||backend\catalog\pg_enum.c|||812|||/* qsort comparison function for tuples by sort order */""",
  """SerializeUncommittedEnums|||backend\catalog\pg_enum.c|||826|||/* qsort comparison function for tuples by sort order */""",
  """find_inheritance_children|||backend\catalog\pg_inherits.c|||57|||/*
 * find_inheritance_children
 *
 * Returns a list containing the OIDs of all relations which
 * inherit *directly* from the relation with OID 'parentrelId'.
 *
 * The specified lock type is acquired on each child relation (but not on the
 * given rel; caller should already have locked it).  If lockmode is NoLock
 * then no locks are acquired, but caller must beware of race conditions
 * against possible DROPs of child relations.
 *
 * Partitions marked as being detached are omitted; see
 * find_inheritance_children_extended for details.
 */""",
  """find_inheritance_children_extended|||backend\catalog\pg_inherits.c|||81|||/*
 * find_inheritance_children_extended
 *
 * As find_inheritance_children, with more options regarding detached
 * partitions.
 *
 * If a partition's pg_inherits row is marked "detach pending",
 * *detached_exist (if not null) is set true.
 *
 * If omit_detached is true and there is an active snapshot (not the same as
 * the catalog snapshot used to scan pg_inherits!) and a pg_inherits tuple
 * marked "detach pending" is visible to that snapshot, then that partition is
 * omitted from the output list.  This makes partitions invisible depending on
 * whether the transaction that marked those partitions as detached appears
 * committed to the active snapshot.  In addition, *detached_xmin (if not null)
 * is set to the xmin of the row of the detached partition.
 */""",
  """find_all_inheritors|||backend\catalog\pg_inherits.c|||254|||/*
 * find_all_inheritors -
 *		Returns a list of relation OIDs including the given rel plus
 *		all relations that inherit from it, directly or indirectly.
 *		Optionally, it also returns the number of parents found for
 *		each such relation within the inheritance tree rooted at the
 *		given rel.
 *
 * The specified lock type is acquired on all child relations (but not on the
 * given rel; caller should already have locked it).  If lockmode is NoLock
 * then no locks are acquired, but caller must beware of race conditions
 * against possible DROPs of child relations.
 *
 * NB - No current callers of this routine are interested in children being
 * concurrently detached, so there's no provision to include them.
 */""",
  """has_subclass|||backend\catalog\pg_inherits.c|||354|||/*
 * has_subclass - does this relation have any children?
 *
 * In the current implementation, has_subclass returns whether a
 * particular class *might* have a subclass. It will not return the
 * correct result if a class had a subclass which was later dropped.
 * This is because relhassubclass in pg_class is not updated immediately
 * when a subclass is dropped, primarily because of concurrency concerns.
 *
 * Currently has_subclass is only used as an efficiency hack to skip
 * unnecessary inheritance searches, so this is OK.  Note that ANALYZE
 * on a childless table will clean up the obsolete relhassubclass flag.
 *
 * Although this doesn't actually touch pg_inherits, it seems reasonable
 * to keep it here since it's normally used with the other routines here.
 */""",
  """has_superclass|||backend\catalog\pg_inherits.c|||376|||/*
 * has_superclass - does this relation inherit from another?
 *
 * Unlike has_subclass, this can be relied on to give an accurate answer.
 * However, the caller must hold a lock on the given relation so that it
 * can't be concurrently added to or removed from an inheritance hierarchy.
 */""",
  """typeInheritsFrom|||backend\catalog\pg_inherits.c|||405|||/*
 * Given two type OIDs, determine whether the first is a complex type
 * (class type) that inherits from the second.
 *
 * This essentially asks whether the first type is guaranteed to be coercible
 * to the second.  Therefore, we allow the first type to be a domain over a
 * complex type that inherits from the second; that creates no difficulties.
 * But the second type cannot be a domain.
 */""",
  """StoreSingleInheritance|||backend\catalog\pg_inherits.c|||507|||/*
 * Create a single pg_inherits row with the given data
 */""",
  """DeleteInheritsTuple|||backend\catalog\pg_inherits.c|||551|||/*
 * DeleteInheritsTuple
 *
 * Delete pg_inherits tuples with the given inhrelid.  inhparent may be given
 * as InvalidOid, in which case all tuples matching inhrelid are deleted;
 * otherwise only delete tuples with the specified inhparent.
 *
 * expect_detach_pending is the expected state of the inhdetachpending flag.
 * If the catalog row does not match that state, an error is raised.
 *
 * childname is the partition name, if a table; pass NULL for regular
 * inheritance or when working with other relation kinds.
 *
 * Returns whether at least one row was deleted.
 */""",
  """PartitionHasPendingDetach|||backend\catalog\pg_inherits.c|||619|||/*
 * Return whether the pg_inherits tuple for a partition has the "detach
 * pending" flag set.
 */""",
  """LargeObjectCreate|||backend\catalog\pg_largeobject.c|||36|||/*
 * Create a large object having the given LO identifier.
 *
 * We create a new large object by inserting an entry into
 * pg_largeobject_metadata without any data pages, so that the object
 * will appear to exist with size 0.
 */""",
  """LargeObjectDrop|||backend\catalog\pg_largeobject.c|||82|||/*
 * Drop a large object having the given LO identifier.  Both the data pages
 * and metadata must be dropped.
 */""",
  """LargeObjectExists|||backend\catalog\pg_largeobject.c|||154|||/*
 * LargeObjectExists
 *
 * We don't use the system cache for large object metadata, for fear of
 * using too much local memory.
 *
 * This function always scans the system catalog using an up-to-date snapshot,
 * so it should not be used when a large object is opened in read-only mode
 * (because large objects opened in read only mode are supposed to be viewed
 * relative to the caller's snapshot, whereas in read-write mode they are
 * relative to a current snapshot).
 */""",
  """NamespaceCreate|||backend\catalog\pg_namespace.c|||42|||/* ----------------
 * NamespaceCreate
 *
 * Create a namespace (schema) with the given name and owner OID.
 *
 * If isTemp is true, this schema is a per-backend schema for holding
 * temporary tables.  Currently, it is used to prevent it from being
 * linked as a member of any active extension.  (If someone does CREATE
 * TEMP TABLE in an extension script, we don't want the temp schema to
 * become part of the extension). And to avoid checking for default ACL
 * for temp namespace (as it is not necessary).
 * ---------------
 */""",
  """validOperatorName|||backend\catalog\pg_operator.c|||67|||/*
 * Check whether a proposed operator name is legal
 *
 * This had better match the behavior of parser/scan.l!
 *
 * We need this because the parser is not smart enough to check that
 * the arguments of CREATE OPERATOR's COMMUTATOR, NEGATOR, etc clauses
 * are operator names rather than some other lexical entity.
 */""",
  """OperatorGet|||backend\catalog\pg_operator.c|||123|||/*
 * OperatorGet
 *
 *		finds an operator given an exact specification (name, namespace,
 *		left and right type IDs).
 *
 *		*defined is set true if defined (not a shell)
 */""",
  """OperatorLookup|||backend\catalog\pg_operator.c|||163|||/*
 * OperatorLookup
 *
 *		looks up an operator given a possibly-qualified name and
 *		left and right type IDs.
 *
 *		*defined is set true if defined (not a shell)
 */""",
  """OperatorShellMake|||backend\catalog\pg_operator.c|||192|||/*
 * OperatorShellMake
 *		Make a "shell" entry for a not-yet-existing operator.
 */""",
  """OperatorCreate|||backend\catalog\pg_operator.c|||320|||/*
 * OperatorCreate
 *
 * "X" indicates an optional argument (i.e. one that can be NULL or 0)
 *		operatorName			name for new operator
 *		operatorNamespace		namespace for new operator
 *		leftTypeId				X left type ID
 *		rightTypeId				X right type ID
 *		procedureId				procedure ID for operator
 *		commutatorName			X commutator operator
 *		negatorName				X negator operator
 *		restrictionId			X restriction selectivity procedure ID
 *		joinId					X join selectivity procedure ID
 *		canMerge				merge join can be used with this operator
 *		canHash					hash join can be used with this operator
 *
 * The caller should have validated properties and permissions for the
 * objects passed as OID references.  We must handle the commutator and
 * negator operator references specially, however, since those need not
 * exist beforehand.
 *
 * This routine gets complicated because it allows the user to
 * specify operators that do not exist.  For example, if operator
 * "op" is being defined, the negator operator "negop" and the
 * commutator "commop" can also be defined without specifying
 * any information other than their names.  Since in order to
 * add "op" to the PG_OPERATOR catalog, all the Oid's for these
 * operators must be placed in the fields of "op", a forward
 * declaration is done on the commutator and negator operators.
 * This is called creating a shell, and its main effect is to
 * create a tuple in the PG_OPERATOR catalog with minimal
 * information about the operator (just its name and types).
 * Forward declaration is used only for this purpose, it is
 * not available to the user as it is for type definition.
 */""",
  """OperatorValidateParams|||backend\catalog\pg_operator.c|||555|||/*
 * OperatorValidateParams
 *
 * Check that an operator with argument types leftTypeId and rightTypeId,
 * returning operResultType, can have the attributes that are set to true.
 * Raise an error for any disallowed attribute.
 *
 * Note: in ALTER OPERATOR, we only bother to pass "true" for attributes
 * the command is trying to set, not those that may already be set.
 * This is OK as long as the attribute checks are independent.
 */""",
  """get_other_operator|||backend\catalog\pg_operator.c|||621|||/*
 * Try to lookup another operator (commutator, etc); return its OID
 *
 * If not found, check to see if it would be the same operator we are trying
 * to define; if so, return InvalidOid.  (Caller must decide whether
 * that is sensible.)  If it is not the same operator, create a shell
 * operator.
 */""",
  """OperatorUpd|||backend\catalog\pg_operator.c|||683|||/*
 * OperatorUpd
 *
 *	For a given operator, look up its negator and commutator operators.
 *	When isDelete is false, update their negator and commutator fields to
 *	point back to the given operator; when isDelete is true, update those
 *	fields to be InvalidOid.
 *
 *	The !isDelete case solves a problem for users who need to insert two new
 *	operators that are the negator or commutator of each other, while the
 *	isDelete case is needed so as not to leave dangling OID links behind
 *	after dropping an operator.
 */""",
  """makeOperatorDependencies|||backend\catalog\pg_operator.c|||852|||/*
 * Create dependencies for an operator (either a freshly inserted
 * complete operator, a new shell operator, a just-updated shell,
 * or an operator that's being modified by ALTER OPERATOR).
 *
 * makeExtensionDep should be true when making a new operator or
 * replacing a shell, false for ALTER OPERATOR.  Passing false
 * will prevent any change in the operator's extension membership.
 *
 * NB: the OidIsValid tests in this routine are necessary, in case
 * the given operator is a shell.
 */""",
  """ParameterAclLookup|||backend\catalog\pg_parameter_acl.c|||34|||/*
 * ParameterAclLookup - Given a configuration parameter name,
 * look up the associated configuration parameter ACL's OID.
 *
 * If missing_ok is false, throw an error if ACL entry not found.  If
 * true, just return InvalidOid.
 */""",
  """ParameterAclCreate|||backend\catalog\pg_parameter_acl.c|||67|||/*
 * ParameterAclCreate
 *
 * Add a new tuple to pg_parameter_acl.
 *
 * parameter: the parameter name to create an entry for.
 * Caller should have verified that there's no such entry already.
 *
 * Returns the new entry's OID.
 */""",
  """ProcedureCreate|||backend\catalog\pg_proc.c|||69|||/* ----------------------------------------------------------------
 *		ProcedureCreate
 *
 * Note: allParameterTypes, parameterModes, parameterNames, trftypes, and proconfig
 * are either arrays of the proper types or NULL.  We declare them Datum,
 * not "ArrayType *", to avoid importing array.h into pg_proc.h.
 * ----------------------------------------------------------------
 */""",
  """fmgr_internal_validator|||backend\catalog\pg_proc.c|||724|||/*
 * Validator for internal functions
 *
 * Check that the given internal function name (the "prosrc" value) is
 * a known builtin function.
 */""",
  """fmgr_c_validator|||backend\catalog\pg_proc.c|||767|||/*
 * Validator for C language functions
 *
 * Make sure that the library file exists, is loadable, and contains
 * the specified link symbol. Also check for a valid function
 * information record.
 */""",
  """fmgr_sql_validator|||backend\catalog\pg_proc.c|||810|||/*
 * Validator for SQL language functions
 *
 * Parse it here in order to be sure that it contains no syntax errors.
 */""",
  """sql_function_parse_error_callback|||backend\catalog\pg_proc.c|||977|||/*
 * Error context callback for handling errors in SQL function definitions
 */""",
  """function_parse_error_transpose|||backend\catalog\pg_proc.c|||1001|||/*
 * Adjust a syntax error occurring inside the function body of a CREATE
 * FUNCTION or DO command.  This can be used by any function validator or
 * anonymous-block handler, not only for SQL-language functions.
 * It is assumed that the syntax error position is initially relative to the
 * function body string (as passed in).  If possible, we adjust the position
 * to reference the original command text; if we can't manage that, we set
 * up an "internal query" syntax error instead.
 *
 * Returns true if a syntax error was processed, false if not.
 */""",
  """match_prosrc_to_query|||backend\catalog\pg_proc.c|||1068|||/*
 * Try to locate the string literal containing the function body in the
 * given text of the CREATE FUNCTION or DO command.  If successful, return
 * the character (not byte) index within the command corresponding to the
 * given character index within the literal.  If not successful, return 0.
 */""",
  """match_prosrc_to_literal|||backend\catalog\pg_proc.c|||1126|||/*
 * Try to match the given source text to a single-quoted literal.
 * If successful, adjust newcursorpos to correspond to the character
 * (not byte) index corresponding to cursorpos in the source text.
 *
 * At entry, literal points just past a ' character.  We must check for the
 * trailing quote.
 */""",
  """check_publication_add_relation|||backend\catalog\pg_publication.c|||58|||/*
 * Check if relation can be in given publication and throws appropriate
 * error if not.
 */""",
  """check_publication_add_schema|||backend\catalog\pg_publication.c|||97|||/*
 * Check if schema can be in given publication and throw appropriate error if
 * not.
 */""",
  """is_publishable_class|||backend\catalog\pg_publication.c|||136|||/*
 * Returns if relation represented by oid and Form_pg_class entry
 * is publishable.
 *
 * Does same checks as check_publication_add_relation() above, but does not
 * need relation to be opened and also does not throw errors.
 *
 * XXX  This also excludes all tables with relid < FirstNormalObjectId,
 * ie all tables created during initdb.  This mainly affects the preinstalled
 * information_schema.  IsCatalogRelationOid() only excludes tables with
 * relid < FirstUnpinnedObjectId, making that test rather redundant,
 * but really we should get rid of the FirstNormalObjectId test not
 * IsCatalogRelationOid.  We can't do so today because we don't want
 * information_schema tables to be considered publishable; but this test
 * is really inadequate for that, since the information_schema could be
 * dropped and reloaded and then it'll be considered publishable.  The best
 * long-term solution may be to add a "relispublishable" bool to pg_class,
 * and depend on that instead of OID checks.
 */""",
  """is_publishable_relation|||backend\catalog\pg_publication.c|||149|||/*
 * Another variant of is_publishable_class(), taking a Relation.
 */""",
  """pg_relation_is_publishable|||backend\catalog\pg_publication.c|||162|||/*
 * SQL-callable variant of the above
 *
 * This returns null when the relation does not exist.  This is intended to be
 * used for example in psql to avoid gratuitous errors when there are
 * concurrent catalog changes.
 */""",
  """is_ancestor_member_tableinfos|||backend\catalog\pg_publication.c|||181|||/*
 * Returns true if the ancestor is in the list of published relations.
 * Otherwise, returns false.
 */""",
  """filter_partitions|||backend\catalog\pg_publication.c|||200|||/*
 * Filter out the partitions whose parent tables are also present in the list.
 */""",
  """is_schema_publication|||backend\catalog\pg_publication.c|||235|||/*
 * Returns true if any schema is associated with the publication, false if no
 * schema is associated with the publication.
 */""",
  """GetPubPartitionOptionRelations|||backend\catalog\pg_publication.c|||266|||/*
 * Gets the relations based on the publication partition option for a specified
 * relation.
 */""",
  """GetTopMostAncestorInPublication|||backend\catalog\pg_publication.c|||310|||/*
 * Returns the relid of the topmost ancestor that is published via this
 * publication if any and set its ancestor level to ancestor_level,
 * otherwise returns InvalidOid.
 *
 * The ancestor_level value allows us to compare the results for multiple
 * publications, and decide which value is higher up.
 *
 * Note that the list of ancestors should be ordered such that the topmost
 * ancestor is at the end of the list.
 */""",
  """publication_add_relation|||backend\catalog\pg_publication.c|||357|||/*
 * Insert new publication / relation mapping.
 */""",
  """compare_int16|||backend\catalog\pg_publication.c|||480|||/* qsort comparator for attnums */""",
  """publication_translate_columns|||backend\catalog\pg_publication.c|||501|||/*
 * Translate a list of column names to an array of attribute numbers
 * and a Bitmapset with them; verify that each attribute is appropriate
 * to have in a publication column list (no system or generated attributes,
 * no duplicates).  Additional checks with replica identity are done later;
 * see pub_collist_contains_invalid_column.
 *
 * Note that the attribute numbers are *not* offset by
 * FirstLowInvalidHeapAttributeNumber; system columns are forbidden so this
 * is okay.
 */""",
  """pub_collist_to_bitmapset|||backend\catalog\pg_publication.c|||569|||/*
 * Transform a column list (represented by an array Datum) to a bitmapset.
 *
 * If columns isn't NULL, add the column numbers to that set.
 *
 * If mcxt isn't NULL, build the bitmapset in that context.
 */""",
  """publication_add_schema|||backend\catalog\pg_publication.c|||605|||/*
 * Insert new publication / schema mapping.
 */""",
  """GetRelationPublications|||backend\catalog\pg_publication.c|||686|||/* Gets list of publication oids for a relation */""",
  """GetPublicationRelations|||backend\catalog\pg_publication.c|||715|||/*
 * Gets list of relation oids for a publication.
 *
 * This should only be used FOR TABLE publications, the FOR ALL TABLES
 * should use GetAllTablesPublicationRelations().
 */""",
  """GetAllTablesPublications|||backend\catalog\pg_publication.c|||758|||/*
 * Gets list of publication oids for publications marked as FOR ALL TABLES.
 */""",
  """GetAllTablesPublicationRelations|||backend\catalog\pg_publication.c|||799|||/*
 * Gets list of all relation published by FOR ALL TABLES publication(s).
 *
 * If the publication publishes partition changes via their respective root
 * partitioned tables, we must exclude partitions in favor of including the
 * root partitioned tables.
 */""",
  """GetPublicationSchemas|||backend\catalog\pg_publication.c|||860|||/*
 * Gets the list of schema oids for a publication.
 *
 * This should only be used FOR TABLES IN SCHEMA publications.
 */""",
  """GetSchemaPublications|||backend\catalog\pg_publication.c|||898|||/*
 * Gets the list of publication oids associated with a specified schema.
 */""",
  """GetSchemaPublicationRelations|||backend\catalog\pg_publication.c|||924|||/*
 * Get the list of publishable relation oids for a specified schema.
 */""",
  """GetAllSchemaPublicationRelations|||backend\catalog\pg_publication.c|||981|||/*
 * Gets the list of all relations published by FOR TABLES IN SCHEMA
 * publication.
 */""",
  """GetPublication|||backend\catalog\pg_publication.c|||1005|||/*
 * Get publication using oid
 *
 * The Publication struct and its data are palloc'ed here.
 */""",
  """GetPublicationByName|||backend\catalog\pg_publication.c|||1036|||/*
 * Get Publication using name.
 */""",
  """pg_get_publication_tables|||backend\catalog\pg_publication.c|||1051|||/*
 * Get information of the tables in the given publication array.
 *
 * Returns pubid, relid, column list, row filter for each table.
 */""",
  """RangeCreate|||backend\catalog\pg_range.c|||35|||/*
 * RangeCreate
 *		Create an entry in pg_range.
 */""",
  """RangeDelete|||backend\catalog\pg_range.c|||112|||/*
 * RangeDelete
 *		Remove the pg_range entry for the specified type.
 */""",
  """recordSharedDependencyOn|||backend\catalog\pg_shdepend.c|||124|||/*
 * recordSharedDependencyOn
 *
 * Record a dependency between 2 objects via their respective ObjectAddresses.
 * The first argument is the dependent object, the second the one it
 * references (which must be a shared object).
 *
 * This locks the referenced object and makes sure it still exists.
 * Then it creates an entry in pg_shdepend.  The lock is kept until
 * the end of the transaction.
 *
 * Dependencies on pinned objects are not recorded.
 */""",
  """recordDependencyOnOwner|||backend\catalog\pg_shdepend.c|||167|||/*
 * recordDependencyOnOwner
 *
 * A convenient wrapper of recordSharedDependencyOn -- register the specified
 * user as owner of the given object.
 *
 * Note: it's the caller's responsibility to ensure that there isn't an owner
 * entry for the object already.
 */""",
  """shdepChangeDep|||backend\catalog\pg_shdepend.c|||205|||/*
 * shdepChangeDep
 *
 * Update shared dependency records to account for an updated referenced
 * object.  This is an internal workhorse for operations such as changing
 * an object's owner.
 *
 * There must be no more than one existing entry for the given dependent
 * object and dependency type!	So in practice this can only be used for
 * updating SHARED_DEPENDENCY_OWNER and SHARED_DEPENDENCY_TABLESPACE
 * entries, which should have that property.
 *
 * If there is no previous entry, we assume it was referencing a PINned
 * object, so we create a new entry.  If the new referenced object is
 * PINned, we don't create an entry (and drop the old one, if any).
 * (For tablespaces, we don't record dependencies in certain cases, so
 * there are other possible reasons for entries to be missing.)
 *
 * sdepRel must be the pg_shdepend relation, already opened and suitably
 * locked.
 */""",
  """changeDependencyOnOwner|||backend\catalog\pg_shdepend.c|||315|||/*
 * changeDependencyOnOwner
 *
 * Update the shared dependencies to account for the new owner.
 *
 * Note: we don't need an objsubid argument because only whole objects
 * have owners.
 */""",
  """recordDependencyOnTablespace|||backend\catalog\pg_shdepend.c|||369|||/*
 * recordDependencyOnTablespace
 *
 * A convenient wrapper of recordSharedDependencyOn -- register the specified
 * tablespace as default for the given object.
 *
 * Note: it's the caller's responsibility to ensure that there isn't a
 * tablespace entry for the object already.
 */""",
  """changeDependencyOnTablespace|||backend\catalog\pg_shdepend.c|||390|||/*
 * changeDependencyOnTablespace
 *
 * Update the shared dependencies to account for the new tablespace.
 *
 * Note: we don't need an objsubid argument because only whole objects
 * have tablespaces.
 */""",
  """getOidListDiff|||backend\catalog\pg_shdepend.c|||420|||/*
 * getOidListDiff
 *		Helper for updateAclDependencies.
 *
 * Takes two Oid arrays and removes elements that are common to both arrays,
 * leaving just those that are in one input but not the other.
 * We assume both arrays have been sorted and de-duped.
 */""",
  """updateAclDependencies|||backend\catalog\pg_shdepend.c|||490|||/*
 * updateAclDependencies
 *		Update the pg_shdepend info for an object's ACL during GRANT/REVOKE.
 *
 * classId, objectId, objsubId: identify the object whose ACL this is
 * ownerId: role owning the object
 * noldmembers, oldmembers: array of roleids appearing in old ACL
 * nnewmembers, newmembers: array of roleids appearing in new ACL
 *
 * We calculate the differences between the new and old lists of roles,
 * and then insert or delete from pg_shdepend as appropriate.
 *
 * Note that we can't just insert all referenced roles blindly during GRANT,
 * because we would end up with duplicate registered dependencies.  We could
 * check for existence of the tuples before inserting, but that seems to be
 * more expensive than what we are doing here.  Likewise we can't just delete
 * blindly during REVOKE, because the user may still have other privileges.
 * It is also possible that REVOKE actually adds dependencies, due to
 * instantiation of a formerly implicit default ACL (although at present,
 * all such dependencies should be for the owning role, which we ignore here).
 *
 * NOTE: Both input arrays must be sorted and de-duped.  (Typically they
 * are extracted from an ACL array by aclmembers(), which takes care of
 * both requirements.)	The arrays are pfreed before return.
 */""",
  """updateInitAclDependencies|||backend\catalog\pg_shdepend.c|||511|||/*
 * updateInitAclDependencies
 *		Update the pg_shdepend info for a pg_init_privs entry.
 *
 * Exactly like updateAclDependencies, except we are considering a
 * pg_init_privs ACL for the specified object.  Since recording of
 * pg_init_privs role dependencies is the same for owners and non-owners,
 * we do not need an ownerId argument.
 */""",
  """updateAclDependenciesWorker|||backend\catalog\pg_shdepend.c|||524|||/* Common code for the above two functions */""",
  """shared_dependency_comparator|||backend\catalog\pg_shdepend.c|||609|||/*
 * qsort comparator for ShDependObjectInfo items
 */""",
  """checkSharedDependencies|||backend\catalog\pg_shdepend.c|||675|||/*
 * checkSharedDependencies
 *
 * Check whether there are shared dependency entries for a given shared
 * object; return true if so.
 *
 * In addition, return a string containing a newline-separated list of object
 * descriptions that depend on the shared object, or NULL if none is found.
 * We actually return two such strings; the "detail" result is suitable for
 * returning to the client as an errdetail() string, and is limited in size.
 * The "detail_log" string is potentially much longer, and should be emitted
 * to the server log only.
 *
 * We can find three different kinds of dependencies: dependencies on objects
 * of the current database; dependencies on shared objects; and dependencies
 * on objects local to other databases.  We can (and do) provide descriptions
 * of the two former kinds of objects, but we can't do that for "remote"
 * objects, so we just provide a count of them.
 */""",
  """copyTemplateDependencies|||backend\catalog\pg_shdepend.c|||894|||/*
 * copyTemplateDependencies
 *
 * Routine to create the initial shared dependencies of a new database.
 * We simply copy the dependencies from the template database.
 */""",
  """dropDatabaseDependencies|||backend\catalog\pg_shdepend.c|||998|||/*
 * dropDatabaseDependencies
 *
 * Delete pg_shdepend entries corresponding to a database that's being
 * dropped.
 */""",
  """deleteSharedDependencyRecordsFor|||backend\catalog\pg_shdepend.c|||1046|||/*
 * deleteSharedDependencyRecordsFor
 *
 * Delete all pg_shdepend entries corresponding to an object that's being
 * dropped or modified.  The object is assumed to be either a shared object
 * or local to the current database (the classId tells us which).
 *
 * If objectSubId is zero, we are deleting a whole object, so get rid of
 * pg_shdepend entries for subobjects as well.
 */""",
  """shdepAddDependency|||backend\catalog\pg_shdepend.c|||1068|||/*
 * shdepAddDependency
 *		Internal workhorse for inserting into pg_shdepend
 *
 * sdepRel must be the pg_shdepend relation, already opened and suitably
 * locked.
 */""",
  """shdepDropDependency|||backend\catalog\pg_shdepend.c|||1123|||/*
 * shdepDropDependency
 *		Internal workhorse for deleting entries from pg_shdepend.
 *
 * We drop entries having the following properties:
 *	dependent object is the one identified by classId/objectId/objsubId
 *	if refclassId isn't InvalidOid, it must match the entry's refclassid
 *	if refobjId isn't InvalidOid, it must match the entry's refobjid
 *	if deptype isn't SHARED_DEPENDENCY_INVALID, it must match entry's deptype
 *
 * If drop_subobjects is true, we ignore objsubId and consider all entries
 * matching classId/objectId.
 *
 * sdepRel must be the pg_shdepend relation, already opened and suitably
 * locked.
 */""",
  """classIdGetDbId|||backend\catalog\pg_shdepend.c|||1189|||/*
 * classIdGetDbId
 *
 * Get the database Id that should be used in pg_shdepend, given the OID
 * of the catalog containing the object.  For shared objects, it's 0
 * (InvalidOid); for all other objects, it's the current database Id.
 */""",
  """shdepLockAndCheckObject|||backend\catalog\pg_shdepend.c|||1210|||/*
 * shdepLockAndCheckObject
 *
 * Lock the object that we are about to record a dependency on.
 * After it's locked, verify that it hasn't been dropped while we
 * weren't looking.  If the object has been dropped, this function
 * does not return!
 */""",
  """storeObjectDescription|||backend\catalog\pg_shdepend.c|||1275|||/*
 * storeObjectDescription
 *		Append the description of a dependent object to "descs"
 *
 * While searching for dependencies of a shared object, we stash the
 * descriptions of dependent objects we find in a single string, which we
 * later pass to ereport() in the DETAIL field when somebody attempts to
 * drop a referenced shared object.
 *
 * When type is LOCAL_OBJECT or SHARED_OBJECT, we expect object to be the
 * dependent object, deptype is the dependency type, and count is not used.
 * When type is REMOTE_OBJECT, we expect object to be the database object,
 * and count to be nonzero; deptype is not used in this case.
 */""",
  """shdepDropOwned|||backend\catalog\pg_shdepend.c|||1341|||/*
 * shdepDropOwned
 *
 * Drop the objects owned by any one of the given RoleIds.  If a role has
 * access to an object, the grant will be removed as well (but the object
 * will not, of course).
 *
 * We can revoke grants immediately while doing the scan, but drops are
 * saved up and done all at once with performMultipleDeletions.  This
 * is necessary so that we don't get failures from trying to delete
 * interdependent objects in the wrong order.
 */""",
  """shdepReassignOwned|||backend\catalog\pg_shdepend.c|||1529|||/*
 * shdepReassignOwned
 *
 * Change the owner of objects owned by any of the roles in roleids to
 * newrole.  Grants are not touched.
 */""",
  """shdepReassignOwned_Owner|||backend\catalog\pg_shdepend.c|||1646|||/*
 * shdepReassignOwned_Owner
 *
 * shdepReassignOwned's processing of SHARED_DEPENDENCY_OWNER entries
 */""",
  """shdepReassignOwned_InitAcl|||backend\catalog\pg_shdepend.c|||1733|||/*
 * shdepReassignOwned_InitAcl
 *
 * shdepReassignOwned's processing of SHARED_DEPENDENCY_INITACL entries
 */""",
  """GetSubscription|||backend\catalog\pg_subscription.c|||40|||/*
 * Fetch the subscription from the syscache.
 */""",
  """CountDBSubscriptions|||backend\catalog\pg_subscription.c|||122|||/*
 * Return number of subscriptions defined in given database.
 * Used by dropdb() to check if database can indeed be dropped.
 */""",
  """FreeSubscription|||backend\catalog\pg_subscription.c|||154|||/*
 * Free memory allocated by subscription struct.
 */""",
  """DisableSubscription|||backend\catalog\pg_subscription.c|||168|||/*
 * Disable the given subscription.
 */""",
  """textarray_to_stringlist|||backend\catalog\pg_subscription.c|||209|||/*
 * Convert text array to list of strings.
 *
 * Note: the resulting list of strings is pallocated here.
 */""",
  """AddSubscriptionRelState|||backend\catalog\pg_subscription.c|||235|||/*
 * Add new state record for a subscription table.
 *
 * If retain_lock is true, then don't release the locks taken in this function.
 * We normally release the locks at the end of transaction but in binary-upgrade
 * mode, we expect to release those immediately.
 */""",
  """UpdateSubscriptionRelStateEx|||backend\catalog\pg_subscription.c|||289|||/*
 * Update the state of a subscription table.
 */""",
  """UpdateSubscriptionRelState|||backend\catalog\pg_subscription.c|||353|||/*
 * Update the state of a subscription table.
 */""",
  """GetSubscriptionRelState|||backend\catalog\pg_subscription.c|||365|||/*
 * Get state of subscription table.
 *
 * Returns SUBREL_STATE_UNKNOWN when the table is not in the subscription.
 */""",
  """RemoveSubscriptionRel|||backend\catalog\pg_subscription.c|||415|||/*
 * Drop subscription relation mapping. These can be for a particular
 * subscription, or for a particular relation, or both.
 */"""
)
INFO:__main__:Extracted 7 methods with comments in this batch
INFO:__main__:Fetching batch: offset=2700, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=2700, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres358[0m: [32mList[0m[[32mString[0m] = List(
  """HasSubscriptionRelations|||backend\catalog\pg_subscription.c|||490|||/*
 * Does the subscription have any relations?
 *
 * Use this function only to know true/false, and when you have no need for the
 * List returned by GetSubscriptionRelations.
 */""",
  """GetSubscriptionRelations|||backend\catalog\pg_subscription.c|||525|||/*
 * Get the relations for the subscription.
 *
 * If not_ready is true, return only the relations that are not in a ready
 * state, otherwise return all the relations of the subscription.  The
 * returned list is palloc'ed in the current memory context.
 */""",
  """TypeShellMake|||backend\catalog\pg_type.c|||56|||/* ----------------------------------------------------------------
 *		TypeShellMake
 *
 *		This procedure inserts a "shell" tuple into the pg_type relation.
 *		The type tuple inserted has valid but dummy values, and its
 *		"typisdefined" field is false indicating it's not really defined.
 *
 *		This is used so that a tuple exists in the catalogs.  The I/O
 *		functions for the type will link to this tuple.  When the full
 *		CREATE TYPE command is issued, the bogus values will be replaced
 *		with correct ones, and "typisdefined" will be set to true.
 * ----------------------------------------------------------------
 */""",
  """TypeCreate|||backend\catalog\pg_type.c|||194|||/* ----------------------------------------------------------------
 *		TypeCreate
 *
 *		This does all the necessary work needed to define a new type.
 *
 *		Returns the ObjectAddress assigned to the new type.
 *		If newTypeOid is zero (the normal case), a new OID is created;
 *		otherwise we use exactly that OID.
 * ----------------------------------------------------------------
 */""",
  """GenerateTypeDependencies|||backend\catalog\pg_type.c|||556|||/*
 * GenerateTypeDependencies: build the dependencies needed for a type
 *
 * Most of what this function needs to know about the type is passed as the
 * new pg_type row, typeTuple.  We make callers pass the pg_type Relation
 * as well, so that we have easy access to a tuple descriptor for the row.
 *
 * While this is able to extract the defaultExpr and typacl from the tuple,
 * doing so is relatively expensive, and callers may have those values at
 * hand already.  Pass those if handy, otherwise pass NULL.  (typacl is really
 * "Acl *", but we declare it "void *" to avoid including acl.h in pg_type.h.)
 *
 * relationKind and isImplicitArray are likewise somewhat expensive to deduce
 * from the tuple, so we make callers pass those (they're not optional).
 *
 * isDependentType is true if this is an implicit array, multirange, or
 * relation rowtype; that means it doesn't need its own dependencies on owner
 * etc.
 *
 * We make an extension-membership dependency if we're in an extension
 * script and makeExtensionDep is true.
 * makeExtensionDep should be true when creating a new type or replacing a
 * shell type, but not for ALTER TYPE on an existing type.  Passing false
 * causes the type's extension membership to be left alone.
 *
 * rebuild should be true if this is a pre-existing type.  We will remove
 * existing dependencies and rebuild them from scratch.  This is needed for
 * ALTER TYPE, and also when replacing a shell type.  We don't remove any
 * existing extension dependency, though; hence, if makeExtensionDep is also
 * true and we're in an extension script, an error will occur unless the
 * type already belongs to the current extension.  That's the behavior we
 * want when replacing a shell type, which is the only case where both flags
 * are true.
 */""",
  """RenameTypeInternal|||backend\catalog\pg_type.c|||764|||/*
 * RenameTypeInternal
 *		This renames a type, as well as any associated array type.
 *
 * Caller must have already checked privileges.
 *
 * Currently this is used for renaming table rowtypes and for
 * ALTER TYPE RENAME TO command.
 */""",
  """makeArrayTypeName|||backend\catalog\pg_type.c|||839|||/*
 * makeArrayTypeName
 *	  - given a base type name, make an array type name for it
 *
 * the caller is responsible for pfreeing the result
 */""",
  """moveArrayTypeName|||backend\catalog\pg_type.c|||904|||/*
 * moveArrayTypeName
 *	  - try to reassign an array type name that the user wants to use.
 *
 * The given type name has been discovered to already exist (with the given
 * OID).  If it is an autogenerated array type, change the array type's name
 * to not conflict.  This allows the user to create type "foo" followed by
 * type "_foo" without problems.  (Of course, there are race conditions if
 * two backends try to create similarly-named types concurrently, but the
 * worst that can happen is an unnecessary failure --- anything we do here
 * will be rolled back if the type creation fails due to conflicting names.)
 *
 * Note that this must be called *before* calling makeArrayTypeName to
 * determine the new type's own array type name; else the latter will
 * certainly pick the same name.
 *
 * Returns true if successfully moved the type, false if not.
 *
 * We also return true if the given type is a shell type.  In this case
 * the type has not been renamed out of the way, but nonetheless it can
 * be expected that TypeCreate will succeed.  This behavior is convenient
 * for most callers --- those that need to distinguish the shell-type case
 * must do their own typisdefined test.
 */""",
  """makeMultirangeTypeName|||backend\catalog\pg_type.c|||949|||/*
 * makeMultirangeTypeName
 *	  - given a range type name, make a multirange type name for it
 *
 * caller is responsible for pfreeing the result
 */""",
  """AddPendingSync|||backend\catalog\storage.c|||84|||/*
 * AddPendingSync
 *		Queue an at-commit fsync.
 */""",
  """RelationCreateStorage|||backend\catalog\storage.c|||120|||/*
 * RelationCreateStorage
 *		Create physical storage for a relation.
 *
 * Create the underlying disk file storage for the relation. This only
 * creates the main fork; additional forks are created lazily by the
 * modules that need them.
 *
 * This function is transactional. The creation is WAL-logged, and if the
 * transaction aborts later on, the storage will be destroyed.  A caller
 * that does not want the storage to be destroyed in case of an abort may
 * pass register_delete = false.
 */""",
  """log_smgrcreate|||backend\catalog\storage.c|||185|||/*
 * Perform XLogInsert of an XLOG_SMGR_CREATE record to WAL.
 */""",
  """RelationDropStorage|||backend\catalog\storage.c|||205|||/*
 * RelationDropStorage
 *		Schedule unlinking of physical storage at transaction commit.
 */""",
  """RelationPreserveStorage|||backend\catalog\storage.c|||250|||/*
 * RelationPreserveStorage
 *		Mark a relation as not to be deleted after all.
 *
 * We need this function because relation mapping changes are committed
 * separately from commit of the whole transaction, so it's still possible
 * for the transaction to abort after the mapping update is done.
 * When a new physical relation is installed in the map, it would be
 * scheduled for delete-on-abort, so we'd delete it, and be in trouble.
 * The relation mapper fixes this by telling us to not delete such relations
 * after all as part of its commit.
 *
 * We also use this to reuse an old build of an index during ALTER TABLE, this
 * time removing the delete-at-commit entry.
 *
 * No-op if the relation is not among those scheduled for deletion.
 */""",
  """RelationTruncate|||backend\catalog\storage.c|||287|||/*
 * RelationTruncate
 *		Physically truncate a relation to the specified number of blocks.
 *
 * This includes getting rid of any buffers for the blocks that are to be
 * dropped.
 */""",
  """RelationPreTruncate|||backend\catalog\storage.c|||448|||/*
 * RelationPreTruncate
 *		Perform AM-independent work before a physical truncation.
 *
 * If an access method's relation_nontransactional_truncate does not call
 * RelationTruncate(), it must call this before decreasing the table size.
 */""",
  """RelationCopyStorage|||backend\catalog\storage.c|||476|||/*
 * Copy a fork's data, block by block.
 *
 * Note that this requires that there is no dirty data in shared buffers. If
 * it's possible that there are, callers need to flush those using
 * e.g. FlushRelationBuffers(rel).
 *
 * Also note that this is frequently called via locutions such as
 *		RelationCopyStorage(RelationGetSmgr(rel), ...);
 * That's safe only because we perform only smgr and WAL operations here.
 * If we invoked anything else, a relcache flush could cause our SMgrRelation
 * argument to become a dangling pointer.
 */""",
  """RelFileLocatorSkippingWAL|||backend\catalog\storage.c|||556|||/*
 * RelFileLocatorSkippingWAL
 *		Check if a BM_PERMANENT relfilelocator is using WAL.
 *
 * Changes to certain relations must not write WAL; see "Skipping WAL for
 * New RelFileLocator" in src/backend/access/transam/README.  Though it is
 * known from Relation efficiently, this function is intended for the code
 * paths not having access to Relation.
 */""",
  """EstimatePendingSyncsSpace|||backend\catalog\storage.c|||570|||/*
 * EstimatePendingSyncsSpace
 *		Estimate space needed to pass syncs to parallel workers.
 */""",
  """SerializePendingSyncs|||backend\catalog\storage.c|||583|||/*
 * SerializePendingSyncs
 *		Serialize syncs for parallel workers.
 */""",
  """RestorePendingSyncs|||backend\catalog\storage.c|||634|||/*
 * RestorePendingSyncs
 *		Restore syncs within a parallel worker.
 *
 * RelationNeedsWAL() and RelFileLocatorSkippingWAL() must offer the correct
 * answer to parallel workers.  Only smgrDoPendingSyncs() reads the
 * is_truncated field, at end of transaction.  Hence, don't restore it.
 */""",
  """smgrDoPendingDeletes|||backend\catalog\storage.c|||656|||/*
 *	smgrDoPendingDeletes() -- Take care of relation deletes at end of xact.
 *
 * This also runs when aborting a subxact; we want to clean up a failed
 * subxact immediately.
 *
 * Note: It's possible that we're being asked to remove a relation that has
 * no physical storage in any fork. In particular, it's possible that we're
 * cleaning up an old temporary relation for which RemovePgTempFiles has
 * already recovered the physical storage.
 */""",
  """smgrDoPendingSyncs|||backend\catalog\storage.c|||724|||/*
 *	smgrDoPendingSyncs() -- Take care of relation syncs at end of xact.
 */""",
  """smgrGetPendingDeletes|||backend\catalog\storage.c|||876|||/*
 * smgrGetPendingDeletes() -- Get a list of non-temp relations to be deleted.
 *
 * The return value is the number of relations scheduled for termination.
 * *ptr is set to point to a freshly-palloc'd array of RelFileLocators.
 * If there are no relations to be deleted, *ptr is set to NULL.
 *
 * Only non-temporary relations are included in the returned list.  This is OK
 * because the list is used only in contexts where temporary relations don't
 * matter: we're either writing to the two-phase state file (and transactions
 * that have touched temp tables can't be prepared) or we're writing to xlog
 * (and all temporary files will be zapped if we restart anyway, so no need
 * for redo to do it also).
 *
 * Note that the list does not include anything scheduled for termination
 * by upper-level transactions.
 */""",
  """PostPrepare_smgr|||backend\catalog\storage.c|||917|||/*
 *	PostPrepare_smgr -- Clean up after a successful PREPARE
 *
 * What we have to do here is throw away the in-memory state about pending
 * relation deletes.  It's all been recorded in the 2PC state file and
 * it's no longer smgr's job to worry about it.
 */""",
  """AtSubCommit_smgr|||backend\catalog\storage.c|||938|||/*
 * AtSubCommit_smgr() --- Take care of subtransaction commit.
 *
 * Reassign all items in the pending-deletes list to the parent transaction.
 */""",
  """AtSubAbort_smgr|||backend\catalog\storage.c|||958|||/*
 * AtSubAbort_smgr() --- Take care of subtransaction abort.
 *
 * Delete created relations and forget about deleted relations.
 * We can execute these operations immediately because we know this
 * subtransaction will not commit.
 */""",
  """smgr_redo|||backend\catalog\storage.c|||964|||/*
 * AtSubAbort_smgr() --- Take care of subtransaction abort.
 *
 * Delete created relations and forget about deleted relations.
 * We can execute these operations immediately because we know this
 * subtransaction will not commit.
 */""",
  """AlterTableCreateToastTable|||backend\catalog\toasting.c|||57|||/*
 * CreateToastTable variants
 *		If the table needs a toast table, and doesn't already have one,
 *		then create a toast table for it.
 *
 * reloptions for the toast table can be passed, too.  Pass (Datum) 0
 * for default reloptions.
 *
 * We expect the caller to have verified that the relation is a table and have
 * already done any necessary permission checks.  Callers expect this function
 * to end with CommandCounterIncrement if it makes any changes.
 */""",
  """NewHeapCreateToastTable|||backend\catalog\toasting.c|||63|||/*
 * CreateToastTable variants
 *		If the table needs a toast table, and doesn't already have one,
 *		then create a toast table for it.
 *
 * reloptions for the toast table can be passed, too.  Pass (Datum) 0
 * for default reloptions.
 *
 * We expect the caller to have verified that the relation is a table and have
 * already done any necessary permission checks.  Callers expect this function
 * to end with CommandCounterIncrement if it makes any changes.
 */""",
  """NewRelationCreateToastTable|||backend\catalog\toasting.c|||70|||/*
 * CreateToastTable variants
 *		If the table needs a toast table, and doesn't already have one,
 *		then create a toast table for it.
 *
 * reloptions for the toast table can be passed, too.  Pass (Datum) 0
 * for default reloptions.
 *
 * We expect the caller to have verified that the relation is a table and have
 * already done any necessary permission checks.  Callers expect this function
 * to end with CommandCounterIncrement if it makes any changes.
 */""",
  """CheckAndCreateToastTable|||backend\catalog\toasting.c|||77|||/*
 * CreateToastTable variants
 *		If the table needs a toast table, and doesn't already have one,
 *		then create a toast table for it.
 *
 * reloptions for the toast table can be passed, too.  Pass (Datum) 0
 * for default reloptions.
 *
 * We expect the caller to have verified that the relation is a table and have
 * already done any necessary permission checks.  Callers expect this function
 * to end with CommandCounterIncrement if it makes any changes.
 */""",
  """BootstrapToastTable|||backend\catalog\toasting.c|||97|||/*
 * Create a toast table during bootstrap
 *
 * Here we need to prespecify the OIDs of the toast table and its index
 */""",
  """create_toast_table|||backend\catalog\toasting.c|||126|||/*
 * create_toast_table --- internal workhorse
 *
 * rel is already opened and locked
 * toastOid and toastIndexOid are normally InvalidOid, but during
 * bootstrap they can be nonzero to specify hand-assigned OIDs
 */""",
  """needs_toast_table|||backend\catalog\toasting.c|||400|||/*
 * Check to see whether the table needs a TOAST table.
 */""",
  """DefineAggregate|||backend\commands\aggregatecmds.c|||52|||/*
 *	DefineAggregate
 *
 * "oldstyle" signals the old (pre-8.2) style where the aggregate input type
 * is specified by a BASETYPE element in the parameters.  Otherwise,
 * "args" is a pair, whose first element is a list of FunctionParameter structs
 * defining the agg's arguments (both direct and aggregated), and whose second
 * element is an Integer node with the number of direct args, or -1 if this
 * isn't an ordered-set aggregate.
 * "parameters" is a list of DefElem representing the agg's definition clauses.
 */""",
  """extractModify|||backend\commands\aggregatecmds.c|||477|||/*
 * Convert the string form of [m]finalfunc_modify to the catalog representation
 */""",
  """report_name_conflict|||backend\commands\alter.c|||75|||/*
 * Raise an error to the effect that an object of the given name is already
 * present in the given namespace.
 */""",
  """AlterObjectRename_internal|||backend\commands\alter.c|||164|||/*
 * AlterObjectRename_internal
 *
 * Generic function to rename the given object, for simple cases (won't
 * work for tables, nor other cases where we need to do more than change
 * the name column of a single catalog entry).
 *
 * rel: catalog relation containing object (RowExclusiveLock'd by caller)
 * objectId: OID of object to be renamed
 * new_name: CString representation of new name
 */""",
  """ExecRenameStmt|||backend\commands\alter.c|||356|||/*
 * Executes an ALTER OBJECT / RENAME TO statement.  Based on the object
 * type, the function appropriate to that type is executed.
 *
 * Return value is the address of the renamed object.
 */""",
  """ExecAlterObjectDependsStmt|||backend\commands\alter.c|||456|||/*
 * Executes an ALTER OBJECT / [NO] DEPENDS ON EXTENSION statement.
 *
 * Return value is the address of the altered object.  refAddress is an output
 * argument which, if not null, receives the address of the object that the
 * altered object now depends on.
 */""",
  """ExecAlterObjectSchemaStmt|||backend\commands\alter.c|||520|||/*
 * Executes an ALTER OBJECT / SET SCHEMA statement.  Based on the object
 * type, the function appropriate to that type is executed.
 *
 * Return value is that of the altered object.
 *
 * oldSchemaAddr is an output argument which, if not NULL, is set to the object
 * address of the original schema.
 */""",
  """AlterObjectNamespace_oid|||backend\commands\alter.c|||613|||/*
 * Change an object's namespace given its classOid and object Oid.
 *
 * Objects that don't have a namespace should be ignored, as should
 * dependent types such as array types.
 *
 * This function is currently used only by ALTER EXTENSION SET SCHEMA,
 * so it only needs to cover object kinds that can be members of an
 * extension, and it can silently ignore dependent types --- we assume
 * those will be moved when their parent object is moved.
 *
 * Returns the OID of the object's previous namespace, or InvalidOid if
 * object doesn't have a schema or was ignored due to being a dependent type.
 */""",
  """AlterObjectNamespace_internal|||backend\commands\alter.c|||680|||/*
 * Generic function to change the namespace of a given object, for simple
 * cases (won't work for tables, nor other cases where we need to do more
 * than change the namespace column of a single catalog entry).
 *
 * rel: catalog relation containing object (RowExclusiveLock'd by caller)
 * objid: OID of object to change the namespace of
 * nspOid: OID of new namespace
 *
 * Returns the OID of the object's previous namespace.
 */""",
  """ExecAlterOwnerStmt|||backend\commands\alter.c|||825|||/*
 * Executes an ALTER OBJECT / OWNER TO statement.  Based on the object
 * type, the function appropriate to that type is executed.
 */""",
  """AlterObjectOwner_internal|||backend\commands\alter.c|||916|||/*
 * Generic function to change the ownership of a given object, for simple
 * cases (won't work for tables, nor other cases where we need to do more than
 * change the ownership column of a single catalog entry).
 *
 * classId: OID of catalog containing object
 * objectId: OID of object to change the ownership of
 * new_ownerId: OID of new object owner
 *
 * This will work on large objects, but we have to beware of the fact that
 * classId isn't the OID of the catalog to modify in that case.
 */""",
  """CreateAccessMethod|||backend\commands\amcmds.c|||42|||/*
 * CreateAccessMethod
 *		Registers a new access method.
 */""",
  """get_am_type_oid|||backend\commands\amcmds.c|||128|||/*
 * get_am_type_oid
 *		Worker for various get_am_*_oid variants
 *
 * If missing_ok is false, throw an error if access method not found.  If
 * true, just return InvalidOid.
 *
 * If amtype is not '\0', an error is raised if the AM found is not of the
 * given type.
 */""",
  """get_index_am_oid|||backend\commands\amcmds.c|||162|||/*
 * get_index_am_oid - given an access method name, look up its OID
 *		and verify it corresponds to an index AM.
 */""",
  """get_table_am_oid|||backend\commands\amcmds.c|||172|||/*
 * get_table_am_oid - given an access method name, look up its OID
 *		and verify it corresponds to a table AM.
 */""",
  """get_am_oid|||backend\commands\amcmds.c|||182|||/*
 * get_am_oid - given an access method name, look up its OID.
 *		The type is not checked.
 */""",
  """get_am_name|||backend\commands\amcmds.c|||191|||/*
 * get_am_name - given an access method OID, look up its name.
 */""",
  """get_am_type_string|||backend\commands\amcmds.c|||211|||/*
 * Convert single-character access method type into string for error reporting.
 */""",
  """lookup_am_handler_func|||backend\commands\amcmds.c|||233|||/*
 * Convert a handler function name to an Oid.  If the return type of the
 * function doesn't match the given AM type, an error is raised.
 *
 * This function either return valid function Oid or throw an error.
 */""",
  """analyze_rel|||backend\commands\analyze.c|||110|||/*
 *	analyze_rel() -- analyze one relation
 *
 * relid identifies the relation to analyze.  If relation is supplied, use
 * the name therein for reporting any failure to open/lock the rel; do not
 * use it once we've successfully opened the rel, since it might be stale.
 */""",
  """do_analyze_rel|||backend\commands\analyze.c|||279|||/*
 *	do_analyze_rel() -- analyze one relation, recursively or not
 *
 * Note that "acquirefunc" is only relevant for the non-inherited case.
 * For the inherited case, acquire_inherited_sample_rows() determines the
 * appropriate acquirefunc for each child table.
 */""",
  """compute_index_stats|||backend\commands\analyze.c|||827|||/*
 * Compute statistics about indexes of a relation
 */""",
  """examine_attribute|||backend\commands\analyze.c|||998|||/*
 * examine_attribute -- pre-analysis of a single column
 *
 * Determine whether the column is analyzable; if so, create and initialize
 * a VacAttrStats struct for it.  If not, return NULL.
 *
 * If index_expr isn't NULL, then we're trying to analyze an expression index,
 * and index_expr is the expression tree representing the column's data.
 */""",
  """block_sampling_read_stream_next|||backend\commands\analyze.c|||1114|||/*
 * Read stream callback returning the next BlockNumber as chosen by the
 * BlockSampling algorithm.
 */""",
  """acquire_sample_rows|||backend\commands\analyze.c|||1157|||/*
 * acquire_sample_rows -- acquire a random sample of rows from the table
 *
 * Selected rows are returned in the caller-allocated array rows[], which
 * must have at least targrows entries.
 * The actual number of rows selected is returned as the function result.
 * We also estimate the total numbers of live and dead rows in the table,
 * and return them into *totalrows and *totaldeadrows, respectively.
 *
 * The returned list of tuples is in order by physical position in the table.
 * (We will rely on this later to derive correlation estimates.)
 *
 * As of May 2004 we use a new two-stage method:  Stage one selects up
 * to targrows random blocks (or all blocks, if there aren't so many).
 * Stage two scans these blocks and uses the Vitter algorithm to create
 * a random sample of targrows rows (or less, if there are less in the
 * sample of blocks).  The two stages are executed simultaneously: each
 * block is processed as soon as stage one returns its number and while
 * the rows are read stage two controls which ones are to be inserted
 * into the sample.
 *
 * Although every row has an equal chance of ending up in the final
 * sample, this sampling method is not perfect: not every possible
 * sample has an equal chance of being selected.  For large relations
 * the number of different blocks represented by the sample tends to be
 * too small.  We can live with that for now.  Improvements are welcome.
 *
 * An important property of this sampling method is that because we do
 * look at a statistically unbiased set of blocks, we should get
 * unbiased estimates of the average numbers of live and dead rows per
 * block.  The previous sampling method put too much credence in the row
 * density near the start of the table.
 */""",
  """compare_rows|||backend\commands\analyze.c|||1314|||/*
 * Comparator for sorting rows[] array
 */""",
  """acquire_inherited_sample_rows|||backend\commands\analyze.c|||1344|||/*
 * acquire_inherited_sample_rows -- acquire sample rows from inheritance tree
 *
 * This has the same API as acquire_sample_rows, except that rows are
 * collected from all inheritance children as well as the specified table.
 * We fail and return zero if there are no inheritance children, or if all
 * children are foreign tables that don't support ANALYZE.
 */""",
  """update_attstats|||backend\commands\analyze.c|||1608|||/*
 *	update_attstats() -- update attribute statistics for one relation
 *
 *		Statistics are stored in several places: the pg_class row for the
 *		relation has stats about the whole relation, and there is a
 *		pg_statistic row for each (non-system) attribute that has ever
 *		been analyzed.  The pg_class values are updated by VACUUM, not here.
 *
 *		pg_statistic rows are just added or updated normally.  This means
 *		that pg_statistic will probably contain some deleted rows at the
 *		completion of a vacuum cycle, unless it happens to get vacuumed last.
 *
 *		To keep things simple, we punt for pg_statistic, and don't try
 *		to compute or store rows for pg_statistic itself in pg_statistic.
 *		This could possibly be made to work, but it's not worth the trouble.
 *		Note analyze_rel() has seen to it that we won't come here when
 *		vacuuming pg_statistic itself.
 *
 *		Note: there would be a race condition here if two backends could
 *		ANALYZE the same table concurrently.  Presently, we lock that out
 *		by taking a self-exclusive lock on the relation in analyze_rel().
 */""",
  """std_fetch_func|||backend\commands\analyze.c|||1751|||/*
 * Standard fetch function for use by compute_stats subroutines.
 *
 * This exists to provide some insulation between compute_stats routines
 * and the actual storage of the sample data.
 */""",
  """ind_fetch_func|||backend\commands\analyze.c|||1767|||/*
 * Fetch function for analyzing index expressions.
 *
 * We have not bothered to construct index tuples, instead the data is
 * just in Datum arrays.
 */""",
  """std_typanalyze|||backend\commands\analyze.c|||1844|||/*
 * std_typanalyze -- the default type-specific typanalyze function
 */""",
  """compute_trivial_stats|||backend\commands\analyze.c|||1922|||/*
 *	compute_trivial_stats() -- compute very basic column statistics
 *
 *	We use this when we cannot find a hash "=" operator for the datatype.
 *
 *	We determine the fraction of non-null rows and the average datum width.
 */""",
  """compute_distinct_stats|||backend\commands\analyze.c|||2012|||/*
 *	compute_distinct_stats() -- compute column statistics including ndistinct
 *
 *	We use this when we can find only an "=" operator for the datatype.
 *
 *	We determine the fraction of non-null rows, the average width, the
 *	most common values, and the (estimated) number of distinct values.
 *
 *	The most common values are determined by brute force: we keep a list
 *	of previously seen values, ordered by number of times seen, as we scan
 *	the samples.  A newly seen value is inserted just after the last
 *	multiply-seen value, causing the bottommost (oldest) singly-seen value
 *	to drop off the list.  The accuracy of this method, and also its cost,
 *	depend mainly on the length of the list we are willing to keep.
 */""",
  """compute_scalar_stats|||backend\commands\analyze.c|||2355|||/*
 *	compute_scalar_stats() -- compute column statistics
 *
 *	We use this when we can find "=" and "<" operators for the datatype.
 *
 *	We determine the fraction of non-null rows, the average width, the
 *	most common values, the (estimated) number of distinct values, the
 *	distribution histogram, and the correlation of physical to logical order.
 *
 *	The desired stats can be determined fairly easily after sorting the
 *	data values into order.
 */""",
  """compare_scalars|||backend\commands\analyze.c|||2884|||/*
 * Comparator for sorting ScalarItems
 *
 * Aside from sorting the items, we update the tupnoLink[] array
 * whenever two ScalarItems are found to contain equal datums.  The array
 * is indexed by tupno; for each ScalarItem, it contains the highest
 * tupno that that item's datum has been found to be equal to.  This allows
 * us to avoid additional comparisons in compute_scalar_stats().
 */""",
  """compare_mcvs|||backend\commands\analyze.c|||2915|||/*
 * Comparator for sorting ScalarMCVItems by position
 */""",
  """analyze_mcv_list|||backend\commands\analyze.c|||2933|||/*
 * Analyze the list of common values in the sample and decide how many are
 * worth storing in the table's MCV list.
 *
 * mcv_counts is assumed to be a list of the counts of the most common values
 * seen in the sample, starting with the most common.  The return value is the
 * number that are significantly more common than the values not in the list,
 * and which are therefore deemed worth storing in the table's MCV list.
 */""",
  """<clinit>|||backend\commands\async.c|||177|||/*
 * Struct representing an entry in the global notify queue
 *
 * This struct declaration has the maximal length, but in a real queue entry
 * the data area is only big enough for the actual channel and payload strings
 * (each null-terminated).  AsyncQueueEntryEmptySize is the minimum possible
 * entry size, if both channel and payload strings are empty (but note it
 * doesn't include alignment padding).
 *
 * The "length" field should always be rounded up to the next QUEUEALIGN
 * multiple so that all fields are properly aligned.
 */""",
  """<clinit>|||backend\commands\async.c|||281|||/*
 * Shared memory state for LISTEN/NOTIFY (excluding its SLRU stuff)
 *
 * The AsyncQueueControl structure is protected by the NotifyQueueLock and
 * NotifyQueueTailLock.
 *
 * When holding NotifyQueueLock in SHARED mode, backends may only inspect
 * their own entries as well as the head and tail pointers. Consequently we
 * can allow a backend to update its own record while holding only SHARED lock
 * (since no other backend will inspect it).
 *
 * When holding NotifyQueueLock in EXCLUSIVE mode, backends can inspect the
 * entries of other backends and also change the head pointer. When holding
 * both NotifyQueueLock and NotifyQueueTailLock in EXCLUSIVE mode, backends
 * can change the tail pointers.
 *
 * SLRU buffer pool is divided in banks and bank wise SLRU lock is used as
 * the control lock for the pg_notify SLRU buffers.
 * In order to avoid deadlocks, whenever we need multiple locks, we first get
 * NotifyQueueTailLock, then NotifyQueueLock, and lastly SLRU bank lock.
 *
 * Each backend uses the backend[] array entry with index equal to its
 * ProcNumber.  We rely on this to make SendProcSignal fast.
 *
 * The backend[] array entries for actively-listening backends are threaded
 * together using firstListener and the nextListener links, so that we can
 * scan them without having to iterate over inactive entries.  We keep this
 * list in order by ProcNumber so that the scan is cache-friendly when there
 * are many active entries.
 */""",
  """<clinit>|||backend\commands\async.c|||339|||/*
 * State for pending LISTEN/UNLISTEN actions consists of an ordered list of
 * all actions requested in the current transaction.  As explained above,
 * we don't actually change listenChannels until we reach transaction commit.
 *
 * The list is kept in CurTransactionContext.  In subtransactions, each
 * subtransaction has its own list in its own CurTransactionContext, but
 * successful subtransactions attach their lists to their parent's list.
 * Failed subtransactions simply discard their lists.
 */""",
  """<clinit>|||backend\commands\async.c|||381|||/*
 * State for outbound notifies consists of a list of all channels+payloads
 * NOTIFYed in the current transaction.  We do not actually perform a NOTIFY
 * until and unless the transaction commits.  pendingNotifies is NULL if no
 * NOTIFYs have been done in the current (sub) transaction.
 *
 * We discard duplicate notify events issued in the same transaction.
 * Hence, in addition to the list proper (which we need to track the order
 * of the events, since we guarantee to deliver them in order), we build a
 * hash table which we can probe to detect duplicates.  Since building the
 * hash table is somewhat expensive, we do so only once we have at least
 * MIN_HASHABLE_NOTIFIES events queued in the current (sub) transaction;
 * before that we just scan the events linearly.
 *
 * The list is kept in CurTransactionContext.  In subtransactions, each
 * subtransaction has its own list in its own CurTransactionContext, but
 * successful subtransactions add their entries to their parent's list.
 * Failed subtransactions simply discard their lists.  Since these lists
 * are independent, there may be notify events in a subtransaction's list
 * that duplicate events in some ancestor (sub) transaction; we get rid of
 * the dups when merging the subtransaction's list into its parent's.
 *
 * Note: the action and notify lists do not interact within a transaction.
 * In particular, if a transaction does NOTIFY and then LISTEN on the same
 * condition name, it will get a self-notify at commit.  This is a bit odd
 * but is consistent with our historical behavior.
 */""",
  """asyncQueuePageDiff|||backend\commands\async.c|||465|||/*
 * Compute the difference between two queue page numbers.
 * Previously this function accounted for a wraparound.
 */""",
  """asyncQueuePagePrecedes|||backend\commands\async.c|||475|||/*
 * Determines whether p precedes q.
 * Previously this function accounted for a wraparound.
 */""",
  """AsyncShmemSize|||backend\commands\async.c|||484|||/*
 * Report space needed for our shared memory area
 */""",
  """AsyncShmemInit|||backend\commands\async.c|||501|||/*
 * Initialize our shared memory area
 */""",
  """pg_notify|||backend\commands\async.c|||556|||/*
 * pg_notify -
 *	  SQL function to send a notification event
 */""",
  """Async_Notify|||backend\commands\async.c|||590|||/*
 * Async_Notify
 *
 *		This is executed by the SQL notify command.
 *
 *		Adds the message to the list of pending notifies.
 *		Actual notification happens during transaction commit.
 *		^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 */""",
  """queue_listen|||backend\commands\async.c|||689|||/*
 * queue_listen
 *		Common code for listen, unlisten, unlisten all commands.
 *
 *		Adds the request to the list of pending actions.
 *		Actual update of the listenChannels list happens during transaction
 *		commit.
 */""",
  """Async_Listen|||backend\commands\async.c|||737|||/*
 * Async_Listen
 *
 *		This is executed by the SQL listen command.
 */""",
  """Async_Unlisten|||backend\commands\async.c|||751|||/*
 * Async_Unlisten
 *
 *		This is executed by the SQL unlisten command.
 */""",
  """Async_UnlistenAll|||backend\commands\async.c|||769|||/*
 * Async_UnlistenAll
 *
 *		This is invoked by UNLISTEN * command, and also at backend exit.
 */""",
  """pg_listening_channels|||backend\commands\async.c|||789|||/*
 * SQL function: return a set of the channel names this backend is actively
 * listening to.
 *
 * Note: this coding relies on the fact that the listenChannels list cannot
 * change within a transaction.
 */""",
  """Async_UnlistenOnExit|||backend\commands\async.c|||822|||/*
 * Async_UnlistenOnExit
 *
 * This is executed at backend exit if we have done any LISTENs in this
 * backend.  It might not be necessary anymore, if the user UNLISTENed
 * everything, but we don't try to detect that case.
 */""",
  """AtPrepare_Notify|||backend\commands\async.c|||835|||/*
 * AtPrepare_Notify
 *
 *		This is called at the prepare phase of a two-phase
 *		transaction.  Save the state for possible commit later.
 */""",
  """PreCommit_Notify|||backend\commands\async.c|||860|||/*
 * PreCommit_Notify
 *
 *		This is called at transaction commit, before actually committing to
 *		clog.
 *
 *		If there are pending LISTEN actions, make sure we are listed in the
 *		shared-memory listener array.  This must happen before commit to
 *		ensure we don't miss any notifies from transactions that commit
 *		just after ours.
 *
 *		If there are outbound notify requests in the pendingNotifies list,
 *		add them to the global queue.  We do that before commit so that
 *		we can still throw error if we run out of queue space.
 */""",
  """AtCommit_Notify|||backend\commands\async.c|||967|||/*
 * AtCommit_Notify
 *
 *		This is called at transaction commit, after committing to clog.
 *
 *		Update listenChannels and clear transaction-local state.
 *
 *		If we issued any notifications in the transaction, send signals to
 *		listening backends (possibly including ourselves) to process them.
 *		Also, if we filled enough queue pages with new notifies, try to
 *		advance the queue tail pointer.
 */""",
  """Exec_ListenPreCommit|||backend\commands\async.c|||1040|||/*
 * Exec_ListenPreCommit --- subroutine for PreCommit_Notify
 *
 * This function must make sure we are ready to catch any incoming messages.
 */""",
  """Exec_ListenCommit|||backend\commands\async.c|||1135|||/*
 * Exec_ListenCommit --- subroutine for AtCommit_Notify
 *
 * Add the channel to the list of channels we are listening on.
 */""",
  """Exec_UnlistenCommit|||backend\commands\async.c|||1162|||/*
 * Exec_UnlistenCommit --- subroutine for AtCommit_Notify
 *
 * Remove the specified channel name from listenChannels.
 */""",
  """Exec_UnlistenAllCommit|||backend\commands\async.c|||1193|||/*
 * Exec_UnlistenAllCommit --- subroutine for AtCommit_Notify
 *
 *		Unlisten on all channels for this backend.
 */""",
  """IsListeningOn|||backend\commands\async.c|||1211|||/*
 * Test whether we are actively listening on the given channel name.
 *
 * Note: this function is executed for every notification found in the queue.
 * Perhaps it is worth further optimization, eg convert the list to a sorted
 * array so we can binary-search it.  In practice the list is likely to be
 * fairly short, though.
 */""",
  """asyncQueueUnregister|||backend\commands\async.c|||1230|||/*
 * Remove our entry from the listeners array when we are no longer listening
 * on any channel.  NB: must not fail if we're already not listening.
 */""",
  """asyncQueueIsFull|||backend\commands\async.c|||1271|||/*
 * Test whether there is room to insert more notification messages.
 *
 * Caller must hold at least shared NotifyQueueLock.
 */""",
  """asyncQueueAdvance|||backend\commands\async.c|||1286|||/*
 * Advance the QueuePosition to the next entry, assuming that the current
 * entry is of length entryLength.  If we jump to a new page the function
 * returns true, else false.
 */""",
  """asyncQueueNotificationToEntry|||backend\commands\async.c|||1319|||/*
 * Fill the AsyncQueueEntry at *qe with an outbound notification message.
 */"""
)
INFO:__main__:Extracted 2 methods with comments in this batch
INFO:__main__:Fetching batch: offset=2800, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=2800, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres359[0m: [32mList[0m[[32mString[0m] = List(
  """asyncQueueAddEntries|||backend\commands\async.c|||1355|||/*
 * Add pending notifications to the queue.
 *
 * We go page by page here, i.e. we stop once we have to go to a new page but
 * we will be called again and then fill that next page. If an entry does not
 * fit into the current page, we write a dummy entry with an InvalidOid as the
 * database OID in order to fill the page. So every page is always used up to
 * the last byte which simplifies reading the page later.
 *
 * We are passed the list cell (in pendingNotifies->events) containing the next
 * notification to write and return the first still-unwritten cell back.
 * Eventually we will return NULL indicating all is done.
 *
 * We are holding NotifyQueueLock already from the caller and grab
 * page specific SLRU bank lock locally in this function.
 */""",
  """pg_notification_queue_usage|||backend\commands\async.c|||1480|||/*
 * SQL function to return the fraction of the notification queue currently
 * occupied.
 */""",
  """asyncQueueUsage|||backend\commands\async.c|||1505|||/*
 * Return the fraction of the queue that is currently occupied.
 *
 * The caller must hold NotifyQueueLock in (at least) shared mode.
 *
 * Note: we measure the distance to the logical tail page, not the physical
 * tail page.  In some sense that's wrong, but the relative position of the
 * physical tail is affected by details such as SLRU segment boundaries,
 * so that a result based on that is unpleasantly unstable.
 */""",
  """asyncQueueFillWarning|||backend\commands\async.c|||1526|||/*
 * Check whether the queue is at least half full, and emit a warning if so.
 *
 * This is unlikely given the size of the queue, but possible.
 * The warnings show up at most once every QUEUE_FULL_WARN_INTERVAL.
 *
 * Caller must hold exclusive NotifyQueueLock.
 */""",
  """SignalBackends|||backend\commands\async.c|||1580|||/*
 * Send signals to listening backends.
 *
 * Normally we signal only backends in our own database, since only those
 * backends could be interested in notifies we send.  However, if there's
 * notify traffic in our database but no traffic in another database that
 * does have listener(s), those listeners will fall further and further
 * behind.  Waken them anyway if they're far enough behind, so that they'll
 * advance their queue position pointers, allowing the global tail to advance.
 *
 * Since we know the ProcNumber and the Pid the signaling is quite cheap.
 *
 * This is called during CommitTransaction(), so it's important for it
 * to have very low probability of failure.
 */""",
  """AtAbort_Notify|||backend\commands\async.c|||1670|||/*
 * AtAbort_Notify
 *
 *	This is called at transaction abort.
 *
 *	Gets rid of pending actions and outbound notifies that we would have
 *	executed if the transaction got committed.
 */""",
  """AtSubCommit_Notify|||backend\commands\async.c|||1690|||/*
 * AtSubCommit_Notify() --- Take care of subtransaction commit.
 *
 * Reassign all items in the pending lists to the parent transaction.
 */""",
  """AtSubAbort_Notify|||backend\commands\async.c|||1760|||/*
 * AtSubAbort_Notify() --- Take care of subtransaction abort.
 */""",
  """HandleNotifyInterrupt|||backend\commands\async.c|||1803|||/*
 * HandleNotifyInterrupt
 *
 *		Signal handler portion of interrupt handling. Let the backend know
 *		that there's a pending notify interrupt. If we're currently reading
 *		from the client, this will interrupt the read and
 *		ProcessClientReadInterrupt() will call ProcessNotifyInterrupt().
 */""",
  """ProcessNotifyInterrupt|||backend\commands\async.c|||1833|||/*
 * ProcessNotifyInterrupt
 *
 *		This is called if we see notifyInterruptPending set, just before
 *		transmitting ReadyForQuery at the end of a frontend command, and
 *		also if a notify signal occurs while reading from the frontend.
 *		HandleNotifyInterrupt() will cause the read to be interrupted
 *		via the process's latch, and this routine will get called.
 *		If we are truly idle (ie, *not* inside a transaction block),
 *		process the incoming notifies.
 *
 *		If "flush" is true, force any frontend messages out immediately.
 *		This can be false when being called at the end of a frontend command,
 *		since we'll flush after sending ReadyForQuery.
 */""",
  """asyncQueueReadAllNotifications|||backend\commands\async.c|||1850|||/*
 * Read all pending notifications from the queue, and deliver appropriate
 * ones to my frontend.  Stop when we reach queue head or an uncommitted
 * notification.
 */""",
  """asyncQueueProcessPageEntries|||backend\commands\async.c|||2015|||/*
 * Fetch notifications from the shared queue, beginning at position current,
 * and deliver relevant ones to my frontend.
 *
 * The current page must have been fetched into page_buffer from shared
 * memory.  (We could access the page right in shared memory, but that
 * would imply holding the SLRU bank lock throughout this routine.)
 *
 * We stop if we reach the "stop" position, or reach a notification from an
 * uncommitted transaction, or reach the end of the page.
 *
 * The function returns true once we have reached the stop position or an
 * uncommitted notification, and false if we have finished with the page.
 * In other words: once it returns true there is no need to look further.
 * The QueuePosition *current is advanced past all processed messages.
 */""",
  """asyncQueueAdvanceTail|||backend\commands\async.c|||2107|||/*
 * Advance the shared queue tail variable to the minimum of all the
 * per-backend tail pointers.  Truncate pg_notify space if possible.
 *
 * This is (usually) called during CommitTransaction(), so it's important for
 * it to have very low probability of failure.
 */""",
  """ProcessIncomingNotify|||backend\commands\async.c|||2182|||/*
 * ProcessIncomingNotify
 *
 *		Scan the queue for arriving notifications and report them to the front
 *		end.  The notifications might be from other sessions, or our own;
 *		there's no need to distinguish here.
 *
 *		If "flush" is true, force any frontend messages out immediately.
 *
 *		NOTE: since we are outside any transaction, we must create our own.
 */""",
  """NotifyMyFrontEnd|||backend\commands\async.c|||2232|||/*
 * Send NOTIFY message to my front end.
 */""",
  """AsyncExistsPendingNotify|||backend\commands\async.c|||2256|||/* Does pendingNotifies include a match for the given event? */""",
  """AddEventToPendingNotifies|||backend\commands\async.c|||2297|||/*
 * Add a notification event to a pre-existing pendingNotifies list.
 *
 * Because pendingNotifies->events is already nonempty, this works
 * correctly no matter what CurrentMemoryContext is.
 */""",
  """notification_hash|||backend\commands\async.c|||2356|||/*
 * notification_hash: hash function for notification hash table
 *
 * The hash "keys" are pointers to Notification structs.
 */""",
  """notification_match|||backend\commands\async.c|||2370|||/*
 * notification_match: match function to use with notification_hash
 */""",
  """ClearPendingActionsAndNotifies|||backend\commands\async.c|||2386|||/* Clear the pendingActions and pendingNotifies lists. */""",
  """check_notify_buffers|||backend\commands\async.c|||2402|||/*
 * GUC check_hook for notify_buffers
 */""",
  """cluster|||backend\commands\cluster.c|||107|||/*---------------------------------------------------------------------------
 * This cluster code allows for clustering multiple tables at once. Because
 * of this, we cannot just run everything on a single transaction, or we
 * would be forced to acquire exclusive locks on all the tables being
 * clustered, simultaneously --- very likely leading to deadlock.
 *
 * To solve this we follow a similar strategy to VACUUM code,
 * clustering each relation in a separate transaction. For this to work,
 * we need to:
 *	- provide a separate memory context so that we can pass information in
 *	  a way that survives across transactions
 *	- start a new transaction every time a new relation is clustered
 *	- check for validity of the information on to-be-clustered relations,
 *	  as someone might have deleted a relation behind our back, or
 *	  clustered one on a different index
 *	- end the transaction
 *
 * The single-relation case does not have any such overhead.
 *
 * We also allow a relation to be specified without index.  In that case,
 * the indisclustered bit will be looked up, and an ERROR will be thrown
 * if there is no index with the bit set.
 *---------------------------------------------------------------------------
 */""",
  """cluster_multiple_rels|||backend\commands\cluster.c|||265|||/*
 * Given a list of relations to cluster, process each of them in a separate
 * transaction.
 *
 * We expect to be in a transaction at start, but there isn't one when we
 * return.
 */""",
  """cluster_rel|||backend\commands\cluster.c|||310|||/*
 * cluster_rel
 *
 * This clusters the table by creating a new, clustered table and
 * swapping the relfilenumbers of the new table and the old table, so
 * the OID of the original table is preserved.  Thus we do not lose
 * GRANT, inheritance nor references to this table (this was a bug
 * in releases through 7.3).
 *
 * Indexes are rebuilt too, via REINDEX. Since we are effectively bulk-loading
 * the new table, it's better to create the indexes afterwards than to fill
 * them incrementally while we load the table.
 *
 * If indexOid is InvalidOid, the table will be rewritten in physical order
 * instead of index order.  This is the new implementation of VACUUM FULL,
 * and error messages should refer to the operation as VACUUM not CLUSTER.
 */""",
  """check_index_is_clusterable|||backend\commands\cluster.c|||499|||/*
 * Verify that the specified heap and index are valid to cluster on
 *
 * Side effect: obtains lock on the index.  The caller may
 * in some cases already have AccessExclusiveLock on the table, but
 * not in all cases so we can't rely on the table-level lock for
 * protection here.
 */""",
  """mark_index_clustered|||backend\commands\cluster.c|||559|||/*
 * mark_index_clustered: mark the specified index as the one clustered on
 *
 * With indexOid == InvalidOid, will mark all indexes of rel not-clustered.
 */""",
  """rebuild_relation|||backend\commands\cluster.c|||632|||/*
 * rebuild_relation: rebuild an existing relation in index or physical order
 *
 * OldHeap: table to rebuild --- must be opened and exclusive-locked!
 * indexOid: index to cluster by, or InvalidOid to rewrite in physical order.
 *
 * NB: this routine closes OldHeap at the right time; caller should not.
 */""",
  """make_new_heap|||backend\commands\cluster.c|||687|||/*
 * Create the transient table that will be filled with new data during
 * CLUSTER, ALTER TABLE, and similar operations.  The transient table
 * duplicates the logical structure of the OldHeap; but will have the
 * specified physical storage properties NewTableSpace, NewAccessMethod, and
 * relpersistence.
 *
 * After this, the caller should load the new heap with transferred/modified
 * data, then call finish_heap_swap to complete the operation.
 */""",
  """copy_table_data|||backend\commands\cluster.c|||813|||/*
 * Do the physical copying of table data.
 *
 * There are three output parameters:
 * *pSwapToastByContent is set true if toast tables must be swapped by content.
 * *pFreezeXid receives the TransactionId used as freeze cutoff point.
 * *pCutoffMulti receives the MultiXactId used as a cutoff point.
 */""",
  """swap_relation_files|||backend\commands\cluster.c|||1060|||/*
 * Swap the physical files of two given relations.
 *
 * We swap the physical identity (reltablespace, relfilenumber) while keeping
 * the same logical identities of the two relations.  relpersistence is also
 * swapped, which is critical since it determines where buffers live for each
 * relation.
 *
 * We can swap associated TOAST data in either of two ways: recursively swap
 * the physical content of the toast tables (and their indexes), or swap the
 * TOAST links in the given relations' pg_class entries.  The former is needed
 * to manage rewrites of shared catalogs (where we cannot change the pg_class
 * links) while the latter is the only way to handle cases in which a toast
 * table is added or removed altogether.
 *
 * Additionally, the first relation is marked with relfrozenxid set to
 * frozenXid.  It seems a bit ugly to have this here, but the caller would
 * have to do it anyway, so having it here saves a heap_update.  Note: in
 * the swap-toast-links case, we assume we don't need to change the toast
 * table's relfrozenxid: the new version of the toast table should already
 * have relfrozenxid set to RecentXmin, which is good enough.
 *
 * Lastly, if r2 and its toast table and toast index (if any) are mapped,
 * their OIDs are emitted into mapped_tables[].  This is hacky but beats
 * having to look the information up again later in finish_heap_swap.
 */""",
  """finish_heap_swap|||backend\commands\cluster.c|||1437|||/*
 * Remove the transient table that was built by make_new_heap, and finish
 * cleaning up (including rebuilding all indexes on the old heap).
 */""",
  """get_tables_to_cluster|||backend\commands\cluster.c|||1635|||/*
 * Get a list of tables that the current user has privileges on and
 * have indisclustered set.  Return the list in a List * of RelToCluster
 * (stored in the specified memory context), each one giving the tableOid
 * and the indexOid on which the table is already clustered.
 */""",
  """get_tables_to_cluster_partitioned|||backend\commands\cluster.c|||1689|||/*
 * Given an index on a partitioned table, return a list of RelToCluster for
 * all the children leaves tables/indexes.
 *
 * Like expand_vacuum_rel, but here caller must hold AccessExclusiveLock
 * on the table containing the index.
 */""",
  """cluster_is_permitted_for_relation|||backend\commands\cluster.c|||1737|||/*
 * Return whether userid has privileges to CLUSTER relid.  If not, this
 * function emits a WARNING.
 */""",
  """DefineCollation|||backend\commands\collationcmds.c|||52|||/*
 * CREATE COLLATION
 */""",
  """IsThereCollationInNamespace|||backend\commands\collationcmds.c|||399|||/*
 * Subroutine for ALTER COLLATION SET SCHEMA and RENAME
 *
 * Is there a collation with the same name of the given collation already in
 * the given namespace?  If so, raise an appropriate error message.
 */""",
  """AlterCollation|||backend\commands\collationcmds.c|||427|||/*
 * ALTER COLLATION
 */""",
  """normalize_libc_locale_name|||backend\commands\collationcmds.c|||599|||/*
 * "Normalize" a libc locale name, stripping off encoding tags such as
 * ".utf8" (e.g., "en_US.utf8" -> "en_US", but "br_FR.iso885915@euro"
 * -> "br_FR@euro").  Return true if a new, different name was
 * generated.
 */""",
  """cmpaliases|||backend\commands\collationcmds.c|||630|||/*
 * qsort comparator for CollAliasData items
 */""",
  """get_icu_locale_comment|||backend\commands\collationcmds.c|||649|||/*
 * Get a comment (specifically, the display name) for an ICU locale.
 * The result is a palloc'd string, or NULL if we can't get a comment
 * or find that it's not all ASCII.  (We can *not* accept non-ASCII
 * comments, because the contents of template0 must be encoding-agnostic.)
 */""",
  """create_collation_from_locale|||backend\commands\collationcmds.c|||699|||/*
 * Create a new collation using the input locale 'locale'. (subroutine for
 * pg_import_system_collations())
 *
 * 'nspid' is the namespace id where the collation will be created.
 *
 * 'nvalidp' is incremented if the locale has a valid encoding.
 *
 * 'ncreatedp' is incremented if the collation is actually created.  If the
 * collation already exists it will quietly do nothing.
 *
 * The returned value is the encoding of the locale, -1 if the locale is not
 * valid for creating a collation.
 *
 */""",
  """win32_read_locale|||backend\commands\collationcmds.c|||777|||/*
 * Callback function for EnumSystemLocalesEx() in
 * pg_import_system_collations().  Creates a collation for every valid locale
 * and a POSIX alias collation.
 *
 * The callback contract is to return TRUE to continue enumerating and FALSE
 * to stop enumerating.  We always want to continue.
 */""",
  """pg_import_system_collations|||backend\commands\collationcmds.c|||839|||/*
 * pg_import_system_collations: add known system collations to pg_collation
 */""",
  """CommentObject|||backend\commands\comment.c|||39|||/*
 * CommentObject --
 *
 * This routine is used to add the associated comment into
 * pg_description for the object specified by the given SQL command.
 */""",
  """CreateComments|||backend\commands\comment.c|||142|||/*
 * CreateComments --
 *
 * Create a comment for the specified object descriptor.  Inserts a new
 * pg_description tuple, or replaces an existing one with the same key.
 *
 * If the comment given is null or an empty string, instead delete any
 * existing comment for the specified key.
 */""",
  """CreateSharedComments|||backend\commands\comment.c|||237|||/*
 * CreateSharedComments --
 *
 * Create a comment for the specified shared object descriptor.  Inserts a
 * new pg_shdescription tuple, or replaces an existing one with the same key.
 *
 * If the comment given is null or an empty string, instead delete any
 * existing comment for the specified key.
 */""",
  """DeleteComments|||backend\commands\comment.c|||325|||/*
 * DeleteComments -- remove comments for an object
 *
 * If subid is nonzero then only comments matching it will be removed.
 * If subid is zero, all comments matching the oid/classoid will be removed
 * (this corresponds to deleting a whole object).
 */""",
  """DeleteSharedComments|||backend\commands\comment.c|||373|||/*
 * DeleteSharedComments -- remove comments for a shared object
 */""",
  """GetComment|||backend\commands\comment.c|||409|||/*
 * GetComment -- get the comment for an object, or null if not found.
 */""",
  """unique_key_recheck|||backend\commands\constraint.c|||38|||/*
 * unique_key_recheck - trigger function to do a deferred uniqueness check.
 *
 * This now also does deferred exclusion-constraint checks, so the name is
 * somewhat historical.
 *
 * This is invoked as an AFTER ROW trigger for both INSERT and UPDATE,
 * for any rows recorded as potentially violating a deferrable unique
 * or exclusion constraint.
 *
 * This may be an end-of-statement check, a commit-time check, or a
 * check triggered by a SET CONSTRAINTS command.
 */""",
  """CreateConversionCommand|||backend\commands\conversioncmds.c|||31|||/*
 * CREATE CONVERSION
 */""",
  """DoCopy|||backend\commands\copy.c|||61|||/*
 *	 DoCopy executes the SQL COPY statement
 *
 * Either unload or reload contents of table <relation>, depending on <from>.
 * (<from> = true means we are inserting into the table.)  In the "TO" case
 * we also support copying the output of an arbitrary SELECT, INSERT, UPDATE
 * or DELETE query.
 *
 * If <pipe> is false, transfer is between the table and the file named
 * <filename>.  Otherwise, transfer is between the table and our regular
 * input/output stream. The latter could be either stdin/stdout or a
 * socket, depending on whether we're running under Postmaster control.
 *
 * Do not allow a Postgres user without the 'pg_read_server_files' or
 * 'pg_write_server_files' role to read from or write to a file.
 *
 * Do not allow the copy if user doesn't have proper permission to access
 * the table or the specifically requested columns.
 */""",
  """defGetCopyHeaderChoice|||backend\commands\copy.c|||328|||/*
 * Extract a CopyHeaderChoice value from a DefElem.  This is like
 * defGetBoolean() but also accepts the special value "match".
 */""",
  """defGetCopyOnErrorChoice|||backend\commands\copy.c|||392|||/*
 * Extract a CopyOnErrorChoice value from a DefElem.
 */""",
  """defGetCopyLogVerbosityChoice|||backend\commands\copy.c|||424|||/*
 * Extract a CopyLogVerbosityChoice value from a DefElem.
 */""",
  """ProcessCopyOptions|||backend\commands\copy.c|||462|||/*
 * Process the statement option list for COPY.
 *
 * Scan the options list (a list of DefElem) and transpose the information
 * into *opts_out, applying appropriate error checking.
 *
 * If 'opts_out' is not NULL, it is assumed to be filled with zeroes initially.
 *
 * This is exported so that external users of the COPY API can sanity-check
 * a list of options.  In that usage, 'opts_out' can be passed as NULL and
 * the collected data is just leaked until CurrentMemoryContext is reset.
 *
 * Note that additional checking, such as whether column names listed in FORCE
 * QUOTE actually exist, has to be applied later.  This just checks for
 * self-consistency of the options list.
 */""",
  """CopyGetAttnums|||backend\commands\copy.c|||895|||/*
 * CopyGetAttnums - build an integer list of attnums to be copied
 *
 * The input attnamelist is either the user-specified column list,
 * or NIL if there was none (in which case we want all the non-dropped
 * columns).
 *
 * We don't include generated columns in the generated full list and we don't
 * allow them to be specified explicitly.  They don't make sense for COPY
 * FROM, but we could possibly allow them for COPY TO.  But this way it's at
 * least ensured that whatever we copy out can be copied back in.
 *
 * rel can be NULL ... it's only used for error reports.
 */""",
  """<clinit>|||backend\commands\copyfrom.c|||75|||/* Stores multi-insert data related to a single relation in CopyFrom. */""",
  """CopyFromErrorCallback|||backend\commands\copyfrom.c|||111|||/*
 * error context callback for COPY FROM
 *
 * The argument for the error context must be CopyFromState.
 */""",
  """CopyLimitPrintoutLength|||backend\commands\copyfrom.c|||190|||/*
 * Make sure we don't print an unreasonable amount of COPY data in a message.
 *
 * Returns a pstrdup'd copy of the input.
 */""",
  """CopyMultiInsertBufferInit|||backend\commands\copyfrom.c|||220|||/*
 * Allocate memory and initialize a new CopyMultiInsertBuffer for this
 * ResultRelInfo.
 */""",
  """CopyMultiInsertInfoSetupBuffer|||backend\commands\copyfrom.c|||237|||/*
 * Make a new buffer for this ResultRelInfo.
 */""",
  """CopyMultiInsertInfoInit|||backend\commands\copyfrom.c|||257|||/*
 * Initialize an already allocated CopyMultiInsertInfo.
 *
 * If rri is a non-partitioned table then a CopyMultiInsertBuffer is set up
 * for that table.
 */""",
  """CopyMultiInsertInfoIsFull|||backend\commands\copyfrom.c|||282|||/*
 * Returns true if the buffers are full
 */""",
  """CopyMultiInsertInfoIsEmpty|||backend\commands\copyfrom.c|||294|||/*
 * Returns true if we have no buffered tuples
 */""",
  """CopyMultiInsertBufferFlush|||backend\commands\copyfrom.c|||303|||/*
 * Write the tuples stored in 'buffer' out to the table.
 */""",
  """CopyMultiInsertBufferCleanup|||backend\commands\copyfrom.c|||477|||/*
 * Drop used slots and free member for this buffer.
 *
 * The buffer must be flushed before cleanup.
 */""",
  """CopyMultiInsertInfoFlush|||backend\commands\copyfrom.c|||519|||/*
 * Write out all stored tuples in all buffers out to the tables.
 *
 * Once flushed we also trim the tracked buffers list down to size by removing
 * the buffers created earliest first.
 *
 * Callers should pass 'curr_rri' as the ResultRelInfo that's currently being
 * used.  When cleaning up old buffers we'll never remove the one for
 * 'curr_rri'.
 */""",
  """CopyMultiInsertInfoCleanup|||backend\commands\copyfrom.c|||566|||/*
 * Cleanup allocated buffers and free memory
 */""",
  """CopyMultiInsertInfoNextFreeSlot|||backend\commands\copyfrom.c|||585|||/*
 * Get the next TupleTableSlot that the next tuple should be stored in.
 *
 * Callers must ensure that the buffer is not full.
 *
 * Note: 'miinfo' is unused but has been included for consistency with the
 * other functions in this area.
 */""",
  """CopyMultiInsertInfoStore|||backend\commands\copyfrom.c|||604|||/*
 * Record the previously reserved TupleTableSlot that was reserved by
 * CopyMultiInsertInfoNextFreeSlot as being consumed.
 */""",
  """CopyFrom|||backend\commands\copyfrom.c|||627|||/*
 * Copy FROM file to relation.
 */""",
  """BeginCopyFrom|||backend\commands\copyfrom.c|||1367|||/*
 * Setup to read tuples from a file for COPY FROM.
 *
 * 'rel': Used as a template for the tuples
 * 'whereClause': WHERE clause from the COPY FROM command
 * 'filename': Name of server-local file to read, NULL for STDIN
 * 'is_program': true if 'filename' is program to execute
 * 'data_source_cb': callback that provides the input data
 * 'attnamelist': List of char *, columns to include. NIL selects all cols.
 * 'options': List of DefElem. See copy_opt_item in gram.y for selections.
 *
 * Returns a CopyFromState, to be passed to NextCopyFrom and related functions.
 */""",
  """EndCopyFrom|||backend\commands\copyfrom.c|||1786|||/*
 * Clean up storage and release resources for COPY FROM.
 */""",
  """ClosePipeFromProgram|||backend\commands\copyfrom.c|||1812|||/*
 * Closes the pipe from an external program, checking the pclose() return code.
 */""",
  """ReceiveCopyBegin|||backend\commands\copyfromparse.c|||169|||/* Low-level communications functions */""",
  """ReceiveCopyBinaryHeader|||backend\commands\copyfromparse.c|||189|||/* Low-level communications functions */""",
  """CopyGetData|||backend\commands\copyfromparse.c|||244|||/*
 * CopyGetData reads data from the source (file or frontend)
 *
 * We attempt to read at least minread, and at most maxread, bytes from
 * the source.  The actual number of bytes read is returned; if this is
 * less than minread, EOF was detected.
 *
 * Note: when copying from the frontend, we expect a proper EOF mark per
 * protocol; if the frontend simply drops the connection, we raise error.
 * It seems unwise to allow the COPY IN to complete normally in that case.
 *
 * NB: no data conversion is applied here.
 */""",
  """CopyGetInt32|||backend\commands\copyfromparse.c|||361|||/*
 * CopyGetInt32 reads an int32 that appears in network byte order
 *
 * Returns true if OK, false if EOF
 */""",
  """CopyGetInt16|||backend\commands\copyfromparse.c|||378|||/*
 * CopyGetInt16 reads an int16 that appears in network byte order
 */""",
  """CopyConvertBuf|||backend\commands\copyfromparse.c|||399|||/*
 * Perform encoding conversion on data in 'raw_buf', writing the converted
 * data into 'input_buf'.
 *
 * On entry, there must be some data to convert in 'raw_buf'.
 */""",
  """CopyConversionError|||backend\commands\copyfromparse.c|||532|||/*
 * Report an encoding or conversion error.
 */""",
  """CopyLoadRawBuf|||backend\commands\copyfromparse.c|||589|||/*
 * Load more data from data source to raw_buf.
 *
 * If RAW_BUF_BYTES(cstate) > 0, the unprocessed bytes are moved to the
 * beginning of the buffer, and we load new data after that.
 */""",
  """CopyLoadInputBuf|||backend\commands\copyfromparse.c|||649|||/*
 * CopyLoadInputBuf loads some more data into input_buf
 *
 * On return, at least one more input character is loaded into
 * input_buf, or input_reached_eof is set.
 *
 * If INPUT_BUF_BYTES(cstate) > 0, the unprocessed bytes are moved to the start
 * of the buffer and then we load more data after that.
 */""",
  """CopyReadBinaryData|||backend\commands\copyfromparse.c|||700|||/*
 * CopyReadBinaryData
 *
 * Reads up to 'nbytes' bytes from cstate->copy_file via cstate->raw_buf
 * and writes them to 'dest'.  Returns the number of bytes read (which
 * would be less than 'nbytes' only if we reach EOF).
 */""",
  """NextCopyFromRawFields|||backend\commands\copyfromparse.c|||753|||/*
 * Read raw fields in the next line for COPY FROM in text or csv mode.
 * Return false if no more lines.
 *
 * An internal temporary buffer is returned via 'fields'. It is valid until
 * the next call of the function. Since the function returns all raw fields
 * in the input file, 'nfields' could be different from the number of columns
 * in the relation.
 *
 * NOTE: force_not_null option are not applied to the returned fields.
 */""",
  """NextCopyFrom|||backend\commands\copyfromparse.c|||853|||/*
 * Read next tuple from file for COPY FROM. Return false if no more tuples.
 *
 * 'econtext' is used to evaluate default expression for each column that is
 * either not read from the file or is using the DEFAULT option of COPY FROM.
 * It can be NULL when no default values are used, i.e. when all columns are
 * read from the file, and DEFAULT option is unset.
 *
 * 'values' and 'nulls' arrays must be the same length as columns of the
 * relation passed to BeginCopyFrom. This function fills the arrays.
 */""",
  """CopyReadLine|||backend\commands\copyfromparse.c|||1098|||/*
 * Read the next input line and stash it in line_buf.
 *
 * Result is true if read was terminated by EOF, false if terminated
 * by newline.  The terminating newline or EOF marker is not included
 * in the final value of line_buf.
 */""",
  """CopyReadLineText|||backend\commands\copyfromparse.c|||1174|||/*
 * CopyReadLineText - inner loop of CopyReadLine for text mode
 */""",
  """GetDecimalFromHex|||backend\commands\copyfromparse.c|||1508|||/*
 *	Return decimal value for a hexadecimal digit
 */""",
  """CopyReadAttributesText|||backend\commands\copyfromparse.c|||1536|||/*
 * Parse the current line into separate attributes (fields),
 * performing de-escaping as needed.
 *
 * The input is in line_buf.  We use attribute_buf to hold the result
 * strings.  cstate->raw_fields[k] is set to point to the k'th attribute
 * string, or NULL when the input matches the null marker string.
 * This array is expanded as necessary.
 *
 * (Note that the caller cannot check for nulls since the returned
 * string would be the post-de-escaping equivalent, which may look
 * the same as some valid data string.)
 *
 * delim is the column delimiter string (must be just one byte for now).
 * null_print is the null marker string.  Note that this is compared to
 * the pre-de-escaped input string.
 *
 * The return value is the number of fields actually read.
 */""",
  """CopyReadAttributesCSV|||backend\commands\copyfromparse.c|||1790|||/*
 * Parse the current line into separate attributes (fields),
 * performing de-escaping as needed.  This has exactly the same API as
 * CopyReadAttributesText, except we parse the fields according to
 * "standard" (i.e. common) CSV usage.
 */""",
  """CopyReadBinaryAttribute|||backend\commands\copyfromparse.c|||1985|||/*
 * Read a binary attribute
 */""",
  """SendCopyBegin|||backend\commands\copyto.c|||132|||/*
 * Send copy start/stop messages for frontend copies.  These have changed
 * in past protocol redesigns.
 */""",
  """SendCopyEnd|||backend\commands\copyto.c|||149|||	/* Shouldn't have any unsent data */""",
  """CopySendData|||backend\commands\copyto.c|||168|||/*----------
 * CopySendData sends output data to the destination (file or frontend)
 * CopySendString does the same for null-terminated strings
 * CopySendChar does the same for single characters
 * CopySendEndOfRow does the appropriate thing at end of each data row
 *	(data is not actually flushed except by CopySendEndOfRow)
 *
 * NB: no data conversion is applied by these functions
 *----------
 */""",
  """CopySendString|||backend\commands\copyto.c|||174|||/*----------
 * CopySendData sends output data to the destination (file or frontend)
 * CopySendString does the same for null-terminated strings
 * CopySendChar does the same for single characters
 * CopySendEndOfRow does the appropriate thing at end of each data row
 *	(data is not actually flushed except by CopySendEndOfRow)
 *
 * NB: no data conversion is applied by these functions
 *----------
 */""",
  """CopySendChar|||backend\commands\copyto.c|||180|||/*----------
 * CopySendData sends output data to the destination (file or frontend)
 * CopySendString does the same for null-terminated strings
 * CopySendChar does the same for single characters
 * CopySendEndOfRow does the appropriate thing at end of each data row
 *	(data is not actually flushed except by CopySendEndOfRow)
 *
 * NB: no data conversion is applied by these functions
 *----------
 */""",
  """CopySendEndOfRow|||backend\commands\copyto.c|||186|||/*----------
 * CopySendData sends output data to the destination (file or frontend)
 * CopySendString does the same for null-terminated strings
 * CopySendChar does the same for single characters
 * CopySendEndOfRow does the appropriate thing at end of each data row
 *	(data is not actually flushed except by CopySendEndOfRow)
 *
 * NB: no data conversion is applied by these functions
 *----------
 */""",
  """CopySendInt32|||backend\commands\copyto.c|||264|||/*
 * CopySendInt32 sends an int32 in network byte order
 */"""
)
INFO:__main__:Extracted 9 methods with comments in this batch
INFO:__main__:Fetching batch: offset=2900, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=2900, batch_size=100)...
WARNING:src.execution.joern_client:Query failed: [33mval[0m [36mres360[0m: [32mList[0m[[32mString[0m] = List(
  """CopySendInt16|||backend\commands\copyto.c|||276|||/*
 * CopySendInt16 sends an int16 in network byte order
 */""",
  """ClosePipeToProgram|||backend\commands\copyto.c|||288|||/*
 * Closes the pipe to an external program, checking the pclose() return code.
 */""",
  """EndCopy|||backend\commands\copyto.c|||313|||/*
 * Release resources allocated in a cstate for COPY TO/FROM.
 */""",
  """BeginCopyTo|||backend\commands\copyto.c|||349|||/*
 * Setup CopyToState to read tuples from a table or a query for COPY TO.
 *
 * 'rel': Relation to be copied
 * 'raw_query': Query whose results are to be copied
 * 'queryRelId': OID of base relation to convert to a query (for RLS)
 * 'filename': Name of server-local file to write, NULL for STDOUT
 * 'is_program': true if 'filename' is program to execute
 * 'data_dest_cb': Callback that processes the output data
 * 'attnamelist': List of char *, columns to include. NIL selects all cols.
 * 'options': List of DefElem. See copy_opt_item in gram.y for selections.
 *
 * Returns a CopyToState, to be passed to DoCopyTo() and related functions.
 */""",
  """EndCopyTo|||backend\commands\copyto.c|||725|||/*
 * Clean up storage and release resources for COPY TO.
 */""",
  """DoCopyTo|||backend\commands\copyto.c|||746|||/*
 * Copy from relation or query TO file.
 *
 * Returns the number of rows processed.
 */""",
  """CopyOneRowTo|||backend\commands\copyto.c|||906|||/*
 * Emit one row during DoCopyTo().
 */""",
  """CopyAttributeOutText|||backend\commands\copyto.c|||986|||/*
 * Send text representation of one attribute, with conversion and escaping
 */""",
  """CopyAttributeOutCSV|||backend\commands\copyto.c|||1139|||/*
 * Send text representation of one attribute, with conversion and
 * CSV-style escaping
 */""",
  """copy_dest_startup|||backend\commands\copyto.c|||1225|||/*
 * copy_dest_startup --- executor startup
 */""",
  """copy_dest_receive|||backend\commands\copyto.c|||1234|||/*
 * copy_dest_receive --- receive one tuple
 */""",
  """copy_dest_shutdown|||backend\commands\copyto.c|||1253|||/*
 * copy_dest_shutdown --- executor end
 */""",
  """copy_dest_destroy|||backend\commands\copyto.c|||1262|||/*
 * copy_dest_destroy --- release DestReceiver object
 */""",
  """CreateCopyDestReceiver|||backend\commands\copyto.c|||1271|||/*
 * CreateCopyDestReceiver -- create a suitable DestReceiver object
 */""",
  """create_ctas_internal|||backend\commands\createas.c|||79|||/*
 * create_ctas_internal
 *
 * Internal utility used for the creation of the definition of a relation
 * created via CREATE TABLE AS or a materialized view.  Caller needs to
 * provide a list of attributes (ColumnDef nodes).
 */""",
  """create_ctas_nodata|||backend\commands\createas.c|||152|||/*
 * create_ctas_nodata
 *
 * Create CTAS or materialized view when WITH NO DATA is used, starting from
 * the targetlist of the SELECT or view definition.
 */""",
  """ExecCreateTableAs|||backend\commands\createas.c|||220|||/*
 * ExecCreateTableAs -- execute a CREATE TABLE AS command
 */""",
  """GetIntoRelEFlags|||backend\commands\createas.c|||367|||/*
 * GetIntoRelEFlags --- compute executor flags needed for CREATE TABLE AS
 *
 * This is exported because EXPLAIN and PREPARE need it too.  (Note: those
 * callers still need to deal explicitly with the skipData flag; since they
 * use different methods for suppressing execution, it doesn't seem worth
 * trying to encapsulate that part.)
 */""",
  """CreateTableAsRelExists|||backend\commands\createas.c|||385|||/*
 * CreateTableAsRelExists --- check existence of relation for CreateTableAsStmt
 *
 * Utility wrapper checking if the relation pending for creation in this
 * CreateTableAsStmt query already exists or not.  Returns true if the
 * relation exists, otherwise false.
 */""",
  """CreateIntoRelDestReceiver|||backend\commands\createas.c|||432|||/*
 * CreateIntoRelDestReceiver -- create a suitable DestReceiver object
 *
 * intoClause will be NULL if called from CreateDestReceiver(), in which
 * case it has to be provided later.  However, it is convenient to allow
 * self->into to be filled in immediately for other callers.
 */""",
  """intorel_startup|||backend\commands\createas.c|||451|||/*
 * intorel_startup --- executor startup
 */""",
  """intorel_receive|||backend\commands\createas.c|||575|||/*
 * intorel_receive --- receive one tuple
 */""",
  """intorel_shutdown|||backend\commands\createas.c|||606|||/*
 * intorel_shutdown --- executor end
 */""",
  """intorel_destroy|||backend\commands\createas.c|||626|||/*
 * intorel_destroy --- release DestReceiver object
 */""",
  """CreateDatabaseUsingWalLog|||backend\commands\dbcommands.c|||147|||/*
 * Create a new database using the WAL_LOG strategy.
 *
 * Each copied block is separately written to the write-ahead log.
 */""",
  """ScanSourceDatabasePgClass|||backend\commands\dbcommands.c|||249|||/*
 * Scan the pg_class table in the source database to identify the relations
 * that need to be copied to the destination database.
 *
 * This is an exception to the usual rule that cross-database access is
 * not possible. We can make it work here because we know that there are no
 * connections to the source database and (since there can't be prepared
 * transactions touching that database) no in-doubt tuples either. This
 * means that we don't need to worry about pruning removing anything from
 * under us, and we don't need to be too picky about our snapshot either.
 * As long as it sees all previously-committed XIDs as committed and all
 * aborted XIDs as aborted, we should be fine: nothing else is possible
 * here.
 *
 * We can't rely on the relcache for anything here, because that only knows
 * about the database to which we are connected, and can't handle access to
 * other databases. That also means we can't rely on the heap scan
 * infrastructure, which would be a bad idea anyway since it might try
 * to do things like HOT pruning which we definitely can't do safely in
 * a database to which we're not even connected.
 */""",
  """ScanSourceDatabasePgClassPage|||backend\commands\dbcommands.c|||327|||/*
 * Scan one page of the source database's pg_class relation and add relevant
 * entries to rlocatorlist. The return value is the updated list.
 */""",
  """ScanSourceDatabasePgClassTuple|||backend\commands\dbcommands.c|||390|||/*
 * Decide whether a certain pg_class tuple represents something that
 * needs to be copied from the source database to the destination database,
 * and if so, construct a CreateDBRelInfo for it.
 *
 * Visibility checks are handled by the caller, so our job here is just
 * to assess the data stored in the tuple.
 */""",
  """CreateDirAndVersionFile|||backend\commands\dbcommands.c|||455|||/*
 * Create database directory and write out the PG_VERSION file in the database
 * path.  If isRedo is true, it's okay for the database directory to exist
 * already.
 */""",
  """CreateDatabaseUsingFileCopy|||backend\commands\dbcommands.c|||549|||/*
 * Create a new database using the FILE_COPY strategy.
 *
 * Copy each tablespace at the filesystem level, and log a single WAL record
 * for each tablespace copied.  This requires a checkpoint before and after the
 * copy, which may be expensive, but it does greatly reduce WAL generation
 * if the copied database is large.
 */""",
  """createdb|||backend\commands\dbcommands.c|||669|||/*
 * CREATE DATABASE
 */""",
  """check_encoding_locale_matches|||backend\commands\dbcommands.c|||1556|||/*
 * Check whether chosen encoding matches chosen locale settings.  This
 * restriction is necessary because libc's locale-specific code usually
 * fails when presented with data in an encoding it's not expecting. We
 * allow mismatch in four cases:
 *
 * 1. locale encoding = SQL_ASCII, which means that the locale is C/POSIX
 * which works with any encoding.
 *
 * 2. locale encoding = -1, which means that we couldn't determine the
 * locale's encoding and have to trust the user to get it right.
 *
 * 3. selected encoding is UTF8 and platform is win32. This is because
 * UTF8 is a pseudo codepage that is supported in all locales since it's
 * converted to UTF16 before being used.
 *
 * 4. selected encoding is SQL_ASCII, but only if you're a superuser. This
 * is risky but we have historically allowed it --- notably, the
 * regression tests require it.
 *
 * Note: if you change this policy, fix initdb to match.
 */""",
  """createdb_failure_callback|||backend\commands\dbcommands.c|||1594|||/* Error cleanup callback for createdb */""",
  """dropdb|||backend\commands\dbcommands.c|||1633|||/*
 * DROP DATABASE
 */""",
  """RenameDatabase|||backend\commands\dbcommands.c|||1862|||/*
 * Rename database
 */""",
  """movedb|||backend\commands\dbcommands.c|||1963|||/*
 * ALTER DATABASE SET TABLESPACE
 */""",
  """movedb_failure_callback|||backend\commands\dbcommands.c|||2285|||/* Error cleanup callback for movedb */""",
  """DropDatabase|||backend\commands\dbcommands.c|||2302|||/*
 * Process options and call dropdb function.
 */""",
  """AlterDatabase|||backend\commands\dbcommands.c|||2327|||/*
 * ALTER DATABASE name ...
 */""",
  """AlterDatabaseRefreshColl|||backend\commands\dbcommands.c|||2500|||/*
 * ALTER DATABASE name REFRESH COLLATION VERSION
 */""",
  """AlterDatabaseSet|||backend\commands\dbcommands.c|||2597|||/*
 * ALTER DATABASE name SET ...
 */""",
  """AlterDatabaseOwner|||backend\commands\dbcommands.c|||2623|||/*
 * ALTER DATABASE name OWNER TO newowner
 */""",
  """get_db_info|||backend\commands\dbcommands.c|||2780|||/*
 * Look up info about the database named "name".  If the database exists,
 * obtain the specified lock type on it, fill in any of the remaining
 * parameters that aren't NULL, and return true.  If no such database,
 * return false.
 */""",
  """have_createdb_privilege|||backend\commands\dbcommands.c|||2938|||/* Check if current user has createdb privileges */""",
  """remove_dbtablespaces|||backend\commands\dbcommands.c|||2963|||/*
 * Remove tablespace directories
 *
 * We don't know what tablespaces db_id is using, so iterate through all
 * tablespaces removing <tablespace>/db_id
 */""",
  """check_db_file_conflict|||backend\commands\dbcommands.c|||3053|||/*
 * Check for existing files that conflict with a proposed new DB OID;
 * return true if there are any
 *
 * If there were a subdirectory in any tablespace matching the proposed new
 * OID, we'd get a create failure due to the duplicate name ... and then we'd
 * try to remove that already-existing subdirectory during the cleanup in
 * remove_dbtablespaces.  Nuking existing files seems like a bad idea, so
 * instead we make this extra check before settling on the OID of the new
 * database.  This exactly parallels what GetNewRelFileNumber() does for table
 * relfilenumber values.
 */""",
  """errdetail_busy_db|||backend\commands\dbcommands.c|||3096|||/*
 * Issue a suitable errdetail message for a busy database
 */""",
  """get_database_oid|||backend\commands\dbcommands.c|||3126|||/*
 * get_database_oid - given a database name, look up the OID
 *
 * If missing_ok is false, throw an error if database name not found.  If
 * true, just return InvalidOid.
 */""",
  """get_database_name|||backend\commands\dbcommands.c|||3173|||/*
 * get_database_name - given a database OID, look up the name
 *
 * Returns a palloc'd string, or NULL if no such database.
 */""",
  """database_is_invalid_form|||backend\commands\dbcommands.c|||3197|||/*
 * While dropping a database the pg_database row is marked invalid, but the
 * catalog contents still exist. Connections to such a database are not
 * allowed.
 */""",
  """database_is_invalid_oid|||backend\commands\dbcommands.c|||3207|||/*
 * Convenience wrapper around database_is_invalid_form()
 */""",
  """recovery_create_dbdir|||backend\commands\dbcommands.c|||3240|||/*
 * recovery_create_dbdir()
 *
 * During recovery, there's a case where we validly need to recover a missing
 * tablespace directory so that recovery can continue.  This happens when
 * recovery wants to create a database but the holding tablespace has been
 * removed before the server stopped.  Since we expect that the directory will
 * be gone before reaching recovery consistency, and we have no knowledge about
 * the tablespace other than its OID here, we create a real directory under
 * pg_tblspc here instead of restoring the symlink.
 *
 * If only_tblspc is true, then the requested directory must be in pg_tblspc/
 */""",
  """dbase_redo|||backend\commands\dbcommands.c|||3269|||/*
 * DATABASE resource manager's routines
 */""",
  """defGetString|||backend\commands\define.c|||47|||/*
 * Extract a string value (otherwise uninterpreted) from a DefElem.
 */""",
  """defGetNumeric|||backend\commands\define.c|||80|||/*
 * Extract a numeric value (actually double) from a DefElem.
 */""",
  """defGetBoolean|||backend\commands\define.c|||106|||/*
 * Extract a boolean value from a DefElem.
 */""",
  """defGetInt32|||backend\commands\define.c|||161|||/*
 * Extract an int32 value from a DefElem.
 */""",
  """defGetInt64|||backend\commands\define.c|||185|||/*
 * Extract an int64 value from a DefElem.
 */""",
  """defGetObjectId|||backend\commands\define.c|||218|||/*
 * Extract an OID value from a DefElem.
 */""",
  """defGetQualifiedName|||backend\commands\define.c|||251|||/*
 * Extract a possibly-qualified name (as a List of Strings) from a DefElem.
 */""",
  """defGetTypeName|||backend\commands\define.c|||283|||/*
 * Extract a TypeName from a DefElem.
 *
 * Note: we do not accept a List arg here, because the parser will only
 * return a bare List when the name looks like an operator name.
 */""",
  """defGetTypeLength|||backend\commands\define.c|||311|||/*
 * Extract a type length indicator (either absolute bytes, or
 * -1 for "variable") from a DefElem.
 */""",
  """defGetStringList|||backend\commands\define.c|||355|||/*
 * Extract a list of string values (otherwise uninterpreted) from a DefElem.
 */""",
  """errorConflictingDefElem|||backend\commands\define.c|||383|||/*
 * Raise an error about a conflicting DefElem.
 */""",
  """DiscardCommand|||backend\commands\discard.c|||30|||/*
 * DISCARD { ALL | SEQUENCES | TEMP | PLANS }
 */""",
  """DiscardAll|||backend\commands\discard.c|||56|||	/*
	 * Disallow DISCARD ALL in a transaction block. This is arguably
	 * inconsistent (we don't make a similar check in the command sequence
	 * that DISCARD ALL is equivalent to), but the idea is to catch mistakes:
	 * DISCARD ALL inside a transaction block would leave the transaction
	 * still uncommitted.
	 */""",
  """RemoveObjects|||backend\commands\dropcmds.c|||52|||/*
 * Drop one or more objects.
 *
 * We don't currently handle all object types here.  Relations, for example,
 * require special handling, because (for example) indexes have additional
 * locking requirements.
 *
 * We look up all the objects first, and then delete them in a single
 * performMultipleDeletions() call.  This avoids unnecessary DROP RESTRICT
 * errors if there are dependencies between them.
 */""",
  """owningrel_does_not_exist_skipping|||backend\commands\dropcmds.c|||138|||/*
 * owningrel_does_not_exist_skipping
 *		Subroutine for RemoveObjects
 *
 * After determining that a specification for a rule or trigger returns that
 * the specified object does not exist, test whether its owning relation, and
 * its schema, exist or not; if they do, return false --- the trigger or rule
 * itself is missing instead.  If the owning relation or its schema do not
 * exist, fill the error message format string and name, and return true.
 */""",
  """schema_does_not_exist_skipping|||backend\commands\dropcmds.c|||173|||/*
 * schema_does_not_exist_skipping
 *		Subroutine for RemoveObjects
 *
 * After determining that a specification for a schema-qualifiable object
 * refers to an object that does not exist, test whether the specified schema
 * exists or not.  If no schema was specified, or if the schema does exist,
 * return false -- the object itself is missing instead.  If the specified
 * schema does not exist, fill the error message format string and the
 * specified schema name, and return true.
 */""",
  """type_in_list_does_not_exist_skipping|||backend\commands\dropcmds.c|||205|||/*
 * type_in_list_does_not_exist_skipping
 *		Subroutine for RemoveObjects
 *
 * After determining that a specification for a function, cast, aggregate or
 * operator returns that the specified object does not exist, test whether the
 * involved datatypes, and their schemas, exist or not; if they do, return
 * false --- the original object itself is missing instead.  If the datatypes
 * or schemas do not exist, fill the error message format string and the
 * missing name, and return true.
 *
 * First parameter is a list of TypeNames.
 */""",
  """does_not_exist_skipping|||backend\commands\dropcmds.c|||242|||/*
 * does_not_exist_skipping
 *		Subroutine for RemoveObjects
 *
 * Generate a NOTICE stating that the named object was not found, and is
 * being skipped.  This is only relevant when "IF EXISTS" is used; otherwise,
 * get_object_address() in RemoveObjects would have thrown an ERROR.
 */""",
  """CreateEventTrigger|||backend\commands\event_trigger.c|||119|||/*
 * Create an event trigger.
 */""",
  """validate_ddl_tags|||backend\commands\event_trigger.c|||211|||/*
 * Validate DDL command tags.
 */""",
  """validate_table_rewrite_tags|||backend\commands\event_trigger.c|||238|||/*
 * Validate DDL command tags for event table_rewrite.
 */""",
  """error_duplicate_filter_variable|||backend\commands\event_trigger.c|||260|||/*
 * Complain about a duplicate filter variable.
 */""",
  """insert_event_trigger_tuple|||backend\commands\event_trigger.c|||272|||/*
 * Insert the new pg_event_trigger row and record dependencies.
 */""",
  """filter_list_to_array|||backend\commands\event_trigger.c|||355|||/*
 * In the parser, a clause like WHEN tag IN ('cmd1', 'cmd2') is represented
 * by a DefElem whose value is a List of String nodes; in the catalog, we
 * store the list of strings as a text array.  This function transforms the
 * former representation into the latter one.
 *
 * For cleanliness, we store command tags in the catalog as text.  It's
 * possible (although not currently anticipated) that we might have
 * a case-sensitive filter variable in the future, in which case this would
 * need some further adjustment.
 */""",
  """SetDatabaseHasLoginEventTriggers|||backend\commands\event_trigger.c|||385|||/*
 * Set pg_database.dathasloginevt flag for current database indicating that
 * current database has on login event triggers.
 */""",
  """AlterEventTrigger|||backend\commands\event_trigger.c|||422|||/*
 * ALTER EVENT TRIGGER foo ENABLE|DISABLE|ENABLE ALWAYS|REPLICA
 */""",
  """AlterEventTriggerOwner|||backend\commands\event_trigger.c|||474|||/*
 * Change event trigger's owner -- by name
 */""",
  """AlterEventTriggerOwner_oid|||backend\commands\event_trigger.c|||509|||/*
 * Change event trigger owner, by OID
 */""",
  """AlterEventTriggerOwner_internal|||backend\commands\event_trigger.c|||534|||/*
 * Internal workhorse for changing an event trigger's owner
 */""",
  """get_event_trigger_oid|||backend\commands\event_trigger.c|||574|||/*
 * get_event_trigger_oid - Look up an event trigger by name to find its OID.
 *
 * If missing_ok is false, throw an error if trigger not found.  If
 * true, just return InvalidOid.
 */""",
  """filter_event_trigger|||backend\commands\event_trigger.c|||593|||/*
 * Return true when we want to fire given Event Trigger and false otherwise,
 * filtering on the session replication role and the event trigger registered
 * tags matching.
 */""",
  """EventTriggerCommonSetup|||backend\commands\event_trigger.c|||633|||/*
 * Setup for running triggers for the given event.  Return value is an OID list
 * of functions to run; if there are any, trigdata is filled with an
 * appropriate EventTriggerData for them to receive.
 */""",
  """EventTriggerDDLCommandStart|||backend\commands\event_trigger.c|||720|||/*
 * Fire ddl_command_start triggers.
 */""",
  """EventTriggerDDLCommandEnd|||backend\commands\event_trigger.c|||771|||/*
 * Fire ddl_command_end triggers.
 */""",
  """EventTriggerSQLDrop|||backend\commands\event_trigger.c|||819|||/*
 * Fire sql_drop triggers.
 */""",
  """EventTriggerOnLogin|||backend\commands\event_trigger.c|||892|||/*
 * Fire login event triggers if any are present.  The dathasloginevt
 * pg_database flag is left unchanged when an event trigger is dropped to avoid
 * complicating the codepath in the case of multiple event triggers.  This
 * function will instead unset the flag if no trigger is defined.
 */""",
  """EventTriggerTableRewrite|||backend\commands\event_trigger.c|||1003|||/*
 * Fire table_rewrite triggers.
 */""",
  """EventTriggerInvoke|||backend\commands\event_trigger.c|||1068|||/*
 * Invoke each event trigger in a list of event triggers.
 */""",
  """EventTriggerSupportsObjectType|||backend\commands\event_trigger.c|||1133|||/*
 * Do event triggers support this object type?
 *
 * See also event trigger support matrix in event-trigger.sgml.
 */""",
  """EventTriggerSupportsObject|||backend\commands\event_trigger.c|||1157|||/*
 * Do event triggers support this object class?
 *
 * See also event trigger support matrix in event-trigger.sgml.
 */""",
  """EventTriggerBeginCompleteQuery|||backend\commands\event_trigger.c|||1183|||/*
 * Prepare event trigger state for a new complete query to run, if necessary;
 * returns whether this was done.  If it was, EventTriggerEndCompleteQuery must
 * be called when the query is done, regardless of whether it succeeds or fails
 * -- so use of a PG_TRY block is mandatory.
 */""",
  """EventTriggerEndCompleteQuery|||backend\commands\event_trigger.c|||1227|||/*
 * Query completed (or errored out) -- clean up local state, return to previous
 * one.
 *
 * Note: it's an error to call this routine if EventTriggerBeginCompleteQuery
 * returned false previously.
 *
 * Note: this might be called in the PG_CATCH block of a failing transaction,
 * so be wary of running anything unnecessary.  (In particular, it's probably
 * unwise to try to allocate memory.)
 */""",
  """trackDroppedObjectsNeeded|||backend\commands\event_trigger.c|||1245|||/*
 * Do we need to keep close track of objects being dropped?
 *
 * This is useful because there is a cost to running with them enabled.
 */""",
  """EventTriggerSQLDropAddObject|||backend\commands\event_trigger.c|||1277|||/*
 * Register one object as being dropped by the current command.
 */""",
  """pg_event_trigger_dropped_objects|||backend\commands\event_trigger.c|||1396|||/*
 * pg_event_trigger_dropped_objects
 *
 * Make the list of dropped objects available to the user function run by the
 * Event Trigger.
 */""",
  """pg_event_trigger_table_rewrite_oid|||backend\commands\event_trigger.c|||1492|||/*
 * pg_event_trigger_table_rewrite_oid
 *
 * Make the Oid of the table going to be rewritten available to the user
 * function run by the Event Trigger.
 */""",
  """pg_event_trigger_table_rewrite_reason|||backend\commands\event_trigger.c|||1513|||/*
 * pg_event_trigger_table_rewrite_reason
 *
 * Make the rewrite reason available to the user.
 */"""
)
INFO:__main__:Extracted 42 methods with comments in this batch
INFO:__main__:Fetching batch: offset=3000, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=3000, batch_size=100)...
INFO:src.execution.joern_client:Query executed successfully in 12.26s
INFO:__main__:Extracted 18 methods with comments in this batch
INFO:__main__:Fetching batch: offset=3100, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=3100, batch_size=100)...
ERROR:src.execution.joern_client:Query execution error: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /result/bde00fba-9dfa-4431-b9c8-7dd6fedba3ab (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022FD518FF90>: Failed to establish a new connection: [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full'))
INFO:__main__:Extracted 0 methods with comments in this batch
WARNING:__main__:Empty batch at offset 3100 (count: 1)
INFO:__main__:Fetching batch: offset=3200, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=3200, batch_size=100)...
ERROR:src.execution.joern_client:Query execution error: Multiple exceptions: [WinError 1225] The remote computer refused the network connection, [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full
INFO:__main__:Extracted 0 methods with comments in this batch
WARNING:__main__:Empty batch at offset 3200 (count: 2)
INFO:__main__:Fetching batch: offset=3300, size=100
INFO:__main__:Extracting method-level comments from CPG (limit=100, offset=3300, batch_size=100)...
ERROR:src.execution.joern_client:Query execution error: [WinError 10055] An operation on a socket could not be performed because the system lacked sufficient buffer space or because a queue was full
INFO:__main__:Extracted 0 methods with comments in this batch
WARNING:__main__:Empty batch at offset 3300 (count: 3)
INFO:__main__:Stopping after 3 consecutive empty batches
INFO:__main__:Extraction complete: {'total_method_docs': 638, 'total_documentation_entries': 638, 'batches_processed': 33, 'final_offset': 3300}
INFO:__main__:Saved documentation to data/cpg_documentation_complete.json


============================================================
Documentation Extraction Summary (v4 - batched)
============================================================
total_method_docs: 638
total_documentation_entries: 638
batches_processed: 33
final_offset: 3,300
============================================================

Sample extracted methods:

1. Method: brinhandler
   File: backend/access/brin/brin.c:246
   Description: BRIN handler function: return IndexAmRoutine with access method parameters    and callbacks....

2. Method: initialize_brin_insertstate
   File: backend/access/brin/brin.c:305
   Description: Initialize a BrinInsertState to maintain state to be used across multiple    tuple inserts, within t...

3. Method: brininsert
   File: backend/access/brin/brin.c:334
   Description: A tuple in the heap is being inserted....
============================================================

