[
  {
    "title": "README: backend\\access\\brin",
    "url": "backend\\access\\brin\\README",
    "content": "Block Range Indexes (BRIN)\n==========================\n\nBRIN indexes intend to enable very fast scanning of extremely large tables.\n\nThe essential idea of a BRIN index is to keep track of summarizing values in\nconsecutive groups of heap pages (page ranges); for example, the minimum and\nmaximum values for datatypes with a btree opclass, or the bounding box for\ngeometric types.  These values can be used to avoid scanning such pages\nduring a table scan, depending on query quals.\n\nThe cost of this is having to update the stored summary values of each page\nrange as tuples are inserted into them.\n\n\nAccess Method Design\n--------------------\n\nSince item pointers are not stored inside indexes of this type, it is not\npossible to support the amgettuple interface.  Instead, we only provide\namgetbitmap support.  The amgetbitmap routine returns a lossy TIDBitmap\ncomprising all pages in those page ranges that match the query\nqualifications.  The recheck step in the BitmapHeapScan node prunes tuples\nthat are not visible according to the query qualifications.\n\nAn operator class must have the following entries:\n\n- generic support procedures (pg_amproc), identical to all opclasses:\n  * \"opcinfo\" (BRIN_PROCNUM_OPCINFO) initializes a structure for index\n    creation or scanning\n  * \"addValue\" (BRIN_PROCNUM_ADDVALUE) takes an index tuple and a heap item,\n    and possibly changes the index tuple so that it includes the heap item\n    values\n  * \"consistent\" (BRIN_PROCNUM_CONSISTENT) takes an index tuple and query\n    quals, and returns whether the index tuple values match the query quals.\n  * \"union\" (BRIN_PROCNUM_UNION) takes two index tuples and modifies the first\n    one so that it represents the union of the two.\nProcedure numbers up to 10 are reserved for future expansion.\n\nAdditionally, each opclass needs additional support functions:\n- Minmax-style operator classes:\n  * Proc numbers 11-14 are used for the functions implementing inequality\n    operators for the type, in this order: less than, less or equal,\n    greater or equal, greater than.\n\nOpclasses using a different design will require different additional procedure\nnumbers.\n\nOperator classes also need to have operator (pg_amop) entries so that the\noptimizer can choose the index to execute queries.\n- Minmax-style operator classes:\n  * The same operators as btree (<=, <, =, >=, >)\n\nEach index tuple stores some NULL bits and some opclass-specified values, which\nare stored in a single null bitmask of length twice the number of columns.  The\ngeneric NULL bits indicate, for each column:\n  * bt_hasnulls: Whether there's any NULL value at all in the page range\n  * bt_allnulls: Whether all values are NULLs in the page range\n\nThe opclass-specified values are:\n- Minmax-style operator classes\n  * minimum value across all tuples in the range\n  * maximum value across all tuples in the range\n\nNote that the addValue and Union support procedures  must be careful to\ndatumCopy() the values they want to store in the in-memory BRIN tuple, and\nmust pfree() the old copies when replacing older ones.  Since some values\nreferenced from the tuple persist and others go away, there is no\nwell-defined lifetime for a memory context that would make this automatic.\n\n\nThe Range Map\n-------------\n\nTo find the index tuple for a particular page range, we have an internal\nstructure we call the range map, or \"revmap\" for short.  This stores one TID\nper page range, which is the address of the index tuple summarizing that\nrange.  Since the map entries are fixed size, it is possible to compute the\naddress of the range map entry for any given heap page by simple arithmetic.\n\nWhen a new heap tuple is inserted in a summarized page range, we compare the\nexisting index tuple with the new heap tuple.  If the heap tuple is outside\nthe summarization data given by the index tuple for any indexed column (or\nif the new heap tuple contains null values but the index tuple indicates\nthere are no nulls), the index is updated with the new values.  In many\ncases it is possible to update the index tuple in-place, but if the new\nindex tuple is larger than the old one and there's not enough space in the\npage, it is necessary to create a new index tuple with the new values.  The\nrange map can be updated quickly to point to it; the old index tuple is\nremoved.\n\nIf the range map points to an invalid TID, the corresponding page range is\nconsidered to be not summarized.  When tuples are added to unsummarized\npages, nothing needs to happen.\n\nTo scan a table following a BRIN index, we scan the range map sequentially.\nThis yields index tuples in ascending page range order.  Query quals are\nmatched to each index tuple; if they match, each page within the page range\nis returned as part of the output TID bitmap.  If there's no match, they are\nskipped.  Range map entries returning invalid index TIDs, that is\nunsummarized page ranges, are also returned in the TID bitmap.\n\nThe revmap is stored in the first few blocks of the index main fork,\nimmediately following the metapage.  Whenever the revmap needs to be\nextended by another page, existing tuples in that page are moved to some\nother page.\n\nHeap tuples can be removed from anywhere without restriction.  It might be\nuseful to mark the corresponding index tuple somehow, if the heap tuple is\none of the constraining values of the summary data (i.e. either min or max\nin the case of a btree-opclass-bearing datatype), so that in the future we\nare aware of the need to re-execute summarization on that range, leading to\na possible tightening of the summary values.\n\nSummarization\n-------------\n\nAt index creation time, the whole table is scanned; for each page range the\nsummarizing values of each indexed column and nulls bitmap are collected and\nstored in the index.  The partially-filled page range at the end of the\ntable is also summarized.\n\nAs new tuples get inserted at the end of the table, they may update the\nindex tuple that summarizes the partial page range at the end.  Eventually\nthat page range is complete and new tuples belong in a new page range that\nhasn't yet been summarized.  Those insertions do not create a new index\nentry; instead, the page range remains unsummarized until later.\n\nWhenever VACUUM is run on the table, all unsummarized page ranges are\nsummarized.  This action can also be invoked by the user via\nbrin_summarize_new_values().  Both these procedures scan all the\nunsummarized ranges, and create a summary tuple.  Again, this includes the\npartially-filled page range at the end of the table.\n\nVacuuming\n---------\n\nSince no heap TIDs are stored in a BRIN index, it's not necessary to scan the\nindex when heap tuples are removed.  It might be that some summary values can\nbe tightened if heap tuples have been deleted; but this would represent an\noptimization opportunity only, not a correctness issue.  It's simpler to\nrepresent this as the need to re-run summarization on the affected page range\nrather than \"subtracting\" values from the existing one.  This is not\ncurrently implemented.\n\nNote that if there are no indexes on the table other than the BRIN index,\nusage of maintenance_work_mem by vacuum can be decreased significantly, because\nno detailed index scan needs to take place (and thus it's not necessary for\nvacuum to save TIDs to remove).  It's unlikely that BRIN would be the only\nindexes in a table, though, because primary keys can be btrees only, and so\nwe don't implement this optimization.\n\n\nOptimizer\n---------\n\nThe optimizer selects the index based on the operator class' pg_amop\nentries for the column.\n\n\nFuture improvements\n-------------------\n\n* Different-size page ranges?\n  In the current design, each \"index entry\" in a BRIN index covers the same\n  number of pages.  There's no hard reason for this; it might make sense to\n  allow the index to self-tune so that some index entries cover smaller page\n  ranges, if this allows the summary values to be more compact.  This would incur\n  larger BRIN overhead for the index itself, but might allow better pruning of\n  page ranges during scan.  In the limit of one index tuple per page, the index\n  itself would occupy too much space, even though we would be able to skip\n  reading the most heap pages, because the summary values are tight; in the\n  opposite limit of a single tuple that summarizes the whole table, we wouldn't\n  be able to prune anything even though the index is very small.  This can\n  probably be made to work by using the range map as an index in itself.\n\n* More compact representation for TIDBitmap?\n  TIDBitmap is the structure used to represent bitmap scans.  The\n  representation of lossy page ranges is not optimal for our purposes, because\n  it uses a Bitmapset to represent pages in the range; since we're going to return\n  all pages in a large range, it might be more convenient to allow for a\n  struct that uses start and end page numbers to represent the range, instead.\n\n* Better vacuuming?\n  It might be useful to enable passing more useful info to BRIN indexes during\n  vacuuming about tuples that are deleted, i.e. do not require the callback to\n  pass each tuple's TID.  For instance we might need a callback that passes a\n  block number instead of a TID.  That would help determine when to re-run\n  summarization on blocks that have seen lots of tuple deletions.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\README",
      "directory": "backend\\access\\brin"
    }
  },
  {
    "title": "README: backend\\access\\gin",
    "url": "backend\\access\\gin\\README",
    "content": "src/backend/access/gin/README\n\nGin for PostgreSQL\n==================\n\nGin was sponsored by jfg://networks (http://www.jfg-networks.com/)\n\nGin stands for Generalized Inverted Index and should be considered as a genie,\nnot a drink.\n\nGeneralized means that the index does not know which operation it accelerates.\nIt instead works with custom strategies, defined for specific data types (read\n\"Index Method Strategies\" in the PostgreSQL documentation). In that sense, Gin\nis similar to GiST and differs from btree indices, which have predefined,\ncomparison-based operations.\n\nAn inverted index is an index structure storing a set of (key, posting list)\npairs, where 'posting list' is a set of heap rows in which the key occurs.\n(A text document would usually contain many keys.)  The primary goal of\nGin indices is support for highly scalable, full-text search in PostgreSQL.\n\nA Gin index consists of a B-tree index constructed over key values,\nwhere each key is an element of some indexed items (element of array, lexeme\nfor tsvector) and where each tuple in a leaf page contains either a pointer to\na B-tree over item pointers (posting tree), or a simple list of item pointers\n(posting list) if the list is small enough.\n\nNote: There is no delete operation in the key (entry) tree. The reason for\nthis is that in our experience, the set of distinct words in a large corpus\nchanges very slowly.  This greatly simplifies the code and concurrency\nalgorithms.\n\nCore PostgreSQL includes built-in Gin support for one-dimensional arrays\n(eg. integer[], text[]).  The following operations are available:\n\n  * contains: value_array @> query_array\n  * overlaps: value_array && query_array\n  * is contained by: value_array <@ query_array\n\nSynopsis\n--------\n\n=# create index txt_idx on aa using gin(a);\n\nFeatures\n--------\n\n  * Concurrency\n  * Write-Ahead Logging (WAL).  (Recoverability from crashes.)\n  * User-defined opclasses.  (The scheme is similar to GiST.)\n  * Optimized index creation (Makes use of maintenance_work_mem to accumulate\n    postings in memory.)\n  * Text search support via an opclass\n  * Soft upper limit on the returned results set using a GUC variable:\n    gin_fuzzy_search_limit\n\nGin Fuzzy Limit\n---------------\n\nThere are often situations when a full-text search returns a very large set of\nresults.  Since reading tuples from the disk and sorting them could take a\nlot of time, this is unacceptable for production.  (Note that the search\nitself is very fast.)\n\nSuch queries usually contain very frequent lexemes, so the results are not\nvery helpful. To facilitate execution of such queries Gin has a configurable\nsoft upper limit on the size of the returned set, determined by the\n'gin_fuzzy_search_limit' GUC variable.  This is set to 0 by default (no\nlimit).\n\nIf a non-zero search limit is set, then the returned set is a subset of the\nwhole result set, chosen at random.\n\n\"Soft\" means that the actual number of returned results could differ\nfrom the specified limit, depending on the query and the quality of the\nsystem's random number generator.\n\nFrom experience, a value of 'gin_fuzzy_search_limit' in the thousands\n(eg. 5000-20000) works well.  This means that 'gin_fuzzy_search_limit' will\nhave no effect for queries returning a result set with less tuples than this\nnumber.\n\nIndex structure\n---------------\n\nThe \"items\" that a GIN index indexes are composite values that contain\nzero or more \"keys\".  For example, an item might be an integer array, and\nthen the keys would be the individual integer values.  The index actually\nstores and searches for the key values, not the items per se.  In the\npg_opclass entry for a GIN opclass, the opcintype is the data type of the\nitems, and the opckeytype is the data type of the keys.  GIN is optimized\nfor cases where items contain many keys and the same key values appear\nin many different items.\n\nA GIN index contains a metapage, a btree of key entries, and possibly\n\"posting tree\" pages, which hold the overflow when a key entry acquires\ntoo many heap tuple pointers to fit in a btree page.  Additionally, if the\nfast-update feature is enabled, there can be \"list pages\" holding \"pending\"\nkey entries that haven't yet been merged into the main btree.  The list\npages have to be scanned linearly when doing a search, so the pending\nentries should be merged into the main btree before there get to be too\nmany of them.  The advantage of the pending list is that bulk insertion of\na few thousand entries can be much faster than retail insertion.  (The win\ncomes mainly from not having to do multiple searches/insertions when the\nsame key appears in multiple new heap tuples.)\n\nKey entries are nominally of the same IndexTuple format as used in other\nindex types, but since a leaf key entry typically refers to multiple heap\ntuples, there are significant differences.  (See GinFormTuple, which works\nby building a \"normal\" index tuple and then modifying it.)  The points to\nknow are:\n\n* In a single-column index, a key tuple just contains the key datum, but\nin a multi-column index, a key tuple contains the pair (column number,\nkey datum) where the column number is stored as an int2.  This is needed\nto support different key data types in different columns.  This much of\nthe tuple is built by index_form_tuple according to the usual rules.\nThe column number (if present) can never be null, but the key datum can\nbe, in which case a null bitmap is present as usual.  (As usual for index\ntuples, the size of the null bitmap is fixed at INDEX_MAX_KEYS.)\n\n* If the key datum is null (ie, IndexTupleHasNulls() is true), then\njust after the nominal index data (ie, at offset IndexInfoFindDataOffset\nor IndexInfoFindDataOffset + sizeof(int2)) there is a byte indicating\nthe \"category\" of the null entry.  These are the possible categories:\n\t1 = ordinary null key value extracted from an indexable item\n\t2 = placeholder for zero-key indexable item\n\t3 = placeholder for null indexable item\nPlaceholder null entries are inserted into the index because otherwise\nthere would be no index entry at all for an empty or null indexable item,\nwhich would mean that full index scans couldn't be done and various corner\ncases would give wrong answers.  The different categories of null entries\nare treated as distinct keys by the btree, but heap itempointers for the\nsame category of null entry are merged into one index entry just as happens\nwith ordinary key entries.\n\n* In a key entry at the btree leaf level, at the next SHORTALIGN boundary,\nthere is a list of item pointers, in compressed format (see Posting List\nCompression section), pointing to the heap tuples for which the indexable\nitems contain this key. This is called the \"posting list\".\n\nIf the list would be too big for the index tuple to fit on an index page, the\nItemPointers are pushed out to a separate posting page or pages, and none\nappear in the key entry itself.  The separate pages are called a \"posting\ntree\" (see below); Note that in either case, the ItemPointers associated with\na key can easily be read out in sorted order; this is relied on by the scan\nalgorithms.\n\n* The index tuple header fields of a leaf key entry are abused as follows:\n\n1) Posting list case:\n\n* ItemPointerGetBlockNumber(&itup->t_tid) contains the offset from index\n  tuple start to the posting list.\n  Access macros: GinGetPostingOffset(itup) / GinSetPostingOffset(itup,n)\n\n* ItemPointerGetOffsetNumber(&itup->t_tid) contains the number of elements\n  in the posting list (number of heap itempointers).\n  Access macros: GinGetNPosting(itup) / GinSetNPosting(itup,n)\n\n* If IndexTupleHasNulls(itup) is true, the null category byte can be\n  accessed/set with GinGetNullCategory(itup,gs) / GinSetNullCategory(itup,gs,c)\n\n* The posting list can be accessed with GinGetPosting(itup)\n\n* If GinItupIsCompressed(itup), the posting list is stored in compressed\n  format. Otherwise it is just an array of ItemPointers. New tuples are always\n  stored in compressed format, uncompressed items can be present if the\n  database was migrated from 9.3 or earlier version.\n\n2) Posting tree case:\n\n* ItemPointerGetBlockNumber(&itup->t_tid) contains the index block number\n  of the root of the posting tree.\n  Access macros: GinGetPostingTree(itup) / GinSetPostingTree(itup, blkno)\n\n* ItemPointerGetOffsetNumber(&itup->t_tid) contains the magic number\n  GIN_TREE_POSTING, which distinguishes this from the posting-list case\n  (it's large enough that that many heap itempointers couldn't possibly\n  fit on an index page).  This value is inserted automatically by the\n  GinSetPostingTree macro.\n\n* If IndexTupleHasNulls(itup) is true, the null category byte can be\n  accessed/set with GinGetNullCategory(itup,gs) / GinSetNullCategory(itup,gs,c)\n\n* The posting list is not present and must not be accessed.\n\nUse the macro GinIsPostingTree(itup) to determine which case applies.\n\nIn both cases, itup->t_info & INDEX_SIZE_MASK contains actual total size of\ntuple, and the INDEX_VAR_MASK and INDEX_NULL_MASK bits have their normal\nmeanings as set by index_form_tuple.\n\nIndex tuples in non-leaf levels of the btree contain the optional column\nnumber, key datum, and null category byte as above.  They do not contain\na posting list.  ItemPointerGetBlockNumber(&itup->t_tid) is the downlink\nto the next lower btree level, and ItemPointerGetOffsetNumber(&itup->t_tid)\nis InvalidOffsetNumber.  Use the access macros GinGetDownlink/GinSetDownlink\nto get/set the downlink.\n\nIndex entries that appear in \"pending list\" pages work a tad differently as\nwell.  The optional column number, key datum, and null category byte are as\nfor other GIN index entries.  However, there is always exactly one heap\nitempointer associated with a pending entry, and it is stored in the t_tid\nheader field just as in non-GIN indexes.  There is no posting list.\nFurthermore, the code that searches the pending list assumes that all\nentries for a given heap tuple appear consecutively in the pending list and\nare sorted by the column-number-plus-key-datum.  The GIN_LIST_FULLROW page\nflag bit tells whether entries for a given heap tuple are spread across\nmultiple pending-list pages.  If GIN_LIST_FULLROW is set, the page contains\nall the entries for one or more heap tuples.  If GIN_LIST_FULLROW is clear,\nthe page contains entries for only one heap tuple, *and* they are not all\nthe entries for that tuple.  (Thus, a heap tuple whose entries do not all\nfit on one pending-list page must have those pages to itself, even if this\nresults in wasting much of the space on the preceding page and the last\npage for the tuple.)\n\nGIN packs downlinks and pivot keys into internal page tuples in a different way\nthan nbtree does.  Lehman & Yao defines it as following.\n\nP_0, K_1, P_1, K_2, P_2, ... , K_n, P_n, K_{n+1}\n\nThere P_i is a downlink and K_i is a key.  K_i splits key space between P_{i-1}\nand P_i (0 <= i <= n).  K_{n+1} is high key.\n\nIn internal page tuple is key and downlink grouped together.  nbtree packs\nkeys and downlinks into tuples as following.\n\n(K_{n+1}, None), (-Inf, P_0), (K_1, P_1), ... , (K_n, P_n)\n\nThere tuples are shown in parentheses.  So, highkey is stored separately.  P_i\nis grouped with K_i.  P_0 is grouped with -Inf key.\n\nGIN packs keys and downlinks into tuples in a different way.\n\n(P_0, K_1), (P_1, K_2), ... , (P_n, K_{n+1})\n\nP_i is grouped with K_{i+1}.  -Inf key is not needed.\n\nThere are couple of additional notes regarding K_{n+1} key.\n1) In entry tree rightmost page, a key coupled with P_n doesn't really matter.\nHighkey is assumed to be infinity.\n2) In posting tree, a key coupled with P_n always doesn't matter.  Highkey for\nnon-rightmost pages is stored separately and accessed via\nGinDataPageGetRightBound().\n\nPosting tree\n------------\n\nIf a posting list is too large to store in-line in a key entry, a posting tree\nis created. A posting tree is a B-tree structure, where the ItemPointer is\nused as the key.\n\nInternal posting tree pages use the standard PageHeader and the same \"opaque\"\nstruct as other GIN page, but do not contain regular index tuples. Instead,\nthe contents of the page is an array of PostingItem structs. Each PostingItem\nconsists of the block number of the child page, and the right bound of that\nchild page, as an ItemPointer. The right bound of the page is stored right\nafter the page header, before the PostingItem array.\n\nPosting tree leaf pages also use the standard PageHeader and opaque struct,\nand the right bound of the page is stored right after the page header, but\nthe page content comprises of a number of compressed posting lists. The\ncompressed posting lists are stored one after each other, between page header\nand pd_lower. The space between pd_lower and pd_upper is unused, which allows\nfull-page images of posting tree leaf pages to skip the unused space in middle\n(buffer_std = true in XLogRecData).\n\nThe item pointers are stored in a number of independent compressed posting\nlists (also called segments), instead of one big one, to make random access\nto a given item pointer faster: to find an item in a compressed list, you\nhave to read the list from the beginning, but when the items are split into\nmultiple lists, you can first skip over to the list containing the item you're\nlooking for, and read only that segment. Also, an update only needs to\nre-encode the affected segment.\n\nPosting List Compression\n------------------------\n\nTo fit as many item pointers on a page as possible, posting tree leaf pages\nand posting lists stored inline in entry tree leaf tuples use a lightweight\nform of compression. We take advantage of the fact that the item pointers\nare stored in sorted order. Instead of storing the block and offset number of\neach item pointer separately, we store the difference from the previous item.\nThat in itself doesn't do much, but it allows us to use so-called varbyte\nencoding to compress them.\n\nVarbyte encoding is a method to encode integers, allowing smaller numbers to\ntake less space at the cost of larger numbers. Each integer is represented by\nvariable number of bytes. High bit of each byte in varbyte encoding determines\nwhether the next byte is still part of this number. Therefore, to read a single\nvarbyte encoded number, you have to read bytes until you find a byte with the\nhigh bit not set.\n\nWhen encoding, the block and offset number forming the item pointer are\ncombined into a single integer. The offset number is stored in the 11 low\nbits (see MaxHeapTuplesPerPageBits in ginpostinglist.c), and the block number\nis stored in the higher bits. That requires 43 bits in total, which\nconveniently fits in at most 6 bytes.\n\nA compressed posting list is passed around and stored on disk in a\nGinPostingList struct. The first item in the list is stored uncompressed\nas a regular ItemPointerData, followed by the length of the list in bytes,\nfollowed by the packed items.\n\nConcurrency\n-----------\n\nThe entry tree and each posting tree are B-trees, with right-links connecting\nsibling pages at the same level.  This is the same structure that is used in\nthe regular B-tree indexam (invented by Lehman & Yao), but we don't support\nscanning a GIN trees backwards, so we don't need left-links.  The entry tree\nleaves don't have dedicated high keys, instead greatest leaf tuple serves as\nhigh key.  That works because tuples are never deleted from the entry tree.\n\nThe algorithms used to operate entry and posting trees are considered below.\n\n### Locating the leaf page\n\nWhen we search for leaf page in GIN btree to perform a read, we descend from\nthe root page to the leaf through using downlinks taking pin and shared lock on\none page at once.  So, we release pin and shared lock on previous page before\ngetting them on the next page.\n\nThe picture below shows tree state after finding the leaf page.  Lower case\nletters depicts tree pages.  'S' depicts shared lock on the page.\n\n               a\n           /   |   \\\n       b       c       d\n     / | \\     | \\     | \\\n   eS  f   g   h   i   j   k\n\n### Steping right\n\nConcurrent page splits move the keyspace to right, so after following a\ndownlink, the page actually containing the key we're looking for might be\nsomewhere to the right of the page we landed on.  In that case, we follow the\nright-links until we find the page we're looking for.\n\nDuring stepping right we take pin and shared lock on the right sibling before\nreleasing them from the current page.  This mechanism was designed to protect\nfrom stepping to delete page.  We step to the right sibling while hold lock on\nthe rightlink pointing there.  So, it's guaranteed that nobody updates rightlink\nconcurrently and doesn't delete right sibling accordingly.\n\nThe picture below shows two pages locked at once during stepping right.\n\n               a\n           /   |   \\\n       b       c       d\n     / | \\     | \\     | \\\n   eS  fS  g   h   i   j   k\n\n### Insert\n\nWhile finding appropriate leaf for insertion we also descend from the root to\nleaf, while shared locking one page at once in.  But during insertion we don't\nrelease pins from root and internal pages.  That could save us some lookups to\nthe buffers hash table for downlinks insertion assuming parents are not changed\ndue to concurrent splits.  Once we reach leaf we re-lock the page in exclusive\nmode.\n\nThe picture below shows leaf page locked in exclusive mode and ready for\ninsertion.  'P' and 'E' depict pin and exclusive lock correspondingly.\n\n\n               aP\n           /   |   \\\n       b       cP      d\n     / | \\     | \\     | \\\n   e   f   g   hE  i   j   k\n\n\nIf insert causes a page split, the parent is locked in exclusive mode before\nunlocking the left child.  So, insertion algorithm can exclusively lock both\nparent and child pages at once starting from child.\n\nThe picture below shows tree state after leaf page split.  'q' is new page\nproduced by split.  Parent 'c' is about to have downlink inserted.\n\n                  aP\n            /     |   \\\n       b          cE      d\n     / | \\      / | \\     | \\\n   e   f   g  hE  q   i   j   k\n\n\n### Page deletion\n\nVacuum never deletes tuples or pages from the entry tree. It traverses entry\ntree leafs in logical order by rightlinks and removes deletable TIDs from\nposting lists. Posting trees are processed by links from entry tree leafs. They\nare vacuumed in two stages. At first stage, deletable TIDs are removed from\nleafs. If first stage detects at least one empty page, then at the second stage\nginScanToDelete() deletes empty pages.\n\nginScanToDelete() traverses the whole tree in depth-first manner.  It starts\nfrom the full cleanup lock on the tree root.  This lock prevents all the\nconcurrent insertions into this tree while we're deleting pages.  However,\nthere are still might be some in-progress readers, who traversed root before\nwe locked it.\n\nThe picture below shows tree state after page deletion algorithm traversed to\nleftmost leaf of the tree.\n\n               aE\n           /   |   \\\n       bE      c       d\n     / | \\     | \\     | \\\n   eE  f   g   h   i   j   k\n\nDeletion algorithm keeps exclusive locks on left siblings of pages comprising\ncurrently investigated path.  Thus, if current page is to be removed, all\nrequired pages to remove both downlink and rightlink are already locked.  That\navoids potential right to left page locking order, which could deadlock with\nconcurrent stepping right.\n\nA search concurrent to page deletion might already have read a pointer to the\npage to be deleted, and might be just about to follow it.  A page can be reached\nvia the right-link of its left sibling, or via its downlink in the parent.\n\nTo prevent a backend from reaching a deleted page via a right-link, stepping\nright algorithm doesn't release lock on the current page until lock of the\nright page is acquired.\n\nThe downlink is more tricky.  A search descending the tree must release the lock\non the parent page before locking the child, or it could deadlock with a\nconcurrent split of the child page; a page split locks the parent, while already\nholding a lock on the child page.  So, deleted page cannot be reclaimed\nimmediately.  Instead, we have to wait for every transaction, which might wait\nto reference this page, to finish.  Corresponding processes must observe that\nthe page is marked deleted and recover accordingly.\n\nThe picture below shows tree state after page deletion algorithm further\ntraversed the tree.  Currently investigated path is 'a-c-h'.  Left siblings 'b'\nand 'g' of 'c' and 'h' correspondingly are also exclusively locked.\n\n               aE\n           /   |   \\\n       bE      cE      d\n     / | \\     | \\     | \\\n   e   f   gE  hE  i   j   k\n\nThe next picture shows tree state after page 'h' was deleted.  It's marked with\n'deleted' flag and newest xid, which might visit it.  Downlink from 'c' to 'h'\nis also deleted.\n\n               aE\n           /   |   \\\n       bE      cE      d\n     / | \\       \\     | \\\n   e   f   gE  hD  iE  j   k\n\nHowever, it's still possible that concurrent reader has seen downlink from 'c'\nto 'h' before we deleted it.  In that case this reader will step right from 'h'\nto till find non-deleted page.  Xid-marking of page 'h' guarantees that this\npage wouldn't be reused till all such readers gone.  Next leaf page under\ninvestigation is 'i'.  'g' remains locked as it becomes left sibling of 'i'.\n\nThe next picture shows tree state after 'i' and 'c' was deleted.  Internal page\n'c' was deleted because it appeared to have no downlinks.  The path under\ninvestigation is 'a-d-j'.  Pages 'b' and 'g' are locked as self siblings of 'd'\nand 'j'.\n\n               aE\n           /       \\\n       bE      cD      dE\n     / | \\             | \\\n   e   f   gE  hD  iD  jE  k\n\nDuring the replay of page deletion at standby, the page's left sibling, the\ntarget page, and its parent, are locked in that order.  This order guarantees\nno deadlock with concurrent reads.\n\nPredicate Locking\n-----------------\n\nGIN supports predicate locking, for serializable snapshot isolation.\nA predicate locks represent that a scan has scanned a range of values.  They\nare not concerned with physical pages as such, but the logical key values.\nA predicate lock on a page covers the key range that would belong on that\npage, whether or not there are any matching tuples there currently.  In other\nwords, a predicate lock on an index page covers the \"gaps\" between the index\ntuples.  To minimize false positives, predicate locks are acquired at the\nfinest level possible.\n\n* Like in the B-tree index, it is enough to lock only leaf pages, because all\n  insertions happen at the leaf level.\n\n* In an equality search (i.e. not a partial match search), if a key entry has\n  a posting tree, we lock the posting tree root page, to represent a lock on\n  just that key entry.  Otherwise, we lock the entry tree page.  We also lock\n  the entry tree page if no match is found, to lock the \"gap\" where the entry\n  would've been, had there been one.\n\n* In a partial match search, we lock all the entry leaf pages that we scan,\n  in addition to locks on posting tree roots, to represent the \"gaps\" between\n  values.\n\n* In addition to the locks on entry leaf pages and posting tree roots, all\n  scans grab a lock the metapage.  This is to interlock with insertions to\n  the fast update pending list.  An insertion to the pending list can really\n  belong anywhere in the tree, and the lock on the metapage represents that.\n\nThe interlock for fastupdate pending lists means that with fastupdate=on,\nwe effectively always grab a full-index lock, so you could get a lot of false\npositives.\n\nCompatibility\n-------------\n\nCompression of TIDs was introduced in 9.4. Some GIN indexes could remain in\nuncompressed format because of pg_upgrade from 9.3 or earlier versions.\nFor compatibility, old uncompressed format is also supported. Following\nrules are used to handle it:\n\n* GIN_ITUP_COMPRESSED flag marks index tuples that contain a posting list.\nThis flag is stored in high bit of ItemPointerGetBlockNumber(&itup->t_tid).\nUse GinItupIsCompressed(itup) to check the flag.\n\n* Posting tree pages in the new format are marked with the GIN_COMPRESSED flag.\n  Macros GinPageIsCompressed(page) and GinPageSetCompressed(page) are used to\n  check and set this flag.\n\n* All scan operations check format of posting list add use corresponding code\nto read its content.\n\n* When updating an index tuple containing an uncompressed posting list, it\nwill be replaced with new index tuple containing a compressed list.\n\n* When updating an uncompressed posting tree leaf page, it's compressed.\n\n* If vacuum finds some dead TIDs in uncompressed posting lists, they are\nconverted into compressed posting lists. This assumes that the compressed\nposting list fits in the space occupied by the uncompressed list. IOW, we\nassume that the compressed version of the page, with the dead items removed,\ntakes less space than the old uncompressed version.\n\nLimitations\n-----------\n\n  * Gin doesn't use scan->kill_prior_tuple & scan->ignore_killed_tuples\n  * Gin searches entries only by equality matching, or simple range\n    matching using the \"partial match\" feature.\n\nTODO\n----\n\nNearest future:\n\n  * Opclasses for more types (no programming, just many catalog changes)\n\nDistant future:\n\n  * Replace B-tree of entries to something like GiST\n\nAuthors\n-------\n\nOriginal work was done by Teodor Sigaev (teodor@sigaev.ru) and Oleg Bartunov\n(oleg@sai.msu.su).",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\gin\\README",
      "directory": "backend\\access\\gin"
    }
  },
  {
    "title": "README: backend\\access\\gist",
    "url": "backend\\access\\gist\\README",
    "content": "src/backend/access/gist/README\n\nGiST Indexing\n=============\n\nThis directory contains an implementation of GiST indexing for Postgres.\n\nGiST stands for Generalized Search Tree. It was introduced in the seminal paper\n\"Generalized Search Trees for Database Systems\", 1995, Joseph M. Hellerstein,\nJeffrey F. Naughton, Avi Pfeffer:\n\n    http://www.sai.msu.su/~megera/postgres/gist/papers/gist.ps\n    https://dsf.berkeley.edu/papers/sigmod97-gist.pdf\n\nand implemented by J. Hellerstein and P. Aoki in an early version of\nPostgreSQL (more details are available from The GiST Indexing Project\nat Berkeley at http://gist.cs.berkeley.edu/). As a \"university\"\nproject it had a limited number of features and was in rare use.\n\nThe current implementation of GiST supports:\n\n  * Variable length keys\n  * Composite keys (multi-key)\n  * Ordered search (nearest-neighbor search)\n  * provides NULL-safe interface to GiST core\n  * Concurrency\n  * Recovery support via WAL logging\n  * Buffering build algorithm\n  * Sorted build method\n\nThe support for concurrency implemented in PostgreSQL was developed based on\nthe paper \"Access Methods for Next-Generation Database Systems\" by\nMarcel Kornacker:\n\n    http://www.sai.msu.su/~megera/postgres/gist/papers/concurrency/access-methods-for-next-generation.pdf.gz\n\nBuffering build algorithm for GiST was developed based on the paper \"Efficient\nBulk Operations on Dynamic R-trees\" by Lars Arge, Klaus Hinrichs, Jan Vahrenhold\nand Jeffrey Scott Vitter.\n\n    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.135.9894&rep=rep1&type=pdf\n\nThe original algorithms were modified in several ways:\n\n* They had to be adapted to PostgreSQL conventions. For example, the SEARCH\n  algorithm was considerably changed, because in PostgreSQL the search function\n  should return one tuple (next), not all tuples at once. Also, it should\n  release page locks between calls.\n* Since we added support for variable length keys, it's not possible to\n  guarantee enough free space for all keys on pages after splitting. User\n  defined function picksplit doesn't have information about size of tuples\n  (each tuple may contain several keys as in multicolumn index while picksplit\n  could work with only one key) and pages.\n* We modified original INSERT algorithm for performance reasons. In particular,\n  it is now a single-pass algorithm.\n* Since the papers were theoretical, some details were omitted and we\n  had to find out ourself how to solve some specific problems.\n\nBecause of the above reasons, we have revised the interaction of GiST\ncore and PostgreSQL WAL system. Moreover, we encountered (and solved)\na problem of uncompleted insertions when recovering after crash, which\nwas not touched in the paper.\n\nSearch Algorithm\n----------------\n\nThe search code maintains a queue of unvisited items, where an \"item\" is\neither a heap tuple known to satisfy the search conditions, or an index\npage that is consistent with the search conditions according to inspection\nof its parent page's downlink item.  Initially the root page is searched\nto find unvisited items in it.  Then we pull items from the queue.  A\nheap tuple pointer is just returned immediately; an index page entry\ncauses that page to be searched, generating more queue entries.\n\nThe queue is kept ordered with heap tuple items at the front, then\nindex page entries, with any newly-added index page entry inserted\nbefore existing index page entries.  This ensures depth-first traversal\nof the index, and in particular causes the first few heap tuples to be\nreturned as soon as possible.  That is helpful in case there is a LIMIT\nthat requires only a few tuples to be produced.\n\nTo implement nearest-neighbor search, the queue entries are augmented\nwith distance data: heap tuple entries are labeled with exact distance\nfrom the search argument, while index-page entries must be labeled with\nthe minimum distance that any of their children could have.  Then,\nqueue entries are retrieved in smallest-distance-first order, with\nentries having identical distances managed as stated in the previous\nparagraph.\n\nThe search algorithm keeps an index page locked only long enough to scan\nits entries and queue those that satisfy the search conditions.  Since\ninsertions can occur concurrently with searches, it is possible for an\nindex child page to be split between the time we make a queue entry for it\n(while visiting its parent page) and the time we actually reach and scan\nthe child page.  To avoid missing the entries that were moved to the right\nsibling, we detect whether a split has occurred by comparing the child\npage's NSN (node sequence number, a special-purpose LSN) to the LSN that\nthe parent had when visited.  If it did, the sibling page is immediately\nadded to the front of the queue, ensuring that its items will be scanned\nin the same order as if they were still on the original child page.\n\nAs is usual in Postgres, the search algorithm only guarantees to find index\nentries that existed before the scan started; index entries added during\nthe scan might or might not be visited.  This is okay as long as all\nsearches use MVCC snapshot rules to reject heap tuples newer than the time\nof scan start.  In particular, this means that we need not worry about\ncases where a parent page's downlink key is \"enlarged\" after we look at it.\nAny such enlargement would be to add child items that we aren't interested\nin returning anyway.\n\n\nInsert Algorithm\n----------------\n\nINSERT guarantees that the GiST tree remains balanced. User defined key method\nPenalty is used for choosing a subtree to insert; method PickSplit is used for\nthe node splitting algorithm; method Union is used for propagating changes\nupward to maintain the tree properties.\n\nTo insert a tuple, we first have to find a suitable leaf page to insert to.\nThe algorithm walks down the tree, starting from the root, along the path\nof smallest Penalty. At each step:\n\n1. Has this page been split since we looked at the parent? If so, it's\npossible that we should be inserting to the other half instead, so retreat\nback to the parent.\n2. If this is a leaf node, we've found our target node.\n3. Otherwise use Penalty to pick a new target subtree.\n4. Check the key representing the target subtree. If it doesn't already cover\nthe key we're inserting, replace it with the Union of the old downlink key\nand the key being inserted. (Actually, we always call Union, and just skip\nthe replacement if the Unioned key is the same as the existing key)\n5. Replacing the key in step 4 might cause the page to be split. In that case,\npropagate the change upwards and restart the algorithm from the first parent\nthat didn't need to be split.\n6. Walk down to the target subtree, and goto 1.\n\nThis differs from the insertion algorithm in the original paper. In the\noriginal paper, you first walk down the tree until you reach a leaf page, and\nthen you adjust the downlink in the parent, and propagate the adjustment up,\nall the way up to the root in the worst case. But we adjust the downlinks to\ncover the new key already when we walk down, so that when we reach the leaf\npage, we don't need to update the parents anymore, except to insert the\ndownlinks if we have to split the page. This makes crash recovery simpler:\nafter inserting a key to the page, the tree is immediately self-consistent\nwithout having to update the parents. Even if we split a page and crash before\ninserting the downlink to the parent, the tree is self-consistent because the\nright half of the split is accessible via the rightlink of the left page\n(which replaced the original page).\n\nNote that the algorithm can walk up and down the tree before reaching a leaf\npage, if internal pages need to split while adjusting the downlinks for the\nnew key. Eventually, you should reach the bottom, and proceed with the\ninsertion of the new tuple.\n\nOnce we've found the target page to insert to, we check if there's room\nfor the new tuple. If there is, the tuple is inserted, and we're done.\nIf it doesn't fit, however, the page needs to be split. Note that it is\npossible that a page needs to be split into more than two pages, if keys have\ndifferent lengths or more than one key is being inserted at a time (which can\nhappen when inserting downlinks for a page split that resulted in more than\ntwo pages at the lower level). After splitting a page, the parent page needs\nto be updated. The downlink for the new page needs to be inserted, and the\ndownlink for the old page, which became the left half of the split, needs to\nbe updated to only cover those tuples that stayed on the left page. Inserting\nthe downlink in the parent can again lead to a page split, recursing up to the\nroot page in the worst case.\n\ngistplacetopage is the workhorse function that performs one step of the\ninsertion. If the tuple fits, it inserts it to the given page, otherwise\nit splits the page, and constructs the new downlink tuples for the split\npages. The caller must then call gistplacetopage() on the parent page to\ninsert the downlink tuples. The parent page that holds the downlink to\nthe child might have migrated as a result of concurrent splits of the\nparent, gistFindCorrectParent() is used to find the parent page.\n\nSplitting the root page works slightly differently. At root split,\ngistplacetopage() allocates the new child pages and replaces the old root\npage with the new root containing downlinks to the new children, all in one\noperation.\n\n\nfindPath is a subroutine of findParent, used when the correct parent page\ncan't be found by following the rightlinks at the parent level:\n\nfindPath( stack item )\n\tpush stack, [root, 0, 0] // page, LSN, parent\n\twhile( stack )\n\t\tptr = top of stack\n\t\tlatch( ptr->page, S-mode )\n\t\tif ( ptr->parent->page->lsn < ptr->page->nsn )\n\t\t\tpush stack, [ ptr->page->rightlink, 0, ptr->parent ]\n\t\tend\n\t\tfor( each tuple on page )\n\t\t\tif ( tuple->pagepointer == item->page )\n\t\t\t\treturn stack\n\t\t\telse\n\t\t\t\tadd to stack at the end [tuple->pagepointer,0, ptr]\n\t\t\tend\n\t\tend\n\t\tunlatch( ptr->page )\n\t\tpop stack\n\tend\n\n\ngistFindCorrectParent is used to re-find the parent of a page during\ninsertion. It might have migrated to the right since we traversed down the\ntree because of page splits.\n\nfindParent( stack item )\n\tparent = item->parent\n\tif ( parent->page->lsn != parent->lsn )\n\t\twhile(true)\n\t\t\tsearch parent tuple on parent->page, if found the return\n\t\t\trightlink = parent->page->rightlink\n\t\t\tunlatch( parent->page )\n\t\t\tif ( rightlink is incorrect )\n\t\t\t\tbreak loop\n\t\t\tend\n\t\t\tparent->page = rightlink\n\t\t\tlatch( parent->page, X-mode )\n\t\tend\n\t\tnewstack = findPath( item->parent )\n\t\treplace part of stack to new one\n\t\tlatch( parent->page, X-mode )\n\t\treturn findParent( item )\n\tend\n\npageSplit function decides how to distribute keys to the new pages after\npage split:\n\npageSplit(page, allkeys)\n\t(lkeys, rkeys) = pickSplit( allkeys )\n\tif ( page is root )\n\t\tlpage = new page\n\telse\n\t\tlpage = page\n\trpage = new page\n\tif ( no space left on rpage )\n\t\tnewkeys = pageSplit( rpage, rkeys )\n\telse\n\t\tpush newkeys, union(rkeys)\n\tend\n\tif ( no space left on lpage )\n\t\tpush newkeys, pageSplit( lpage, lkeys )\n\telse\n\t\tpush newkeys, union(lkeys)\n\tend\n\treturn newkeys\n\n\n\nConcurrency control\n-------------------\nAs a rule of thumb, if you need to hold a lock on multiple pages at the\nsame time, the locks should be acquired in the following order: child page\nbefore parent, and left-to-right at the same level. Always acquiring the\nlocks in the same order avoids deadlocks.\n\nThe search algorithm only looks at and locks one page at a time. Consequently\nthere's a race condition between a search and a page split. A page split\nhappens in two phases: 1. The page is split 2. The downlink is inserted to the\nparent. If a search looks at the parent page between those steps, before the\ndownlink is inserted, it will still find the new right half by following the\nrightlink on the left half. But it must not follow the rightlink if it saw the\ndownlink in the parent, or the page will be visited twice!\n\nA split initially marks the left page with the F_FOLLOW_RIGHT flag. If a scan\nsees that flag set, it knows that the right page is missing the downlink, and\nshould be visited too. When split inserts the downlink to the parent, it\nclears the F_FOLLOW_RIGHT flag in the child, and sets the NSN field in the\nchild page header to match the LSN of the insertion on the parent. If the\nF_FOLLOW_RIGHT flag is not set, a scan compares the NSN on the child and the\nLSN it saw in the parent. If the child's NSN is greater than the LSN seen on\nthe parent, the scan looked at the parent page before the downlink was\ninserted, so it should follow the rightlink. Otherwise the scan saw the\ndownlink in the parent page, and will/did follow that as usual.\n\nA scan can't normally see a page with the F_FOLLOW_RIGHT flag set, because\na page split keeps the child pages locked until the downlink has been inserted\nto the parent and the flag cleared again. But if a crash happens in the middle\nof a page split, before the downlinks are inserted into the parent, that will\nleave a page with F_FOLLOW_RIGHT in the tree. Scans handle that just fine,\nbut we'll eventually want to fix that for performance reasons. And more\nimportantly, dealing with pages with missing downlink pointers in the parent\nwould complicate the insertion algorithm. So when an insertion sees a page\nwith F_FOLLOW_RIGHT set, it immediately tries to bring the split that\ncrashed in the middle to completion by adding the downlink in the parent.\n\nBuffering build algorithm\n-------------------------\n\nIn the buffering index build algorithm, some or all internal nodes have a\nbuffer attached to them. When a tuple is inserted at the top, the descend down\nthe tree is stopped as soon as a buffer is reached, and the tuple is pushed to\nthe buffer. When a buffer gets too full, all the tuples in it are flushed to\nthe lower level, where they again hit lower level buffers or leaf pages. This\nmakes the insertions happen in more of a breadth-first than depth-first order,\nwhich greatly reduces the amount of random I/O required.\n\nIn the algorithm, levels are numbered so that leaf pages have level zero,\nand internal node levels count up from 1. This numbering ensures that a page's\nlevel number never changes, even when the root page is split.\n\nLevel                    Tree\n\n3                         *\n                      /       \\\n2                *                 *\n              /  |  \\           /  |  \\\n1          *     *     *     *     *     *\n          / \\   / \\   / \\   / \\   / \\   / \\\n0        o   o o   o o   o o   o o   o o   o\n\n* - internal page\no - leaf page\n\nInternal pages that belong to certain levels have buffers associated with\nthem. Leaf pages never have buffers. Which levels have buffers is controlled\nby \"level step\" parameter: level numbers that are multiples of level_step\nhave buffers, while others do not. For example, if level_step = 2, then\npages on levels 2, 4, 6, ... have buffers. If level_step = 1 then every\ninternal page has a buffer.\n\nLevel        Tree (level_step = 1)                Tree (level_step = 2)\n\n3                      *                                     *\n                   /       \\                             /       \\\n2             *(b)              *(b)                *(b)              *(b)\n           /  |  \\           /  |  \\             /  |  \\           /  |  \\\n1       *(b)  *(b)  *(b)  *(b)  *(b)  *(b)    *     *     *     *     *     *\n       / \\   / \\   / \\   / \\   / \\   / \\     / \\   / \\   / \\   / \\   / \\   / \\\n0     o   o o   o o   o o   o o   o o   o   o   o o   o o   o o   o o   o o   o\n\n(b) - buffer\n\nLogically, a buffer is just bunch of tuples. Physically, it is divided in\npages, backed by a temporary file. Each buffer can be in one of two states:\na) Last page of the buffer is kept in main memory. A node buffer is\nautomatically switched to this state when a new index tuple is added to it,\nor a tuple is removed from it.\nb) All pages of the buffer are swapped out to disk. When a buffer becomes too\nfull, and we start to flush it, all other buffers are switched to this state.\n\nWhen an index tuple is inserted, its initial processing can end in one of the\nfollowing points:\n1) Leaf page, if the depth of the index <= level_step, meaning that\n   none of the internal pages have buffers associated with them.\n2) Buffer of topmost level page that has buffers.\n\nNew index tuples are processed until one of the buffers in the topmost\nbuffered level becomes half-full. When a buffer becomes half-full, it's added\nto the emptying queue, and will be emptied before a new tuple is processed.\n\nBuffer emptying process means that index tuples from the buffer are moved\ninto buffers at a lower level, or leaf pages. First, all the other buffers are\nswapped to disk to free up the memory. Then tuples are popped from the buffer\none by one, and cascaded down the tree to the next buffer or leaf page below\nthe buffered node.\n\nEmptying a buffer has the interesting dynamic property that any intermediate\npages between the buffer being emptied, and the next buffered or leaf level\nbelow it, become cached. If there are no more buffers below the node, the leaf\npages where the tuples finally land on get cached too. If there are, the last\nbuffer page of each buffer below is kept in memory. This is illustrated in\nthe figures below:\n\n   Buffer being emptied to\n     lower-level buffers               Buffer being emptied to leaf pages\n\n               +(fb)                                 +(fb)\n            /     \\                                /     \\\n        +             +                        +             +\n      /   \\         /   \\                    /   \\         /   \\\n    *(ab)   *(ab) *(ab)   *(ab)            x       x     x       x\n\n+    - cached internal page\nx    - cached leaf page\n*    - non-cached internal page\n(fb) - buffer being emptied\n(ab) - buffers being appended to, with last page in memory\n\nIn the beginning of the index build, the level-step is chosen so that all those\npages involved in emptying one buffer fit in cache, so after each of those\npages have been accessed once and cached, emptying a buffer doesn't involve\nany more I/O. This locality is where the speedup of the buffering algorithm\ncomes from.\n\nEmptying one buffer can fill up one or more of the lower-level buffers,\ntriggering emptying of them as well. Whenever a buffer becomes too full, it's\nadded to the emptying queue, and will be emptied after the current buffer has\nbeen processed.\n\nTo keep the size of each buffer limited even in the worst case, buffer emptying\nis scheduled as soon as a buffer becomes half-full, and emptying it continues\nuntil 1/2 of the nominal buffer size worth of tuples has been emptied. This\nguarantees that when buffer emptying begins, all the lower-level buffers\nare at most half-full. In the worst case that all the tuples are cascaded down\nto the same lower-level buffer, that buffer therefore has enough space to\naccommodate all the tuples emptied from the upper-level buffer. There is no\nhard size limit in any of the data structures used, though, so this only needs\nto be approximate; small overfilling of some buffers doesn't matter.\n\nIf an internal page that has a buffer associated with it is split, the buffer\nneeds to be split too. All tuples in the buffer are scanned through and\nrelocated to the correct sibling buffers, using the penalty function to decide\nwhich buffer each tuple should go to.\n\nAfter all tuples from the heap have been processed, there are still some index\ntuples in the buffers. At this point, final buffer emptying starts. All buffers\nare emptied in top-down order. This is slightly complicated by the fact that\nnew buffers can be allocated during the emptying, due to page splits. However,\nthe new buffers will always be siblings of buffers that haven't been fully\nemptied yet; tuples never move upwards in the tree. The final emptying loops\nthrough buffers at a given level until all buffers at that level have been\nemptied, and then moves down to the next level.\n\nSorted build method\n-------------------\n\nSort all input tuples, pack them into GiST leaf pages in the sorted order,\nand create downlinks and internal pages as we go. This method builds the index\nfrom the bottom up, similar to how the B-tree index is built.\n\nThe sorted method is used if the operator classes for all columns have a\n\"sortsupport\" defined. Otherwise, we fall back on inserting tuples one by one\nwith optional buffering.\n\nSorting GiST build requires good linearization of the sort opclass. That is not\nalways the case in multidimensional data. To tackle the anomalies, we buffer\nindex tuples and apply a picksplit function that can be multidimensional-aware.\n\nBulk delete algorithm (VACUUM)\n------------------------------\n\nVACUUM works in two stages:\n\nIn the first stage, we scan the whole index in physical order. To make sure\nthat we don't miss any dead tuples because a concurrent page split moved them,\nwe check the F_FOLLOW_RIGHT flags and NSN on each page, to detect if the\npage has been concurrently split. If a concurrent page split is detected, and\none half of the page was moved to a position that we already scanned, we\n\"jump backwards\" to scan the page again. This is the same mechanism that\nB-tree VACUUM uses, but because we already have NSNs on pages, to detect page\nsplits during searches, we don't need a \"vacuum cycle ID\" concept for that\nlike B-tree does.\n\nWhile we scan all the pages, we also make note of any completely empty leaf\npages. We will try to unlink them from the tree after the scan. We also record\nthe block numbers of all internal pages; they are needed to locate parents of\nthe empty pages while unlinking them.\n\nWe try to unlink any empty leaf pages from the tree, so that their space can\nbe reused. In order to delete an empty page, its downlink must be removed from\nthe parent. We scan all the internal pages, whose block numbers we memorized\nin the first stage, and look for downlinks to pages that we have memorized as\nbeing empty. Whenever we find one, we acquire a lock on the parent and child\npage, re-check that the child page is still empty. Then, we remove the\ndownlink and mark the child as deleted, and release the locks.\n\nThe insertion algorithm would get confused, if an internal page was completely\nempty. So we never delete the last child of an internal page, even if it's\nempty. Currently, we only support deleting leaf pages.\n\nThis page deletion algorithm works on a best-effort basis. It might fail to\nfind a downlink, if a concurrent page split moved it after the first stage.\nIn that case, we won't be able to remove all empty pages. That's OK, it's\nnot expected to happen very often, and hopefully the next VACUUM will clean\nit up.\n\nWhen we have deleted a page, it's possible that an in-progress search will\nstill descend on the page, if it saw the downlink before we removed it. The\nsearch will see that it is deleted, and ignore it, but as long as that can\nhappen, we cannot reuse the page. To \"wait out\" any in-progress searches, when\na page is deleted, it's labeled with the current next-transaction counter\nvalue. The page is not recycled, until that XID is no longer visible to\nanyone. That's much more conservative than necessary, but let's keep it\nsimple.\n\n\nAuthors:\n\tTeodor Sigaev\t<teodor@sigaev.ru>\n\tOleg Bartunov\t<oleg@sai.msu.su>",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\gist\\README",
      "directory": "backend\\access\\gist"
    }
  },
  {
    "title": "README: backend\\access\\hash",
    "url": "backend\\access\\hash\\README",
    "content": "src/backend/access/hash/README\n\nHash Indexing\n=============\n\nThis directory contains an implementation of hash indexing for Postgres.\nMost of the core ideas are taken from Margo Seltzer and Ozan Yigit,\nA New Hashing Package for UNIX, Proceedings of the Winter USENIX Conference,\nJanuary 1991.  (Our in-memory hashtable implementation,\nsrc/backend/utils/hash/dynahash.c, also relies on some of the same concepts;\nit is derived from code written by Esmond Pitt and later improved by Margo\namong others.)\n\nA hash index consists of two or more \"buckets\", into which tuples are\nplaced whenever their hash key maps to the bucket number.  The\nkey-to-bucket-number mapping is chosen so that the index can be\nincrementally expanded.  When a new bucket is to be added to the index,\nexactly one existing bucket will need to be \"split\", with some of its\ntuples being transferred to the new bucket according to the updated\nkey-to-bucket-number mapping.  This is essentially the same hash table\nmanagement technique embodied in src/backend/utils/hash/dynahash.c for\nin-memory hash tables.\n\nEach bucket in the hash index comprises one or more index pages.  The\nbucket's first page is permanently assigned to it when the bucket is\ncreated.  Additional pages, called \"overflow pages\", are added if the\nbucket receives too many tuples to fit in the primary bucket page.\nThe pages of a bucket are chained together in a doubly-linked list\nusing fields in the index page special space.\n\nThere is currently no provision to shrink a hash index, other than by\nrebuilding it with REINDEX.  Overflow pages can be recycled for reuse\nin other buckets, but we never give them back to the operating system.\nThere is no provision for reducing the number of buckets, either.\n\nAs of PostgreSQL 8.4, hash index entries store only the hash code, not the\nactual data value, for each indexed item.  This makes the index entries\nsmaller (perhaps very substantially so) and speeds up various operations.\nIn particular, we can speed searches by keeping the index entries in any\none index page sorted by hash code, thus allowing binary search to be used\nwithin an index page.  Note however that there is *no* assumption about the\nrelative ordering of hash codes across different index pages of a bucket.\n\n\nPage Addressing\n---------------\n\nThere are four kinds of pages in a hash index: the meta page (page zero),\nwhich contains statically allocated control information; primary bucket\npages; overflow pages; and bitmap pages, which keep track of overflow\npages that have been freed and are available for re-use.  For addressing\npurposes, bitmap pages are regarded as a subset of the overflow pages.\n\nPrimary bucket pages and overflow pages are allocated independently (since\nany given index might need more or fewer overflow pages relative to its\nnumber of buckets).  The hash code uses an interesting set of addressing\nrules to support a variable number of overflow pages while not having to\nmove primary bucket pages around after they are created.\n\nPrimary bucket pages (henceforth just \"bucket pages\") are allocated in\npower-of-2 groups, called \"split points\" in the code.  That means at every new\nsplitpoint we double the existing number of buckets.  Allocating huge chunks\nof bucket pages all at once isn't optimal and we will take ages to consume\nthose.  To avoid this exponential growth of index size, we did use a trick to\nbreak up allocation of buckets at the splitpoint into 4 equal phases.  If\n(2 ^ x) are the total buckets need to be allocated at a splitpoint (from now on\nwe shall call this as a splitpoint group), then we allocate 1/4th (2 ^ (x - 2))\nof total buckets at each phase of splitpoint group.  Next quarter of allocation\nwill only happen if buckets of the previous phase have been already consumed.\nFor the initial splitpoint groups < 10 we will allocate all of their buckets in\nsingle phase only, as number of buckets allocated at initial groups are small\nin numbers.  And for the groups >= 10 the allocation process is distributed\namong four equal phases.  At group 10 we allocate (2 ^ 9) buckets in 4\ndifferent phases {2 ^ 7, 2 ^ 7, 2 ^ 7, 2 ^ 7}, the numbers in curly braces\nindicate the number of buckets allocated within each phase of splitpoint group\n10.  And, for splitpoint group 11 and 12 allocation phases will be\n{2 ^ 8, 2 ^ 8, 2 ^ 8, 2 ^ 8} and {2 ^ 9, 2 ^ 9, 2 ^ 9, 2 ^ 9} respectively.  We\ncan see that at each splitpoint group we double the total number of buckets\nfrom the previous group but in an incremental phase.  The bucket pages\nallocated within one phase of a splitpoint group will appear consecutively in\nthe index.  This addressing scheme allows the physical location of a bucket\npage to be computed from the bucket number relatively easily, using only a\nsmall amount of control information.  If we look at the function\n_hash_spareindex for a given bucket number we first compute the\nsplitpoint group it belongs to and then the phase to which the bucket belongs\nto.  Adding them we get the global splitpoint phase number S to which the\nbucket belongs and then simply add \"hashm_spares[S] + 1\" (where hashm_spares[]\nis an array stored in the metapage) with given bucket number to compute its\nphysical address.  The hashm_spares[S] can be interpreted as the total number\nof overflow pages that have been allocated before the bucket pages of\nsplitpoint phase S.  The hashm_spares[0] is always 0, so that buckets 0 and 1\nalways appear at block numbers 1 and 2, just after the meta page.  We always\nhave hashm_spares[N] <= hashm_spares[N+1], since the latter count includes the\nformer.  The difference between the two represents the number of overflow pages\nappearing between the bucket page groups of splitpoints phase N and N+1.\n(Note: the above describes what happens when filling an initially minimally\nsized hash index.  In practice, we try to estimate the required index size and\nallocate a suitable number of splitpoints phases immediately, to avoid\nexpensive re-splitting during initial index build.)\n\nWhen S splitpoints exist altogether, the array entries hashm_spares[0]\nthrough hashm_spares[S] are valid; hashm_spares[S] records the current\ntotal number of overflow pages.  New overflow pages are created as needed\nat the end of the index, and recorded by incrementing hashm_spares[S].\nWhen it is time to create a new splitpoint phase's worth of bucket pages, we\ncopy hashm_spares[S] into hashm_spares[S+1] and increment S (which is\nstored in the hashm_ovflpoint field of the meta page).  This has the\neffect of reserving the correct number of bucket pages at the end of the\nindex, and preparing to allocate additional overflow pages after those\nbucket pages.  hashm_spares[] entries before S cannot change anymore,\nsince that would require moving already-created bucket pages.\n\nThe last page nominally used by the index is always determinable from\nhashm_spares[S].  To avoid complaints from smgr, the logical EOF as seen by\nthe filesystem and smgr must always be greater than or equal to this page.\nWe have to allow the case \"greater than\" because it's possible that during\nan index extension we crash after allocating filesystem space and before\nupdating the metapage.  Note that on filesystems that allow \"holes\" in\nfiles, it's entirely likely that pages before the logical EOF are not yet\nallocated: when we allocate a new splitpoint phase's worth of bucket pages, we\nphysically zero the last such page to force the EOF up, and the first such\npage will be used immediately, but the intervening pages are not written\nuntil needed.\n\nSince overflow pages may be recycled if enough tuples are deleted from\ntheir bucket, we need a way to keep track of currently-free overflow\npages.  The state of each overflow page (0 = available, 1 = not available)\nis recorded in \"bitmap\" pages dedicated to this purpose.  The entries in\nthe bitmap are indexed by \"bit number\", a zero-based count in which every\noverflow page has a unique entry.  We can convert between an overflow\npage's physical block number and its bit number using the information in\nhashm_spares[] (see hashovfl.c for details).  The bit number sequence\nincludes the bitmap pages, which is the reason for saying that bitmap\npages are a subset of the overflow pages.  It turns out in fact that each\nbitmap page's first bit represents itself --- this is not an essential\nproperty, but falls out of the fact that we only allocate another bitmap\npage when we really need one.  Bit number zero always corresponds to the\nfirst bitmap page, which is allocated during index creation just after all\nthe initially created buckets.\n\n\nLock Definitions\n----------------\n\nConcurrency control for hash indexes is provided using buffer content\nlocks, buffer pins, and cleanup locks.   Here as elsewhere in PostgreSQL,\ncleanup lock means that we hold an exclusive lock on the buffer and have\nobserved at some point after acquiring the lock that we hold the only pin\non that buffer.  For hash indexes, a cleanup lock on a primary bucket page\nrepresents the right to perform an arbitrary reorganization of the entire\nbucket.  Therefore, scans retain a pin on the primary bucket page for the\nbucket they are currently scanning.  Splitting a bucket requires a cleanup\nlock on both the old and new primary bucket pages.  VACUUM therefore takes\na cleanup lock on every bucket page in order to remove tuples.  It can also\nremove tuples copied to a new bucket by any previous split operation, because\nthe cleanup lock taken on the primary bucket page guarantees that no scans\nwhich started prior to the most recent split can still be in progress.  After\ncleaning each page individually, it attempts to take a cleanup lock on the\nprimary bucket page in order to \"squeeze\" the bucket down to the minimum\npossible number of pages.\n\nTo avoid deadlocks, we must be consistent about the lock order in which we\nlock the buckets for operations that requires locks on two different buckets.\nWe choose to always lock the lower-numbered bucket first.  The metapage is\nonly ever locked after all bucket locks have been taken.\n\n\nMetapage Caching\n----------------\n\nBoth scanning the index and inserting tuples require locating the bucket\nwhere a given tuple ought to be located.  To do this, we need the bucket\ncount, highmask, and lowmask from the metapage; however, it's undesirable\nfor performance reasons to have to have to lock and pin the metapage for\nevery such operation.  Instead, we retain a cached copy of the metapage\nin each backend's relcache entry.  This will produce the correct\nbucket mapping as long as the target bucket hasn't been split since the\nlast cache refresh.\n\nTo guard against the possibility that such a split has occurred, the\nprimary page of each bucket chain stores the number of buckets that\nexisted as of the time the bucket was last split, or if never split as\nof the time it was created, in the space normally used for the\nprevious block number (that is, hasho_prevblkno).  This doesn't cost\nanything because the primary bucket page is always the first page in\nthe chain, and the previous block number is therefore always, in\nreality, InvalidBlockNumber.\n\nAfter computing the ostensibly-correct bucket number based on our cached\ncopy of the metapage, we lock the corresponding primary bucket page and\ncheck whether the bucket count stored in hasho_prevblkno is greater than\nthe number of buckets stored in our cached copy of the metapage.  If\nso, the bucket has certainly been split, because the count must originally\nhave been less than the number of buckets that existed at that time and\ncan't have increased except due to a split.  If not, the bucket can't have\nbeen split, because a split would have created a new bucket with a higher\nbucket number than any we'd seen previously.  In the latter case, we've\nlocked the correct bucket and can proceed; in the former case, we must\nrelease the lock on this bucket, lock the metapage, update our cache,\nunlock the metapage, and retry.\n\nNeeding to retry occasionally might seem expensive, but the number of times\nany given bucket can be split is limited to a few dozen no matter how\nmany times the hash index is accessed, because the total number of\nbuckets is limited to less than 2^32.  On the other hand, the number of\ntimes we access a bucket is unbounded and will be several orders of\nmagnitude larger even in unsympathetic cases.\n\n(The metapage cache is new in v10.  Older hash indexes had the primary\nbucket page's hasho_prevblkno initialized to InvalidBuffer.)\n\nPseudocode Algorithms\n---------------------\n\nVarious flags that are used in hash index operations are described as below:\n\nThe bucket-being-split and bucket-being-populated flags indicate that split\noperation is in progress for a bucket.  During split operation, a\nbucket-being-split flag is set on the old bucket and bucket-being-populated\nflag is set on new bucket.  These flags are cleared once the split operation\nis finished.\n\nThe split-cleanup flag indicates that a bucket which has been recently split\nstill contains tuples that were also copied to the new bucket; it essentially\nmarks the split as incomplete.  Once we're certain that no scans which\nstarted before the new bucket was fully populated are still in progress, we\ncan remove the copies from the old bucket and clear the flag.  We insist that\nthis flag must be clear before splitting a bucket; thus, a bucket can't be\nsplit again until the previous split is totally complete.\n\nThe moved-by-split flag on a tuple indicates that tuple is moved from old to\nnew bucket.  Concurrent scans will skip such tuples until the split operation\nis finished.  Once the tuple is marked as moved-by-split, it will remain so\nforever but that does no harm.  We have intentionally not cleared it as that\ncan generate an additional I/O which is not necessary.\n\nThe operations we need to support are: readers scanning the index for\nentries of a particular hash code (which by definition are all in the same\nbucket); insertion of a new tuple into the correct bucket; enlarging the\nhash table by splitting an existing bucket; and garbage collection\n(deletion of dead tuples and compaction of buckets).  Bucket splitting is\ndone at conclusion of any insertion that leaves the hash table more full\nthan the target load factor, but it is convenient to consider it as an\nindependent operation.  Note that we do not have a bucket-merge operation\n--- the number of buckets never shrinks.  Insertion, splitting, and\ngarbage collection may all need access to freelist management, which keeps\ntrack of available overflow pages.\n\nThe reader algorithm is:\n\n    lock the primary bucket page of the target bucket\n\tif the target bucket is still being populated by a split:\n\t\trelease the buffer content lock on current bucket page\n\t\tpin and acquire the buffer content lock on old bucket in shared mode\n\t\trelease the buffer content lock on old bucket, but not pin\n\t\tretake the buffer content lock on new bucket\n\t\tarrange to scan the old bucket normally and the new bucket for\n         tuples which are not moved-by-split\n-- then, per read request:\n\treacquire content lock on current page\n\tstep to next page if necessary (no chaining of content locks, but keep\n\tthe pin on the primary bucket throughout the scan)\n\tsave all the matching tuples from current index page into an items array\n\trelease pin and content lock (but if it is primary bucket page retain\n\tits pin till the end of the scan)\n\tget tuple from an item array\n-- at scan shutdown:\n\trelease all pins still held\n\nHolding the buffer pin on the primary bucket page for the whole scan prevents\nthe reader's current-tuple pointer from being invalidated by splits or\ncompactions.  (Of course, other buckets can still be split or compacted.)\n\nTo minimize lock/unlock traffic, hash index scan always searches the entire\nhash page to identify all the matching items at once, copying their heap tuple\nIDs into backend-local storage. The heap tuple IDs are then processed while not\nholding any page lock within the index thereby, allowing concurrent insertion\nto happen on the same index page without any requirement of re-finding the\ncurrent scan position for the reader. We do continue to hold a pin on the\nbucket page, to protect against concurrent deletions and bucket split.\n\nTo allow for scans during a bucket split, if at the start of the scan, the\nbucket is marked as bucket-being-populated, it scan all the tuples in that\nbucket except for those that are marked as moved-by-split.  Once it finishes\nthe scan of all the tuples in the current bucket, it scans the old bucket from\nwhich this bucket is formed by split.\n\nThe insertion algorithm is rather similar:\n\n    lock the primary bucket page of the target bucket\n-- (so far same as reader, except for acquisition of buffer content lock in\n\texclusive mode on primary bucket page)\n\tif the bucket-being-split flag is set for a bucket and pin count on it is\n\t one, then finish the split\n\t\trelease the buffer content lock on current bucket\n\t\tget the \"new\" bucket which was being populated by the split\n\t\tscan the new bucket and form the hash table of TIDs\n\t\tconditionally get the cleanup lock on old and new buckets\n\t\tif we get the lock on both the buckets\n\t\t\tfinish the split using algorithm mentioned below for split\n\t\trelease the pin on old bucket and restart the insert from beginning.\n\tif current page is full, first check if this page contains any dead tuples.\n\tif yes, remove dead tuples from the current page and again check for the\n\tavailability of the space. If enough space found, insert the tuple else\n\trelease lock but not pin, read/exclusive-lock\n     next page; repeat as needed\n\t>> see below if no space in any page of bucket\n\ttake buffer content lock in exclusive mode on metapage\n\tinsert tuple at appropriate place in page\n\tmark current page dirty\n\tincrement tuple count, decide if split needed\n\tmark meta page dirty\n\twrite WAL for insertion of tuple\n\trelease the buffer content lock on metapage\n\trelease buffer content lock on current page\n\tif current page is not a bucket page, release the pin on bucket page\n\tif split is needed, enter Split algorithm below\n\trelease the pin on metapage\n\nTo speed searches, the index entries within any individual index page are\nkept sorted by hash code; the insertion code must take care to insert new\nentries in the right place.  It is okay for an insertion to take place in a\nbucket that is being actively scanned, because readers can cope with this\nas explained above.  We only need the short-term buffer locks to ensure\nthat readers do not see a partially-updated page.\n\nTo avoid deadlock between readers and inserters, whenever there is a need\nto lock multiple buckets, we always take in the order suggested in Lock\nDefinitions above.  This algorithm allows them a very high degree of\nconcurrency.  (The exclusive metapage lock taken to update the tuple count\nis stronger than necessary, since readers do not care about the tuple count,\nbut the lock is held for such a short time that this is probably not an\nissue.)\n\nWhen an inserter cannot find space in any existing page of a bucket, it\nmust obtain an overflow page and add that page to the bucket's chain.\nDetails of that part of the algorithm appear later.\n\nThe page split algorithm is entered whenever an inserter observes that the\nindex is overfull (has a higher-than-wanted ratio of tuples to buckets).\nThe algorithm attempts, but does not necessarily succeed, to split one\nexisting bucket in two, thereby lowering the fill ratio:\n\n    pin meta page and take buffer content lock in exclusive mode\n    check split still needed\n    if split not needed anymore, drop buffer content lock and pin and exit\n    decide which bucket to split\n    try to take a cleanup lock on that bucket; if fail, give up\n    if that bucket is still being split or has split-cleanup work:\n       try to finish the split and the cleanup work\n       if that succeeds, start over; if it fails, give up\n\tmark the old and new buckets indicating split is in progress\n\tmark both old and new buckets as dirty\n\twrite WAL for allocation of new page for split\n\tcopy the tuples that belongs to new bucket from old bucket, marking\n     them as moved-by-split\n\twrite WAL record for moving tuples to new page once the new page is full\n\tor all the pages of old bucket are finished\n\trelease lock but not pin for primary bucket page of old bucket,\n\t read/shared-lock next page; repeat as needed\n\tclear the bucket-being-split and bucket-being-populated flags\n\tmark the old bucket indicating split-cleanup\n\twrite WAL for changing the flags on both old and new buckets\n\nThe split operation's attempt to acquire cleanup-lock on the old bucket number\ncould fail if another process holds any lock or pin on it.  We do not want to\nwait if that happens, because we don't want to wait while holding the metapage\nexclusive-lock.  So, this is a conditional LWLockAcquire operation, and if\nit fails we just abandon the attempt to split.  This is all right since the\nindex is overfull but perfectly functional.  Every subsequent inserter will\ntry to split, and eventually one will succeed.  If multiple inserters failed\nto split, the index might still be overfull, but eventually, the index will\nnot be overfull and split attempts will stop.  (We could make a successful\nsplitter loop to see if the index is still overfull, but it seems better to\ndistribute the split overhead across successive insertions.)\n\nIf a split fails partway through (e.g. due to insufficient disk space or an\ninterrupt), the index will not be corrupted.  Instead, we'll retry the split\nevery time a tuple is inserted into the old bucket prior to inserting the new\ntuple; eventually, we should succeed.  The fact that a split is left\nunfinished doesn't prevent subsequent buckets from being split, but we won't\ntry to split the bucket again until the prior split is finished.  In other\nwords, a bucket can be in the middle of being split for some time, but it can't\nbe in the middle of two splits at the same time.\n\nThe fourth operation is garbage collection (bulk deletion):\n\n\tnext bucket := 0\n\tpin metapage and take buffer content lock in exclusive mode\n\tfetch current max bucket number\n\trelease meta page buffer content lock and pin\n\twhile next bucket <= max bucket do\n\t\tacquire cleanup lock on primary bucket page\n\t\tloop:\n\t\t\tscan and remove tuples\n\t\t\tmark the target page dirty\n\t\t\twrite WAL for deleting tuples from target page\n\t\t\tif this is the last bucket page, break out of loop\n\t\t\tpin and x-lock next page\n\t\t\trelease prior lock and pin (except keep pin on primary bucket page)\n\t\tif the page we have locked is not the primary bucket page:\n\t\t\trelease lock and take exclusive lock on primary bucket page\n\t\tif there are no other pins on the primary bucket page:\n\t\t\tsqueeze the bucket to remove free space\n\t\trelease the pin on primary bucket page\n\t\tnext bucket ++\n\tend loop\n\tpin metapage and take buffer content lock in exclusive mode\n\tcheck if number of buckets changed\n\tif so, release content lock and pin and return to for-each-bucket loop\n\telse update metapage tuple count\n\tmark meta page dirty and write WAL for update of metapage\n\trelease buffer content lock and pin\n\nNote that this is designed to allow concurrent splits and scans.  If a split\noccurs, tuples relocated into the new bucket will be visited twice by the\nscan, but that does no harm.  See also \"Interlocking Between Scans and\nVACUUM\", below.\n\nWe must be careful about the statistics reported by the VACUUM operation.\nWhat we can do is count the number of tuples scanned, and believe this in\npreference to the stored tuple count if the stored tuple count and number of\nbuckets did *not* change at any time during the scan.  This provides a way of\ncorrecting the stored tuple count if it gets out of sync for some reason.  But\nif a split or insertion does occur concurrently, the scan count is\nuntrustworthy; instead, subtract the number of tuples deleted from the stored\ntuple count and use that.\n\nInterlocking Between Scans and VACUUM\n-------------------------------------\n\nSince we release the lock on bucket page during a cleanup scan of a bucket, a\nconcurrent scan could start in that bucket before we've finished vacuuming it.\nIf a scan gets ahead of cleanup, we could have the following problem: (1) the\nscan sees heap TIDs that are about to be removed before they are processed by\nVACUUM, (2) the scan decides that one or more of those TIDs are dead, (3)\nVACUUM completes, (4) one or more of the TIDs the scan decided were dead are\nreused for an unrelated tuple, and finally (5) the scan wakes up and\nerroneously kills the new tuple.\n\nNote that this requires VACUUM and a scan to be active in the same bucket at\nthe same time.  If VACUUM completes before the scan starts, the scan never has\na chance to see the dead tuples; if the scan completes before the VACUUM\nstarts, the heap TIDs can't have been reused meanwhile.  Furthermore, VACUUM\ncan't start on a bucket that has an active scan, because the scan holds a pin\non the primary bucket page, and VACUUM must take a cleanup lock on that page\nin order to begin cleanup.  Therefore, the only way this problem can occur is\nfor a scan to start after VACUUM has released the cleanup lock on the bucket\nbut before it has processed the entire bucket and then overtake the cleanup\noperation.\n\nCurrently, we prevent this using lock chaining: cleanup locks the next page\nin the chain before releasing the lock and pin on the page just processed.\n\nFree Space Management\n---------------------\n\n(Question: why is this so complicated?  Why not just have a linked list\nof free pages with the list head in the metapage?  It's not like we\navoid needing to modify the metapage with all this.)\n\nFree space management consists of two sub-algorithms, one for reserving\nan overflow page to add to a bucket chain, and one for returning an empty\noverflow page to the free pool.\n\nObtaining an overflow page:\n\n\ttake metapage content lock in exclusive mode\n\tdetermine next bitmap page number; if none, exit loop\n\trelease meta page content lock\n\tpin bitmap page and take content lock in exclusive mode\n\tsearch for a free page (zero bit in bitmap)\n\tif found:\n\t\tset bit in bitmap\n\t\tmark bitmap page dirty\n\t\ttake metapage buffer content lock in exclusive mode\n\t\tif first-free-bit value did not change,\n\t\t\tupdate it and mark meta page dirty\n\telse (not found):\n\trelease bitmap page buffer content lock\n\tloop back to try next bitmap page, if any\n-- here when we have checked all bitmap pages; we hold meta excl. lock\n\textend index to add another overflow page; update meta information\n\tmark meta page dirty\n\treturn page number\n\nIt is slightly annoying to release and reacquire the metapage lock\nmultiple times, but it seems best to do it that way to minimize loss of\nconcurrency against processes just entering the index.  We don't want\nto hold the metapage exclusive lock while reading in a bitmap page.\n(We can at least avoid repeated buffer pin/unpin here.)\n\nThe normal path for extending the index does not require doing I/O while\nholding the metapage lock.  We do have to do I/O when the extension\nrequires adding a new bitmap page as well as the required overflow page\n... but that is an infrequent case, so the loss of concurrency seems\nacceptable.\n\nThe portion of tuple insertion that calls the above subroutine looks\nlike this:\n\n\t-- having determined that no space is free in the target bucket:\n\tremember last page of bucket, drop write lock on it\n\tre-write-lock last page of bucket\n\tif it is not last anymore, step to the last page\n\texecute free-page-acquire (obtaining an overflow page) mechanism\n      described above\n\tupdate (former) last page to point to the new page and mark buffer dirty\n\twrite-lock and initialize new page, with back link to former last page\n\twrite WAL for addition of overflow page\n\trelease the locks on meta page and bitmap page acquired in\n      free-page-acquire algorithm\n\trelease the lock on former last page\n\trelease the lock on new overflow page\n\tinsert tuple into new page\n\t-- etc.\n\nNotice this handles the case where two concurrent inserters try to extend\nthe same bucket.  They will end up with a valid, though perhaps\nspace-inefficient, configuration: two overflow pages will be added to the\nbucket, each containing one tuple.\n\nThe last part of this violates the rule about holding write lock on two\npages concurrently, but it should be okay to write-lock the previously\nfree page; there can be no other process holding lock on it.\n\nBucket splitting uses a similar algorithm if it has to extend the new\nbucket, but it need not worry about concurrent extension since it has\nbuffer content lock in exclusive mode on the new bucket.\n\nFreeing an overflow page requires the process to hold buffer content lock in\nexclusive mode on the containing bucket, so need not worry about other\naccessors of pages in the bucket.  The algorithm is:\n\n\tdelink overflow page from bucket chain\n\t(this requires read/update/write/release of fore and aft siblings)\n\tpin meta page and take buffer content lock in shared mode\n\tdetermine which bitmap page contains the free space bit for page\n\trelease meta page buffer content lock\n\tpin bitmap page and take buffer content lock in exclusive mode\n\tretake meta page buffer content lock in exclusive mode\n\tmove (insert) tuples that belong to the overflow page being freed\n\tupdate bitmap bit\n\tmark bitmap page dirty\n\tif page number is still less than first-free-bit,\n\t\tupdate first-free-bit field and mark meta page dirty\n\twrite WAL for delinking overflow page operation\n\trelease buffer content lock and pin\n\trelease meta page buffer content lock and pin\n\nWe have to do it this way because we must clear the bitmap bit before\nchanging the first-free-bit field (hashm_firstfree).  It is possible that\nwe set first-free-bit too small (because someone has already reused the\npage we just freed), but that is okay; the only cost is the next overflow\npage acquirer will scan more bitmap bits than he needs to.  What must be\navoided is having first-free-bit greater than the actual first free bit,\nbecause then that free page would never be found by searchers.\n\nThe reason of moving tuples from overflow page while delinking the later is\nto make that as an atomic operation.  Not doing so could lead to spurious reads\non standby.  Basically, the user might see the same tuple twice.\n\n\nWAL Considerations\n------------------\n\nThe hash index operations like create index, insert, delete, bucket split,\nallocate overflow page, and squeeze in themselves don't guarantee hash index\nconsistency after a crash.  To provide robustness, we write WAL for each of\nthese operations.\n\nCREATE INDEX writes multiple WAL records.  First, we write a record to cover\nthe initialization of the metapage, followed by one for each new bucket\ncreated, followed by one for the initial bitmap page.  It's not important for\nindex creation to appear atomic, because the index isn't yet visible to any\nother transaction, and the creating transaction will roll back in the event of\na crash.  It would be difficult to cover the whole operation with a single\nwrite-ahead log record anyway, because we can log only a fixed number of\npages, as given by XLR_MAX_BLOCK_ID (32), with current XLog machinery.\n\nOrdinary item insertions (that don't force a page split or need a new overflow\npage) are single WAL entries.  They touch a single bucket page and the\nmetapage.  The metapage is updated during replay as it is updated during\noriginal operation.\n\nIf an insertion causes the addition of an overflow page, there will be one\nWAL entry for the new overflow page and second entry for insert itself.\n\nIf an insertion causes a bucket split, there will be one WAL entry for insert\nitself, followed by a WAL entry for allocating a new bucket, followed by a WAL\nentry for each overflow bucket page in the new bucket to which the tuples are\nmoved from old bucket, followed by a WAL entry to indicate that split is\ncomplete for both old and new buckets.  A split operation which requires\noverflow pages to complete the operation will need to write a WAL record for\neach new allocation of an overflow page.\n\nAs splitting involves multiple atomic actions, it's possible that the system\ncrashes between moving tuples from bucket pages of the old bucket to new\nbucket.  In such a case, after recovery, the old and new buckets will be\nmarked with bucket-being-split and bucket-being-populated flags respectively\nwhich indicates that split is in progress for those buckets.  The reader\nalgorithm works correctly, as it will scan both the old and new buckets when\nthe split is in progress as explained in the reader algorithm section above.\n\nWe finish the split at next insert or split operation on the old bucket as\nexplained in insert and split algorithm above.  It could be done during\nsearches, too, but it seems best not to put any extra updates in what would\notherwise be a read-only operation (updating is not possible in hot standby\nmode anyway).  It would seem natural to complete the split in VACUUM, but since\nsplitting a bucket might require allocating a new page, it might fail if you\nrun out of disk space.  That would be bad during VACUUM - the reason for\nrunning VACUUM in the first place might be that you run out of disk space,\nand now VACUUM won't finish because you're out of disk space.  In contrast,\nan insertion can require enlarging the physical file anyway.\n\nDeletion of tuples from a bucket is performed for two reasons: to remove dead\ntuples, and to remove tuples that were moved by a bucket split.  A WAL entry\nis made for each bucket page from which tuples are removed, and then another\nWAL entry is made when we clear the needs-split-cleanup flag.  If dead tuples\nare removed, a separate WAL entry is made to update the metapage.\n\nAs deletion involves multiple atomic operations, it is quite possible that\nsystem crashes after (a) removing tuples from some of the bucket pages, (b)\nbefore clearing the garbage flag, or (c) before updating the metapage.  If the\nsystem crashes before completing (b), it will again try to clean the bucket\nduring next vacuum or insert after recovery which can have some performance\nimpact, but it will work fine. If the system crashes before completing (c),\nafter recovery there could be some additional splits until the next vacuum\nupdates the metapage, but the other operations like insert, delete and scan\nwill work correctly.  We can fix this problem by actually updating the\nmetapage based on delete operation during replay, but it's not clear whether\nit's worth the complication.\n\nA squeeze operation moves tuples from one of the buckets later in the chain to\none of the bucket earlier in chain and writes WAL record when either the\nbucket to which it is writing tuples is filled or bucket from which it\nis removing the tuples becomes empty.\n\nAs a squeeze operation involves writing multiple atomic operations, it is\nquite possible that the system crashes before completing the operation on\nentire bucket.  After recovery, the operations will work correctly, but\nthe index will remain bloated and this can impact performance of read and\ninsert operations until the next vacuum squeeze the bucket completely.\n\n\nOther Notes\n-----------\n\nClean up locks prevent a split from occurring while *another* process is stopped\nin a given bucket.  It also ensures that one of our *own* backend's scans is not\nstopped in the bucket.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\hash\\README",
      "directory": "backend\\access\\hash"
    }
  },
  {
    "title": "README: backend\\access\\heap",
    "url": "backend\\access\\heap\\README.HOT",
    "content": "src/backend/access/heap/README.HOT\n\nHeap Only Tuples (HOT)\n======================\n\nThe Heap Only Tuple (HOT) feature eliminates redundant index entries and\nallows the re-use of space taken by DELETEd or obsoleted UPDATEd tuples\nwithout performing a table-wide vacuum.  It does this by allowing\nsingle-page vacuuming, also called \"defragmentation\" or \"pruning\".\n\nNote: there is a Glossary at the end of this document that may be helpful\nfor first-time readers.\n\n\nTechnical Challenges\n--------------------\n\nPage-at-a-time vacuuming is normally impractical because of the costs of\nfinding and removing the index entries that link to the tuples to be\nreclaimed.  Standard vacuuming scans the indexes to ensure all such index\nentries are removed, amortizing the index scan cost across as many dead\ntuples as possible; this approach does not scale down well to the case of\nreclaiming just a few tuples.  In principle one could recompute the index\nkeys and do standard index searches to find the index entries, but this is\nrisky in the presence of possibly-buggy user-defined functions in\nfunctional indexes.  An allegedly immutable function that in fact is not\nimmutable might prevent us from re-finding an index entry (and we cannot\nthrow an error for not finding it, in view of the fact that dead index\nentries are sometimes reclaimed early).  That would lead to a seriously\ncorrupt index, in the form of entries pointing to tuple slots that by now\ncontain some unrelated content.  In any case we would prefer to be able\nto do vacuuming without invoking any user-written code.\n\nHOT solves this problem for two restricted but useful special cases:\n\nFirst, where a tuple is repeatedly updated in ways that do not change\nits indexed columns.  (Here, \"indexed column\" means any column referenced\nat all in an index definition, including for example columns that are\ntested in a partial-index predicate but are not stored in the index.)\n\nSecond, where the modified columns are only used in indexes that do not\ncontain tuple IDs, but maintain summaries of the indexed data by block.\nAs these indexes don't contain references to individual tuples, they\ncan't remove tuple references in VACUUM, and thus don't need to get a new\nand unique reference to a tuple.  These indexes still need to be notified\nof the new column data, but don't need a new HOT chain to be established.\n\nAn additional property of HOT is that it reduces index size by avoiding\nthe creation of identically-keyed index entries.  This improves search\nspeeds.\n\n\nUpdate Chains With a Single Index Entry\n---------------------------------------\n\nWithout HOT, every version of a row in an update chain has its own index\nentries, even if all indexed columns are the same.  With HOT, a new tuple\nplaced on the same page and with all indexed columns the same as its\nparent row version does not get new index entries.  This means there is\nonly one index entry for the entire update chain on the heap page.\nAn index-entry-less tuple is marked with the HEAP_ONLY_TUPLE flag.\nThe prior row version is marked HEAP_HOT_UPDATED, and (as always in an\nupdate chain) its t_ctid field links forward to the newer version.\n\nFor example:\n\n\tIndex points to 1\n\tlp [1]  [2]\n\n\t[111111111]->[2222222222]\n\nIn the above diagram, the index points to line pointer 1, and tuple 1 is\nmarked as HEAP_HOT_UPDATED.  Tuple 2 is a HOT tuple, meaning it has\nno index entry pointing to it, and is marked as HEAP_ONLY_TUPLE.\nAlthough tuple 2 is not directly referenced by the index, it can still be\nfound by an index search: after traversing from the index to tuple 1,\nthe index search proceeds forward to child tuples as long as it sees the\nHEAP_HOT_UPDATED flag set.  Since we restrict the HOT chain to lie within\na single page, this requires no additional page fetches and doesn't\nintroduce much performance penalty.\n\nEventually, tuple 1 will no longer be visible to any transaction.\nAt that point its space could be reclaimed, but its line pointer cannot,\nsince the index still links to that line pointer and we still need to\nbe able to find tuple 2 in an index search.  HOT handles this by turning\nline pointer 1 into a \"redirecting line pointer\", which links to tuple 2\nbut has no actual tuple attached.  This state of affairs looks like\n\n\tIndex points to 1\n\tlp [1]->[2]\n\n\t[2222222222]\n\nIf now the row is updated again, to version 3, the page looks like this:\n\n\tIndex points to 1\n\tlp [1]->[2]  [3]\n\n\t[2222222222]->[3333333333]\n\nAt some later time when no transaction can see tuple 2 in its snapshot,\ntuple 2 and its line pointer can be pruned entirely:\n\n\tIndex points to 1\n\tlp [1]------>[3]\n\n\t[3333333333]\n\nThis is safe because no index entry points to line pointer 2.  Subsequent\ninsertions into the page can now recycle both line pointer 2 and the\nspace formerly used by tuple 2.\n\nIf an update changes any column indexed by a non-summarizing indexes, or\nif there is not room on the same page for the new tuple, then the HOT\nchain ends: the last member has a regular t_ctid link to the next version\nand is not marked HEAP_HOT_UPDATED.  (In principle we could continue a\nHOT chain across pages, but this would destroy the desired property of\nbeing able to reclaim space with just page-local manipulations.  Anyway,\nwe don't want to have to chase through multiple heap pages to get from an\nindex entry to the desired tuple, so it seems better to create a new\nindex entry for the new tuple.)  If further updates occur, the next\nversion could become the root of a new HOT chain.\n\nLine pointer 1 has to remain as long as there is any non-dead member of\nthe chain on the page.  When there is not, it is marked \"dead\".\nThis lets us reclaim the last child line pointer and associated tuple\nimmediately.  The next regular VACUUM pass can reclaim the index entries\npointing at the line pointer and then the line pointer itself.  Since a\nline pointer is small compared to a tuple, this does not represent an\nundue space cost.\n\nNote: we can use a \"dead\" line pointer for any DELETEd tuple,\nwhether it was part of a HOT chain or not.  This allows space reclamation\nin advance of running VACUUM for plain DELETEs as well as HOT updates.\n\nThe requirement for doing a HOT update is that indexes which point to\nthe root line pointer (and thus need to be cleaned up by VACUUM when the\ntuple is dead) do not reference columns which are updated in that HOT\nchain.  Summarizing indexes (such as BRIN) are assumed to have no\nreferences to individual tuples and thus are ignored when checking HOT\napplicability.  The updated columns are checked at execution time by\ncomparing the binary representation of the old and new values.  We insist\non bitwise equality rather than using datatype-specific equality routines.\nThe main reason to avoid the latter is that there might be multiple\nnotions of equality for a datatype, and we don't know exactly which one\nis relevant for the indexes at hand.  We assume that bitwise equality\nguarantees equality for all purposes.\n\nIf any columns that are included by non-summarizing indexes are updated,\nthe HOT optimization is not applied, and the new tuple is inserted into\nall indexes of the table.  If none of the updated columns are included in\nthe table's indexes, the HOT optimization is applied and no indexes are\nupdated.  If instead the updated columns are only indexed by summarizing\nindexes, the HOT optimization is applied, but the update is propagated to\nall summarizing indexes.  (Realistically, we only need to propagate the\nupdate to the indexes that contain the updated values, but that is yet to\nbe implemented.)\n\nAbort Cases\n-----------\n\nIf a heap-only tuple's xmin is aborted, then it can be removed immediately:\nit was never visible to any other transaction, and all descendant row\nversions must be aborted as well.  Therefore we need not consider it part\nof a HOT chain.  By the same token, if a HOT-updated tuple's xmax is\naborted, there is no need to follow the chain link.  However, there is a\nrace condition here: the transaction that did the HOT update might abort\nbetween the time we inspect the HOT-updated tuple and the time we reach\nthe descendant heap-only tuple.  It is conceivable that someone prunes\nthe heap-only tuple before that, and even conceivable that the line pointer\nis re-used for another purpose.  Therefore, when following a HOT chain,\nit is always necessary to be prepared for the possibility that the\nlinked-to line pointer is unused, dead, or redirected; and if it is a\nnormal line pointer, we still have to check that XMIN of the tuple matches\nthe XMAX of the tuple we left.  Otherwise we should assume that we have\ncome to the end of the HOT chain.  Note that this sort of XMIN/XMAX\nmatching is required when following ordinary update chains anyway.\n\n(Early versions of the HOT code assumed that holding pin on the page\nbuffer while following a HOT link would prevent this type of problem,\nbut checking XMIN/XMAX matching is a much more robust solution.)\n\n\nIndex/Sequential Scans\n----------------------\n\nWhen doing an index scan, whenever we reach a HEAP_HOT_UPDATED tuple whose\nxmax is not aborted, we need to follow its t_ctid link and check that\nentry as well; possibly repeatedly until we reach the end of the HOT\nchain.  (When using an MVCC snapshot it is possible to optimize this a\nbit: there can be at most one visible tuple in the chain, so we can stop\nwhen we find it.  This rule does not work for non-MVCC snapshots, though.)\n\nSequential scans do not need to pay attention to the HOT links because\nthey scan every line pointer on the page anyway.  The same goes for a\nbitmap heap scan with a lossy bitmap.\n\n\nPruning\n-------\n\nHOT pruning means updating line pointers so that HOT chains are\nreduced in length, by collapsing out line pointers for intermediate dead\ntuples.  Although this makes those line pointers available for re-use,\nit does not immediately make the space occupied by their tuples available.\n\n\nDefragmentation\n---------------\n\nDefragmentation centralizes unused space.  After we have converted root\nline pointers to redirected line pointers and pruned away any dead\nintermediate line pointers, the tuples they linked to are free space.\nBut unless that space is adjacent to the central \"hole\" on the page\n(the pd_lower-to-pd_upper area) it cannot be used by tuple insertion.\nDefragmentation moves the surviving tuples to coalesce all the free\nspace into one \"hole\".  This is done with the same PageRepairFragmentation\nfunction that regular VACUUM uses.\n\n\nWhen can/should we prune or defragment?\n---------------------------------------\n\nThis is the most interesting question in HOT implementation, since there\nis no simple right answer: we must use heuristics to determine when it's\nmost efficient to perform pruning and/or defragmenting.\n\nWe cannot prune or defragment unless we can get a \"buffer cleanup lock\"\non the target page; otherwise, pruning might destroy line pointers that\nother backends have live references to, and defragmenting might move\ntuples that other backends have live pointers to.  Thus the general\napproach must be to heuristically decide if we should try to prune\nor defragment, and if so try to acquire the buffer cleanup lock without\nblocking.  If we succeed we can proceed with our housekeeping work.\nIf we cannot get the lock (which should not happen often, except under\nvery heavy contention) then the housekeeping has to be postponed till\nsome other time.  The worst-case consequence of this is only that an\nUPDATE cannot be made HOT but has to link to a new tuple version placed on\nsome other page, for lack of centralized space on the original page.\n\nIdeally we would do defragmenting only when we are about to attempt\nheap_update on a HOT-safe tuple.  The difficulty with this approach\nis that the update query has certainly got a pin on the old tuple, and\ntherefore our attempt to acquire a buffer cleanup lock will always fail.\n(This corresponds to the idea that we don't want to move the old tuple\nout from under where the query's HeapTuple pointer points.  It might\nbe possible to finesse that, but it seems fragile.)\n\nPruning, however, is potentially useful even when we are not about to\ninsert a new tuple, since shortening a HOT chain reduces the cost of\nsubsequent index searches.  However it is unclear that this gain is\nlarge enough to accept any extra maintenance burden for.\n\nThe currently planned heuristic is to prune and defrag when first accessing\na page that potentially has prunable tuples (as flagged by the pd_prune_xid\npage hint field) and that either has free space less than MAX(fillfactor\ntarget free space, BLCKSZ/10) *or* has recently had an UPDATE fail to\nfind enough free space to store an updated tuple version.  (These rules\nare subject to change.)\n\nWe have effectively implemented the \"truncate dead tuples to just line\npointer\" idea that has been proposed and rejected before because of fear\nof line pointer bloat: we might end up with huge numbers of line pointers\nand just a few actual tuples on a page.  To limit the damage in the worst\ncase, and to keep various work arrays as well as the bitmaps in bitmap\nscans reasonably sized, the maximum number of line pointers per page\nis arbitrarily capped at MaxHeapTuplesPerPage (the most tuples that\ncould fit without HOT pruning).\n\nEffectively, space reclamation happens during tuple retrieval when the\npage is nearly full (<10% free) and a buffer cleanup lock can be\nacquired.  This means that UPDATE, DELETE, and SELECT can trigger space\nreclamation, but often not during INSERT ... VALUES because it does\nnot retrieve a row.\n\n\nVACUUM\n------\n\nThere is little change to regular vacuum.  It performs pruning to remove\ndead heap-only tuples, and cleans up any dead line pointers as if they were\nregular dead tuples.\n\n\nStatistics\n----------\n\nCurrently, we count HOT updates the same as cold updates for statistics\npurposes, though there is an additional per-table counter that counts\nonly HOT updates.  When a page pruning operation is able to remove a\nphysical tuple by eliminating an intermediate heap-only tuple or\nreplacing a physical root tuple by a redirect pointer, a decrement in\nthe table's number of dead tuples is reported to pgstats, which may\npostpone autovacuuming.  Note that we do not count replacing a root tuple\nby a DEAD line pointer as decrementing dead_tuples; we still want\nautovacuum to run to clean up the index entries and DEAD item.\n\nThis area probably needs further work ...\n\n\nCREATE INDEX\n------------\n\nCREATE INDEX presents a problem for HOT updates.  While the existing HOT\nchains all have the same index values for existing indexes, the columns\nin the new index might change within a pre-existing HOT chain, creating\na \"broken\" chain that can't be indexed properly.\n\nTo address this issue, regular (non-concurrent) CREATE INDEX makes the\nnew index usable only by new transactions and transactions that don't\nhave snapshots older than the CREATE INDEX command.  This prevents\nqueries that can see the inconsistent HOT chains from trying to use the\nnew index and getting incorrect results.  Queries that can see the index\ncan only see the rows that were visible after the index was created,\nhence the HOT chains are consistent for them.\n\nEntries in the new index point to root tuples (tuples with current index\npointers) so that our index uses the same index pointers as all other\nindexes on the table.  However the row we want to index is actually at\nthe *end* of the chain, ie, the most recent live tuple on the HOT chain.\nThat is the one we compute the index entry values for, but the TID\nwe put into the index is that of the root tuple.  Since queries that\nwill be allowed to use the new index cannot see any of the older tuple\nversions in the chain, the fact that they might not match the index entry\nisn't a problem.  (Such queries will check the tuple visibility\ninformation of the older versions and ignore them, without ever looking at\ntheir contents, so the content inconsistency is OK.)  Subsequent updates\nto the live tuple will be allowed to extend the HOT chain only if they are\nHOT-safe for all the indexes.\n\nBecause we have ShareLock on the table, any DELETE_IN_PROGRESS or\nINSERT_IN_PROGRESS tuples should have come from our own transaction.\nTherefore we can consider them committed since if the CREATE INDEX\ncommits, they will be committed, and if it aborts the index is discarded.\nAn exception to this is that early lock release is customary for system\ncatalog updates, and so we might find such tuples when reindexing a system\ncatalog.  In that case we deal with it by waiting for the source\ntransaction to commit or roll back.  (We could do that for user tables\ntoo, but since the case is unexpected we prefer to throw an error.)\n\nPractically, we prevent certain transactions from using the new index by\nsetting pg_index.indcheckxmin to TRUE.  Transactions are allowed to use\nsuch an index only after pg_index.xmin is below their TransactionXmin\nhorizon, thereby ensuring that any incompatible rows in HOT chains are\ndead to them. (pg_index.xmin will be the XID of the CREATE INDEX\ntransaction.  The reason for using xmin rather than a normal column is\nthat the regular vacuum freezing mechanism will take care of converting\nxmin to FrozenTransactionId before it can wrap around.)\n\nThis means in particular that the transaction creating the index will be\nunable to use the index if the transaction has old snapshots.  We\nalleviate that problem somewhat by not setting indcheckxmin unless the\ntable actually contains HOT chains with RECENTLY_DEAD members.\n\nAnother unpleasant consequence is that it is now risky to use SnapshotAny\nin an index scan: if the index was created more recently than the last\nvacuum, it's possible that some of the visited tuples do not match the\nindex entry they are linked to.  This does not seem to be a fatal\nobjection, since there are few users of SnapshotAny and most use seqscans.\nThe only exception at this writing is CLUSTER, which is okay because it\ndoes not require perfect ordering of the indexscan readout (and especially\nso because CLUSTER tends to write recently-dead tuples out of order anyway).\n\n\nCREATE INDEX CONCURRENTLY\n-------------------------\n\nIn the concurrent case we must take a different approach.  We create the\npg_index entry immediately, before we scan the table.  The pg_index entry\nis marked as \"not ready for inserts\".  Then we commit and wait for any\ntransactions which have the table open to finish.  This ensures that no\nnew HOT updates will change the key value for our new index, because all\ntransactions will see the existence of the index and will respect its\nconstraint on which updates can be HOT.  Other transactions must include\nsuch an index when determining HOT-safety of updates, even though they\nmust ignore it for both insertion and searching purposes.\n\nWe must do this to avoid making incorrect index entries.  For example,\nsuppose we are building an index on column X and we make an index entry for\na non-HOT tuple with X=1.  Then some other backend, unaware that X is an\nindexed column, HOT-updates the row to have X=2, and commits.  We now have\nan index entry for X=1 pointing at a HOT chain whose live row has X=2.\nWe could make an index entry with X=2 during the validation pass, but\nthere is no nice way to get rid of the wrong entry with X=1.  So we must\nhave the HOT-safety property enforced before we start to build the new\nindex.\n\nAfter waiting for transactions which had the table open, we build the index\nfor all rows that are valid in a fresh snapshot.  Any tuples visible in the\nsnapshot will have only valid forward-growing HOT chains.  (They might have\nolder HOT updates behind them which are broken, but this is OK for the same\nreason it's OK in a regular index build.)  As above, we point the index\nentry at the root of the HOT-update chain but we use the key value from the\nlive tuple.\n\nWe mark the index open for inserts (but still not ready for reads) then\nwe again wait for transactions which have the table open.  Then we take\na second reference snapshot and validate the index.  This searches for\ntuples missing from the index, and inserts any missing ones.  Again,\nthe index entries have to have TIDs equal to HOT-chain root TIDs, but\nthe value to be inserted is the one from the live tuple.\n\nThen we wait until every transaction that could have a snapshot older than\nthe second reference snapshot is finished.  This ensures that nobody is\nalive any longer who could need to see any tuples that might be missing\nfrom the index, as well as ensuring that no one can see any inconsistent\nrows in a broken HOT chain (the first condition is stronger than the\nsecond).  Finally, we can mark the index valid for searches.\n\nNote that we do not need to set pg_index.indcheckxmin in this code path,\nbecause we have outwaited any transactions that would need to avoid using\nthe index.  (indcheckxmin is only needed because non-concurrent CREATE\nINDEX doesn't want to wait; its stronger lock would create too much risk of\ndeadlock if it did.)\n\n\nDROP INDEX CONCURRENTLY\n-----------------------\n\nDROP INDEX CONCURRENTLY is sort of the reverse sequence of CREATE INDEX\nCONCURRENTLY.  We first mark the index as not indisvalid, and then wait for\nany transactions that could be using it in queries to end.  (During this\ntime, index updates must still be performed as normal, since such\ntransactions might expect freshly inserted tuples to be findable.)\nThen, we clear indisready and indislive, and again wait for transactions\nthat could be updating the index to end.  Finally we can drop the index\nnormally (though taking only ShareUpdateExclusiveLock on its parent table).\n\nThe reason we need the pg_index.indislive flag is that after the second\nwait step begins, we don't want transactions to be touching the index at\nall; otherwise they might suffer errors if the DROP finally commits while\nthey are reading catalog entries for the index.  If we had only indisvalid\nand indisready, this state would be indistinguishable from the first stage\nof CREATE INDEX CONCURRENTLY --- but in that state, we *do* want\ntransactions to examine the index, since they must consider it in\nHOT-safety checks.\n\n\nLimitations and Restrictions\n----------------------------\n\nIt is worth noting that HOT forever forecloses alternative approaches\nto vacuuming, specifically the recompute-the-index-keys approach alluded\nto in Technical Challenges above.  It'll be tough to recompute the index\nkeys for a root line pointer you don't have data for anymore ...\n\n\nGlossary\n--------\n\nBroken HOT Chain\n\n\tA HOT chain in which the key value for an index has changed.\n\n\tThis is not allowed to occur normally but if a new index is created\n\tit can happen.  In that case various strategies are used to ensure\n\tthat no transaction for which the older tuples are visible can\n\tuse the index.\n\nCold update\n\n\tA normal, non-HOT update, in which index entries are made for\n\tthe new version of the tuple.\n\nDead line pointer\n\n\tA stub line pointer, that does not point to anything, but cannot\n\tbe removed or reused yet because there are index pointers to it.\n\tSemantically same as a dead tuple.  It has state LP_DEAD.\n\nHeap-only tuple\n\n\tA heap tuple with no index pointers, which can only be reached\n\tfrom indexes indirectly through its ancestral root tuple.\n\tMarked with HEAP_ONLY_TUPLE flag.\n\nHOT-safe\n\n\tA proposed tuple update is said to be HOT-safe if it changes\n\tnone of the tuple's indexed columns.  It will only become an\n\tactual HOT update if we can find room on the same page for\n\tthe new tuple version.\n\nHOT update\n\n\tAn UPDATE where the new tuple becomes a heap-only tuple, and no\n\tnew index entries are made.\n\nHOT-updated tuple\n\n\tAn updated tuple, for which the next tuple in the chain is a\n\theap-only tuple.  Marked with HEAP_HOT_UPDATED flag.\n\nIndexed column\n\n\tA column used in an index definition.  The column might not\n\tactually be stored in the index --- it could be used in a\n\tfunctional index's expression, or used in a partial index\n\tpredicate.  HOT treats all these cases alike.\n\nRedirecting line pointer\n\n\tA line pointer that points to another line pointer and has no\n\tassociated tuple.  It has the special lp_flags state LP_REDIRECT,\n\tand lp_off is the OffsetNumber of the line pointer it links to.\n\tThis is used when a root tuple becomes dead but we cannot prune\n\tthe line pointer because there are non-dead heap-only tuples\n\tfurther down the chain.\n\nRoot tuple\n\n\tThe first tuple in a HOT update chain; the one that indexes point to.\n\nUpdate chain\n\n\tA chain of updated tuples, in which each tuple's ctid points to\n\tthe next tuple in the chain. A HOT update chain is an update chain\n\t(or portion of an update chain) that consists of a root tuple and\n\tone or more heap-only tuples.  A complete update chain can contain\n\tboth HOT and non-HOT (cold) updated tuples.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\heap\\README.HOT",
      "directory": "backend\\access\\heap"
    }
  },
  {
    "title": "README: backend\\access\\heap",
    "url": "backend\\access\\heap\\README.tuplock",
    "content": "Locking tuples\n--------------\n\nLocking tuples is not as easy as locking tables or other database objects.\nThe problem is that transactions might want to lock large numbers of tuples at\nany one time, so it's not possible to keep the locks objects in shared memory.\nTo work around this limitation, we use a two-level mechanism.  The first level\nis implemented by storing locking information in the tuple header: a tuple is\nmarked as locked by setting the current transaction's XID as its XMAX, and\nsetting additional infomask bits to distinguish this case from the more normal\ncase of having deleted the tuple.  When multiple transactions concurrently\nlock a tuple, a MultiXact is used; see below.  This mechanism can accommodate\narbitrarily large numbers of tuples being locked simultaneously.\n\nWhen it is necessary to wait for a tuple-level lock to be released, the basic\ndelay is provided by XactLockTableWait or MultiXactIdWait on the contents of\nthe tuple's XMAX.  However, that mechanism will release all waiters\nconcurrently, so there would be a race condition as to which waiter gets the\ntuple, potentially leading to indefinite starvation of some waiters.  The\npossibility of share-locking makes the problem much worse --- a steady stream\nof share-lockers can easily block an exclusive locker forever.  To provide\nmore reliable semantics about who gets a tuple-level lock first, we use the\nstandard lock manager, which implements the second level mentioned above.  The\nprotocol for waiting for a tuple-level lock is really\n\n     LockTuple()\n     XactLockTableWait()\n     mark tuple as locked by me\n     UnlockTuple()\n\nWhen there are multiple waiters, arbitration of who is to get the lock next\nis provided by LockTuple().  However, at most one tuple-level lock will\nbe held or awaited per backend at any time, so we don't risk overflow\nof the lock table.  Note that incoming share-lockers are required to\ndo LockTuple as well, if there is any conflict, to ensure that they don't\nstarve out waiting exclusive-lockers.  However, if there is not any active\nconflict for a tuple, we don't incur any extra overhead.\n\nWe make an exception to the above rule for those lockers that already hold\nsome lock on a tuple and attempt to acquire a stronger one on it.  In that\ncase, we skip the LockTuple() call even when there are conflicts, provided\nthat the target tuple is being locked, updated or deleted by multiple sessions\nconcurrently.  Failing to skip the lock would risk a deadlock, e.g., between a\nsession that was first to record its weaker lock in the tuple header and would\nbe waiting on the LockTuple() call to upgrade to the stronger lock level, and\nanother session that has already done LockTuple() and is waiting for the first\nsession transaction to release its tuple header-level lock.\n\nWe provide four levels of tuple locking strength: SELECT FOR UPDATE obtains an\nexclusive lock which prevents any kind of modification of the tuple. This is\nthe lock level that is implicitly taken by DELETE operations, and also by\nUPDATE operations if they modify any of the tuple's key fields. SELECT FOR NO\nKEY UPDATE likewise obtains an exclusive lock, but only prevents tuple removal\nand modifications which might alter the tuple's key. This is the lock that is\nimplicitly taken by UPDATE operations which leave all key fields unchanged.\nSELECT FOR SHARE obtains a shared lock which prevents any kind of tuple\nmodification. Finally, SELECT FOR KEY SHARE obtains a shared lock which only\nprevents tuple removal and modifications of key fields. This lock level is\njust strong enough to implement RI checks, i.e. it ensures that tuples do not\ngo away from under a check, without blocking transactions that want to update\nthe tuple without changing its key.\n\nThe conflict table is:\n\n                  UPDATE       NO KEY UPDATE    SHARE        KEY SHARE\nUPDATE           conflict        conflict      conflict      conflict\nNO KEY UPDATE    conflict        conflict      conflict\nSHARE            conflict        conflict\nKEY SHARE        conflict\n\nWhen there is a single locker in a tuple, we can just store the locking info\nin the tuple itself.  We do this by storing the locker's Xid in XMAX, and\nsetting infomask bits specifying the locking strength.  See \"Infomask Bits\"\nbelow for details on the bit patterns we use.\n\nMultiXacts\n----------\n\nA tuple header provides very limited space for storing information about tuple\nlocking and updates: there is room only for a single Xid and a small number of\ninfomask bits.  Whenever we need to store more than one lock, we replace the\nfirst locker's Xid with a new MultiXactId.  Each MultiXact provides extended\nlocking data; it comprises an array of Xids plus some flags bits for each one.\nThe flags are currently used to store the locking strength of each member\ntransaction.  (The flags also distinguish a pure locker from an updater.)\n\nIn earlier PostgreSQL releases, a MultiXact always meant that the tuple was\nlocked in shared mode by multiple transactions.  This is no longer the case; a\nMultiXact may contain an update or delete Xid.  (Keep in mind that tuple locks\nin a transaction do not conflict with other tuple locks in the same\ntransaction, so it's possible to have otherwise conflicting locks in a\nMultiXact if they belong to the same transaction).\n\nNote that each lock is attributed to the subtransaction that acquires it.\nThis means that a subtransaction that aborts is seen as though it releases the\nlocks it acquired; concurrent transactions can then proceed without having to\nwait for the main transaction to finish.  It also means that a subtransaction\ncan upgrade to a stronger lock level than an earlier transaction had, and if\nthe subxact aborts, the earlier, weaker lock is kept.\n\nThe possibility of having an update within a MultiXact means that they must\npersist across crashes and restarts: a future reader of the tuple needs to\nfigure out whether the update committed or aborted.  So we have a requirement\nthat pg_multixact needs to retain pages of its data until we're certain that\nthe MultiXacts in them are no longer of interest.\n\nVACUUM is in charge of removing old MultiXacts at the time of tuple freezing.\nThe lower bound used by vacuum (that is, the value below which all multixacts\nare removed) is stored as pg_class.relminmxid for each table; the minimum of\nall such values is stored in pg_database.datminmxid.  The minimum across\nall databases, in turn, is recorded in checkpoint records, and CHECKPOINT\nremoves pg_multixact/ segments older than that value once the checkpoint\nrecord has been flushed.\n\nInfomask Bits\n-------------\n\nThe following infomask bits are applicable:\n\n- HEAP_XMAX_INVALID\n  Any tuple with this bit set does not have a valid value stored in XMAX.\n\n- HEAP_XMAX_IS_MULTI\n  This bit is set if the tuple's Xmax is a MultiXactId (as opposed to a\n  regular TransactionId).\n\n- HEAP_XMAX_LOCK_ONLY\n  This bit is set when the XMAX is a locker only; that is, if it's a\n  multixact, it does not contain an update among its members.  It's set when\n  the XMAX is a plain Xid that locked the tuple, as well.\n\n- HEAP_XMAX_KEYSHR_LOCK\n- HEAP_XMAX_SHR_LOCK\n- HEAP_XMAX_EXCL_LOCK\n  These bits indicate the strength of the lock acquired; they are useful when\n  the XMAX is not a MultiXactId.  If it's a multi, the info is to be found in\n  the member flags.  If HEAP_XMAX_IS_MULTI is not set and HEAP_XMAX_LOCK_ONLY\n  is set, then one of these *must* be set as well.\n\n  Note that HEAP_XMAX_EXCL_LOCK does not distinguish FOR NO KEY UPDATE from\n  FOR UPDATE; this is implemented by the HEAP_KEYS_UPDATED bit.\n\n- HEAP_KEYS_UPDATED\n  This bit lives in t_infomask2.  If set, indicates that the operation(s) done\n  by the XMAX compromise the tuple key, such as a SELECT FOR UPDATE, an UPDATE\n  that modifies the columns of the key, or a DELETE.  It's set regardless of\n  whether the XMAX is a TransactionId or a MultiXactId.\n\nWe currently never set the HEAP_XMAX_COMMITTED when the HEAP_XMAX_IS_MULTI bit\nis set.\n\nLocking to write inplace-updated tables\n---------------------------------------\n\nIf IsInplaceUpdateRelation() returns true for a table, the table is a system\ncatalog that receives systable_inplace_update_begin() calls.  Preparing a\nheap_update() of these tables follows additional locking rules, to ensure we\ndon't lose the effects of an inplace update.  In particular, consider a moment\nwhen a backend has fetched the old tuple to modify, not yet having called\nheap_update().  Another backend's inplace update starting then can't conclude\nuntil the heap_update() places its new tuple in a buffer.  We enforce that\nusing locktags as follows.  While DDL code is the main audience, the executor\nfollows these rules to make e.g. \"MERGE INTO pg_class\" safer.  Locking rules\nare per-catalog:\n\n  pg_class systable_inplace_update_begin() callers: before the call, acquire a\n  lock on the relation in mode ShareUpdateExclusiveLock or stricter.  If the\n  update targets a row of RELKIND_INDEX (but not RELKIND_PARTITIONED_INDEX),\n  that lock must be on the table.  Locking the index rel is not necessary.\n  (This allows VACUUM to overwrite per-index pg_class while holding a lock on\n  the table alone.) systable_inplace_update_begin() acquires and releases\n  LOCKTAG_TUPLE in InplaceUpdateTupleLock, an alias for ExclusiveLock, on each\n  tuple it overwrites.\n\n  pg_class heap_update() callers: before copying the tuple to modify, take a\n  lock on the tuple, a ShareUpdateExclusiveLock on the relation, or a\n  ShareRowExclusiveLock or stricter on the relation.\n\n  SearchSysCacheLocked1() is one convenient way to acquire the tuple lock.\n  Most heap_update() callers already hold a suitable lock on the relation for\n  other reasons and can skip the tuple lock.  If you do acquire the tuple\n  lock, release it immediately after the update.\n\n\n  pg_database: before copying the tuple to modify, all updaters of pg_database\n  rows acquire LOCKTAG_TUPLE.  (Few updaters acquire LOCKTAG_OBJECT on the\n  database OID, so it wasn't worth extending that as a second option.)\n\nIdeally, DDL might want to perform permissions checks before LockTuple(), as\nwe do with RangeVarGetRelidExtended() callbacks.  We typically don't bother.\nLOCKTAG_TUPLE acquirers release it after each row, so the potential\ninconvenience is lower.\n\nReading inplace-updated columns\n-------------------------------\n\nInplace updates create an exception to the rule that tuple data won't change\nunder a reader holding a pin.  A reader of a heap_fetch() result tuple may\nwitness a torn read.  Current inplace-updated fields are aligned and are no\nwider than four bytes, and current readers don't need consistency across\nfields.  Hence, they get by with just fetching each field once.  XXX such a\ncaller may also read a value that has not reached WAL; see\nsystable_inplace_update_finish().",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\heap\\README.tuplock",
      "directory": "backend\\access\\heap"
    }
  },
  {
    "title": "README: backend\\access\\nbtree",
    "url": "backend\\access\\nbtree\\README",
    "content": "src/backend/access/nbtree/README\n\nBtree Indexing\n==============\n\nThis directory contains a correct implementation of Lehman and Yao's\nhigh-concurrency B-tree management algorithm (P. Lehman and S. Yao,\nEfficient Locking for Concurrent Operations on B-Trees, ACM Transactions\non Database Systems, Vol 6, No. 4, December 1981, pp 650-670).  We also\nuse a simplified version of the deletion logic described in Lanin and\nShasha (V. Lanin and D. Shasha, A Symmetric Concurrent B-Tree Algorithm,\nProceedings of 1986 Fall Joint Computer Conference, pp 380-389).\n\nThe basic Lehman & Yao Algorithm\n--------------------------------\n\nCompared to a classic B-tree, L&Y adds a right-link pointer to each page,\nto the page's right sibling.  It also adds a \"high key\" to each page, which\nis an upper bound on the keys that are allowed on that page.  These two\nadditions make it possible to detect a concurrent page split, which allows\nthe tree to be searched without holding any read locks (except to keep a\nsingle page from being modified while reading it).\n\nWhen a search follows a downlink to a child page, it compares the page's\nhigh key with the search key.  If the search key is greater than the high\nkey, the page must've been split concurrently, and you must follow the\nright-link to find the new page containing the key range you're looking\nfor.  This might need to be repeated, if the page has been split more than\nonce.\n\nLehman and Yao talk about alternating \"separator\" keys and downlinks in\ninternal pages rather than tuples or records.  We use the term \"pivot\"\ntuple to refer to tuples which don't point to heap tuples, that are used\nonly for tree navigation.  All tuples on non-leaf pages and high keys on\nleaf pages are pivot tuples.  Since pivot tuples are only used to represent\nwhich part of the key space belongs on each page, they can have attribute\nvalues copied from non-pivot tuples that were deleted and killed by VACUUM\nsome time ago.  A pivot tuple may contain a \"separator\" key and downlink,\njust a separator key (i.e. the downlink value is implicitly undefined), or\njust a downlink (i.e. all attributes are truncated away).\n\nThe requirement that all btree keys be unique is satisfied by treating heap\nTID as a tiebreaker attribute.  Logical duplicates are sorted in heap TID\norder.  This is necessary because Lehman and Yao also require that the key\nrange for a subtree S is described by Ki < v <= Ki+1 where Ki and Ki+1 are\nthe adjacent keys in the parent page (Ki must be _strictly_ less than v,\nwhich is assured by having reliably unique keys).  Keys are always unique\non their level, with the exception of a leaf page's high key, which can be\nfully equal to the last item on the page.\n\nThe Postgres implementation of suffix truncation must make sure that the\nLehman and Yao invariants hold, and represents that absent/truncated\nattributes in pivot tuples have the sentinel value \"minus infinity\".  The\nlater section on suffix truncation will be helpful if it's unclear how the\nLehman & Yao invariants work with a real world example.\n\nDifferences to the Lehman & Yao algorithm\n-----------------------------------------\n\nWe have made the following changes in order to incorporate the L&Y algorithm\ninto Postgres:\n\nLehman and Yao don't require read locks, but assume that in-memory\ncopies of tree pages are unshared.  Postgres shares in-memory buffers\namong backends.  As a result, we do page-level read locking on btree\npages in order to guarantee that no record is modified while we are\nexamining it.  This reduces concurrency but guarantees correct\nbehavior.\n\nWe support the notion of an ordered \"scan\" of an index as well as\ninsertions, deletions, and simple lookups.  A scan in the forward\ndirection is no problem, we just use the right-sibling pointers that\nL&Y require anyway.  (Thus, once we have descended the tree to the\ncorrect start point for the scan, the scan looks only at leaf pages\nand never at higher tree levels.)  To support scans in the backward\ndirection, we also store a \"left sibling\" link much like the \"right\nsibling\".  (This adds an extra step to the L&Y split algorithm: while\nholding the write lock on the page being split, we also lock its former\nright sibling to update that page's left-link.  This is safe since no\nwriter of that page can be interested in acquiring a write lock on our\npage.)  A backwards scan has one additional bit of complexity: after\nfollowing the left-link we must account for the possibility that the\nleft sibling page got split before we could read it.  So, we have to\nmove right until we find a page whose right-link matches the page we\ncame from.  (Actually, it's even harder than that; see page deletion\ndiscussion below.)\n\nPage read locks are held only for as long as a scan is examining a page.\nTo minimize lock/unlock traffic, an index scan always searches a leaf page\nto identify all the matching items at once, copying their heap tuple IDs\ninto backend-local storage.  The heap tuple IDs are then processed while\nnot holding any page lock within the index.  We do continue to hold a pin\non the leaf page in some circumstances, to protect against concurrent\ndeletions (see below).  In this state the scan is effectively stopped\n\"between\" pages, either before or after the page it has pinned.  This is\nsafe in the presence of concurrent insertions and even page splits, because\nitems are never moved across pre-existing page boundaries --- so the scan\ncannot miss any items it should have seen, nor accidentally return the same\nitem twice.  The scan must remember the page's right-link at the time it\nwas scanned, since that is the page to move right to; if we move right to\nthe current right-link then we'd re-scan any items moved by a page split.\nWe don't similarly remember the left-link, since it's best to use the most\nup-to-date left-link when trying to move left (see detailed move-left\nalgorithm below).\n\nIn most cases we release our lock and pin on a page before attempting\nto acquire pin and lock on the page we are moving to.  In a few places\nit is necessary to lock the next page before releasing the current one.\nThis is safe when moving right or up, but not when moving left or down\n(else we'd create the possibility of deadlocks).\n\nLehman and Yao fail to discuss what must happen when the root page\nbecomes full and must be split.  Our implementation is to split the\nroot in the same way that any other page would be split, then construct\na new root page holding pointers to both of the resulting pages (which\nnow become siblings on the next level of the tree).  The new root page\nis then installed by altering the root pointer in the meta-data page (see\nbelow).  This works because the root is not treated specially in any\nother way --- in particular, searches will move right using its link\npointer if the link is set.  Therefore, searches will find the data\nthat's been moved into the right sibling even if they read the meta-data\npage before it got updated.  This is the same reasoning that makes a\nsplit of a non-root page safe.  The locking considerations are similar too.\n\nWhen an inserter recurses up the tree, splitting internal pages to insert\nlinks to pages inserted on the level below, it is possible that it will\nneed to access a page above the level that was the root when it began its\ndescent (or more accurately, the level that was the root when it read the\nmeta-data page).  In this case the stack it made while descending does not\nhelp for finding the correct page.  When this happens, we find the correct\nplace by re-descending the tree until we reach the level one above the\nlevel we need to insert a link to, and then moving right as necessary.\n(Typically this will take only two fetches, the meta-data page and the new\nroot, but in principle there could have been more than one root split\nsince we saw the root.  We can identify the correct tree level by means of\nthe level numbers stored in each page.  The situation is rare enough that\nwe do not need a more efficient solution.)\n\nLehman and Yao must couple/chain locks as part of moving right when\nrelocating a child page's downlink during an ascent of the tree.  This is\nthe only point where Lehman and Yao have to simultaneously hold three\nlocks (a lock on the child, the original parent, and the original parent's\nright sibling).  We don't need to couple internal page locks for pages on\nthe same level, though.  We match a child's block number to a downlink\nfrom a pivot tuple one level up, whereas Lehman and Yao match on the\nseparator key associated with the downlink that was followed during the\ninitial descent.  We can release the lock on the original parent page\nbefore acquiring a lock on its right sibling, since there is never any\nneed to deal with the case where the separator key that we must relocate\nbecomes the original parent's high key.  Lanin and Shasha don't couple\nlocks here either, though they also don't couple locks between levels\nduring ascents.  They are willing to \"wait and try again\" to avoid races.\nTheir algorithm is optimistic, which means that \"an insertion holds no\nmore than one write lock at a time during its ascent\".  We more or less\nstick with Lehman and Yao's approach of conservatively coupling parent and\nchild locks when ascending the tree, since it's far simpler.\n\nLehman and Yao assume fixed-size keys, but we must deal with\nvariable-size keys.  Therefore there is not a fixed maximum number of\nkeys per page; we just stuff in as many as will fit.  When we split a\npage, we try to equalize the number of bytes, not items, assigned to\npages (though suffix truncation is also considered).  Note we must include\nthe incoming item in this calculation, otherwise it is possible to find\nthat the incoming item doesn't fit on the split page where it needs to go!\n\nDeleting index tuples during VACUUM\n-----------------------------------\n\nBefore deleting a leaf item, we get a full cleanup lock on the target\npage, so that no other backend has a pin on the page when the deletion\nstarts.  This is not necessary for correctness in terms of the btree index\noperations themselves; as explained above, index scans logically stop\n\"between\" pages and so can't lose their place.  The reason we do it is to\nprovide an interlock between VACUUM and index scans that are not prepared\nto deal with concurrent TID recycling when visiting the heap.  Since only\nVACUUM can ever mark pointed-to items LP_UNUSED in the heap, and since\nthis only ever happens _after_ btbulkdelete returns, having index scans\nhold on to the pin (used when reading from the leaf page) until _after_\nthey're done visiting the heap (for TIDs from pinned leaf page) prevents\nconcurrent TID recycling.  VACUUM cannot get a conflicting cleanup lock\nuntil the index scan is totally finished processing its leaf page.\n\nThis approach is fairly coarse, so we avoid it whenever possible.  In\npractice most index scans won't hold onto their pin, and so won't block\nVACUUM.  These index scans must deal with TID recycling directly, which is\nmore complicated and not always possible.  See later section on making\nconcurrent TID recycling safe.\n\nOpportunistic index tuple deletion performs almost the same page-level\nmodifications while only holding an exclusive lock.  This is safe because\nthere is no question of TID recycling taking place later on -- only VACUUM\ncan make TIDs recyclable.  See also simple deletion and bottom-up\ndeletion, below.\n\nBecause a pin is not always held, and a page can be split even while\nsomeone does hold a pin on it, it is possible that an indexscan will\nreturn items that are no longer stored on the page it has a pin on, but\nrather somewhere to the right of that page.  To ensure that VACUUM can't\nprematurely make TIDs recyclable in this scenario, we require btbulkdelete\nto obtain a cleanup lock on every leaf page in the index, even pages that\ndon't contain any deletable tuples.  Note that this requirement does not\nsay that btbulkdelete must visit the pages in any particular order.\n\nVACUUM's linear scan, concurrent page splits\n--------------------------------------------\n\nVACUUM accesses the index by doing a linear scan to search for deletable\nTIDs, while considering the possibility of deleting empty pages in\npassing.  This is in physical/block order, not logical/keyspace order.\nThe tricky part of this is avoiding missing any deletable tuples in the\npresence of concurrent page splits: a page split could easily move some\ntuples from a page not yet passed over by the sequential scan to a\nlower-numbered page already passed over.\n\nTo implement this, we provide a \"vacuum cycle ID\" mechanism that makes it\npossible to determine whether a page has been split since the current\nbtbulkdelete cycle started.  If btbulkdelete finds a page that has been\nsplit since it started, and has a right-link pointing to a lower page\nnumber, then it temporarily suspends its sequential scan and visits that\npage instead.  It must continue to follow right-links and vacuum dead\ntuples until reaching a page that either hasn't been split since\nbtbulkdelete started, or is above the location of the outer sequential\nscan.  Then it can resume the sequential scan.  This ensures that all\ntuples are visited.  It may be that some tuples are visited twice, but\nthat has no worse effect than an inaccurate index tuple count (and we\ncan't guarantee an accurate count anyway in the face of concurrent\nactivity).  Note that this still works if the has-been-recently-split test\nhas a small probability of false positives, so long as it never gives a\nfalse negative.  This makes it possible to implement the test with a small\ncounter value stored on each index page.\n\nDeleting entire pages during VACUUM\n-----------------------------------\n\nWe consider deleting an entire page from the btree only when it's become\ncompletely empty of items.  (Merging partly-full pages would allow better\nspace reuse, but it seems impractical to move existing data items left or\nright to make this happen --- a scan moving in the opposite direction\nmight miss the items if so.)  Also, we *never* delete the rightmost page\non a tree level (this restriction simplifies the traversal algorithms, as\nexplained below).  Page deletion always begins from an empty leaf page.  An\ninternal page can only be deleted as part of deleting an entire subtree.\nThis is always a \"skinny\" subtree consisting of a \"chain\" of internal pages\nplus a single leaf page.  There is one page on each level of the subtree,\nand each level/page covers the same key space.\n\nDeleting a leaf page is a two-stage process.  In the first stage, the page\nis unlinked from its parent, and marked as half-dead.  The parent page must\nbe found using the same type of search as used to find the parent during an\ninsertion split.  We lock the target and the parent pages, change the\ntarget's downlink to point to the right sibling, and remove its old\ndownlink.  This causes the target page's key space to effectively belong to\nits right sibling.  (Neither the left nor right sibling pages need to\nchange their \"high key\" if any; so there is no problem with possibly not\nhaving enough space to replace a high key.)  At the same time, we mark the\ntarget page as half-dead, which causes any subsequent searches to ignore it\nand move right (or left, in a backwards scan).  This leaves the tree in a\nsimilar state as during a page split: the page has no downlink pointing to\nit, but it's still linked to its siblings.\n\n(Note: Lanin and Shasha prefer to make the key space move left, but their\nargument for doing so hinges on not having left-links, which we have\nanyway.  So we simplify the algorithm by moving the key space right.  This\nis only possible because we don't match on a separator key when ascending\nthe tree during a page split, unlike Lehman and Yao/Lanin and Shasha -- it\ndoesn't matter if the downlink is re-found in a pivot tuple whose separator\nkey does not match the one encountered when inserter initially descended\nthe tree.)\n\nTo preserve consistency on the parent level, we cannot merge the key space\nof a page into its right sibling unless the right sibling is a child of\nthe same parent --- otherwise, the parent's key space assignment changes\ntoo, meaning we'd have to make bounding-key updates in its parent, and\nperhaps all the way up the tree.  Since we can't possibly do that\natomically, we forbid this case.  That means that the rightmost child of a\nparent node can't be deleted unless it's the only remaining child, in which\ncase we will delete the parent too (see below).\n\nIn the second-stage, the half-dead leaf page is unlinked from its siblings.\nWe first lock the left sibling (if any) of the target, the target page\nitself, and its right sibling (there must be one) in that order.  Then we\nupdate the side-links in the siblings, and mark the target page deleted.\n\nWhen we're about to delete the last remaining child of a parent page, things\nare slightly more complicated.  In the first stage, we leave the immediate\nparent of the leaf page alone, and remove the downlink to the parent page\ninstead, from the grandparent.  If it's the last child of the grandparent\ntoo, we recurse up until we find a parent with more than one child, and\nremove the downlink of that page.  The leaf page is marked as half-dead, and\nthe block number of the page whose downlink was removed is stashed in the\nhalf-dead leaf page.  This leaves us with a chain of internal pages, with\none downlink each, leading to the half-dead leaf page, and no downlink\npointing to the topmost page in the chain.\n\nWhile we recurse up to find the topmost parent in the chain, we keep the\nleaf page locked, but don't need to hold locks on the intermediate pages\nbetween the leaf and the topmost parent -- insertions into upper tree levels\nhappen only as a result of splits of child pages, and that can't happen as\nlong as we're keeping the leaf locked.  The internal pages in the chain\ncannot acquire new children afterwards either, because the leaf page is\nmarked as half-dead and won't be split.\n\nRemoving the downlink to the top of the to-be-deleted subtree/chain\neffectively transfers the key space to the right sibling for all the\nintermediate levels too, in one atomic operation.  A concurrent search might\nstill visit the intermediate pages, but it will move right when it reaches\nthe half-dead page at the leaf level.  In particular, the search will move to\nthe subtree to the right of the half-dead leaf page/to-be-deleted subtree,\nsince the half-dead leaf page's right sibling must be a \"cousin\" page, not a\n\"true\" sibling page (or a second cousin page when the to-be-deleted chain\nstarts at leaf page's grandparent page, and so on).\n\nIn the second stage, the topmost page in the chain is unlinked from its\nsiblings, and the half-dead leaf page is updated to point to the next page\ndown in the chain.  This is repeated until there are no internal pages left\nin the chain.  Finally, the half-dead leaf page itself is unlinked from its\nsiblings.\n\nA deleted page cannot be recycled immediately, since there may be other\nprocesses waiting to reference it (ie, search processes that just left the\nparent, or scans moving right or left from one of the siblings).  These\nprocesses must be able to observe a deleted page for some time after the\ndeletion operation, in order to be able to at least recover from it (they\nrecover by moving right, as with concurrent page splits).  Searchers never\nhave to worry about concurrent page recycling.\n\nSee \"Placing deleted pages in the FSM\" section below for a description of\nwhen and how deleted pages become safe for VACUUM to make recyclable.\n\nPage deletion and backwards scans\n---------------------------------\n\nMoving left in a backward scan is complicated because we must consider\nthe possibility that the left sibling was just split (meaning we must find\nthe rightmost page derived from the left sibling), plus the possibility\nthat the page we were just on has now been deleted and hence isn't in the\nsibling chain at all anymore.  So the move-left algorithm becomes:\n\n0. Remember the page we are on as the \"original page\".\n1. Follow the original page's left-link (we're done if this is zero).\n2. If the current page is live and its right-link matches the \"original\n   page\", we are done.\n3. Otherwise, move right one or more times looking for a live page whose\n   right-link matches the \"original page\".  If found, we are done.  (In\n   principle we could scan all the way to the right end of the index, but\n   in practice it seems better to give up after a small number of tries.\n   It's unlikely the original page's sibling split more than a few times\n   while we were in flight to it; if we do not find a matching link in a\n   few tries, then most likely the original page is deleted.)\n4. Return to the \"original page\".  If it is still live, return to step 1\n   (we guessed wrong about it being deleted, and should restart with its\n   current left-link).  If it is dead, move right until a non-dead page\n   is found (there must be one, since rightmost pages are never deleted),\n   mark that as the new \"original page\", and return to step 1.\n\nThis algorithm is correct because the live page found by step 4 will have\nthe same left keyspace boundary as the page we started from.  Therefore,\nwhen we ultimately exit, it must be on a page whose right keyspace\nboundary matches the left boundary of where we started --- which is what\nwe need to be sure we don't miss or re-scan any items.\n\nPage deletion and tree height\n-----------------------------\n\nBecause we never delete the rightmost page of any level (and in particular\nnever delete the root), it's impossible for the height of the tree to\ndecrease.  After massive deletions we might have a scenario in which the\ntree is \"skinny\", with several single-page levels below the root.\nOperations will still be correct in this case, but we'd waste cycles\ndescending through the single-page levels.  To handle this we use an idea\nfrom Lanin and Shasha: we keep track of the \"fast root\" level, which is\nthe lowest single-page level.  The meta-data page keeps a pointer to this\nlevel as well as the true root.  All ordinary operations initiate their\nsearches at the fast root not the true root.  When we split a page that is\nalone on its level or delete the next-to-last page on a level (both cases\nare easily detected), we have to make sure that the fast root pointer is\nadjusted appropriately.  In the split case, we do this work as part of the\natomic update for the insertion into the parent level; in the delete case\nas part of the atomic update for the delete (either way, the metapage has\nto be the last page locked in the update to avoid deadlock risks).  This\navoids race conditions if two such operations are executing concurrently.\n\nPlacing deleted pages in the FSM\n--------------------------------\n\nRecycling a page is decoupled from page deletion.  A deleted page can only\nbe put in the FSM to be recycled once there is no possible scan or search\nthat has a reference to it; until then, it must stay in place with its\nsibling links undisturbed, as a tombstone that allows concurrent searches\nto detect and then recover from concurrent deletions (which are rather\nlike concurrent page splits to searchers).  This design is an\nimplementation of what Lanin and Shasha call \"the drain technique\".\n\nWe implement the technique by waiting until all active snapshots and\nregistered snapshots as of the page deletion are gone; which is overly\nstrong, but is simple to implement within Postgres.  When marked fully\ndead, a deleted page is labeled with the next-transaction counter value.\nVACUUM can reclaim the page for re-use when the stored XID is guaranteed\nto be \"visible to everyone\".  As collateral damage, we wait for snapshots\ntaken until the next transaction to allocate an XID commits.  We also wait\nfor running XIDs with no snapshots.\n\nPrior to PostgreSQL 14, VACUUM would only place _old_ deleted pages that\nit encounters during its linear scan (pages deleted by a previous VACUUM\noperation) in the FSM.  Newly deleted pages were never placed in the FSM,\nbecause that was assumed to _always_ be unsafe.  That assumption was\nunnecessarily pessimistic in practice, though -- it often doesn't take\nvery long for newly deleted pages to become safe to place in the FSM.\nThere is no truly principled way to predict when deleted pages will become\nsafe to place in the FSM for recycling -- it might become safe almost\nimmediately (long before the current VACUUM completes), or it might not\neven be safe by the time the next VACUUM takes place.  Recycle safety is\npurely a question of maintaining the consistency (or at least the apparent\nconsistency) of a physical data structure.  The state within the backend\nrunning VACUUM is simply not relevant.\n\nPostgreSQL 14 added the ability for VACUUM to consider if it's possible to\nrecycle newly deleted pages at the end of the full index scan where the\npage deletion took place.  It is convenient to check if it's safe at that\npoint.  This does require that VACUUM keep around a little bookkeeping\ninformation about newly deleted pages, but that's very cheap.  Using\nin-memory state for this avoids the need to revisit newly deleted pages a\nsecond time later on -- we can just use safexid values from the local\nbookkeeping state to determine recycle safety in a deferred fashion.\n\nThe need for additional FSM indirection after a page deletion operation\ntakes place is a natural consequence of the highly permissive rules for\nindex scans with Lehman and Yao's design.  In general an index scan\ndoesn't have to hold a lock or even a pin on any page when it descends the\ntree (nothing that you'd usually think of as an interlock is held \"between\nlevels\").  At the same time, index scans cannot be allowed to land on a\ntruly unrelated page due to concurrent recycling (not to be confused with\nconcurrent deletion), because that results in wrong answers to queries.\nSimpler approaches to page deletion that don't need to defer recycling are\npossible, but none seem compatible with Lehman and Yao's design.\n\nPlacing an already-deleted page in the FSM to be recycled when needed\ndoesn't actually change the state of the page.  The page will be changed\nwhenever it is subsequently taken from the FSM for reuse.  The deleted\npage's contents will be overwritten by the split operation (it will become\nthe new right sibling page).\n\nMaking concurrent TID recycling safe\n------------------------------------\n\nAs explained in the earlier section about deleting index tuples during\nVACUUM, we implement a locking protocol that allows individual index scans\nto avoid concurrent TID recycling.  Index scans opt-out (and so drop their\nleaf page pin when visiting the heap) whenever it's safe to do so, though.\nDropping the pin early is useful because it avoids blocking progress by\nVACUUM.  This is particularly important with index scans used by cursors,\nsince idle cursors sometimes stop for relatively long periods of time.  In\nextreme cases, a client application may hold on to an idle cursors for\nhours or even days.  Blocking VACUUM for that long could be disastrous.\n\nIndex scans that don't hold on to a buffer pin are protected by holding an\nMVCC snapshot instead.  This more limited interlock prevents wrong answers\nto queries, but it does not prevent concurrent TID recycling itself (only\nholding onto the leaf page pin while accessing the heap ensures that).\n\nIndex-only scans can never drop their buffer pin, since they are unable to\ntolerate having a referenced TID become recyclable.  Index-only scans\ntypically just visit the visibility map (not the heap proper), and so will\nnot reliably notice that any stale TID reference (for a TID that pointed\nto a dead-to-all heap item at first) was concurrently marked LP_UNUSED in\nthe heap by VACUUM.  This could easily allow VACUUM to set the whole heap\npage to all-visible in the visibility map immediately afterwards.  An MVCC\nsnapshot is only sufficient to avoid problems during plain index scans\nbecause they must access granular visibility information from the heap\nproper.  A plain index scan will even recognize LP_UNUSED items in the\nheap (items that could be recycled but haven't been just yet) as \"not\nvisible\" -- even when the heap page is generally considered all-visible.\n\nLP_DEAD setting of index tuples by the kill_prior_tuple optimization\n(described in full in simple deletion, below) is also more complicated for\nindex scans that drop their leaf page pins.  We must be careful to avoid\nLP_DEAD-marking any new index tuple that looks like a known-dead index\ntuple because it happens to share the same TID, following concurrent TID\nrecycling.  It's just about possible that some other session inserted a\nnew, unrelated index tuple, on the same leaf page, which has the same\noriginal TID.  It would be totally wrong to LP_DEAD-set this new,\nunrelated index tuple.\n\nWe handle this kill_prior_tuple race condition by having affected index\nscans conservatively assume that any change to the leaf page at all\nimplies that it was reached by btbulkdelete in the interim period when no\nbuffer pin was held.  This is implemented by not setting any LP_DEAD bits\non the leaf page at all when the page's LSN has changed.  (That won't work\nwith an unlogged index, so for now we don't ever apply the \"don't hold\nonto pin\" optimization there.)\n\nFastpath For Index Insertion\n----------------------------\n\nWe optimize for a common case of insertion of increasing index key\nvalues by caching the last page to which this backend inserted the last\nvalue, if this page was the rightmost leaf page. For the next insert, we\ncan then quickly check if the cached page is still the rightmost leaf\npage and also the correct place to hold the current value. We can avoid\nthe cost of walking down the tree in such common cases.\n\nThe optimization works on the assumption that there can only be one\nnon-ignorable leaf rightmost page, and so not even a visible-to-everyone\nstyle interlock is required.  We cannot fail to detect that our hint was\ninvalidated, because there can only be one such page in the B-Tree at\nany time. It's possible that the page will be deleted and recycled\nwithout a backend's cached page also being detected as invalidated, but\nonly when we happen to recycle a block that once again gets recycled as the\nrightmost leaf page.\n\nSimple deletion\n---------------\n\nIf a process visits a heap tuple and finds that it's dead and removable\n(ie, dead to all open transactions, not only that process), then we can\nreturn to the index and mark the corresponding index entry \"known dead\",\nallowing subsequent index scans to skip visiting the heap tuple.  The\n\"known dead\" marking works by setting the index item's lp_flags state\nto LP_DEAD.  This is currently only done in plain indexscans, not bitmap\nscans, because only plain scans visit the heap and index \"in sync\" and so\nthere's not a convenient way to do it for bitmap scans.  Note also that\nLP_DEAD bits are often set when checking a unique index for conflicts on\ninsert (this is simpler because it takes place when we hold an exclusive\nlock on the leaf page).\n\nOnce an index tuple has been marked LP_DEAD it can actually be deleted\nfrom the index immediately; since index scans only stop \"between\" pages,\nno scan can lose its place from such a deletion.  We separate the steps\nbecause we allow LP_DEAD to be set with only a share lock (it's like a\nhint bit for a heap tuple), but physically deleting tuples requires an\nexclusive lock.  We also need to generate a snapshotConflictHorizon for\neach deletion operation's WAL record, which requires additional\ncoordinating with the tableam when the deletion actually takes place.\n(snapshotConflictHorizon value may be used to generate a conflict during\nsubsequent REDO of the record by a standby.)\n\nDelaying and batching index tuple deletion like this enables a further\noptimization: opportunistic checking of \"extra\" nearby index tuples\n(tuples that are not LP_DEAD-set) when they happen to be very cheap to\ncheck in passing (because we already know that the tableam will be\nvisiting their table block to generate a snapshotConflictHorizon).  Any\nindex tuples that turn out to be safe to delete will also be deleted.\nSimple deletion will behave as if the extra tuples that actually turn\nout to be delete-safe had their LP_DEAD bits set right from the start.\n\nDeduplication can also prevent a page split, but index tuple deletion is\nour preferred approach.  Note that posting list tuples can only have\ntheir LP_DEAD bit set when every table TID within the posting list is\nknown dead.  This isn't much of a problem in practice because LP_DEAD\nbits are just a starting point for deletion.  What really matters is\nthat _some_ deletion operation that targets related nearby-in-table TIDs\ntakes place at some point before the page finally splits.  That's all\nthat's required for the deletion process to perform granular removal of\ngroups of dead TIDs from posting list tuples (without the situation ever\nbeing allowed to get out of hand).\n\nBottom-Up deletion\n------------------\n\nWe attempt to delete whatever duplicates happen to be present on the page\nwhen the duplicates are suspected to be caused by version churn from\nsuccessive UPDATEs.  This only happens when we receive an executor hint\nindicating that optimizations like heapam's HOT have not worked out for\nthe index -- the incoming tuple must be a logically unchanged duplicate\nwhich is needed for MVCC purposes, suggesting that that might well be the\ndominant source of new index tuples on the leaf page in question.  (Also,\nbottom-up deletion is triggered within unique indexes in cases with\ncontinual INSERT and DELETE related churn, since that is easy to detect\nwithout any external hint.)\n\nSimple deletion will already have failed to prevent a page split when a\nbottom-up deletion pass takes place (often because no LP_DEAD bits were\never set on the page).  The two mechanisms have closely related\nimplementations.  The same WAL records are used for each operation, and\nthe same tableam infrastructure is used to determine what TIDs/tuples are\nactually safe to delete.  The implementations only differ in how they pick\nTIDs to consider for deletion, and whether or not the tableam will give up\nbefore accessing all table blocks (bottom-up deletion lives with the\nuncertainty of its success by keeping the cost of failure low).  Even\nstill, the two mechanisms are clearly distinct at the conceptual level.\n\nBottom-up index deletion is driven entirely by heuristics (whereas simple\ndeletion is guaranteed to delete at least those index tuples that are\nalready LP_DEAD marked -- there must be at least one).  We have no\ncertainty that we'll find even one index tuple to delete.  That's why we\nclosely cooperate with the tableam to keep the costs it pays in balance\nwith the benefits we receive.  The interface that we use for this is\ndescribed in detail in access/tableam.h.\n\nBottom-up index deletion can be thought of as a backstop mechanism against\nunnecessary version-driven page splits.  It is based in part on an idea\nfrom generational garbage collection: the \"generational hypothesis\".  This\nis the empirical observation that \"most objects die young\".  Within\nnbtree, new index tuples often quickly appear in the same place, and then\nquickly become garbage.  There can be intense concentrations of garbage in\nrelatively few leaf pages with certain workloads (or there could be in\nearlier versions of PostgreSQL without bottom-up index deletion, at\nleast).  See doc/src/sgml/btree.sgml for a high-level description of the\ndesign principles behind bottom-up index deletion in nbtree, including\ndetails of how it complements VACUUM.\n\nWe expect to find a reasonably large number of tuples that are safe to\ndelete within each bottom-up pass.  If we don't then we won't need to\nconsider the question of bottom-up deletion for the same leaf page for\nquite a while (usually because the page splits, which resolves the\nsituation for the time being).  We expect to perform regular bottom-up\ndeletion operations against pages that are at constant risk of unnecessary\npage splits caused only by version churn.  When the mechanism works well\nwe'll constantly be \"on the verge\" of having version-churn-driven page\nsplits, but never actually have even one.\n\nOur duplicate heuristics work well despite being fairly simple.\nUnnecessary page splits only occur when there are truly pathological\nlevels of version churn (in theory a small amount of version churn could\nmake a page split occur earlier than strictly necessary, but that's pretty\nharmless).  We don't have to understand the underlying workload; we only\nhave to understand the general nature of the pathology that we target.\nVersion churn is easy to spot when it is truly pathological.  Affected\nleaf pages are fairly homogeneous.\n\nWAL Considerations\n------------------\n\nThe insertion and deletion algorithms in themselves don't guarantee btree\nconsistency after a crash.  To provide robustness, we depend on WAL\nreplay.  A single WAL entry is effectively an atomic action --- we can\nredo it from the log if it fails to complete.\n\nOrdinary item insertions (that don't force a page split) are of course\nsingle WAL entries, since they only affect one page.  The same for\nleaf-item deletions (if the deletion brings the leaf page to zero items,\nit is now a candidate to be deleted, but that is a separate action).\n\nAn insertion that causes a page split is logged as a single WAL entry for\nthe changes occurring on the insertion's level --- including update of the\nright sibling's left-link --- followed by a second WAL entry for the\ninsertion on the parent level (which might itself be a page split, requiring\nan additional insertion above that, etc).\n\nFor a root split, the follow-on WAL entry is a \"new root\" entry rather than\nan \"insertion\" entry, but details are otherwise much the same.\n\nBecause splitting involves multiple atomic actions, it's possible that the\nsystem crashes between splitting a page and inserting the downlink for the\nnew half to the parent.  After recovery, the downlink for the new page will\nbe missing.  The search algorithm works correctly, as the page will be found\nby following the right-link from its left sibling, although if a lot of\ndownlinks in the tree are missing, performance will suffer.  A more serious\nconsequence is that if the page without a downlink gets split again, the\ninsertion algorithm will fail to find the location in the parent level to\ninsert the downlink.\n\nOur approach is to create any missing downlinks on-the-fly, when searching\nthe tree for a new insertion.  It could be done during searches, too, but\nit seems best not to put any extra updates in what would otherwise be a\nread-only operation (updating is not possible in hot standby mode anyway).\nIt would seem natural to add the missing downlinks in VACUUM, but since\ninserting a downlink might require splitting a page, it might fail if you\nrun out of disk space.  That would be bad during VACUUM - the reason for\nrunning VACUUM in the first place might be that you run out of disk space,\nand now VACUUM won't finish because you're out of disk space.  In contrast,\nan insertion can require enlarging the physical file anyway.  There is one\nminor exception: VACUUM finishes interrupted splits of internal pages when\ndeleting their children.  This allows the code for re-finding parent items\nto be used by both page splits and page deletion.\n\nTo identify missing downlinks, when a page is split, the left page is\nflagged to indicate that the split is not yet complete (INCOMPLETE_SPLIT).\nWhen the downlink is inserted to the parent, the flag is cleared atomically\nwith the insertion.  The child page is kept locked until the insertion in\nthe parent is finished and the flag in the child cleared, but can be\nreleased immediately after that, before recursing up the tree if the parent\nalso needs to be split.  This ensures that incompletely split pages should\nnot be seen under normal circumstances; only if insertion to the parent\nhas failed for some reason. (It's also possible for a reader to observe\na page with the incomplete split flag set during recovery; see later\nsection on \"Scans during Recovery\" for details.)\n\nWe flag the left page, even though it's the right page that's missing the\ndownlink, because it's more convenient to know already when following the\nright-link from the left page to the right page that it will need to have\nits downlink inserted to the parent.\n\nWhen splitting a non-root page that is alone on its level, the required\nmetapage update (of the \"fast root\" link) is performed and logged as part\nof the insertion into the parent level.  When splitting the root page, the\nmetapage update is handled as part of the \"new root\" action.\n\nEach step in page deletion is logged as a separate WAL entry: marking the\nleaf as half-dead and removing the downlink is one record, and unlinking a\npage is a second record.  If vacuum is interrupted for some reason, or the\nsystem crashes, the tree is consistent for searches and insertions.  The\nnext VACUUM will find the half-dead leaf page and continue the deletion.\n\nBefore 9.4, we used to keep track of incomplete splits and page deletions\nduring recovery and finish them immediately at end of recovery, instead of\ndoing it lazily at the next insertion or vacuum.  However, that made the\nrecovery much more complicated, and only fixed the problem when crash\nrecovery was performed.  An incomplete split can also occur if an otherwise\nrecoverable error, like out-of-memory or out-of-disk-space, happens while\ninserting the downlink to the parent.\n\nScans during Recovery\n---------------------\n\nnbtree indexes support read queries in Hot Standby mode. Every atomic\naction/WAL record makes isolated changes that leave the tree in a\nconsistent state for readers. Readers lock pages according to the same\nrules that readers follow on the primary. (Readers may have to move\nright to recover from a \"concurrent\" page split or page deletion, just\nlike on the primary.)\n\nHowever, there are a couple of differences in how pages are locked by\nreplay/the startup process as compared to the original write operation\non the primary. The exceptions involve page splits and page deletions.\nThe first phase and second phase of a page split are processed\nindependently during replay, since they are independent atomic actions.\nWe do not attempt to recreate the coupling of parent and child page\nwrite locks that took place on the primary. This is safe because readers\nnever care about the incomplete split flag anyway. Holding on to an\nextra write lock on the primary is only necessary so that a second\nwriter cannot observe the incomplete split flag before the first writer\nfinishes the split. If we let concurrent writers on the primary observe\nan incomplete split flag on the same page, each writer would attempt to\ncomplete the unfinished split, corrupting the parent page.  (Similarly,\nreplay of page deletion records does not hold a write lock on the target\nleaf page throughout; only the primary needs to block out concurrent\nwriters that insert on to the page being deleted.)\n\nWAL replay holds same-level locks in a way that matches the approach\ntaken during original execution, though. This prevent readers from\nobserving same-level inconsistencies. It's probably possible to be more\nlax about how same-level locks are acquired during recovery (most kinds\nof readers could still move right to recover if we didn't couple\nsame-level locks), but we prefer to be conservative here.\n\nDuring recovery all index scans start with ignore_killed_tuples = false\nand we never set kill_prior_tuple. We do this because the oldest xmin\non the standby server can be older than the oldest xmin on the primary\nserver, which means tuples can be marked LP_DEAD even when they are\nstill visible on the standby. We don't WAL log tuple LP_DEAD bits, but\nthey can still appear in the standby because of full page writes. So\nwe must always ignore them in standby, and that means it's not worth\nsetting them either.  (When LP_DEAD-marked tuples are eventually deleted\non the primary, the deletion is WAL-logged.  Queries that run on a\nstandby therefore get much of the benefit of any LP_DEAD setting that\ntakes place on the primary.)\n\nNote that we talk about scans that are started during recovery. We go to\na little trouble to allow a scan to start during recovery and end during\nnormal running after recovery has completed. This is a key capability\nbecause it allows running applications to continue while the standby\nchanges state into a normally running server.\n\nThe interlocking required to avoid returning incorrect results from\nnon-MVCC scans is not required on standby nodes. We still get a full\ncleanup lock when replaying VACUUM records during recovery, but recovery\ndoes not need to lock every leaf page (only those leaf pages that have\nitems to delete) -- that's sufficient to avoid breaking index-only scans\nduring recovery (see section above about making TID recycling safe). That\nleaves concern only for plain index scans. (XXX: Not actually clear why\nthis is totally unnecessary during recovery.)\n\nMVCC snapshot plain index scans are always safe, for the same reasons that\nthey're safe during original execution.  HeapTupleSatisfiesToast() doesn't\nuse MVCC semantics, though that's because it doesn't need to - if the main\nheap row is visible then the toast rows will also be visible. So as long\nas we follow a toast pointer from a visible (live) tuple the corresponding\ntoast rows will also be visible, so we do not need to recheck MVCC on\nthem.\n\nOther Things That Are Handy to Know\n-----------------------------------\n\nPage zero of every btree is a meta-data page.  This page stores the\nlocation of the root page --- both the true root and the current effective\nroot (\"fast\" root).  To avoid fetching the metapage for every single index\nsearch, we cache a copy of the meta-data information in the index's\nrelcache entry (rd_amcache).  This is a bit ticklish since using the cache\nimplies following a root page pointer that could be stale.  However, a\nbackend following a cached pointer can sufficiently verify whether it\nreached the intended page; either by checking the is-root flag when it\nis going to the true root, or by checking that the page has no siblings\nwhen going to the fast root.  At worst, this could result in descending\nsome extra tree levels if we have a cached pointer to a fast root that is\nnow above the real fast root.  Such cases shouldn't arise often enough to\nbe worth optimizing; and in any case we can expect a relcache flush will\ndiscard the cached metapage before long, since a VACUUM that's moved the\nfast root pointer can be expected to issue a statistics update for the\nindex.\n\nThe algorithm assumes we can fit at least three items per page\n(a \"high key\" and two real data items).  Therefore it's unsafe\nto accept items larger than 1/3rd page size.  Larger items would\nwork sometimes, but could cause failures later on depending on\nwhat else gets put on their page.\n\n\"ScanKey\" data structures are used in two fundamentally different ways\nin this code, which we describe as \"search\" scankeys and \"insertion\"\nscankeys.  A search scankey is the kind passed to btbeginscan() or\nbtrescan() from outside the btree code.  The sk_func pointers in a search\nscankey point to comparison functions that return boolean, such as int4lt.\nThere might be more than one scankey entry for a given index column, or\nnone at all.  (We require the keys to appear in index column order, but\nthe order of multiple keys for a given column is unspecified.)  An\ninsertion scankey (\"BTScanInsert\" data structure) uses a similar\narray-of-ScanKey data structure, but the sk_func pointers point to btree\ncomparison support functions (ie, 3-way comparators that return int4 values\ninterpreted as <0, =0, >0).  In an insertion scankey there is at most one\nentry per index column.  There is also other data about the rules used to\nlocate where to begin the scan, such as whether or not the scan is a\n\"nextkey\" scan.  Insertion scankeys are built within the btree code (eg, by\n_bt_mkscankey()) and are used to locate the starting point of a scan, as\nwell as for locating the place to insert a new index tuple.  (Note: in the\ncase of an insertion scankey built from a search scankey or built from a\ntruncated pivot tuple, there might be fewer keys than index columns,\nindicating that we have no constraints for the remaining index columns.)\nAfter we have located the starting point of a scan, the original search\nscankey is consulted as each index entry is sequentially scanned to decide\nwhether to return the entry and whether the scan can stop (see\n_bt_checkkeys()).\n\nNotes about suffix truncation\n-----------------------------\n\nWe truncate away suffix key attributes that are not needed for a page high\nkey during a leaf page split.  The remaining attributes must distinguish\nthe last index tuple on the post-split left page as belonging on the left\npage, and the first index tuple on the post-split right page as belonging\non the right page.  Tuples logically retain truncated key attributes,\nthough they implicitly have \"negative infinity\" as their value, and have no\nstorage overhead.  Since the high key is subsequently reused as the\ndownlink in the parent page for the new right page, suffix truncation makes\npivot tuples short.  INCLUDE indexes are guaranteed to have non-key\nattributes truncated at the time of a leaf page split, but may also have\nsome key attributes truncated away, based on the usual criteria for key\nattributes.  They are not a special case, since non-key attributes are\nmerely payload to B-Tree searches.\n\nThe goal of suffix truncation of key attributes is to improve index\nfan-out.  The technique was first described by Bayer and Unterauer (R.Bayer\nand K.Unterauer, Prefix B-Trees, ACM Transactions on Database Systems, Vol\n2, No. 1, March 1977, pp 11-26).  The Postgres implementation is loosely\nbased on their paper.  Note that Postgres only implements what the paper\nrefers to as simple prefix B-Trees.  Note also that the paper assumes that\nthe tree has keys that consist of single strings that maintain the \"prefix\nproperty\", much like strings that are stored in a suffix tree (comparisons\nof earlier bytes must always be more significant than comparisons of later\nbytes, and, in general, the strings must compare in a way that doesn't\nbreak transitive consistency as they're split into pieces).  Suffix\ntruncation in Postgres currently only works at the whole-attribute\ngranularity, but it would be straightforward to invent opclass\ninfrastructure that manufactures a smaller attribute value in the case of\nvariable-length types, such as text.  An opclass support function could\nmanufacture the shortest possible key value that still correctly separates\neach half of a leaf page split.\n\nThere is sophisticated criteria for choosing a leaf page split point.  The\ngeneral idea is to make suffix truncation effective without unduly\ninfluencing the balance of space for each half of the page split.  The\nchoice of leaf split point can be thought of as a choice among points\n*between* items on the page to be split, at least if you pretend that the\nincoming tuple was placed on the page already (you have to pretend because\nthere won't actually be enough space for it on the page).  Choosing the\nsplit point between two index tuples where the first non-equal attribute\nappears as early as possible results in truncating away as many suffix\nattributes as possible.  Evenly balancing space among each half of the\nsplit is usually the first concern, but even small adjustments in the\nprecise split point can allow truncation to be far more effective.\n\nSuffix truncation is primarily valuable because it makes pivot tuples\nsmaller, which delays splits of internal pages, but that isn't the only\nreason why it's effective.  Even truncation that doesn't make pivot tuples\nsmaller due to alignment still prevents pivot tuples from being more\nrestrictive than truly necessary in how they describe which values belong\non which pages.\n\nWhile it's not possible to correctly perform suffix truncation during\ninternal page splits, it's still useful to be discriminating when splitting\nan internal page.  The split point that implies a downlink be inserted in\nthe parent that's the smallest one available within an acceptable range of\nthe fillfactor-wise optimal split point is chosen.  This idea also comes\nfrom the Prefix B-Tree paper.  This process has much in common with what\nhappens at the leaf level to make suffix truncation effective.  The overall\neffect is that suffix truncation tends to produce smaller, more\ndiscriminating pivot tuples, especially early in the lifetime of the index,\nwhile biasing internal page splits makes the earlier, smaller pivot tuples\nend up in the root page, delaying root page splits.\n\nLogical duplicates are given special consideration.  The logic for\nselecting a split point goes to great lengths to avoid having duplicates\nspan more than one page, and almost always manages to pick a split point\nbetween two user-key-distinct tuples, accepting a completely lopsided split\nif it must.  When a page that's already full of duplicates must be split,\nthe fallback strategy assumes that duplicates are mostly inserted in\nascending heap TID order.  The page is split in a way that leaves the left\nhalf of the page mostly full, and the right half of the page mostly empty.\nThe overall effect is that leaf page splits gracefully adapt to inserts of\nlarge groups of duplicates, maximizing space utilization.  Note also that\n\"trapping\" large groups of duplicates on the same leaf page like this makes\ndeduplication more efficient.  Deduplication can be performed infrequently,\nwithout merging together existing posting list tuples too often.\n\nNotes about deduplication\n-------------------------\n\nWe deduplicate non-pivot tuples in non-unique indexes to reduce storage\noverhead, and to avoid (or at least delay) page splits.  Note that the\ngoals for deduplication in unique indexes are rather different; see later\nsection for details.  Deduplication alters the physical representation of\ntuples without changing the logical contents of the index, and without\nadding overhead to read queries.  Non-pivot tuples are merged together\ninto a single physical tuple with a posting list (a simple array of heap\nTIDs with the standard item pointer format).  Deduplication is always\napplied lazily, at the point where it would otherwise be necessary to\nperform a page split.  It occurs only when LP_DEAD items have been\nremoved, as our last line of defense against splitting a leaf page\n(bottom-up index deletion may be attempted first, as our second last line\nof defense).  We can set the LP_DEAD bit with posting list tuples, though\nonly when all TIDs are known dead.\n\nOur lazy approach to deduplication allows the page space accounting used\nduring page splits to have absolutely minimal special case logic for\nposting lists.  Posting lists can be thought of as extra payload that\nsuffix truncation will reliably truncate away as needed during page\nsplits, just like non-key columns from an INCLUDE index tuple.\nIncoming/new tuples can generally be treated as non-overlapping plain\nitems (though see section on posting list splits for information about how\noverlapping new/incoming items are really handled).\n\nThe representation of posting lists is almost identical to the posting\nlists used by GIN, so it would be straightforward to apply GIN's varbyte\nencoding compression scheme to individual posting lists.  Posting list\ncompression would break the assumptions made by posting list splits about\npage space accounting (see later section), so it's not clear how\ncompression could be integrated with nbtree.  Besides, posting list\ncompression does not offer a compelling trade-off for nbtree, since in\ngeneral nbtree is optimized for consistent performance with many\nconcurrent readers and writers.  Compression would also make the deletion\nof a subset of TIDs from a posting list slow and complicated, which would\nbe a big problem for workloads that depend heavily on bottom-up index\ndeletion.\n\nA major goal of our lazy approach to deduplication is to limit the\nperformance impact of deduplication with random updates.  Even concurrent\nappend-only inserts of the same key value will tend to have inserts of\nindividual index tuples in an order that doesn't quite match heap TID\norder.  Delaying deduplication minimizes page level fragmentation.\n\nDeduplication in unique indexes\n-------------------------------\n\nVery often, the number of distinct values that can ever be placed on\nalmost any given leaf page in a unique index is fixed and permanent.  For\nexample, a primary key on an identity column will usually only have leaf\npage splits caused by the insertion of new logical rows within the\nrightmost leaf page.  If there is a split of a non-rightmost leaf page,\nthen the split must have been triggered by inserts associated with UPDATEs\nof existing logical rows.  Splitting a leaf page purely to store multiple\nversions is a false economy.  In effect, we're permanently degrading the\nindex structure just to absorb a temporary burst of duplicates.\n\nDeduplication in unique indexes helps to prevent these pathological page\nsplits.  Storing duplicates in a space efficient manner is not the goal,\nsince in the long run there won't be any duplicates anyway.  Rather, we're\nbuying time for standard garbage collection mechanisms to run before a\npage split is needed.\n\nUnique index leaf pages only get a deduplication pass when an insertion\n(that might have to split the page) observed an existing duplicate on the\npage in passing.  This is based on the assumption that deduplication will\nonly work out when _all_ new insertions are duplicates from UPDATEs.  This\nmay mean that we miss an opportunity to delay a page split, but that's\nokay because our ultimate goal is to delay leaf page splits _indefinitely_\n(i.e. to prevent them altogether).  There is little point in trying to\ndelay a split that is probably inevitable anyway.  This allows us to avoid\nthe overhead of attempting to deduplicate with unique indexes that always\nhave few or no duplicates.\n\nNote: Avoiding \"unnecessary\" page splits driven by version churn is also\nthe goal of bottom-up index deletion, which was added to PostgreSQL 14.\nBottom-up index deletion is now the preferred way to deal with this\nproblem (with all kinds of indexes, though especially with unique\nindexes).  Still, deduplication can sometimes augment bottom-up index\ndeletion.  When deletion cannot free tuples (due to an old snapshot\nholding up cleanup), falling back on deduplication provides additional\ncapacity.  Delaying the page split by deduplicating can allow a future\nbottom-up deletion pass of the same page to succeed.\n\nPosting list splits\n-------------------\n\nWhen the incoming tuple happens to overlap with an existing posting list,\na posting list split is performed.  Like a page split, a posting list\nsplit resolves a situation where a new/incoming item \"won't fit\", while\ninserting the incoming item in passing (i.e. as part of the same atomic\naction).  It's possible (though not particularly likely) that an insert of\na new item on to an almost-full page will overlap with a posting list,\nresulting in both a posting list split and a page split.  Even then, the\natomic action that splits the posting list also inserts the new item\n(since page splits always insert the new item in passing).  Including the\nposting list split in the same atomic action as the insert avoids problems\ncaused by concurrent inserts into the same posting list --  the exact\ndetails of how we change the posting list depend upon the new item, and\nvice-versa.  A single atomic action also minimizes the volume of extra\nWAL required for a posting list split, since we don't have to explicitly\nWAL-log the original posting list tuple.\n\nDespite piggy-backing on the same atomic action that inserts a new tuple,\nposting list splits can be thought of as a separate, extra action to the\ninsert itself (or to the page split itself).  Posting list splits\nconceptually \"rewrite\" an insert that overlaps with an existing posting\nlist into an insert that adds its final new item just to the right of the\nposting list instead.  The size of the posting list won't change, and so\npage space accounting code does not need to care about posting list splits\nat all.  This is an important upside of our design; the page split point\nchoice logic is very subtle even without it needing to deal with posting\nlist splits.\n\nOnly a few isolated extra steps are required to preserve the illusion that\nthe new item never overlapped with an existing posting list in the first\nplace: the heap TID of the incoming tuple has its TID replaced with the\nrightmost/max heap TID from the existing/originally overlapping posting\nlist.  Similarly, the original incoming item's TID is relocated to the\nappropriate offset in the posting list (we usually shift TIDs out of the\nway to make a hole for it).  Finally, the posting-split-with-page-split\ncase must generate a new high key based on an imaginary version of the\noriginal page that has both the final new item and the after-list-split\nposting tuple (page splits usually just operate against an imaginary\nversion that contains the new item/item that won't fit).\n\nThis approach avoids inventing an \"eager\" atomic posting split operation\nthat splits the posting list without simultaneously finishing the insert\nof the incoming item.  This alternative design might seem cleaner, but it\ncreates subtle problems for page space accounting.  In general, there\nmight not be enough free space on the page to split a posting list such\nthat the incoming/new item no longer overlaps with either posting list\nhalf --- the operation could fail before the actual retail insert of the\nnew item even begins.  We'd end up having to handle posting list splits\nthat need a page split anyway.  Besides, supporting variable \"split points\"\nwhile splitting posting lists won't actually improve overall space\nutilization.\n\nNotes About Data Representation\n-------------------------------\n\nThe right-sibling link required by L&Y is kept in the page \"opaque\ndata\" area, as is the left-sibling link, the page level, and some flags.\nThe page level counts upwards from zero at the leaf level, to the tree\ndepth minus 1 at the root.  (Counting up from the leaves ensures that we\ndon't need to renumber any existing pages when splitting the root.)\n\nThe Postgres disk block data format (an array of items) doesn't fit\nLehman and Yao's alternating-keys-and-pointers notion of a disk page,\nso we have to play some games.  (The alternating-keys-and-pointers\nnotion is important for internal page splits, which conceptually split\nat the middle of an existing pivot tuple -- the tuple's \"separator\" key\ngoes on the left side of the split as the left side's new high key,\nwhile the tuple's pointer/downlink goes on the right side as the\nfirst/minus infinity downlink.)\n\nOn a page that is not rightmost in its tree level, the \"high key\" is\nkept in the page's first item, and real data items start at item 2.\nThe link portion of the \"high key\" item goes unused.  A page that is\nrightmost has no \"high key\" (it's implicitly positive infinity), so\ndata items start with the first item.  Putting the high key at the\nleft, rather than the right, may seem odd, but it avoids moving the\nhigh key as we add data items.\n\nOn a leaf page, the data items are simply links to (TIDs of) tuples\nin the relation being indexed, with the associated key values.\n\nOn a non-leaf page, the data items are down-links to child pages with\nbounding keys.  The key in each data item is a strict lower bound for\nkeys on that child page, so logically the key is to the left of that\ndownlink.  The high key (if present) is the upper bound for the last\ndownlink.  The first data item on each such page has no lower bound\n--- or lower bound of minus infinity, if you prefer.  The comparison\nroutines must treat it accordingly.  The actual key stored in the\nitem is irrelevant, and need not be stored at all.  This arrangement\ncorresponds to the fact that an L&Y non-leaf page has one more pointer\nthan key.  Suffix truncation's negative infinity attributes behave in\nthe same way.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\nbtree\\README",
      "directory": "backend\\access\\nbtree"
    }
  },
  {
    "title": "README: backend\\access\\rmgrdesc",
    "url": "backend\\access\\rmgrdesc\\README",
    "content": "src/backend/access/rmgrdesc/README\n\nWAL resource manager description functions\n==========================================\n\nFor debugging purposes, there is a \"description function\", or rmgrdesc\nfunction, for each WAL resource manager. The rmgrdesc function parses the WAL\nrecord and prints the contents of the WAL record in a somewhat human-readable\nformat.\n\nThe rmgrdesc functions for all resource managers are gathered in this\ndirectory, because they are also used in the stand-alone pg_waldump program.\nThey could potentially be used by out-of-tree debugging tools too, although\nneither the description functions nor the output format should be considered\npart of a stable API\n\nGuidelines for rmgrdesc output format\n-------------------------------------\n\nThe goal of these guidelines is to avoid gratuitous inconsistencies across\neach rmgr, and to allow users to parse desc output strings without too much\ndifficulty.  This is not an API specification or an interchange format.\n(Only heapam and nbtree desc routines follow these guidelines at present, in\nany case.)\n\nRecord descriptions are similar to JSON style key/value objects.  However,\nthere is no explicit \"string\" type/string escaping.  Top-level { } brackets\nshould be omitted.  For example:\n\nsnapshotConflictHorizon: 0, flags: 0x03\n\nRecord descriptions may contain variable-length arrays.  For example:\n\nnunused: 5, unused: [1, 2, 3, 4, 5]\n\nNested objects are supported via { } brackets.  They generally appear inside\nvariable-length arrays.  For example:\n\nndeleted: 0, nupdated: 1, deleted: [], updated: [{ off: 45, nptids: 1, ptids: [0] }]\n\nTry to output things in an order that faithfully represents the order of\nfields from the underlying physical WAL record struct.  Key names should be\nunique (at the same nesting level) to make parsing easy.  It's a good idea if\nthe number of items in the array appears before the array.\n\nIt's okay for individual WAL record types to invent their own conventions.\nFor example, Heap2's PRUNE record descriptions use a custom array format for\nthe record's \"redirected\" field:\n\n... redirected: [1->4, 5->9], dead: [10, 11], unused: [3, 7, 8]\n\nArguably the desc routine should be using object notation for this instead.\nHowever, there is value in using a custom format when it conveys useful\ninformation about the underlying physical data structures.\n\nThis ad-hoc format has the advantage of being close to the format used for\nthe \"dead\" and \"unused\" arrays (which follow the standard desc convention for\npage offset number arrays).  It suggests that the \"redirected\" elements shown\nare just pairs of page offset numbers (which is how it really works).\n\nrmgrdesc_utils.c contains some helper functions to print data in this format.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\rmgrdesc\\README",
      "directory": "backend\\access\\rmgrdesc"
    }
  },
  {
    "title": "README: backend\\access\\spgist",
    "url": "backend\\access\\spgist\\README",
    "content": "src/backend/access/spgist/README\n\nSP-GiST is an abbreviation of space-partitioned GiST.  It provides a\ngeneralized infrastructure for implementing space-partitioned data\nstructures, such as quadtrees, k-d trees, and radix trees (tries).  When\nimplemented in main memory, these structures are usually designed as a set of\ndynamically-allocated nodes linked by pointers.  This is not suitable for\ndirect storing on disk, since the chains of pointers can be rather long and\nrequire too many disk accesses. In contrast, disk based data structures\nshould have a high fanout to minimize I/O.  The challenge is to map tree\nnodes to disk pages in such a way that the search algorithm accesses only a\nfew disk pages, even if it traverses many nodes.\n\n\nCOMMON STRUCTURE DESCRIPTION\n\nLogically, an SP-GiST tree is a set of tuples, each of which can be either\nan inner or leaf tuple.  Each inner tuple contains \"nodes\", which are\n(label,pointer) pairs, where the pointer (ItemPointerData) is a pointer to\nanother inner tuple or to the head of a list of leaf tuples.  Inner tuples\ncan have different numbers of nodes (children).  Branches can be of different\ndepth (actually, there is no control or code to support balancing), which\nmeans that the tree is non-balanced.  However, leaf and inner tuples cannot\nbe intermixed at the same level: a downlink from a node of an inner tuple\nleads either to one inner tuple, or to a list of leaf tuples.\n\nThe SP-GiST core requires that inner and leaf tuples fit on a single index\npage, and even more stringently that the list of leaf tuples reached from a\nsingle inner-tuple node all be stored on the same index page.  (Restricting\nsuch lists to not cross pages reduces seeks, and allows the list links to be\nstored as simple 2-byte OffsetNumbers.)  SP-GiST index opclasses should\ntherefore ensure that not too many nodes can be needed in one inner tuple,\nand that inner-tuple prefixes and leaf-node datum values not be too large.\n\nInner and leaf tuples are stored separately: the former are stored only on\n\"inner\" pages, the latter only on \"leaf\" pages.  Also, there are special\nrestrictions on the root page.  Early in an index's life, when there is only\none page's worth of data, the root page contains an unorganized set of leaf\ntuples.  After the first page split has occurred, the root is required to\ncontain exactly one inner tuple.\n\nWhen the search traversal algorithm reaches an inner tuple, it chooses a set\nof nodes to continue tree traverse in depth.  If it reaches a leaf page it\nscans a list of leaf tuples to find the ones that match the query. SP-GiST\nalso supports ordered (nearest-neighbor) searches - that is during scan pending\nnodes are put into priority queue, so traversal is performed by the\nclosest-first model.\n\n\nThe insertion algorithm descends the tree similarly, except it must choose\njust one node to descend to from each inner tuple.  Insertion might also have\nto modify the inner tuple before it can descend: it could add a new node, or\nit could \"split\" the tuple to obtain a less-specific prefix that can match\nthe value to be inserted.  If it's necessary to append a new leaf tuple to a\nlist and there is no free space on page, then SP-GiST creates a new inner\ntuple and distributes leaf tuples into a set of lists on, perhaps, several\npages.\n\nAn inner tuple consists of:\n\n  optional prefix value - all successors must be consistent with it.\n    Example:\n        radix tree   - prefix value is a common prefix string\n        quad tree    - centroid\n        k-d tree     - one coordinate\n\n  list of nodes, where node is a (label, pointer) pair.\n    Example of a label: a single character for radix tree\n\nA leaf tuple consists of:\n\n  a leaf value\n    Example:\n        radix tree - the rest of string (postfix)\n        quad and k-d tree - the point itself\n\n  ItemPointer to the corresponding heap tuple\n  nextOffset number of next leaf tuple in a chain on a leaf page\n\n  optional nulls bitmask\n  optional INCLUDE-column values\n\nFor compatibility with pre-v14 indexes, a leaf tuple has a nulls bitmask\nonly if there are null values (among the leaf value and the INCLUDE values)\n*and* there is at least one INCLUDE column.  The null-ness of the leaf\nvalue can be inferred from whether the tuple is on a \"nulls page\" (see below)\nso it is not necessary to represent it explicitly.  But we include it anyway\nin a bitmask used with INCLUDE values, so that standard tuple deconstruction\ncode can be used.\n\n\nNULLS HANDLING\n\nWe assume that SPGiST-indexable operators are strict (can never succeed for\nnull inputs).  It is still desirable to index nulls, so that whole-table\nindexscans are possible and so that \"x IS NULL\" can be implemented by an\nSPGiST indexscan.  However, we prefer that SPGiST index opclasses not have\nto cope with nulls.  Therefore, the main tree of an SPGiST index does not\ninclude any null entries.  We store null entries in a separate SPGiST tree\noccupying a disjoint set of pages (in particular, its own root page).\nInsertions and searches in the nulls tree do not use any of the\nopclass-supplied functions, but just use hardwired logic comparable to\nAllTheSame cases in the normal tree.\n\n\nINSERTION ALGORITHM\n\nInsertion algorithm is designed to keep the tree in a consistent state at\nany moment.  Here is a simplified insertion algorithm specification\n(numbers refer to notes below):\n\n  Start with the first tuple on the root page (1)\n\n  loop:\n    if (page is leaf) then\n        if (enough space)\n            insert on page and exit (5)\n        else (7)\n            call PickSplitFn() (2)\n        end if\n    else\n        switch (chooseFn())\n            case MatchNode  - descend through selected node\n            case AddNode    - add node and then retry chooseFn (3, 6)\n            case SplitTuple - split inner tuple to prefix and postfix, then\n                              retry chooseFn with the prefix tuple (4, 6)\n    end if\n\nNotes:\n\n(1) Initially, we just dump leaf tuples into the root page until it is full;\nthen we split it.  Once the root is not a leaf page, it can have only one\ninner tuple, so as to keep the amount of free space on the root as large as\npossible.  Both of these rules are meant to postpone doing PickSplit on the\nroot for as long as possible, so that the topmost partitioning of the search\nspace is as good as we can easily make it.\n\n(2) Current implementation allows to do picksplit and insert a new leaf tuple\nin one operation, if the new list of leaf tuples fits on one page. It's\nalways possible for trees with small nodes like quad tree or k-d tree, but\nradix trees may require another picksplit.\n\n(3) Addition of node must keep size of inner tuple small enough to fit on a\npage.  After addition, inner tuple could become too large to be stored on\ncurrent page because of other tuples on page. In this case it will be moved\nto another inner page (see notes about page management). When moving tuple to\nanother page, we can't change the numbers of other tuples on the page, else\nwe'd make downlink pointers to them invalid. To prevent that, SP-GiST leaves\na \"placeholder\" tuple, which can be reused later whenever another tuple is\nadded to the page. See also Concurrency and Vacuum sections below. Right now\nonly radix trees could add a node to the tuple; quad trees and k-d trees\nmake all possible nodes at once in PickSplitFn() call.\n\n(4) Prefix value could only partially match a new value, so the SplitTuple\naction allows breaking the current tree branch into upper and lower sections.\nAnother way to say it is that we can split the current inner tuple into\n\"prefix\" and \"postfix\" parts, where the prefix part is able to match the\nincoming new value. Consider example of insertion into a radix tree. We use\nthe following notation, where tuple's id is just for discussion (no such id\nis actually stored):\n\ninner tuple: {tuple id}(prefix string)[ comma separated list of node labels ]\nleaf tuple: {tuple id}<value>\n\nSuppose we need to insert string 'www.gogo.com' into inner tuple\n\n    {1}(www.google.com/)[a, i]\n\nThe string does not match the prefix so we cannot descend.  We must\nsplit the inner tuple into two tuples:\n\n    {2}(www.go)[o]  - prefix tuple\n                |\n                {3}(gle.com/)[a,i] - postfix tuple\n\nOn the next iteration of loop we find that 'www.gogo.com' matches the\nprefix, but not any node label, so we add a node [g] to tuple {2}:\n\n                   NIL (no child exists yet)\n                   |\n    {2}(www.go)[o, g]\n                |\n                {3}(gle.com/)[a,i]\n\nNow we can descend through the [g] node, which will cause us to update\nthe target string to just 'o.com'.  Finally, we'll insert a leaf tuple\nbearing that string:\n\n                  {4}<o.com>\n                   |\n    {2}(www.go)[o, g]\n                |\n                {3}(gle.com/)[a,i]\n\nAs we can see, the original tuple's node array moves to postfix tuple without\nany changes.  Note also that SP-GiST core assumes that prefix tuple is not\nlarger than old inner tuple.  That allows us to store prefix tuple directly\nin place of old inner tuple.  SP-GiST core will try to store postfix tuple on\nthe same page if possible, but will use another page if there is not enough\nfree space (see notes 5 and 6).  Currently, quad and k-d trees don't use this\nfeature, because they have no concept of a prefix being \"inconsistent\" with\nany new value.  They grow their depth only by PickSplitFn() call.\n\n(5) If pointer from node of parent is a NIL pointer, algorithm chooses a leaf\npage to store on.  At first, it tries to use the last-used leaf page with the\nlargest free space (which we track in each backend) to better utilize disk\nspace.  If that's not large enough, then the algorithm allocates a new page.\n\n(6) Management of inner pages is very similar to management of leaf pages,\ndescribed in (5).\n\n(7) Actually, current implementation can move the whole list of leaf tuples\nand a new tuple to another page, if the list is short enough. This improves\nspace utilization, but doesn't change the basis of the algorithm.\n\n\nCONCURRENCY\n\nWhile descending the tree, the insertion algorithm holds exclusive lock on\ntwo tree levels at a time, ie both parent and child pages (but parent and\nchild pages can be the same, see notes above).  There is a possibility of\ndeadlock between two insertions if there are cross-referenced pages in\ndifferent branches.  That is, if inner tuple on page M has a child on page N\nwhile an inner tuple from another branch is on page N and has a child on\npage M, then two insertions descending the two branches could deadlock,\nsince they will each hold their parent page's lock while trying to get the\nchild page's lock.\n\nCurrently, we deal with this by conditionally locking buffers as we descend\nthe tree.  If we fail to get lock on a buffer, we release both buffers and\nrestart the insertion process.  This is potentially inefficient, but the\nlocking costs of a more deterministic approach seem very high.\n\nTo reduce the number of cases where that happens, we introduce a concept of\n\"triple parity\" of pages: if inner tuple is on page with BlockNumber N, then\nits child tuples should be placed on the same page, or else on a page with\nBlockNumber M where (N+1) mod 3 == M mod 3.  This rule ensures that tuples\non page M will have no children on page N, since (M+1) mod 3 != N mod 3.\nThat makes it unlikely that two insertion processes will conflict against\neach other while descending the tree.  It's not perfect though: in the first\nplace, we could still get a deadlock among three or more insertion processes,\nand in the second place, it's impractical to preserve this invariant in every\ncase when we expand or split an inner tuple.  So we still have to allow for\ndeadlocks.\n\nInsertion may also need to take locks on an additional inner and/or leaf page\nto add tuples of the right type(s), when there's not enough room on the pages\nit descended through.  However, we don't care exactly which such page we add\nto, so deadlocks can be avoided by conditionally locking the additional\nbuffers: if we fail to get lock on an additional page, just try another one.\n\nSearch traversal algorithm is rather traditional.  At each non-leaf level, it\nshare-locks the page, identifies which node(s) in the current inner tuple\nneed to be visited, and puts those addresses on a stack of pages to examine\nlater.  It then releases lock on the current buffer before visiting the next\nstack item.  So only one page is locked at a time, and no deadlock is\npossible.  But instead, we have to worry about race conditions: by the time\nwe arrive at a pointed-to page, a concurrent insertion could have replaced\nthe target inner tuple (or leaf tuple chain) with data placed elsewhere.\nTo handle that, whenever the insertion algorithm changes a nonempty downlink\nin an inner tuple, it places a \"redirect tuple\" in place of the lower-level\ninner tuple or leaf-tuple chain head that the link formerly led to.  Scans\n(though not insertions) must be prepared to honor such redirects.  Only a\nscan that had already visited the parent level could possibly reach such a\nredirect tuple, so we can remove redirects once all active transactions have\nbeen flushed out of the system.\n\n\nDEAD TUPLES\n\nTuples on leaf pages can be in one of four states:\n\nSPGIST_LIVE: normal, live pointer to a heap tuple.\n\nSPGIST_REDIRECT: placeholder that contains a link to another place in the\nindex.  When a chain of leaf tuples has to be moved to another page, a\nredirect tuple is inserted in place of the chain's head tuple.  The parent\ninner tuple's downlink is updated when this happens, but concurrent scans\nmight be \"in flight\" from the parent page to the child page (since they\nrelease lock on the parent page before attempting to lock the child).\nThe redirect pointer serves to tell such a scan where to go.  A redirect\npointer is only needed for as long as such concurrent scans could be in\nprogress.  Eventually, it's converted into a PLACEHOLDER dead tuple by\nVACUUM, and is then a candidate for replacement.  Searches that find such\na tuple (which should never be part of a chain) should immediately proceed\nto the other place, forgetting about the redirect tuple.  Insertions that\nreach such a tuple should raise error, since a valid downlink should never\npoint to such a tuple.\n\nSPGIST_DEAD: tuple is dead, but it cannot be removed or moved to a\ndifferent offset on the page because there is a link leading to it from\nsome inner tuple elsewhere in the index.  (Such a tuple is never part of a\nchain, since we don't need one unless there is nothing live left in its\nchain.)  Searches should ignore such entries.  If an insertion action\narrives at such a tuple, it should either replace it in-place (if there's\nroom on the page to hold the desired new leaf tuple) or replace it with a\nredirection pointer to wherever it puts the new leaf tuple.\n\nSPGIST_PLACEHOLDER: tuple is dead, and there are known to be no links to\nit from elsewhere.  When a live tuple is deleted or moved away, and not\nreplaced by a redirect pointer, it is replaced by a placeholder to keep\nthe offsets of later tuples on the same page from changing.  Placeholders\ncan be freely replaced when adding a new tuple to the page, and also\nVACUUM will delete any that are at the end of the range of valid tuple\noffsets.  Both searches and insertions should complain if a link from\nelsewhere leads them to a placeholder tuple.\n\nWhen the root page is also a leaf, all its tuple should be in LIVE state;\nthere's no need for the others since there are no links and no need to\npreserve offset numbers.\n\nTuples on inner pages can be in LIVE, REDIRECT, or PLACEHOLDER states.\nThe REDIRECT state has the same function as on leaf pages, to send\nconcurrent searches to the place where they need to go after an inner\ntuple is moved to another page.  Expired REDIRECT pointers are converted\nto PLACEHOLDER status by VACUUM, and are then candidates for replacement.\nDEAD state is not currently possible, since VACUUM does not attempt to\nremove unused inner tuples.\n\n\nVACUUM\n\nVACUUM (or more precisely, spgbulkdelete) performs a single sequential scan\nover the entire index.  On both leaf and inner pages, we can convert old\nREDIRECT tuples into PLACEHOLDER status, and then remove any PLACEHOLDERs\nthat are at the end of the page (since they aren't needed to preserve the\noffsets of any live tuples).  On leaf pages, we scan for tuples that need\nto be deleted because their heap TIDs match a vacuum target TID.\n\nIf we find a deletable tuple that is not at the head of its chain, we\ncan simply replace it with a PLACEHOLDER, updating the chain links to\nremove it from the chain.  If it is at the head of its chain, but there's\nat least one live tuple remaining in the chain, we move that live tuple\nto the head tuple's offset, replacing it with a PLACEHOLDER to preserve\nthe offsets of other tuples.  This keeps the parent inner tuple's downlink\nvalid.  If we find ourselves deleting all live tuples in a chain, we\nreplace the head tuple with a DEAD tuple and the rest with PLACEHOLDERS.\nThe parent inner tuple's downlink thus points to the DEAD tuple, and the\nrules explained in the previous section keep everything working.\n\nVACUUM doesn't know a-priori which tuples are heads of their chains, but\nit can easily figure that out by constructing a predecessor array that's\nthe reverse map of the nextOffset links (ie, when we see tuple x links to\ntuple y, we set predecessor[y] = x).  Then head tuples are the ones with\nno predecessor.\n\nBecause insertions can occur while VACUUM runs, a pure sequential scan\ncould miss deleting some target leaf tuples, because they could get moved\nfrom a not-yet-visited leaf page to an already-visited leaf page as a\nconsequence of a PickSplit or MoveLeafs operation.  Failing to delete any\ntarget TID is not acceptable, so we have to extend the algorithm to cope\nwith such cases.  We recognize that such a move might have occurred when\nwe see a leaf-page REDIRECT tuple whose XID indicates it might have been\ncreated after the VACUUM scan started.  We add the redirection target TID\nto a \"pending list\" of places we need to recheck.  Between pages of the\nmain sequential scan, we empty the pending list by visiting each listed\nTID.  If it points to an inner tuple (from a PickSplit), add each downlink\nTID to the pending list.  If it points to a leaf page, vacuum that page.\n(We could just vacuum the single pointed-to chain, but vacuuming the\nwhole page simplifies the code and reduces the odds of VACUUM having to\nmodify the same page multiple times.)  To ensure that pending-list\nprocessing can never get into an endless loop, even in the face of\nconcurrent index changes, we don't remove list entries immediately but\nonly after we've completed all pending-list processing; instead we just\nmark items as done after processing them.  Adding a TID that's already in\nthe list is a no-op, whether or not that item is marked done yet.\n\nspgbulkdelete also updates the index's free space map.\n\nCurrently, spgvacuumcleanup has nothing to do if spgbulkdelete was\nperformed; otherwise, it does an spgbulkdelete scan with an empty target\nlist, so as to clean up redirections and placeholders, update the free\nspace map, and gather statistics.\n\n\nLAST USED PAGE MANAGEMENT\n\nThe list of last used pages contains four pages - a leaf page and three\ninner pages, one from each \"triple parity\" group.  (Actually, there's one\nsuch list for the main tree and a separate one for the nulls tree.)  This\nlist is stored between calls on the index meta page, but updates are never\nWAL-logged to decrease WAL traffic.  Incorrect data on meta page isn't\ncritical, because we could allocate a new page at any moment.\n\n\nAUTHORS\n\n    Teodor Sigaev <teodor@sigaev.ru>\n    Oleg Bartunov <oleg@sai.msu.su>",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\spgist\\README",
      "directory": "backend\\access\\spgist"
    }
  },
  {
    "title": "README: backend\\access\\transam",
    "url": "backend\\access\\transam\\README",
    "content": "src/backend/access/transam/README\n\nThe Transaction System\n======================\n\nPostgreSQL's transaction system is a three-layer system.  The bottom layer\nimplements low-level transactions and subtransactions, on top of which rests\nthe mainloop's control code, which in turn implements user-visible\ntransactions and savepoints.\n\nThe middle layer of code is called by postgres.c before and after the\nprocessing of each query, or after detecting an error:\n\n\t\tStartTransactionCommand\n\t\tCommitTransactionCommand\n\t\tAbortCurrentTransaction\n\nMeanwhile, the user can alter the system's state by issuing the SQL commands\nBEGIN, COMMIT, ROLLBACK, SAVEPOINT, ROLLBACK TO or RELEASE.  The traffic cop\nredirects these calls to the toplevel routines\n\n\t\tBeginTransactionBlock\n\t\tEndTransactionBlock\n\t\tUserAbortTransactionBlock\n\t\tDefineSavepoint\n\t\tRollbackToSavepoint\n\t\tReleaseSavepoint\n\nrespectively.  Depending on the current state of the system, these functions\ncall low level functions to activate the real transaction system:\n\n\t\tStartTransaction\n\t\tCommitTransaction\n\t\tAbortTransaction\n\t\tCleanupTransaction\n\t\tStartSubTransaction\n\t\tCommitSubTransaction\n\t\tAbortSubTransaction\n\t\tCleanupSubTransaction\n\nAdditionally, within a transaction, CommandCounterIncrement is called to\nincrement the command counter, which allows future commands to \"see\" the\neffects of previous commands within the same transaction.  Note that this is\ndone automatically by CommitTransactionCommand after each query inside a\ntransaction block, but some utility functions also do it internally to allow\nsome operations (usually in the system catalogs) to be seen by future\noperations in the same utility command.  (For example, in DefineRelation it is\ndone after creating the heap so the pg_class row is visible, to be able to\nlock it.)\n\n\nFor example, consider the following sequence of user commands:\n\n1)\t\tBEGIN\n2)\t\tSELECT * FROM foo\n3)\t\tINSERT INTO foo VALUES (...)\n4)\t\tCOMMIT\n\nIn the main processing loop, this results in the following function call\nsequence:\n\n     /  StartTransactionCommand;\n    /       StartTransaction;\n1) <    ProcessUtility;                 << BEGIN\n    \\       BeginTransactionBlock;\n     \\  CommitTransactionCommand;\n\n    /   StartTransactionCommand;\n2) /    PortalRunSelect;                << SELECT ...\n   \\    CommitTransactionCommand;\n    \\       CommandCounterIncrement;\n\n    /   StartTransactionCommand;\n3) /    ProcessQuery;                   << INSERT ...\n   \\    CommitTransactionCommand;\n    \\       CommandCounterIncrement;\n\n     /  StartTransactionCommand;\n    /   ProcessUtility;                 << COMMIT\n4) <        EndTransactionBlock;\n    \\   CommitTransactionCommand;\n     \\      CommitTransaction;\n\nThe point of this example is to demonstrate the need for\nStartTransactionCommand and CommitTransactionCommand to be state smart -- they\nshould call CommandCounterIncrement between the calls to BeginTransactionBlock\nand EndTransactionBlock and outside these calls they need to do normal start,\ncommit or abort processing.\n\nFurthermore, suppose the \"SELECT * FROM foo\" caused an abort condition. In\nthis case AbortCurrentTransaction is called, and the transaction is put in\naborted state.  In this state, any user input is ignored except for\ntransaction-termination statements, or ROLLBACK TO <savepoint> commands.\n\nTransaction aborts can occur in two ways:\n\n1) system dies from some internal cause  (syntax error, etc)\n2) user types ROLLBACK\n\nThe reason we have to distinguish them is illustrated by the following two\nsituations:\n\n        case 1                                  case 2\n        ------                                  ------\n1) user types BEGIN                     1) user types BEGIN\n2) user does something                  2) user does something\n3) user does not like what              3) system aborts for some reason\n   she sees and types ABORT                (syntax error, etc)\n\nIn case 1, we want to abort the transaction and return to the default state.\nIn case 2, there may be more commands coming our way which are part of the\nsame transaction block; we have to ignore these commands until we see a COMMIT\nor ROLLBACK.\n\nInternal aborts are handled by AbortCurrentTransaction, while user aborts are\nhandled by UserAbortTransactionBlock.  Both of them rely on AbortTransaction\nto do all the real work.  The only difference is what state we enter after\nAbortTransaction does its work:\n\n* AbortCurrentTransaction leaves us in TBLOCK_ABORT,\n* UserAbortTransactionBlock leaves us in TBLOCK_ABORT_END\n\nLow-level transaction abort handling is divided in two phases:\n* AbortTransaction executes as soon as we realize the transaction has\n  failed.  It should release all shared resources (locks etc) so that we do\n  not delay other backends unnecessarily.\n* CleanupTransaction executes when we finally see a user COMMIT\n  or ROLLBACK command; it cleans things up and gets us out of the transaction\n  completely.  In particular, we mustn't destroy TopTransactionContext until\n  this point.\n\nAlso, note that when a transaction is committed, we don't close it right away.\nRather it's put in TBLOCK_END state, which means that when\nCommitTransactionCommand is called after the query has finished processing,\nthe transaction has to be closed.  The distinction is subtle but important,\nbecause it means that control will leave the xact.c code with the transaction\nopen, and the main loop will be able to keep processing inside the same\ntransaction.  So, in a sense, transaction commit is also handled in two\nphases, the first at EndTransactionBlock and the second at\nCommitTransactionCommand (which is where CommitTransaction is actually\ncalled).\n\nThe rest of the code in xact.c are routines to support the creation and\nfinishing of transactions and subtransactions.  For example, AtStart_Memory\ntakes care of initializing the memory subsystem at main transaction start.\n\n\nSubtransaction Handling\n-----------------------\n\nSubtransactions are implemented using a stack of TransactionState structures,\neach of which has a pointer to its parent transaction's struct.  When a new\nsubtransaction is to be opened, PushTransaction is called, which creates a new\nTransactionState, with its parent link pointing to the current transaction.\nStartSubTransaction is in charge of initializing the new TransactionState to\nsane values, and properly initializing other subsystems (AtSubStart routines).\n\nWhen closing a subtransaction, either CommitSubTransaction has to be called\n(if the subtransaction is committing), or AbortSubTransaction and\nCleanupSubTransaction (if it's aborting).  In either case, PopTransaction is\ncalled so the system returns to the parent transaction.\n\nOne important point regarding subtransaction handling is that several may need\nto be closed in response to a single user command.  That's because savepoints\nhave names, and we allow to commit or rollback a savepoint by name, which is\nnot necessarily the one that was last opened.  Also a COMMIT or ROLLBACK\ncommand must be able to close out the entire stack.  We handle this by having\nthe utility command subroutine mark all the state stack entries as commit-\npending or abort-pending, and then when the main loop reaches\nCommitTransactionCommand, the real work is done.  The main point of doing\nthings this way is that if we get an error while popping state stack entries,\nthe remaining stack entries still show what we need to do to finish up.\n\nIn the case of ROLLBACK TO <savepoint>, we abort all the subtransactions up\nthrough the one identified by the savepoint name, and then re-create that\nsubtransaction level with the same name.  So it's a completely new\nsubtransaction as far as the internals are concerned.\n\nOther subsystems are allowed to start \"internal\" subtransactions, which are\nhandled by BeginInternalSubTransaction.  This is to allow implementing\nexception handling, e.g. in PL/pgSQL.  ReleaseCurrentSubTransaction and\nRollbackAndReleaseCurrentSubTransaction allows the subsystem to close said\nsubtransactions.  The main difference between this and the savepoint/release\npath is that we execute the complete state transition immediately in each\nsubroutine, rather than deferring some work until CommitTransactionCommand.\nAnother difference is that BeginInternalSubTransaction is allowed when no\nexplicit transaction block has been established, while DefineSavepoint is not.\n\n\nTransaction and Subtransaction Numbering\n----------------------------------------\n\nTransactions and subtransactions are assigned permanent XIDs only when/if\nthey first do something that requires one --- typically, insert/update/delete\na tuple, though there are a few other places that need an XID assigned.\nIf a subtransaction requires an XID, we always first assign one to its\nparent.  This maintains the invariant that child transactions have XIDs later\nthan their parents, which is assumed in a number of places.\n\nThe subsidiary actions of obtaining a lock on the XID and entering it into\npg_subtrans and PGPROC are done at the time it is assigned.\n\nA transaction that has no XID still needs to be identified for various\npurposes, notably holding locks.  For this purpose we assign a \"virtual\ntransaction ID\" or VXID to each top-level transaction.  VXIDs are formed from\ntwo fields, the procNumber and a backend-local counter; this arrangement\nallows assignment of a new VXID at transaction start without any contention\nfor shared memory.  To ensure that a VXID isn't re-used too soon after backend\nexit, we store the last local counter value into shared memory at backend\nexit, and initialize it from the previous value for the same PGPROC slot at\nbackend start.  All these counters go back to zero at shared memory\nre-initialization, but that's OK because VXIDs never appear anywhere on-disk.\n\nInternally, a backend needs a way to identify subtransactions whether or not\nthey have XIDs; but this need only lasts as long as the parent top transaction\nendures.  Therefore, we have SubTransactionId, which is somewhat like\nCommandId in that it's generated from a counter that we reset at the start of\neach top transaction.  The top-level transaction itself has SubTransactionId 1,\nand subtransactions have IDs 2 and up.  (Zero is reserved for\nInvalidSubTransactionId.)  Note that subtransactions do not have their\nown VXIDs; they use the parent top transaction's VXID.\n\n\nInterlocking Transaction Begin, Transaction End, and Snapshots\n--------------------------------------------------------------\n\nWe try hard to minimize the amount of overhead and lock contention involved\nin the frequent activities of beginning/ending a transaction and taking a\nsnapshot.  Unfortunately, we must have some interlocking for this, because\nwe must ensure consistency about the commit order of transactions.\nFor example, suppose an UPDATE in xact A is blocked by xact B's prior\nupdate of the same row, and xact B is doing commit while xact C gets a\nsnapshot.  Xact A can complete and commit as soon as B releases its locks.\nIf xact C's GetSnapshotData sees xact B as still running, then it had\nbetter see xact A as still running as well, or it will be able to see two\ntuple versions - one deleted by xact B and one inserted by xact A.  Another\nreason why this would be bad is that C would see (in the row inserted by A)\nearlier changes by B, and it would be inconsistent for C not to see any\nof B's changes elsewhere in the database.\n\nFormally, the correctness requirement is \"if a snapshot A considers\ntransaction X as committed, and any of transaction X's snapshots considered\ntransaction Y as committed, then snapshot A must consider transaction Y as\ncommitted\".\n\nWhat we actually enforce is strict serialization of commits and rollbacks\nwith snapshot-taking: we do not allow any transaction to exit the set of\nrunning transactions while a snapshot is being taken.  (This rule is\nstronger than necessary for consistency, but is relatively simple to\nenforce, and it assists with some other issues as explained below.)  The\nimplementation of this is that GetSnapshotData takes the ProcArrayLock in\nshared mode (so that multiple backends can take snapshots in parallel),\nbut ProcArrayEndTransaction must take the ProcArrayLock in exclusive mode\nwhile clearing the ProcGlobal->xids[] entry at transaction end (either\ncommit or abort). (To reduce context switching, when multiple transactions\ncommit nearly simultaneously, we have one backend take ProcArrayLock and\nclear the XIDs of multiple processes at once.)\n\nProcArrayEndTransaction also holds the lock while advancing the shared\nlatestCompletedXid variable.  This allows GetSnapshotData to use\nlatestCompletedXid + 1 as xmax for its snapshot: there can be no\ntransaction >= this xid value that the snapshot needs to consider as\ncompleted.\n\nIn short, then, the rule is that no transaction may exit the set of\ncurrently-running transactions between the time we fetch latestCompletedXid\nand the time we finish building our snapshot.  However, this restriction\nonly applies to transactions that have an XID --- read-only transactions\ncan end without acquiring ProcArrayLock, since they don't affect anyone\nelse's snapshot nor latestCompletedXid.\n\nTransaction start, per se, doesn't have any interlocking with these\nconsiderations, since we no longer assign an XID immediately at transaction\nstart.  But when we do decide to allocate an XID, GetNewTransactionId must\nstore the new XID into the shared ProcArray before releasing XidGenLock.\nThis ensures that all top-level XIDs <= latestCompletedXid are either\npresent in the ProcArray, or not running anymore.  (This guarantee doesn't\napply to subtransaction XIDs, because of the possibility that there's not\nroom for them in the subxid array; instead we guarantee that they are\npresent or the overflow flag is set.)  If a backend released XidGenLock\nbefore storing its XID into ProcGlobal->xids[], then it would be possible for\nanother backend to allocate and commit a later XID, causing latestCompletedXid\nto pass the first backend's XID, before that value became visible in the\nProcArray.  That would break ComputeXidHorizons, as discussed below.\n\nWe allow GetNewTransactionId to store the XID into ProcGlobal->xids[] (or the\nsubxid array) without taking ProcArrayLock.  This was once necessary to\navoid deadlock; while that is no longer the case, it's still beneficial for\nperformance.  We are thereby relying on fetch/store of an XID to be atomic,\nelse other backends might see a partially-set XID.  This also means that\nreaders of the ProcArray xid fields must be careful to fetch a value only\nonce, rather than assume they can read it multiple times and get the same\nanswer each time.  (Use volatile-qualified pointers when doing this, to\nensure that the C compiler does exactly what you tell it to.)\n\nAnother important activity that uses the shared ProcArray is\nComputeXidHorizons, which must determine a lower bound for the oldest xmin\nof any active MVCC snapshot, system-wide.  Each individual backend\nadvertises the smallest xmin of its own snapshots in MyProc->xmin, or zero\nif it currently has no live snapshots (eg, if it's between transactions or\nhasn't yet set a snapshot for a new transaction).  ComputeXidHorizons takes\nthe MIN() of the valid xmin fields.  It does this with only shared lock on\nProcArrayLock, which means there is a potential race condition against other\nbackends doing GetSnapshotData concurrently: we must be certain that a\nconcurrent backend that is about to set its xmin does not compute an xmin\nless than what ComputeXidHorizons determines.  We ensure that by including\nall the active XIDs into the MIN() calculation, along with the valid xmins.\nThe rule that transactions can't exit without taking exclusive ProcArrayLock\nensures that concurrent holders of shared ProcArrayLock will compute the\nsame minimum of currently-active XIDs: no xact, in particular not the\noldest, can exit while we hold shared ProcArrayLock.  So\nComputeXidHorizons's view of the minimum active XID will be the same as that\nof any concurrent GetSnapshotData, and so it can't produce an overestimate.\nIf there is no active transaction at all, ComputeXidHorizons uses\nlatestCompletedXid + 1, which is a lower bound for the xmin that might\nbe computed by concurrent or later GetSnapshotData calls.  (We know that no\nXID less than this could be about to appear in the ProcArray, because of the\nXidGenLock interlock discussed above.)\n\nAs GetSnapshotData is performance critical, it does not perform an accurate\noldest-xmin calculation (it used to, until v14). The contents of a snapshot\nonly depend on the xids of other backends, not their xmin. As backend's xmin\nchanges much more often than its xid, having GetSnapshotData look at xmins\ncan lead to a lot of unnecessary cacheline ping-pong.  Instead\nGetSnapshotData updates approximate thresholds (one that guarantees that all\ndeleted rows older than it can be removed, another determining that deleted\nrows newer than it can not be removed). GlobalVisTest* uses those thresholds\nto make invisibility decision, falling back to ComputeXidHorizons if\nnecessary.\n\nNote that while it is certain that two concurrent executions of\nGetSnapshotData will compute the same xmin for their own snapshots, there is\nno such guarantee for the horizons computed by ComputeXidHorizons.  This is\nbecause we allow XID-less transactions to clear their MyProc->xmin\nasynchronously (without taking ProcArrayLock), so one execution might see\nwhat had been the oldest xmin, and another not.  This is OK since the\nthresholds need only be a valid lower bound.  As noted above, we are already\nassuming that fetch/store of the xid fields is atomic, so assuming it for\nxmin as well is no extra risk.\n\n\npg_xact and pg_subtrans\n-----------------------\n\npg_xact and pg_subtrans are permanent (on-disk) storage of transaction related\ninformation.  There is a limited number of pages of each kept in memory, so\nin many cases there is no need to actually read from disk.  However, if\nthere's a long running transaction or a backend sitting idle with an open\ntransaction, it may be necessary to be able to read and write this information\nfrom disk.  They also allow information to be permanent across server restarts.\n\npg_xact records the commit status for each transaction that has been assigned\nan XID.  A transaction can be in progress, committed, aborted, or\n\"sub-committed\".  This last state means that it's a subtransaction that's no\nlonger running, but its parent has not updated its state yet.  It is not\nnecessary to update a subtransaction's transaction status to subcommit, so we\ncan just defer it until main transaction commit.  The main role of marking\ntransactions as sub-committed is to provide an atomic commit protocol when\ntransaction status is spread across multiple clog pages. As a result, whenever\ntransaction status spreads across multiple pages we must use a two-phase commit\nprotocol: the first phase is to mark the subtransactions as sub-committed, then\nwe mark the top level transaction and all its subtransactions committed (in\nthat order).  Thus, subtransactions that have not aborted appear as in-progress\neven when they have already finished, and the subcommit status appears as a\nvery short transitory state during main transaction commit.  Subtransaction\nabort is always marked in clog as soon as it occurs.  When the transaction\nstatus all fit in a single CLOG page, we atomically mark them all as committed\nwithout bothering with the intermediate sub-commit state.\n\nSavepoints are implemented using subtransactions.  A subtransaction is a\ntransaction inside a transaction; its commit or abort status is not only\ndependent on whether it committed itself, but also whether its parent\ntransaction committed.  To implement multiple savepoints in a transaction we\nallow unlimited transaction nesting depth, so any particular subtransaction's\ncommit state is dependent on the commit status of each and every ancestor\ntransaction.\n\nThe \"subtransaction parent\" (pg_subtrans) mechanism records, for each\ntransaction with an XID, the TransactionId of its parent transaction.  This\ninformation is stored as soon as the subtransaction is assigned an XID.\nTop-level transactions do not have a parent, so they leave their pg_subtrans\nentries set to the default value of zero (InvalidTransactionId).\n\npg_subtrans is used to check whether the transaction in question is still\nrunning --- the main Xid of a transaction is recorded in ProcGlobal->xids[],\nwith a copy in PGPROC->xid, but since we allow arbitrary nesting of\nsubtransactions, we can't fit all Xids in shared memory, so we have to store\nthem on disk.  Note, however, that for each transaction we keep a \"cache\" of\nXids that are known to be part of the transaction tree, so we can skip looking\nat pg_subtrans unless we know the cache has been overflowed.  See\nstorage/ipc/procarray.c for the gory details.\n\nslru.c is the supporting mechanism for both pg_xact and pg_subtrans.  It\nimplements the LRU policy for in-memory buffer pages.  The high-level routines\nfor pg_xact are implemented in transam.c, while the low-level functions are in\nclog.c.  pg_subtrans is contained completely in subtrans.c.\n\n\nWrite-Ahead Log Coding\n----------------------\n\nThe WAL subsystem (also called XLOG in the code) exists to guarantee crash\nrecovery.  It can also be used to provide point-in-time recovery, as well as\nhot-standby replication via log shipping.  Here are some notes about\nnon-obvious aspects of its design.\n\nA basic assumption of a write AHEAD log is that log entries must reach stable\nstorage before the data-page changes they describe.  This ensures that\nreplaying the log to its end will bring us to a consistent state where there\nare no partially-performed transactions.  To guarantee this, each data page\n(either heap or index) is marked with the LSN (log sequence number --- in\npractice, a WAL file location) of the latest XLOG record affecting the page.\nBefore the bufmgr can write out a dirty page, it must ensure that xlog has\nbeen flushed to disk at least up to the page's LSN.  This low-level\ninteraction improves performance by not waiting for XLOG I/O until necessary.\nThe LSN check exists only in the shared-buffer manager, not in the local\nbuffer manager used for temp tables; hence operations on temp tables must not\nbe WAL-logged.\n\nDuring WAL replay, we can check the LSN of a page to detect whether the change\nrecorded by the current log entry is already applied (it has been, if the page\nLSN is >= the log entry's WAL location).\n\nUsually, log entries contain just enough information to redo a single\nincremental update on a page (or small group of pages).  This will work only\nif the filesystem and hardware implement data page writes as atomic actions,\nso that a page is never left in a corrupt partly-written state.  Since that's\noften an untenable assumption in practice, we log additional information to\nallow complete reconstruction of modified pages.  The first WAL record\naffecting a given page after a checkpoint is made to contain a copy of the\nentire page, and we implement replay by restoring that page copy instead of\nredoing the update.  (This is more reliable than the data storage itself would\nbe because we can check the validity of the WAL record's CRC.)  We can detect\nthe \"first change after checkpoint\" by noting whether the page's old LSN\nprecedes the end of WAL as of the last checkpoint (the RedoRecPtr).\n\nThe general schema for executing a WAL-logged action is\n\n1. Pin and exclusive-lock the shared buffer(s) containing the data page(s)\nto be modified.\n\n2. START_CRIT_SECTION()  (Any error during the next three steps must cause a\nPANIC because the shared buffers will contain unlogged changes, which we\nhave to ensure don't get to disk.  Obviously, you should check conditions\nsuch as whether there's enough free space on the page before you start the\ncritical section.)\n\n3. Apply the required changes to the shared buffer(s).\n\n4. Mark the shared buffer(s) as dirty with MarkBufferDirty().  (This must\nhappen before the WAL record is inserted; see notes in SyncOneBuffer().)\nNote that marking a buffer dirty with MarkBufferDirty() should only\nhappen iff you write a WAL record; see Writing Hints below.\n\n5. If the relation requires WAL-logging, build a WAL record using\nXLogBeginInsert and XLogRegister* functions, and insert it.  (See\n\"Constructing a WAL record\" below).  Then update the page's LSN using the\nreturned XLOG location.  For instance,\n\n\t\tXLogBeginInsert();\n\t\tXLogRegisterBuffer(...)\n\t\tXLogRegisterData(...)\n\t\trecptr = XLogInsert(rmgr_id, info);\n\n\t\tPageSetLSN(dp, recptr);\n\n6. END_CRIT_SECTION()\n\n7. Unlock and unpin the buffer(s).\n\nComplex changes (such as a multilevel index insertion) normally need to be\ndescribed by a series of atomic-action WAL records.  The intermediate states\nmust be self-consistent, so that if the replay is interrupted between any\ntwo actions, the system is fully functional.  In btree indexes, for example,\na page split requires a new page to be allocated, and an insertion of a new\nkey in the parent btree level, but for locking reasons this has to be\nreflected by two separate WAL records.  Replaying the first record, to\nallocate the new page and move tuples to it, sets a flag on the page to\nindicate that the key has not been inserted to the parent yet.  Replaying the\nsecond record clears the flag.  This intermediate state is never seen by\nother backends during normal operation, because the lock on the child page\nis held across the two actions, but will be seen if the operation is\ninterrupted before writing the second WAL record.  The search algorithm works\nwith the intermediate state as normal, but if an insertion encounters a page\nwith the incomplete-split flag set, it will finish the interrupted split by\ninserting the key to the parent, before proceeding.\n\n\nConstructing a WAL record\n-------------------------\n\nA WAL record consists of a header common to all WAL record types,\nrecord-specific data, and information about the data blocks modified.  Each\nmodified data block is identified by an ID number, and can optionally have\nmore record-specific data associated with the block.  If XLogInsert decides\nthat a full-page image of a block needs to be taken, the data associated\nwith that block is not included.\n\nThe API for constructing a WAL record consists of five functions:\nXLogBeginInsert, XLogRegisterBuffer, XLogRegisterData, XLogRegisterBufData,\nand XLogInsert.  First, call XLogBeginInsert().  Then register all the buffers\nmodified, and data needed to replay the changes, using XLogRegister*\nfunctions.  Finally, insert the constructed record to the WAL by calling\nXLogInsert().\n\n\tXLogBeginInsert();\n\n\t/* register buffers modified as part of this WAL-logged action */\n\tXLogRegisterBuffer(0, lbuffer, REGBUF_STANDARD);\n\tXLogRegisterBuffer(1, rbuffer, REGBUF_STANDARD);\n\n\t/* register data that is always included in the WAL record */\n\tXLogRegisterData(&xlrec, SizeOfFictionalAction);\n\n\t/*\n\t * register data associated with a buffer. This will not be included\n\t * in the record if a full-page image is taken.\n\t */\n\tXLogRegisterBufData(0, tuple->data, tuple->len);\n\n\t/* more data associated with the buffer */\n\tXLogRegisterBufData(0, data2, len2);\n\n\t/*\n\t * Ok, all the data and buffers to include in the WAL record have\n\t * been registered. Insert the record.\n\t */\n\trecptr = XLogInsert(RM_FOO_ID, XLOG_FOOBAR_DO_STUFF);\n\nDetails of the API functions:\n\nvoid XLogBeginInsert(void)\n\n    Must be called before XLogRegisterBuffer and XLogRegisterData.\n\nvoid XLogResetInsertion(void)\n\n    Clear any currently registered data and buffers from the WAL record\n    construction workspace.  This is only needed if you have already called\n    XLogBeginInsert(), but decide to not insert the record after all.\n\nvoid XLogEnsureRecordSpace(int max_block_id, int ndatas)\n\n    Normally, the WAL record construction buffers have the following limits:\n\n    * highest block ID that can be used is 4 (allowing five block references)\n    * Max 20 chunks of registered data\n\n    These default limits are enough for most record types that change some\n    on-disk structures.  For the odd case that requires more data, or needs to\n    modify more buffers, these limits can be raised by calling\n    XLogEnsureRecordSpace().  XLogEnsureRecordSpace() must be called before\n    XLogBeginInsert(), and outside a critical section.\n\nvoid XLogRegisterBuffer(uint8 block_id, Buffer buf, uint8 flags);\n\n    XLogRegisterBuffer adds information about a data block to the WAL record.\n    block_id is an arbitrary number used to identify this page reference in\n    the redo routine.  The information needed to re-find the page at redo -\n    relfilelocator, fork, and block number - are included in the WAL record.\n\n    XLogInsert will automatically include a full copy of the page contents, if\n    this is the first modification of the buffer since the last checkpoint.\n    It is important to register every buffer modified by the action with\n    XLogRegisterBuffer, to avoid torn-page hazards.\n\n    The flags control when and how the buffer contents are included in the\n    WAL record.  Normally, a full-page image is taken only if the page has not\n    been modified since the last checkpoint, and only if full_page_writes=on\n    or an online backup is in progress.  The REGBUF_FORCE_IMAGE flag can be\n    used to force a full-page image to always be included; that is useful\n    e.g. for an operation that rewrites most of the page, so that tracking the\n    details is not worth it.  For the rare case where it is not necessary to\n    protect from torn pages, REGBUF_NO_IMAGE flag can be used to suppress\n    full page image from being taken.  REGBUF_WILL_INIT also suppresses a full\n    page image, but the redo routine must re-generate the page from scratch,\n    without looking at the old page contents.  Re-initializing the page\n    protects from torn page hazards like a full page image does.\n\n    The REGBUF_STANDARD flag can be specified together with the other flags to\n    indicate that the page follows the standard page layout.  It causes the\n    area between pd_lower and pd_upper to be left out from the image, reducing\n    WAL volume.\n\n    If the REGBUF_KEEP_DATA flag is given, any per-buffer data registered with\n    XLogRegisterBufData() is included in the WAL record even if a full-page\n    image is taken.\n\nvoid XLogRegisterData(char *data, int len);\n\n    XLogRegisterData is used to include arbitrary data in the WAL record.  If\n    XLogRegisterData() is called multiple times, the data are appended, and\n    will be made available to the redo routine as one contiguous chunk.\n\nvoid XLogRegisterBufData(uint8 block_id, char *data, int len);\n\n    XLogRegisterBufData is used to include data associated with a particular\n    buffer that was registered earlier with XLogRegisterBuffer().  If\n    XLogRegisterBufData() is called multiple times with the same block ID, the\n    data are appended, and will be made available to the redo routine as one\n    contiguous chunk.\n\n    If a full-page image of the buffer is taken at insertion, the data is not\n    included in the WAL record, unless the REGBUF_KEEP_DATA flag is used.\n\n\nWriting a REDO routine\n----------------------\n\nA REDO routine uses the data and page references included in the WAL record\nto reconstruct the new state of the page.  The record decoding functions\nand macros in xlogreader.c/h can be used to extract the data from the record.\n\nWhen replaying a WAL record that describes changes on multiple pages, you\nmust be careful to lock the pages properly to prevent concurrent Hot Standby\nqueries from seeing an inconsistent state.  If this requires that two\nor more buffer locks be held concurrently, you must lock the pages in\nappropriate order, and not release the locks until all the changes are done.\n\nNote that we must only use PageSetLSN/PageGetLSN() when we know the action\nis serialised. Only Startup process may modify data blocks during recovery,\nso Startup process may execute PageGetLSN() without fear of serialisation\nproblems. All other processes must only call PageSet/GetLSN when holding\neither an exclusive buffer lock or a shared lock plus buffer header lock,\nor be writing the data block directly rather than through shared buffers\nwhile holding AccessExclusiveLock on the relation.\n\n\nWriting Hints\n-------------\n\nIn some cases, we write additional information to data blocks without\nwriting a preceding WAL record. This should only happen iff the data can\nbe reconstructed later following a crash and the action is simply a way\nof optimising for performance. When a hint is written we use\nMarkBufferDirtyHint() to mark the block dirty.\n\nIf the buffer is clean and checksums are in use then MarkBufferDirtyHint()\ninserts an XLOG_FPI_FOR_HINT record to ensure that we take a full page image\nthat includes the hint. We do this to avoid a partial page write, when we\nwrite the dirtied page. WAL is not written during recovery, so we simply skip\ndirtying blocks because of hints when in recovery.\n\nIf you do decide to optimise away a WAL record, then any calls to\nMarkBufferDirty() must be replaced by MarkBufferDirtyHint(),\notherwise you will expose the risk of partial page writes.\n\nThe all-visible hint in a heap page (PD_ALL_VISIBLE) is a special\ncase, because it is treated like a durable change in some respects and\na hint in other respects. It must satisfy the invariant that, if a\nheap page's associated visibilitymap (VM) bit is set, then\nPD_ALL_VISIBLE is set on the heap page itself. Clearing of\nPD_ALL_VISIBLE is always treated like a fully-durable change to\nmaintain this invariant. Additionally, if checksums or wal_log_hints\nare enabled, setting PD_ALL_VISIBLE is also treated like a\nfully-durable change to protect against torn pages.\n\nBut, if neither checksums nor wal_log_hints are enabled, torn pages\nare of no consequence if the only change is to PD_ALL_VISIBLE; so no\nfull heap page image is taken, and the heap page's LSN is not\nupdated. NB: it would be incorrect to update the heap page's LSN when\napplying this optimization, even though there is an associated WAL\nrecord, because subsequent modifiers (e.g. an unrelated UPDATE) of the\npage may falsely believe that a full page image is not required.\n\nWrite-Ahead Logging for Filesystem Actions\n------------------------------------------\n\nThe previous section described how to WAL-log actions that only change page\ncontents within shared buffers.  For that type of action it is generally\npossible to check all likely error cases (such as insufficient space on the\npage) before beginning to make the actual change.  Therefore we can make\nthe change and the creation of the associated WAL log record \"atomic\" by\nwrapping them into a critical section --- the odds of failure partway\nthrough are low enough that PANIC is acceptable if it does happen.\n\nClearly, that approach doesn't work for cases where there's a significant\nprobability of failure within the action to be logged, such as creation\nof a new file or database.  We don't want to PANIC, and we especially don't\nwant to PANIC after having already written a WAL record that says we did\nthe action --- if we did, replay of the record would probably fail again\nand PANIC again, making the failure unrecoverable.  This means that the\nordinary WAL rule of \"write WAL before the changes it describes\" doesn't\nwork, and we need a different design for such cases.\n\nThere are several basic types of filesystem actions that have this\nissue.  Here is how we deal with each:\n\n1. Adding a disk page to an existing table.\n\nThis action isn't WAL-logged at all.  We extend a table by writing a page\nof zeroes at its end.  We must actually do this write so that we are sure\nthe filesystem has allocated the space.  If the write fails we can just\nerror out normally.  Once the space is known allocated, we can initialize\nand fill the page via one or more normal WAL-logged actions.  Because it's\npossible that we crash between extending the file and writing out the WAL\nentries, we have to treat discovery of an all-zeroes page in a table or\nindex as being a non-error condition.  In such cases we can just reclaim\nthe space for re-use.\n\n2. Creating a new table, which requires a new file in the filesystem.\n\nWe try to create the file, and if successful we make a WAL record saying\nwe did it.  If not successful, we can just throw an error.  Notice that\nthere is a window where we have created the file but not yet written any\nWAL about it to disk.  If we crash during this window, the file remains\non disk as an \"orphan\".  It would be possible to clean up such orphans\nby having database restart search for files that don't have any committed\nentry in pg_class, but that currently isn't done because of the possibility\nof deleting data that is useful for forensic analysis of the crash.\nOrphan files are harmless --- at worst they waste a bit of disk space ---\nbecause we check for on-disk collisions when allocating new relfilenumber\nOIDs.  So cleaning up isn't really necessary.\n\n3. Deleting a table, which requires an unlink() that could fail.\n\nOur approach here is to WAL-log the operation first, but to treat failure\nof the actual unlink() call as a warning rather than error condition.\nAgain, this can leave an orphan file behind, but that's cheap compared to\nthe alternatives.  Since we can't actually do the unlink() until after\nwe've committed the DROP TABLE transaction, throwing an error would be out\nof the question anyway.  (It may be worth noting that the WAL entry about\nthe file deletion is actually part of the commit record for the dropping\ntransaction.)\n\n4. Creating and deleting databases and tablespaces, which requires creating\nand deleting directories and entire directory trees.\n\nThese cases are handled similarly to creating individual files, ie, we\ntry to do the action first and then write a WAL entry if it succeeded.\nThe potential amount of wasted disk space is rather larger, of course.\nIn the creation case we try to delete the directory tree again if creation\nfails, so as to reduce the risk of wasted space.  Failure partway through\na deletion operation results in a corrupt database: the DROP failed, but\nsome of the data is gone anyway.  There is little we can do about that,\nthough, and in any case it was presumably data the user no longer wants.\n\nIn all of these cases, if WAL replay fails to redo the original action\nwe must panic and abort recovery.  The DBA will have to manually clean up\n(for instance, free up some disk space or fix directory permissions) and\nthen restart recovery.  This is part of the reason for not writing a WAL\nentry until we've successfully done the original action.\n\n\nSkipping WAL for New RelFileLocator\n--------------------------------\n\nUnder wal_level=minimal, if a change modifies a relfilenumber that ROLLBACK\nwould unlink, in-tree access methods write no WAL for that change.  Code that\nwrites WAL without calling RelationNeedsWAL() must check for this case.  This\nskipping is mandatory.  If a WAL-writing change preceded a WAL-skipping change\nfor the same block, REDO could overwrite the WAL-skipping change.  If a\nWAL-writing change followed a WAL-skipping change for the same block, a\nrelated problem would arise.  When a WAL record contains no full-page image,\nREDO expects the page to match its contents from just before record insertion.\nA WAL-skipping change may not reach disk at all, violating REDO's expectation\nunder full_page_writes=off.  For any access method, CommitTransaction() writes\nand fsyncs affected blocks before recording the commit.\n\nPrefer to do the same in future access methods.  However, two other approaches\ncan work.  First, an access method can irreversibly transition a given fork\nfrom WAL-skipping to WAL-writing by calling FlushRelationBuffers() and\nsmgrimmedsync().  Second, an access method can opt to write WAL\nunconditionally for permanent relations.  Under these approaches, the access\nmethod callbacks must not call functions that react to RelationNeedsWAL().\n\nThis applies only to WAL records whose replay would modify bytes stored in the\nnew relfilenumber.  It does not apply to other records about the relfilenumber,\nsuch as XLOG_SMGR_CREATE.  Because it operates at the level of individual\nrelfilenumbers, RelationNeedsWAL() can differ for tightly-coupled relations.\nConsider \"CREATE TABLE t (); BEGIN; ALTER TABLE t ADD c text; ...\" in which\nALTER TABLE adds a TOAST relation.  The TOAST relation will skip WAL, while\nthe table owning it will not.  ALTER TABLE SET TABLESPACE will cause a table\nto skip WAL, but that won't affect its indexes.\n\n\nAsynchronous Commit\n-------------------\n\nAs of PostgreSQL 8.3 it is possible to perform asynchronous commits - i.e.,\nwe don't wait while the WAL record for the commit is fsync'ed.\nWe perform an asynchronous commit when synchronous_commit = off.  Instead\nof performing an XLogFlush() up to the LSN of the commit, we merely note\nthe LSN in shared memory.  The backend then continues with other work.\nWe record the LSN only for an asynchronous commit, not an abort; there's\nnever any need to flush an abort record, since the presumption after a\ncrash would be that the transaction aborted anyway.\n\nWe always force synchronous commit when the transaction is deleting\nrelations, to ensure the commit record is down to disk before the relations\nare removed from the filesystem.  Also, certain utility commands that have\nnon-roll-backable side effects (such as filesystem changes) force sync\ncommit to minimize the window in which the filesystem change has been made\nbut the transaction isn't guaranteed committed.\n\nThe walwriter regularly wakes up (via wal_writer_delay) or is woken up\n(via its latch, which is set by backends committing asynchronously) and\nperforms an XLogBackgroundFlush().  This checks the location of the last\ncompletely filled WAL page.  If that has moved forwards, then we write all\nthe changed buffers up to that point, so that under full load we write\nonly whole buffers.  If there has been a break in activity and the current\nWAL page is the same as before, then we find out the LSN of the most\nrecent asynchronous commit, and write up to that point, if required (i.e.\nif it's in the current WAL page).  If more than wal_writer_delay has\npassed, or more than wal_writer_flush_after blocks have been written, since\nthe last flush, WAL is also flushed up to the current location.  This\narrangement in itself would guarantee that an async commit record reaches\ndisk after at most two times wal_writer_delay after the transaction\ncompletes. However, we also allow XLogFlush to write/flush full buffers\n\"flexibly\" (ie, not wrapping around at the end of the circular WAL buffer\narea), so as to minimize the number of writes issued under high load when\nmultiple WAL pages are filled per walwriter cycle. This makes the worst-case\ndelay three wal_writer_delay cycles.\n\nThere are some other subtle points to consider with asynchronous commits.\nFirst, for each page of CLOG we must remember the LSN of the latest commit\naffecting the page, so that we can enforce the same flush-WAL-before-write\nrule that we do for ordinary relation pages.  Otherwise the record of the\ncommit might reach disk before the WAL record does.  Again, abort records\nneed not factor into this consideration.\n\nIn fact, we store more than one LSN for each clog page.  This relates to\nthe way we set transaction status hint bits during visibility tests.\nWe must not set a transaction-committed hint bit on a relation page and\nhave that record make it to disk prior to the WAL record of the commit.\nSince visibility tests are normally made while holding buffer share locks,\nwe do not have the option of changing the page's LSN to guarantee WAL\nsynchronization.  Instead, we defer the setting of the hint bit if we have\nnot yet flushed WAL as far as the LSN associated with the transaction.\nThis requires tracking the LSN of each unflushed async commit.  It is\nconvenient to associate this data with clog buffers: because we will flush\nWAL before writing a clog page, we know that we do not need to remember a\ntransaction's LSN longer than the clog page holding its commit status\nremains in memory.  However, the naive approach of storing an LSN for each\nclog position is unattractive: the LSNs are 32x bigger than the two-bit\ncommit status fields, and so we'd need 256K of additional shared memory for\neach 8K clog buffer page.  We choose instead to store a smaller number of\nLSNs per page, where each LSN is the highest LSN associated with any\ntransaction commit in a contiguous range of transaction IDs on that page.\nThis saves storage at the price of some possibly-unnecessary delay in\nsetting transaction hint bits.\n\nHow many transactions should share the same cached LSN (N)?  If the\nsystem's workload consists only of small async-commit transactions, then\nit's reasonable to have N similar to the number of transactions per\nwalwriter cycle, since that is the granularity with which transactions will\nbecome truly committed (and thus hintable) anyway.  The worst case is where\na sync-commit xact shares a cached LSN with an async-commit xact that\ncommits a bit later; even though we paid to sync the first xact to disk,\nwe won't be able to hint its outputs until the second xact is sync'd, up to\nthree walwriter cycles later.  This argues for keeping N (the group size)\nas small as possible.  For the moment we are setting the group size to 32,\nwhich makes the LSN cache space the same size as the actual clog buffer\nspace (independently of BLCKSZ).\n\nIt is useful that we can run both synchronous and asynchronous commit\ntransactions concurrently, but the safety of this is perhaps not\nimmediately obvious.  Assume we have two transactions, T1 and T2.  The Log\nSequence Number (LSN) is the point in the WAL sequence where a transaction\ncommit is recorded, so LSN1 and LSN2 are the commit records of those\ntransactions.  If T2 can see changes made by T1 then when T2 commits it\nmust be true that LSN2 follows LSN1.  Thus when T2 commits it is certain\nthat all of the changes made by T1 are also now recorded in the WAL.  This\nis true whether T1 was asynchronous or synchronous.  As a result, it is\nsafe for asynchronous commits and synchronous commits to work concurrently\nwithout endangering data written by synchronous commits.  Sub-transactions\nare not important here since the final write to disk only occurs at the\ncommit of the top level transaction.\n\nChanges to data blocks cannot reach disk unless WAL is flushed up to the\npoint of the LSN of the data blocks.  Any attempt to write unsafe data to\ndisk will trigger a write which ensures the safety of all data written by\nthat and prior transactions.  Data blocks and clog pages are both protected\nby LSNs.\n\nChanges to a temp table are not WAL-logged, hence could reach disk in\nadvance of T1's commit, but we don't care since temp table contents don't\nsurvive crashes anyway.\n\nDatabase writes that skip WAL for new relfilenumbers are also safe.  In these\ncases it's entirely possible for the data to reach disk before T1's commit,\nbecause T1 will fsync it down to disk without any sort of interlock.  However,\nall these paths are designed to write data that no other transaction can see\nuntil after T1 commits.  The situation is thus not different from ordinary\nWAL-logged updates.\n\nTransaction Emulation during Recovery\n-------------------------------------\n\nDuring Recovery we replay transaction changes in the order they occurred.\nAs part of this replay we emulate some transactional behaviour, so that\nread only backends can take MVCC snapshots. We do this by maintaining a\nlist of XIDs belonging to transactions that are being replayed, so that\neach transaction that has recorded WAL records for database writes exist\nin the array until it commits. Further details are given in comments in\nprocarray.c.\n\nMany actions write no WAL records at all, for example read only transactions.\nThese have no effect on MVCC in recovery and we can pretend they never\noccurred at all. Subtransaction commit does not write a WAL record either\nand has very little effect, since lock waiters need to wait for the\nparent transaction to complete.\n\nNot all transactional behaviour is emulated, for example we do not insert\na transaction entry into the lock table, nor do we maintain the transaction\nstack in memory. Clog, multixact and commit_ts entries are made normally.\nSubtrans is maintained during recovery but the details of the transaction\ntree are ignored and all subtransactions reference the top-level TransactionId\ndirectly. Since commit is atomic this provides correct lock wait behaviour\nyet simplifies emulation of subtransactions considerably.\n\nFurther details on locking mechanics in recovery are given in comments\nwith the Lock rmgr code.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\transam\\README",
      "directory": "backend\\access\\transam"
    }
  },
  {
    "title": "README: backend\\access\\transam",
    "url": "backend\\access\\transam\\README.parallel",
    "content": "Overview\n========\n\nPostgreSQL provides some simple facilities to make writing parallel algorithms\neasier.  Using a data structure called a ParallelContext, you can arrange to\nlaunch background worker processes, initialize their state to match that of\nthe backend which initiated parallelism, communicate with them via dynamic\nshared memory, and write reasonably complex code that can run either in the\nuser backend or in one of the parallel workers without needing to be aware of\nwhere it's running.\n\nThe backend which starts a parallel operation (hereafter, the initiating\nbackend) starts by creating a dynamic shared memory segment which will last\nfor the lifetime of the parallel operation.  This dynamic shared memory segment\nwill contain (1) a shm_mq that can be used to transport errors (and other\nmessages reported via elog/ereport) from the worker back to the initiating\nbackend; (2) serialized representations of the initiating backend's private\nstate, so that the worker can synchronize its state with of the initiating\nbackend; and (3) any other data structures which a particular user of the\nParallelContext data structure may wish to add for its own purposes.  Once\nthe initiating backend has initialized the dynamic shared memory segment, it\nasks the postmaster to launch the appropriate number of parallel workers.\nThese workers then connect to the dynamic shared memory segment, initiate\ntheir state, and then invoke the appropriate entrypoint, as further detailed\nbelow.\n\nError Reporting\n===============\n\nWhen started, each parallel worker begins by attaching the dynamic shared\nmemory segment and locating the shm_mq to be used for error reporting; it\nredirects all of its protocol messages to this shm_mq.  Prior to this point,\nany failure of the background worker will not be reported to the initiating\nbackend; from the point of view of the initiating backend, the worker simply\nfailed to start.  The initiating backend must anyway be prepared to cope\nwith fewer parallel workers than it originally requested, so catering to\nthis case imposes no additional burden.\n\nWhenever a new message (or partial message; very large messages may wrap) is\nsent to the error-reporting queue, PROCSIG_PARALLEL_MESSAGE is sent to the\ninitiating backend.  This causes the next CHECK_FOR_INTERRUPTS() in the\ninitiating backend to read and rethrow the message.  For the most part, this\nmakes error reporting in parallel mode \"just work\".  Of course, to work\nproperly, it is important that the code the initiating backend is executing\nCHECK_FOR_INTERRUPTS() regularly and avoid blocking interrupt processing for\nlong periods of time, but those are good things to do anyway.\n\n(A currently-unsolved problem is that some messages may get written to the\nsystem log twice, once in the backend where the report was originally\ngenerated, and again when the initiating backend rethrows the message.  If\nwe decide to suppress one of these reports, it should probably be second one;\notherwise, if the worker is for some reason unable to propagate the message\nback to the initiating backend, the message will be lost altogether.)\n\nState Sharing\n=============\n\nIt's possible to write C code which works correctly without parallelism, but\nwhich fails when parallelism is used.  No parallel infrastructure can\ncompletely eliminate this problem, because any global variable is a risk.\nThere's no general mechanism for ensuring that every global variable in the\nworker will have the same value that it does in the initiating backend; even\nif we could ensure that, some function we're calling could update the variable\nafter each call, and only the backend where that update is performed will see\nthe new value.  Similar problems can arise with any more-complex data\nstructure we might choose to use.  For example, a pseudo-random number\ngenerator should, given a particular seed value, produce the same predictable\nseries of values every time.  But it does this by relying on some private\nstate which won't automatically be shared between cooperating backends.  A\nparallel-safe PRNG would need to store its state in dynamic shared memory, and\nwould require locking.  The parallelism infrastructure has no way of knowing\nwhether the user intends to call code that has this sort of problem, and can't\ndo anything about it anyway.\n\nInstead, we take a more pragmatic approach. First, we try to make as many of\nthe operations that are safe outside of parallel mode work correctly in\nparallel mode as well.  Second, we try to prohibit common unsafe operations\nvia suitable error checks.  These checks are intended to catch 100% of\nunsafe things that a user might do from the SQL interface, but code written\nin C can do unsafe things that won't trigger these checks.  The error checks\nare engaged via EnterParallelMode(), which should be called before creating\na parallel context, and disarmed via ExitParallelMode(), which should be\ncalled after all parallel contexts have been destroyed.  The most\nsignificant restriction imposed by parallel mode is that all operations must\nbe strictly read-only; we allow no writes to the database and no DDL.  We\nmight try to relax these restrictions in the future.\n\nTo make as many operations as possible safe in parallel mode, we try to copy\nthe most important pieces of state from the initiating backend to each parallel\nworker.  This includes:\n\n  - The set of libraries dynamically loaded by dfmgr.c.\n\n  - The authenticated user ID and current database.  Each parallel worker\n    will connect to the same database as the initiating backend, using the\n    same user ID.\n\n  - The values of all GUCs.  Accordingly, permanent changes to the value of\n    any GUC are forbidden while in parallel mode; but temporary changes,\n    such as entering a function with non-NULL proconfig, are OK.\n\n  - The current subtransaction's XID, the top-level transaction's XID, and\n    the list of XIDs considered current (that is, they are in-progress or\n    subcommitted).  This information is needed to ensure that tuple visibility\n    checks return the same results in the worker as they do in the\n    initiating backend.  See also the section Transaction Integration, below.\n\n  - The combo CID mappings.  This is needed to ensure consistent answers to\n    tuple visibility checks.  The need to synchronize this data structure is\n    a major reason why we can't support writes in parallel mode: such writes\n    might create new combo CIDs, and we have no way to let other workers\n    (or the initiating backend) know about them.\n\n  - The transaction snapshot.\n\n  - The active snapshot, which might be different from the transaction\n    snapshot.\n\n  - The currently active user ID and security context.  Note that this is\n    the fourth user ID we restore: the initial step of binding to the correct\n    database also involves restoring the authenticated user ID.  When GUC\n    values are restored, this incidentally sets SessionUserId and OuterUserId\n    to the correct values.  This final step restores CurrentUserId.\n\n  - State related to pending REINDEX operations, which prevents access to\n    an index that is currently being rebuilt.\n\n  - Active relmapper.c mapping state.  This is needed to allow consistent\n    answers when fetching the current relfilenumber for relation oids of\n    mapped relations.\n\nTo prevent unprincipled deadlocks when running in parallel mode, this code\nalso arranges for the leader and all workers to participate in group\nlocking.  See src/backend/storage/lmgr/README for more details.\n\nTransaction Integration\n=======================\n\nRegardless of what the TransactionState stack looks like in the parallel\nleader, each parallel worker begins with a stack of depth 1.  This stack\nentry is marked with the special transaction block state\nTBLOCK_PARALLEL_INPROGRESS so that it's not confused with an ordinary\ntoplevel transaction.  The XID of this TransactionState is set to the XID of\nthe innermost currently-active subtransaction in the initiating backend.  The\ninitiating backend's toplevel XID, and the XIDs of all current (in-progress\nor subcommitted) XIDs are stored separately from the TransactionState stack,\nbut in such a way that GetTopTransactionId(), GetTopTransactionIdIfAny(), and\nTransactionIdIsCurrentTransactionId() return the same values that they would\nin the initiating backend.  We could copy the entire transaction state stack,\nbut most of it would be useless: for example, you can't roll back to a\nsavepoint from within a parallel worker, and there are no resources to\nassociated with the memory contexts or resource owners of intermediate\nsubtransactions.\n\nNo meaningful change to the transaction state can be made while in parallel\nmode.  No XIDs can be assigned, and no command counter increments can happen,\nbecause we have no way of communicating these state changes to cooperating\nbackends, or of synchronizing them.  It's clearly unworkable for the initiating\nbackend to exit any transaction or subtransaction that was in progress when\nparallelism was started before all parallel workers have exited; and it's even\nmore clearly crazy for a parallel worker to try to subcommit or subabort the\ncurrent subtransaction and execute in some other transaction context than was\npresent in the initiating backend.  However, we allow internal subtransactions\n(e.g. those used to implement a PL/pgSQL EXCEPTION block) to be used in\nparallel mode, provided that they remain XID-less, because other backends\ndon't really need to know about those transactions or do anything differently\nbecause of them.\n\nAt the end of a parallel operation, which can happen either because it\ncompleted successfully or because it was interrupted by an error, parallel\nworkers associated with that operation exit.  In the error case, transaction\nabort processing in the parallel leader kills off any remaining workers, and\nthe parallel leader then waits for them to die.  In the case of a successful\nparallel operation, the parallel leader does not send any signals, but must\nwait for workers to complete and exit of their own volition.  In either\ncase, it is very important that all workers actually exit before the\nparallel leader cleans up the (sub)transaction in which they were created;\notherwise, chaos can ensue.  For example, if the leader is rolling back the\ntransaction that created the relation being scanned by a worker, the\nrelation could disappear while the worker is still busy scanning it.  That's\nnot safe.\n\nGenerally, the cleanup performed by each worker at this point is similar to\ntop-level commit or abort.  Each backend has its own resource owners: buffer\npins, catcache or relcache reference counts, tuple descriptors, and so on\nare managed separately by each backend, and must free them before exiting.\nThere are, however, some important differences between parallel worker\ncommit or abort and a real top-level transaction commit or abort.  Most\nimportantly:\n\n  - No commit or abort record is written; the initiating backend is\n    responsible for this.\n\n  - Cleanup of pg_temp namespaces is not done.  Parallel workers cannot\n    safely access the initiating backend's pg_temp namespace, and should\n    not create one of their own.\n\nCoding Conventions\n===================\n\nBefore beginning any parallel operation, call EnterParallelMode(); after all\nparallel operations are completed, call ExitParallelMode().  To actually\nparallelize a particular operation, use a ParallelContext.  The basic coding\npattern looks like this:\n\n\tEnterParallelMode();\t\t/* prohibit unsafe state changes */\n\n\tpcxt = CreateParallelContext(\"library_name\", \"function_name\", nworkers);\n\n\t/* Allow space for application-specific data here. */\n\tshm_toc_estimate_chunk(&pcxt->estimator, size);\n\tshm_toc_estimate_keys(&pcxt->estimator, keys);\n\n\tInitializeParallelDSM(pcxt);\t/* create DSM and copy state to it */\n\n\t/* Store the data for which we reserved space. */\n\tspace = shm_toc_allocate(pcxt->toc, size);\n\tshm_toc_insert(pcxt->toc, key, space);\n\n\tLaunchParallelWorkers(pcxt);\n\n\t/* do parallel stuff */\n\n\tWaitForParallelWorkersToFinish(pcxt);\n\n\t/* read any final results from dynamic shared memory */\n\n\tDestroyParallelContext(pcxt);\n\n\tExitParallelMode();\n\nIf desired, after WaitForParallelWorkersToFinish() has been called, the\ncontext can be reset so that workers can be launched anew using the same\nparallel context.  To do this, first call ReinitializeParallelDSM() to\nreinitialize state managed by the parallel context machinery itself; then,\nperform any other necessary resetting of state; after that, you can again\ncall LaunchParallelWorkers.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\transam\\README.parallel",
      "directory": "backend\\access\\transam"
    }
  },
  {
    "title": "README: backend\\executor",
    "url": "backend\\executor\\README",
    "content": "src/backend/executor/README\n\nThe Postgres Executor\n=====================\n\nThe executor processes a tree of \"plan nodes\".  The plan tree is essentially\na demand-pull pipeline of tuple processing operations.  Each node, when\ncalled, will produce the next tuple in its output sequence, or NULL if no\nmore tuples are available.  If the node is not a primitive relation-scanning\nnode, it will have child node(s) that it calls in turn to obtain input\ntuples.\n\nRefinements on this basic model include:\n\n* Choice of scan direction (forwards or backwards).  Caution: this is not\ncurrently well-supported.  It works for primitive scan nodes, but not very\nwell for joins, aggregates, etc.\n\n* Rescan command to reset a node and make it generate its output sequence\nover again.\n\n* Parameters that can alter a node's results.  After adjusting a parameter,\nthe rescan command must be applied to that node and all nodes above it.\nThere is a moderately intelligent scheme to avoid rescanning nodes\nunnecessarily (for example, Sort does not rescan its input if no parameters\nof the input have changed, since it can just reread its stored sorted data).\n\nFor a SELECT, it is only necessary to deliver the top-level result tuples\nto the client.  For INSERT/UPDATE/DELETE/MERGE, the actual table modification\noperations happen in a top-level ModifyTable plan node.  If the query\nincludes a RETURNING clause, the ModifyTable node delivers the computed\nRETURNING rows as output, otherwise it returns nothing.  Handling INSERT\nis pretty straightforward: the tuples returned from the plan tree below\nModifyTable are inserted into the correct result relation.  For UPDATE,\nthe plan tree returns the new values of the updated columns, plus \"junk\"\n(hidden) column(s) identifying which table row is to be updated.  The\nModifyTable node must fetch that row to extract values for the unchanged\ncolumns, combine the values into a new row, and apply the update.  (For a\nheap table, the row-identity junk column is a CTID, but other things may\nbe used for other table types.)  For DELETE, the plan tree need only deliver\njunk row-identity column(s), and the ModifyTable node visits each of those\nrows and marks the row deleted.  MERGE is described below.\n\nXXX a great deal more documentation needs to be written here...\n\n\nPlan Trees and State Trees\n--------------------------\n\nThe plan tree delivered by the planner contains a tree of Plan nodes (struct\ntypes derived from struct Plan).  During executor startup we build a parallel\ntree of identical structure containing executor state nodes --- generally,\nevery plan node type has a corresponding executor state node type.  Each node\nin the state tree has a pointer to its corresponding node in the plan tree,\nplus executor state data as needed to implement that node type.  This\narrangement allows the plan tree to be completely read-only so far as the\nexecutor is concerned: all data that is modified during execution is in the\nstate tree.  Read-only plan trees make life much simpler for plan caching and\nreuse.\n\nA corresponding executor state node may not be created during executor startup\nif the executor determines that an entire subplan is not required due to\nexecution time partition pruning determining that no matching records will be\nfound there.  This currently only occurs for Append and MergeAppend nodes.  In\nthis case the non-required subplans are ignored and the executor state's\nsubnode array will become out of sequence to the plan's subplan list.\n\nEach Plan node may have expression trees associated with it, to represent\nits target list, qualification conditions, etc.  These trees are also\nread-only to the executor, but the executor state for expression evaluation\ndoes not mirror the Plan expression's tree shape, as explained below.\nRather, there's just one ExprState node per expression tree, although this\nmay have sub-nodes for some complex expression node types.\n\nAltogether there are four classes of nodes used in these trees: Plan nodes,\ntheir corresponding PlanState nodes, Expr nodes, and ExprState nodes.\n(Actually, there are also List nodes, which are used as \"glue\" in all\nthree tree-based representations.)\n\n\nExpression Trees and ExprState nodes\n------------------------------------\n\nExpression trees, in contrast to Plan trees, are not mirrored into a\ncorresponding tree of state nodes.  Instead each separately executable\nexpression tree (e.g. a Plan's qual or targetlist) is represented by one\nExprState node.  The ExprState node contains the information needed to\nevaluate the expression in a compact, linear form.  That compact form is\nstored as a flat array in ExprState->steps[] (an array of ExprEvalStep,\nnot ExprEvalStep *).\n\nThe reasons for choosing such a representation include:\n- commonly the amount of work needed to evaluate one Expr-type node is\n  small enough that the overhead of having to perform a tree-walk\n  during evaluation is significant.\n- the flat representation can be evaluated non-recursively within a single\n  function, reducing stack depth and function call overhead.\n- such a representation is usable both for fast interpreted execution,\n  and for compiling into native code.\n\nThe Plan-tree representation of an expression is compiled into an\nExprState node by ExecInitExpr().  As much complexity as possible should\nbe handled by ExecInitExpr() (and helpers), instead of execution time\nwhere both interpreted and compiled versions would need to deal with the\ncomplexity.  Besides duplicating effort between execution approaches,\nruntime initialization checks also have a small but noticeable cost every\ntime the expression is evaluated.  Therefore, we allow ExecInitExpr() to\nprecompute information that we do not expect to vary across execution of a\nsingle query, for example the set of CHECK constraint expressions to be\napplied to a domain type.  This could not be done at plan time without\ngreatly increasing the number of events that require plan invalidation.\n(Previously, some information of this kind was rechecked on each\nexpression evaluation, but that seems like unnecessary overhead.)\n\n\nExpression Initialization\n-------------------------\n\nDuring ExecInitExpr() and similar routines, Expr trees are converted\ninto the flat representation.  Each Expr node might be represented by\nzero, one, or more ExprEvalSteps.\n\nEach ExprEvalStep's work is determined by its opcode (of enum ExprEvalOp)\nand it stores the result of its work into the Datum variable and boolean\nnull flag variable pointed to by ExprEvalStep->resvalue/resnull.\nComplex expressions are performed by chaining together several steps.\nFor example, \"a + b\" (one OpExpr, with two Var expressions) would be\nrepresented as two steps to fetch the Var values, and one step for the\nevaluation of the function underlying the + operator.  The steps for the\nVars would have their resvalue/resnull pointing directly to the appropriate\nargs[].value .isnull elements in the FunctionCallInfoBaseData struct that\nis used by the function evaluation step, thus avoiding extra work to copy\nthe result values around.\n\nThe last entry in a completed ExprState->steps array is always an\nEEOP_DONE step; this removes the need to test for end-of-array while\niterating.  Also, if the expression contains any variable references (to\nuser columns of the ExprContext's INNER, OUTER, or SCAN tuples), the steps\narray begins with EEOP_*_FETCHSOME steps that ensure that the relevant\ntuples have been deconstructed to make the required columns directly\navailable (cf. slot_getsomeattrs()).  This allows individual Var-fetching\nsteps to be little more than an array lookup.\n\nMost of ExecInitExpr()'s work is done by the recursive function\nExecInitExprRec() and its subroutines.  ExecInitExprRec() maps one Expr\nnode into the steps required for execution, recursing as needed for\nsub-expressions.\n\nEach ExecInitExprRec() call has to specify where that subexpression's\nresults are to be stored (via the resv/resnull parameters).  This allows\nthe above scenario of evaluating a (sub-)expression directly into\nfcinfo->args[].value/isnull, but also requires some care: target Datum/isnull\nvariables may not be shared with another ExecInitExprRec() unless the\nresults are only needed by steps executing before further usages of those\ntarget Datum/isnull variables.  Due to the non-recursiveness of the\nExprEvalStep representation that's usually easy to guarantee.\n\nExecInitExprRec() pushes new operations into the ExprState->steps array\nusing ExprEvalPushStep().  To keep the steps as a consecutively laid out\narray, ExprEvalPushStep() has to repalloc the entire array when there's\nnot enough space.  Because of that it is *not* allowed to point directly\ninto any of the steps during expression initialization.  Therefore, the\nresv/resnull for a subexpression usually point to some storage that is\npalloc'd separately from the steps array.  For instance, the\nFunctionCallInfoBaseData for a function call step is separately allocated\nrather than being part of the ExprEvalStep array.  The overall result\nof a complete expression is typically returned into the resvalue/resnull\nfields of the ExprState node itself.\n\nSome steps, e.g. boolean expressions, allow skipping evaluation of\ncertain subexpressions.  In the flat representation this amounts to\njumping to some later step rather than just continuing consecutively\nwith the next step.  The target for such a jump is represented by\nthe integer index in the ExprState->steps array of the step to execute\nnext.  (Compare the EEO_NEXT and EEO_JUMP macros in execExprInterp.c.)\n\nTypically, ExecInitExprRec() has to push a jumping step into the steps\narray, then recursively generate steps for the subexpression that might\nget skipped over, then go back and fix up the jump target index using\nthe now-known length of the subexpression's steps.  This is handled by\nadjust_jumps lists in execExpr.c.\n\nThe last step in constructing an ExprState is to apply ExecReadyExpr(),\nwhich readies it for execution using whichever execution method has been\nselected.\n\n\nExpression Evaluation\n---------------------\n\nTo allow for different methods of expression evaluation, and for\nbetter branch/jump target prediction, expressions are evaluated by\ncalling ExprState->evalfunc (via ExecEvalExpr() and friends).\n\nExecReadyExpr() can choose the method of interpretation by setting\nevalfunc to an appropriate function.  The default execution function,\nExecInterpExpr, is implemented in execExprInterp.c; see its header\ncomment for details.  Special-case evalfuncs are used for certain\nespecially-simple expressions.\n\nNote that a lot of the more complex expression evaluation steps, which are\nless performance-critical than the simpler ones, are implemented as\nseparate functions outside the fast-path of expression execution, allowing\ntheir implementation to be shared between interpreted and compiled\nexpression evaluation.  This means that these helper functions are not\nallowed to perform expression step dispatch themselves, as the method of\ndispatch will vary based on the caller.  The helpers therefore cannot call\nfor the execution of subexpressions; all subexpression results they need\nmust be computed by earlier steps.  And dispatch to the following\nexpression step must be performed after returning from the helper.\n\n\nTargetlist Evaluation\n---------------------\n\nExecBuildProjectionInfo builds an ExprState that has the effect of\nevaluating a targetlist into ExprState->resultslot.  A generic targetlist\nexpression is executed by evaluating it as discussed above (storing the\nresult into the ExprState's resvalue/resnull fields) and then using an\nEEOP_ASSIGN_TMP step to move the result into the appropriate tts_values[]\nand tts_isnull[] array elements of the result slot.  There are special\nfast-path step types (EEOP_ASSIGN_*_VAR) to handle targetlist entries that\nare simple Vars using only one step instead of two.\n\n\nMERGE\n-----\n\nMERGE is a multiple-table, multiple-action command: It specifies a target\ntable and a source relation, and can contain multiple WHEN MATCHED and\nWHEN NOT MATCHED clauses, each of which specifies one UPDATE, INSERT,\nDELETE, or DO NOTHING actions.  The target table is modified by MERGE,\nand the source relation supplies additional data for the actions.  Each action\noptionally specifies a qualifying expression that is evaluated for each tuple.\n\nIn the planner, transform_MERGE_to_join constructs a join between the target\ntable and the source relation, with row-identifying junk columns from the target\ntable.  This join is an outer join if the MERGE command contains any WHEN NOT\nMATCHED clauses; the ModifyTable node fetches tuples from the plan tree of that\njoin.  If the row-identifying columns in the fetched tuple are NULL, then the\nsource relation contains a tuple that is not matched by any tuples in the\ntarget table, so the qualifying expression for each WHEN NOT MATCHED clause is\nevaluated given that tuple as returned by the plan.  If the expression returns\ntrue, the action indicated by the clause is executed, and no further clauses\nare evaluated.  On the other hand, if the row-identifying columns are not\nNULL, then the matching tuple from the target table can be fetched; qualifying\nexpression of each WHEN MATCHED clause is evaluated given both the fetched\ntuple and the tuple returned by the plan.\n\nIf no WHEN NOT MATCHED clauses are present, then the join constructed by\nthe planner is an inner join, and the row-identifying junk columns are\nalways non NULL.\n\nIf WHEN MATCHED ends up processing a row that is concurrently updated or deleted,\nEvalPlanQual (see below) is used to find the latest version of the row, and\nthat is re-fetched; if it exists, the search for a matching WHEN MATCHED clause\nto use starts at the top.\n\nMERGE does not allow its own type of triggers, but instead fires UPDATE, DELETE,\nand INSERT triggers: row triggers are fired for each row when an action is\nexecuted for that row.  Statement triggers are fired always, regardless of\nwhether any rows match the corresponding clauses.\n\n\nMemory Management\n-----------------\n\nA \"per query\" memory context is created during CreateExecutorState();\nall storage allocated during an executor invocation is allocated in that\ncontext or a child context.  This allows easy reclamation of storage\nduring executor shutdown --- rather than messing with retail pfree's and\nprobable storage leaks, we just destroy the memory context.\n\nIn particular, the plan state trees and expression state trees described\nin the previous section are allocated in the per-query memory context.\n\nTo avoid intra-query memory leaks, most processing while a query runs\nis done in \"per tuple\" memory contexts, which are so-called because they\nare typically reset to empty once per tuple.  Per-tuple contexts are usually\nassociated with ExprContexts, and commonly each PlanState node has its own\nExprContext to evaluate its qual and targetlist expressions in.\n\n\nQuery Processing Control Flow\n-----------------------------\n\nThis is a sketch of control flow for full query processing:\n\n\tCreateQueryDesc\n\n\tExecutorStart\n\t\tCreateExecutorState\n\t\t\tcreates per-query context\n\t\tswitch to per-query context to run ExecInitNode\n\t\tAfterTriggerBeginQuery\n\t\tExecInitNode --- recursively scans plan tree\n\t\t\tExecInitNode\n\t\t\t\trecurse into subsidiary nodes\n\t\t\tCreateExprContext\n\t\t\t\tcreates per-tuple context\n\t\t\tExecInitExpr\n\n\tExecutorRun\n\t\tExecProcNode --- recursively called in per-query context\n\t\t\tExecEvalExpr --- called in per-tuple context\n\t\t\tResetExprContext --- to free memory\n\n\tExecutorFinish\n\t\tExecPostprocessPlan --- run any unfinished ModifyTable nodes\n\t\tAfterTriggerEndQuery\n\n\tExecutorEnd\n\t\tExecEndNode --- recursively releases resources\n\t\tFreeExecutorState\n\t\t\tfrees per-query context and child contexts\n\n\tFreeQueryDesc\n\nPer above comments, it's not really critical for ExecEndNode to free any\nmemory; it'll all go away in FreeExecutorState anyway.  However, we do need to\nbe careful to close relations, drop buffer pins, etc, so we do need to scan\nthe plan state tree to find these sorts of resources.\n\n\nThe executor can also be used to evaluate simple expressions without any Plan\ntree (\"simple\" meaning \"no aggregates and no sub-selects\", though such might\nbe hidden inside function calls).  This case has a flow of control like\n\n\tCreateExecutorState\n\t\tcreates per-query context\n\n\tCreateExprContext\t-- or use GetPerTupleExprContext(estate)\n\t\tcreates per-tuple context\n\n\tExecPrepareExpr\n\t\ttemporarily switch to per-query context\n\t\trun the expression through expression_planner\n\t\tExecInitExpr\n\n\tRepeatedly do:\n\t\tExecEvalExprSwitchContext\n\t\t\tExecEvalExpr --- called in per-tuple context\n\t\tResetExprContext --- to free memory\n\n\tFreeExecutorState\n\t\tfrees per-query context, as well as ExprContext\n\t\t(a separate FreeExprContext call is not necessary)\n\n\nEvalPlanQual (READ COMMITTED Update Checking)\n---------------------------------------------\n\nFor simple SELECTs, the executor need only pay attention to tuples that are\nvalid according to the snapshot seen by the current transaction (ie, they\nwere inserted by a previously committed transaction, and not deleted by any\npreviously committed transaction).  However, for UPDATE, DELETE, and MERGE it\nis not cool to modify or delete a tuple that's been modified by an open or\nconcurrently-committed transaction.  If we are running in SERIALIZABLE\nisolation level then we just raise an error when this condition is seen to\noccur.  In READ COMMITTED isolation level, we must work a lot harder.\n\nThe basic idea in READ COMMITTED mode is to take the modified tuple\ncommitted by the concurrent transaction (after waiting for it to commit,\nif need be) and re-evaluate the query qualifications to see if it would\nstill meet the quals.  If so, we regenerate the updated tuple (if we are\ndoing an UPDATE) from the modified tuple, and finally update/delete the\nmodified tuple.  SELECT FOR UPDATE/SHARE behaves similarly, except that its\naction is just to lock the modified tuple and return results based on that\nversion of the tuple.\n\nTo implement this checking, we actually re-run the query from scratch for\neach modified tuple (or set of tuples, for SELECT FOR UPDATE), with the\nrelation scan nodes tweaked to return only the current tuples --- either\nthe original ones, or the updated (and now locked) versions of the modified\ntuple(s).  If this query returns a tuple, then the modified tuple(s) pass\nthe quals (and the query output is the suitably modified update tuple, if\nwe're doing UPDATE).  If no tuple is returned, then the modified tuple(s)\nfail the quals, so we ignore the current result tuple and continue the\noriginal query.\n\nIn UPDATE/DELETE/MERGE, only the target relation needs to be handled this way.\nIn SELECT FOR UPDATE, there may be multiple relations flagged FOR UPDATE,\nso we obtain lock on the current tuple version in each such relation before\nexecuting the recheck.\n\nIt is also possible that there are relations in the query that are not\nto be locked (they are neither the UPDATE/DELETE/MERGE target nor specified\nto be locked in SELECT FOR UPDATE/SHARE).  When re-running the test query\nwe want to use the same rows from these relations that were joined to\nthe locked rows.  For ordinary relations this can be implemented relatively\ncheaply by including the row TID in the join outputs and re-fetching that\nTID.  (The re-fetch is expensive, but we're trying to optimize the normal\ncase where no re-test is needed.)  We have also to consider non-table\nrelations, such as a ValuesScan or FunctionScan.  For these, since there\nis no equivalent of TID, the only practical solution seems to be to include\nthe entire row value in the join output row.\n\nWe disallow set-returning functions in the targetlist of SELECT FOR UPDATE,\nso as to ensure that at most one tuple can be returned for any particular\nset of scan tuples.  Otherwise we'd get duplicates due to the original\nquery returning the same set of scan tuples multiple times.  Likewise,\nSRFs are disallowed in an UPDATE's targetlist.  There, they would have the\neffect of the same row being updated multiple times, which is not very\nuseful --- and updates after the first would have no effect anyway.\n\n\nAsynchronous Execution\n----------------------\n\nIn cases where a node is waiting on an event external to the database system,\nsuch as a ForeignScan awaiting network I/O, it's desirable for the node to\nindicate that it cannot return any tuple immediately but may be able to do so\nat a later time.  A process which discovers this type of situation can always\nhandle it simply by blocking, but this may waste time that could be spent\nexecuting some other part of the plan tree where progress could be made\nimmediately.  This is particularly likely to occur when the plan tree contains\nan Append node.  Asynchronous execution runs multiple parts of an Append node\nconcurrently rather than serially to improve performance.\n\nFor asynchronous execution, an Append node must first request a tuple from an\nasync-capable child node using ExecAsyncRequest.  Next, it must execute the\nasynchronous event loop using ExecAppendAsyncEventWait.  Eventually, when a\nchild node to which an asynchronous request has been made produces a tuple,\nthe Append node will receive it from the event loop via ExecAsyncResponse.  In\nthe current implementation of asynchronous execution, the only node type that\nrequests tuples from an async-capable child node is an Append, while the only\nnode type that might be async-capable is a ForeignScan.\n\nTypically, the ExecAsyncResponse callback is the only one required for nodes\nthat wish to request tuples asynchronously.  On the other hand, async-capable\nnodes generally need to implement three methods:\n\n1. When an asynchronous request is made, the node's ExecAsyncRequest callback\n   will be invoked; it should use ExecAsyncRequestPending to indicate that the\n   request is pending for a callback described below.  Alternatively, it can\n   instead use ExecAsyncRequestDone if a result is available immediately.\n\n2. When the event loop wishes to wait or poll for file descriptor events, the\n   node's ExecAsyncConfigureWait callback will be invoked to configure the\n   file descriptor event for which the node wishes to wait.\n\n3. When the file descriptor becomes ready, the node's ExecAsyncNotify callback\n   will be invoked; like #1, it should use ExecAsyncRequestPending for another\n   callback or ExecAsyncRequestDone to return a result immediately.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\executor\\README",
      "directory": "backend\\executor"
    }
  },
  {
    "title": "README: backend\\jit",
    "url": "backend\\jit\\README",
    "content": "What is Just-in-Time Compilation?\n=================================\n\nJust-in-Time compilation (JIT) is the process of turning some form of\ninterpreted program evaluation into a native program, and doing so at\nruntime.\n\nFor example, instead of using a facility that can evaluate arbitrary\nSQL expressions to evaluate an SQL predicate like WHERE a.col = 3, it\nis possible to generate a function than can be natively executed by\nthe CPU that just handles that expression, yielding a speedup.\n\nThis is JIT, rather than ahead-of-time (AOT) compilation, because it\nis done at query execution time, and perhaps only in cases where the\nrelevant task is repeated a number of times. Given the way JIT\ncompilation is used in PostgreSQL, the lines between interpretation,\nAOT and JIT are somewhat blurry.\n\nNote that the interpreted program turned into a native program does\nnot necessarily have to be a program in the classical sense. E.g. it\nis highly beneficial to JIT compile tuple deforming into a native\nfunction just handling a specific type of table, despite tuple\ndeforming not commonly being understood as a \"program\".\n\n\nWhy JIT?\n========\n\nParts of PostgreSQL are commonly bottlenecked by comparatively small\npieces of CPU intensive code. In a number of cases that is because the\nrelevant code has to be very generic (e.g. handling arbitrary SQL\nlevel expressions, over arbitrary tables, with arbitrary extensions\ninstalled). This often leads to a large number of indirect jumps and\nunpredictable branches, and generally a high number of instructions\nfor a given task. E.g. just evaluating an expression comparing a\ncolumn in a database to an integer ends up needing several hundred\ncycles.\n\nBy generating native code large numbers of indirect jumps can be\nremoved by either making them into direct branches (e.g. replacing the\nindirect call to an SQL operator's implementation with a direct call\nto that function), or by removing it entirely (e.g. by evaluating the\nbranch at compile time because the input is constant). Similarly a lot\nof branches can be entirely removed (e.g. by again evaluating the\nbranch at compile time because the input is constant). The latter is\nparticularly beneficial for removing branches during tuple deforming.\n\n\nHow to JIT\n==========\n\nPostgreSQL, by default, uses LLVM to perform JIT. LLVM was chosen\nbecause it is developed by several large corporations and therefore\nunlikely to be discontinued, because it has a license compatible with\nPostgreSQL, and because its IR can be generated from C using the Clang\ncompiler.\n\n\nShared Library Separation\n-------------------------\n\nTo avoid the main PostgreSQL binary directly depending on LLVM, which\nwould prevent LLVM support being independently installed by OS package\nmanagers, the LLVM dependent code is located in a shared library that\nis loaded on-demand.\n\nAn additional benefit of doing so is that it is relatively easy to\nevaluate JIT compilation that does not use LLVM, by changing out the\nshared library used to provide JIT compilation.\n\nTo achieve this, code intending to perform JIT (e.g. expression evaluation)\ncalls an LLVM independent wrapper located in jit.c to do so. If the\nshared library providing JIT support can be loaded (i.e. PostgreSQL was\ncompiled with LLVM support and the shared library is installed), the task\nof JIT compiling an expression gets handed off to the shared library. This\nobviously requires that the function in jit.c is allowed to fail in case\nno JIT provider can be loaded.\n\nWhich shared library is loaded is determined by the jit_provider GUC,\ndefaulting to \"llvmjit\".\n\nCloistering code performing JIT into a shared library unfortunately\nalso means that code doing JIT compilation for various parts of code\nhas to be located separately from the code doing so without\nJIT. E.g. the JIT version of execExprInterp.c is located in jit/llvm/\nrather than executor/.\n\n\nJIT Context\n-----------\n\nFor performance and convenience reasons it is useful to allow JITed\nfunctions to be emitted and deallocated together. It is e.g. very\ncommon to create a number of functions at query initialization time,\nuse them during query execution, and then deallocate all of them\ntogether at the end of the query.\n\nLifetimes of JITed functions are managed via JITContext. Exactly one\nsuch context should be created for work in which all created JITed\nfunction should have the same lifetime. E.g. there's exactly one\nJITContext for each query executed, in the query's EState.  Only the\nrelease of a JITContext is exposed to the provider independent\nfacility, as the creation of one is done on-demand by the JIT\nimplementations.\n\nEmitting individual functions separately is more expensive than\nemitting several functions at once, and emitting them together can\nprovide additional optimization opportunities. To facilitate that, the\nLLVM provider separates defining functions from optimizing and\nemitting functions in an executable manner.\n\nCreating functions into the current mutable module (a module\nessentially is LLVM's equivalent of a translation unit in C) is done\nusing\n  extern LLVMModuleRef llvm_mutable_module(LLVMJitContext *context);\nin which it then can emit as much code using the LLVM APIs as it\nwants. Whenever a function actually needs to be called\n  extern void *llvm_get_function(LLVMJitContext *context, const char *funcname);\nreturns a pointer to it.\n\nE.g. in the expression evaluation case this setup allows most\nfunctions in a query to be emitted during ExecInitNode(), delaying the\nfunction emission to the time the first time a function is actually\nused.\n\n\nError Handling\n--------------\n\nThere are two aspects of error handling.  Firstly, generated (LLVM IR)\nand emitted functions (mmap()ed segments) need to be cleaned up both\nafter a successful query execution and after an error. This is done by\nregistering each created JITContext with the current resource owner,\nand cleaning it up on error / end of transaction. If it is desirable\nto release resources earlier, jit_release_context() can be used.\n\nThe second, less pretty, aspect of error handling is OOM handling\ninside LLVM itself. The above resowner based mechanism takes care of\ncleaning up emitted code upon ERROR, but there's also the chance that\nLLVM itself runs out of memory. LLVM by default does *not* use any C++\nexceptions. Its allocations are primarily funneled through the\nstandard \"new\" handlers, and some direct use of malloc() and\nmmap(). For the former a 'new handler' exists:\nhttp://en.cppreference.com/w/cpp/memory/new/set_new_handler\nFor the latter LLVM provides callbacks that get called upon failure\n(unfortunately mmap() failures are treated as fatal rather than OOM errors).\nWhat we've chosen to do for now is have two functions that LLVM using code\nmust use:\nextern void llvm_enter_fatal_on_oom(void);\nextern void llvm_leave_fatal_on_oom(void);\nbefore interacting with LLVM code.\n\nWhen a libstdc++ new or LLVM error occurs, the handlers set up by the\nabove functions trigger a FATAL error. We have to use FATAL rather\nthan ERROR, as we *cannot* reliably throw ERROR inside a foreign\nlibrary without risking corrupting its internal state.\n\nUsers of the above sections do *not* have to use PG_TRY/CATCH blocks,\nthe handlers instead are reset on toplevel sigsetjmp() level.\n\nUsing a relatively small enter/leave protected section of code, rather\nthan setting up these handlers globally, avoids negative interactions\nwith extensions that might use C++ such as PostGIS. As LLVM code\ngeneration should never execute arbitrary code, just setting these\nhandlers temporarily ought to suffice.\n\n\nType Synchronization\n--------------------\n\nTo be able to generate code that can perform tasks done by \"interpreted\"\nPostgreSQL, it obviously is required that code generation knows about at\nleast a few PostgreSQL types.  While it is possible to inform LLVM about\ntype definitions by recreating them manually in C code, that is failure\nprone and labor intensive.\n\nInstead there is one small file (llvmjit_types.c) which references each of\nthe types required for JITing. That file is translated to bitcode at\ncompile time, and loaded when LLVM is initialized in a backend.\n\nThat works very well to synchronize the type definition, but unfortunately\nit does *not* synchronize offsets as the IR level representation doesn't\nknow field names.  Instead, required offsets are maintained as defines in\nthe original struct definition, like so:\n#define FIELDNO_TUPLETABLESLOT_NVALID 9\n        int                     tts_nvalid;             /* # of valid values in tts_values */\nWhile that still needs to be defined, it's only required for a\nrelatively small number of fields, and it's bunched together with the\nstruct definition, so it's easily kept synchronized.\n\n\nInlining\n--------\n\nOne big advantage of JITing expressions is that it can significantly\nreduce the overhead of PostgreSQL's extensible function/operator\nmechanism, by inlining the body of called functions/operators.\n\nIt obviously is undesirable to maintain a second implementation of\ncommonly used functions, just for inlining purposes. Instead we take\nadvantage of the fact that the Clang compiler can emit LLVM IR.\n\nThe ability to do so allows us to get the LLVM IR for all operators\n(e.g. int8eq, float8pl etc), without maintaining two copies.  These\nbitcode files get installed into the server's\n  $pkglibdir/bitcode/postgres/\nUsing existing LLVM functionality (for parallel LTO compilation),\nadditionally an index is over these is stored to\n$pkglibdir/bitcode/postgres.index.bc\n\nSimilarly extensions can install code into\n  $pkglibdir/bitcode/[extension]/\naccompanied by\n  $pkglibdir/bitcode/[extension].index.bc\n\njust alongside the actual library.  An extension's index will be used\nto look up symbols when located in the corresponding shared\nlibrary. Symbols that are used inside the extension, when inlined,\nwill be first looked up in the main binary and then the extension's.\n\n\nCaching\n-------\n\nCurrently it is not yet possible to cache generated functions, even\nthough that'd be desirable from a performance point of view. The\nproblem is that the generated functions commonly contain pointers into\nper-execution memory. The expression evaluation machinery needs to\nbe redesigned a bit to avoid that. Basically all per-execution memory\nneeds to be referenced as an offset to one block of memory stored in\nan ExprState, rather than absolute pointers into memory.\n\nOnce that is addressed, adding an LRU cache that's keyed by the\ngenerated LLVM IR will allow the usage of optimized functions even for\nfaster queries.\n\nA longer term project is to move expression compilation to the planner\nstage, allowing e.g. to tie compiled expressions to prepared\nstatements.\n\nAn even more advanced approach would be to use JIT with few\noptimizations initially, and build an optimized version in the\nbackground. But that's even further off.\n\n\nWhat to JIT\n===========\n\nCurrently expression evaluation and tuple deforming are JITed. Those\nwere chosen because they commonly are major CPU bottlenecks in\nanalytics queries, but are by no means the only potentially beneficial cases.\n\nFor JITing to be beneficial a piece of code first and foremost has to\nbe a CPU bottleneck. But also importantly, JITing can only be\nbeneficial if overhead can be removed by doing so. E.g. in the tuple\ndeforming case the knowledge about the number of columns and their\ntypes can remove a significant number of branches, and in the\nexpression evaluation case a lot of indirect jumps/calls can be\nremoved.  If neither of these is the case, JITing is a waste of\nresources.\n\nFuture avenues for JITing are tuple sorting, COPY parsing/output\ngeneration, and later compiling larger parts of queries.\n\n\nWhen to JIT\n===========\n\nCurrently there are a number of GUCs that influence JITing:\n\n- jit_above_cost = -1, 0-DBL_MAX - all queries with a higher total cost\n  get JITed, *without* optimization (expensive part), corresponding to\n  -O0. This commonly already results in significant speedups if\n  expression/deforming is a bottleneck (removing dynamic branches\n  mostly).\n- jit_optimize_above_cost = -1, 0-DBL_MAX - all queries with a higher total cost\n  get JITed, *with* optimization (expensive part).\n- jit_inline_above_cost = -1, 0-DBL_MAX - inlining is tried if query has\n  higher cost.\n\nWhenever a query's total cost is above these limits, JITing is\nperformed.\n\nAlternative costing models, e.g. by generating separate paths for\nparts of a query with lower cpu_* costs, are also a possibility, but\nit's doubtful the overhead of doing so is sufficient.  Another\nalternative would be to count the number of times individual\nexpressions are estimated to be evaluated, and perform JITing of these\nindividual expressions.\n\nThe obvious seeming approach of JITing expressions individually after\na number of execution turns out not to work too well. Primarily\nbecause emitting many small functions individually has significant\noverhead. Secondarily because the time until JITing occurs causes\nrelative slowdowns that eat into the gain of JIT compilation.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\jit\\README",
      "directory": "backend\\jit"
    }
  },
  {
    "title": "README: backend\\lib",
    "url": "backend\\lib\\README",
    "content": "This directory contains a general purpose data structures, for use anywhere\nin the backend:\n\nbinaryheap.c - a binary heap\n\nbipartite_match.c - Hopcroft-Karp maximum cardinality algorithm for bipartite graphs\n\nbloomfilter.c - probabilistic, space-efficient set membership testing\n\ndshash.c - concurrent hash tables backed by dynamic shared memory areas\n\nhyperloglog.c - a streaming cardinality estimator\n\nilist.c - single and double-linked lists\n\nintegerset.c - a data structure for holding large set of integers\n\nknapsack.c - knapsack problem solver\n\npairingheap.c - a pairing heap\n\nrbtree.c - a red-black tree\n\nstringinfo.c - an extensible string type\n\n\nAside from the inherent characteristics of the data structures, there are a\nfew practical differences between the binary heap and the pairing heap. The\nbinary heap is fully allocated at creation, and cannot be expanded beyond the\nallocated size. The pairing heap on the other hand has no inherent maximum\nsize, but the caller needs to allocate each element being stored in the heap,\nwhile the binary heap works with plain Datums or pointers.\n\nThe linked-lists in ilist.c can be embedded directly into other structs, as\nopposed to the List interface in nodes/pg_list.h.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\lib\\README",
      "directory": "backend\\lib"
    }
  },
  {
    "title": "README: backend\\libpq",
    "url": "backend\\libpq\\README.SSL",
    "content": "src/backend/libpq/README.SSL\n\nSSL\n===\n\n>From the servers perspective:\n\n\n  Receives StartupPacket\n           |\n           |\n (Is SSL_NEGOTIATE_CODE?) -----------  Normal startup\n           |                  No\n           |\n           | Yes\n           |\n           |\n (Server compiled with USE_SSL?) ------- Send 'N'\n           |                       No        |\n           |                                 |\n           | Yes                         Normal startup\n           |\n           |\n        Send 'S'\n           |\n           |\n      Establish SSL\n           |\n           |\n      Normal startup\n\n\n\n\n\n>From the clients perspective (v6.6 client _with_ SSL):\n\n\n      Connect\n         |\n         |\n  Send packet with SSL_NEGOTIATE_CODE\n         |\n         |\n  Receive single char  ------- 'S' -------- Establish SSL\n         |                                       |\n         | '<else>'                              |\n         |                                  Normal startup\n         |\n         |\n   Is it 'E' for error  ------------------- Retry connection\n         |                  Yes             without SSL\n         | No\n         |\n   Is it 'N' for normal ------------------- Normal startup\n         |                  Yes\n         |\n   Fail with unknown\n\n---------------------------------------------------------------------------\n\nEphemeral DH\n============\n\nSince the server static private key ($DataDir/server.key) will\nnormally be stored unencrypted so that the database backend can\nrestart automatically, it is important that we select an algorithm\nthat continues to provide confidentiality even if the attacker has the\nserver's private key.  Ephemeral DH (EDH) keys provide this and more\n(Perfect Forward Secrecy aka PFS).\n\nN.B., the static private key should still be protected to the largest\nextent possible, to minimize the risk of impersonations.\n\nAnother benefit of EDH is that it allows the backend and clients to\nuse DSA keys.  DSA keys can only provide digital signatures, not\nencryption, and are often acceptable in jurisdictions where RSA keys\nare unacceptable.\n\nThe downside to EDH is that it makes it impossible to use ssldump(1)\nif there's a problem establishing an SSL session.  In this case you'll\nneed to temporarily disable EDH (see initialize_dh()).",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\libpq\\README.SSL",
      "directory": "backend\\libpq"
    }
  },
  {
    "title": "README: backend\\nodes",
    "url": "backend\\nodes\\README",
    "content": "src/backend/nodes/README\n\nNode Structures\n===============\n\nIntroduction\n------------\n\nPostgres uses \"node\" types to organize parse trees, plan trees, and\nexecutor state trees.  All objects that can appear in such trees must\nbe declared as node types.  In addition, a few object types that aren't\npart of parse/plan/execute node trees receive NodeTags anyway for\nidentification purposes, usually because they are involved in APIs\nwhere we want to pass multiple object types through the same pointer.\n\nThe node structures are plain old C structures with the first field\nbeing of type NodeTag.  \"Inheritance\" is achieved by convention:\nthe first field can alternatively be of another node type.\n\nNode types typically have support for being copied by copyObject(),\ncompared by equal(), serialized by outNode(), and deserialized by\nnodeRead().  For some classes of Nodes, not all of these support\nfunctions are required; for example, executor state nodes don't\npresently need any of them.  So far as the system is concerned,\noutput and read functions are only needed for node types that can\nappear in parse trees stored in the catalogs, and for plan tree\nnodes because those are serialized to be passed to parallel workers.\nHowever, we provide output functions for some other node types as well,\nbecause they are very handy for debugging.  Currently, such coverage\nexists for raw parsetrees and most planner data structures.  However,\noutput coverage of raw parsetrees is incomplete: in particular, utility\nstatements are almost entirely unsupported.\n\nRelevant Files\n--------------\n\nUtility functions for manipulating node structures reside in this\ndirectory.  Some support functions are automatically generated by the\ngen_node_support.pl script, other functions are maintained manually.\nTo control the automatic generation of support functions, node types\nand node fields can be annotated with pg_node_attr() specifications;\nsee further documentation in src/include/nodes/nodes.h.\n\n\nFILES IN THIS DIRECTORY (src/backend/nodes/)\n\n    General-purpose node manipulation functions:\n\tcopyfuncs.c\t- copy a node tree (*)\n\tequalfuncs.c\t- compare two node trees (*)\n\toutfuncs.c\t- convert a node tree to text representation (*)\n\treadfuncs.c\t- convert text representation back to a node tree (*)\n\tmakefuncs.c\t- creator functions for some common node types\n\tnodeFuncs.c\t- some other general-purpose manipulation functions\n\tqueryjumblefuncs.c - compute a node tree for query jumbling (*)\n\n    (*) - Most functions in these files are generated by\n    gen_node_support.pl and #include'd there.\n\n    Specialized manipulation functions:\n\tbitmapset.c\t- Bitmapset support\n\tlist.c\t\t- generic list support\n\tmultibitmapset.c - List-of-Bitmapset support\n\tparams.c\t- Param support\n\ttidbitmap.c\t- TIDBitmap support\n\tvalue.c\t\t- support for value nodes\n\nFILES IN src/include/nodes/\n\n    Node definitions primarily appear in:\n\tnodes.h\t\t- define node tags (NodeTag) (*)\n\tprimnodes.h\t- primitive nodes\n\tparsenodes.h\t- parse tree nodes\n\tpathnodes.h\t- path tree nodes and planner internal structures\n\tplannodes.h\t- plan tree nodes\n\texecnodes.h\t- executor nodes\n\tmemnodes.h\t- memory nodes\n\tpg_list.h\t- generic list\n\n    (*) - Also #include's files generated by gen_node_support.pl.\n\n\nSteps to Add a Node\n-------------------\n\nSuppose you want to define a node Foo:\n\n1. Add the structure definition to the appropriate include/nodes/???.h file.\n   If you intend to inherit from, say a Plan node, put Plan as the first field\n   of your struct definition.  (The T_Foo tag is created automatically.)\n2. Check that the generated support functions in copyfuncs.funcs.c,\n   equalfuncs.funcs.c, outfuncs.funcs.c, queryjumblefuncs.funcs.c and\n   readfuncs.funcs.c look correct.  Add attributes as necessary to control the\n   outcome.  (For some classes of node types, you don't need all the support\n   functions.  Use node attributes similar to those of related node types.)\n3. Add cases to the functions in nodeFuncs.c as needed.  There are many\n   other places you'll probably also need to teach about your new node\n   type.  Best bet is to grep for references to one or two similar existing\n   node types to find all the places to touch.\n   (Except for frequently-created nodes, don't bother writing a creator\n   function in makefuncs.c.)\n4. Consider testing your new code with COPY_PARSE_PLAN_TREES,\n   WRITE_READ_PARSE_PLAN_TREES, and RAW_EXPRESSION_COVERAGE_TEST to ensure\n   support has been added everywhere that it's necessary; see\n   pg_config_manual.h about these.\n\nAdding a new node type moves the numbers associated with existing\ntags, so you'll need to recompile the whole tree after doing this.\n(--enable-depend usually helps.)  It doesn't force initdb though,\nbecause the numbers never go to disk.  But altering or removing a node\ntype should usually be accompanied by an initdb-forcing catalog\nversion change, since the interpretation of serialized node trees\nstored in system catalogs is affected by that.  (If the node type\nnever appears in stored parse trees, as for example Plan nodes do not,\nthen a catversion change is not needed to change it.)",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\nodes\\README",
      "directory": "backend\\nodes"
    }
  },
  {
    "title": "README: backend\\optimizer",
    "url": "backend\\optimizer\\README",
    "content": "src/backend/optimizer/README\n\nOptimizer\n=========\n\nThese directories take the Query structure returned by the parser, and\ngenerate a plan used by the executor.  The /plan directory generates the\nactual output plan, the /path code generates all possible ways to join the\ntables, and /prep handles various preprocessing steps for special cases.\n/util is utility stuff.  /geqo is the separate \"genetic optimization\" planner\n--- it does a semi-random search through the join tree space, rather than\nexhaustively considering all possible join trees.  (But each join considered\nby /geqo is given to /path to create paths for, so we consider all possible\nimplementation paths for each specific join pair even in GEQO mode.)\n\n\nPaths and Join Pairs\n--------------------\n\nDuring the planning/optimizing process, we build \"Path\" trees representing\nthe different ways of doing a query.  We select the cheapest Path that\ngenerates the desired relation and turn it into a Plan to pass to the\nexecutor.  (There is pretty nearly a one-to-one correspondence between the\nPath and Plan trees, but Path nodes omit info that won't be needed during\nplanning, and include info needed for planning that won't be needed by the\nexecutor.)\n\nThe optimizer builds a RelOptInfo structure for each base relation used in\nthe query.  Base rels are either primitive tables, or subquery subselects\nthat are planned via a separate recursive invocation of the planner.  A\nRelOptInfo is also built for each join relation that is considered during\nplanning.  A join rel is simply a combination of base rels.  There is only\none join RelOptInfo for any given set of baserels --- for example, the join\n{A B C} is represented by the same RelOptInfo no matter whether we build it\nby joining A and B first and then adding C, or joining B and C first and\nthen adding A, etc.  These different means of building the joinrel are\nrepresented as Paths.  For each RelOptInfo we build a list of Paths that\nrepresent plausible ways to implement the scan or join of that relation.\nOnce we've considered all the plausible Paths for a rel, we select the one\nthat is cheapest according to the planner's cost estimates.  The final plan\nis derived from the cheapest Path for the RelOptInfo that includes all the\nbase rels of the query.\n\nPossible Paths for a primitive table relation include plain old sequential\nscan, plus index scans for any indexes that exist on the table, plus bitmap\nindex scans using one or more indexes.  Specialized RTE types, such as\nfunction RTEs, may have only one possible Path.\n\nJoins always occur using two RelOptInfos.  One is outer, the other inner.\nOuters drive lookups of values in the inner.  In a nested loop, lookups of\nvalues in the inner occur by scanning the inner path once per outer tuple\nto find each matching inner row.  In a mergejoin, inner and outer rows are\nordered, and are accessed in order, so only one scan is required to perform\nthe entire join: both inner and outer paths are scanned in-sync.  (There's\nnot a lot of difference between inner and outer in a mergejoin...)  In a\nhashjoin, the inner is scanned first and all its rows are entered in a\nhashtable, then the outer is scanned and for each row we lookup the join\nkey in the hashtable.\n\nA Path for a join relation is actually a tree structure, with the topmost\nPath node representing the last-applied join method.  It has left and right\nsubpaths that represent the scan or join methods used for the two input\nrelations.\n\n\nJoin Tree Construction\n----------------------\n\nThe optimizer generates optimal query plans by doing a more-or-less\nexhaustive search through the ways of executing the query.  The best Path\ntree is found by a recursive process:\n\n1) Take each base relation in the query, and make a RelOptInfo structure\nfor it.  Find each potentially useful way of accessing the relation,\nincluding sequential and index scans, and make Paths representing those\nways.  All the Paths made for a given relation are placed in its\nRelOptInfo.pathlist.  (Actually, we discard Paths that are obviously\ninferior alternatives before they ever get into the pathlist --- what\nends up in the pathlist is the cheapest way of generating each potentially\nuseful sort ordering and parameterization of the relation.)  Also create a\nRelOptInfo.joininfo list including all the join clauses that involve this\nrelation.  For example, the WHERE clause \"tab1.col1 = tab2.col1\" generates\nentries in both tab1 and tab2's joininfo lists.\n\nIf we have only a single base relation in the query, we are done.\nOtherwise we have to figure out how to join the base relations into a\nsingle join relation.\n\n2) Normally, any explicit JOIN clauses are \"flattened\" so that we just\nhave a list of relations to join.  However, FULL OUTER JOIN clauses are\nnever flattened, and other kinds of JOIN might not be either, if the\nflattening process is stopped by join_collapse_limit or from_collapse_limit\nrestrictions.  Therefore, we end up with a planning problem that contains\nlists of relations to be joined in any order, where any individual item\nmight be a sub-list that has to be joined together before we can consider\njoining it to its siblings.  We process these sub-problems recursively,\nbottom up.  Note that the join list structure constrains the possible join\norders, but it doesn't constrain the join implementation method at each\njoin (nestloop, merge, hash), nor does it say which rel is considered outer\nor inner at each join.  We consider all these possibilities in building\nPaths. We generate a Path for each feasible join method, and select the\ncheapest Path.\n\nFor each planning problem, therefore, we will have a list of relations\nthat are either base rels or joinrels constructed per sub-join-lists.\nWe can join these rels together in any order the planner sees fit.\nThe standard (non-GEQO) planner does this as follows:\n\nConsider joining each RelOptInfo to each other RelOptInfo for which there\nis a usable joinclause, and generate a Path for each possible join method\nfor each such pair.  (If we have a RelOptInfo with no join clauses, we have\nno choice but to generate a clauseless Cartesian-product join; so we\nconsider joining that rel to each other available rel.  But in the presence\nof join clauses we will only consider joins that use available join\nclauses.  Note that join-order restrictions induced by outer joins and\nIN/EXISTS clauses are also checked, to ensure that we find a workable join\norder in cases where those restrictions force a clauseless join to be done.)\n\nIf we only had two relations in the list, we are done: we just pick\nthe cheapest path for the join RelOptInfo.  If we had more than two, we now\nneed to consider ways of joining join RelOptInfos to each other to make\njoin RelOptInfos that represent more than two list items.\n\nThe join tree is constructed using a \"dynamic programming\" algorithm:\nin the first pass (already described) we consider ways to create join rels\nrepresenting exactly two list items.  The second pass considers ways\nto make join rels that represent exactly three list items; the next pass,\nfour items, etc.  The last pass considers how to make the final join\nrelation that includes all list items --- obviously there can be only one\njoin rel at this top level, whereas there can be more than one join rel\nat lower levels.  At each level we use joins that follow available join\nclauses, if possible, just as described for the first level.\n\nFor example:\n\n    SELECT  *\n    FROM    tab1, tab2, tab3, tab4\n    WHERE   tab1.col = tab2.col AND\n        tab2.col = tab3.col AND\n        tab3.col = tab4.col\n\n    Tables 1, 2, 3, and 4 are joined as:\n    {1 2},{2 3},{3 4}\n    {1 2 3},{2 3 4}\n    {1 2 3 4}\n    (other possibilities will be excluded for lack of join clauses)\n\n    SELECT  *\n    FROM    tab1, tab2, tab3, tab4\n    WHERE   tab1.col = tab2.col AND\n        tab1.col = tab3.col AND\n        tab1.col = tab4.col\n\n    Tables 1, 2, 3, and 4 are joined as:\n    {1 2},{1 3},{1 4}\n    {1 2 3},{1 3 4},{1 2 4}\n    {1 2 3 4}\n\nWe consider left-handed plans (the outer rel of an upper join is a joinrel,\nbut the inner is always a single list item); right-handed plans (outer rel\nis always a single item); and bushy plans (both inner and outer can be\njoins themselves).  For example, when building {1 2 3 4} we consider\njoining {1 2 3} to {4} (left-handed), {4} to {1 2 3} (right-handed), and\n{1 2} to {3 4} (bushy), among other choices.  Although the jointree\nscanning code produces these potential join combinations one at a time,\nall the ways to produce the same set of joined base rels will share the\nsame RelOptInfo, so the paths produced from different join combinations\nthat produce equivalent joinrels will compete in add_path().\n\nThe dynamic-programming approach has an important property that's not\nimmediately obvious: we will finish constructing all paths for a given\nrelation before we construct any paths for relations containing that rel.\nThis means that we can reliably identify the \"cheapest path\" for each rel\nbefore higher-level relations need to know that.  Also, we can safely\ndiscard a path when we find that another path for the same rel is better,\nwithout worrying that maybe there is already a reference to that path in\nsome higher-level join path.  Without this, memory management for paths\nwould be much more complicated.\n\nOnce we have built the final join rel, we use either the cheapest path\nfor it or the cheapest path with the desired ordering (if that's cheaper\nthan applying a sort to the cheapest other path).\n\nIf the query contains one-sided outer joins (LEFT or RIGHT joins), or\nIN or EXISTS WHERE clauses that were converted to semijoins or antijoins,\nthen some of the possible join orders may be illegal.  These are excluded\nby having join_is_legal consult a side list of such \"special\" joins to see\nwhether a proposed join is illegal.  (The same consultation allows it to\nsee which join style should be applied for a valid join, ie, JOIN_INNER,\nJOIN_LEFT, etc.)\n\n\nValid OUTER JOIN Optimizations\n------------------------------\n\nThe planner's treatment of outer join reordering is based on the following\nidentities:\n\n1.\t(A leftjoin B on (Pab)) innerjoin C on (Pac)\n\t= (A innerjoin C on (Pac)) leftjoin B on (Pab)\n\nwhere Pac is a predicate referencing A and C, etc (in this case, clearly\nPac cannot reference B, or the transformation is nonsensical).\n\n2.\t(A leftjoin B on (Pab)) leftjoin C on (Pac)\n\t= (A leftjoin C on (Pac)) leftjoin B on (Pab)\n\n3.\t(A leftjoin B on (Pab)) leftjoin C on (Pbc)\n\t= A leftjoin (B leftjoin C on (Pbc)) on (Pab)\n\nIdentity 3 only holds if predicate Pbc must fail for all-null B rows\n(that is, Pbc is strict for at least one column of B).  If Pbc is not\nstrict, the first form might produce some rows with nonnull C columns\nwhere the second form would make those entries null.\n\nRIGHT JOIN is equivalent to LEFT JOIN after switching the two input\ntables, so the same identities work for right joins.\n\nAn example of a case that does *not* work is moving an innerjoin into or\nout of the nullable side of an outer join:\n\n\tA leftjoin (B join C on (Pbc)) on (Pab)\n\t!= (A leftjoin B on (Pab)) join C on (Pbc)\n\nSEMI joins work a little bit differently.  A semijoin can be reassociated\ninto or out of the lefthand side of another semijoin, left join, or\nantijoin, but not into or out of the righthand side.  Likewise, an inner\njoin, left join, or antijoin can be reassociated into or out of the\nlefthand side of a semijoin, but not into or out of the righthand side.\n\nANTI joins work approximately like LEFT joins, except that identity 3\nfails if the join to C is an antijoin (even if Pbc is strict, and in\nboth the cases where the other join is a leftjoin and where it is an\nantijoin).  So we can't reorder antijoins into or out of the RHS of a\nleftjoin or antijoin, even if the relevant clause is strict.\n\nThe current code does not attempt to re-order FULL JOINs at all.\nFULL JOIN ordering is enforced by not collapsing FULL JOIN nodes when\ntranslating the jointree to \"joinlist\" representation.  Other types of\nJOIN nodes are normally collapsed so that they participate fully in the\njoin order search.  To avoid generating illegal join orders, the planner\ncreates a SpecialJoinInfo node for each non-inner join, and join_is_legal\nchecks this list to decide if a proposed join is legal.\n\nWhat we store in SpecialJoinInfo nodes are the minimum sets of Relids\nrequired on each side of the join to form the outer join.  Note that\nthese are minimums; there's no explicit maximum, since joining other\nrels to the OJ's syntactic rels may be legal.  Per identities 1 and 2,\nnon-FULL joins can be freely associated into the lefthand side of an\nOJ, but in some cases they can't be associated into the righthand side.\nSo the restriction enforced by join_is_legal is that a proposed join\ncan't join a rel within or partly within an RHS boundary to one outside\nthe boundary, unless the proposed join is a LEFT join that can associate\ninto the SpecialJoinInfo's RHS using identity 3.\n\nThe use of minimum Relid sets has some pitfalls; consider a query like\n\tA leftjoin (B leftjoin (C innerjoin D) on (Pbcd)) on Pa\nwhere Pa doesn't mention B/C/D at all.  In this case a naive computation\nwould give the upper leftjoin's min LHS as {A} and min RHS as {C,D} (since\nwe know that the innerjoin can't associate out of the leftjoin's RHS, and\nenforce that by including its relids in the leftjoin's min RHS).  And the\nlower leftjoin has min LHS of {B} and min RHS of {C,D}.  Given such\ninformation, join_is_legal would think it's okay to associate the upper\njoin into the lower join's RHS, transforming the query to\n\tB leftjoin (A leftjoin (C innerjoin D) on Pa) on (Pbcd)\nwhich yields totally wrong answers.  We prevent that by forcing the min RHS\nfor the upper join to include B.  This is perhaps overly restrictive, but\nsuch cases don't arise often so it's not clear that it's worth developing a\nmore complicated system.\n\n\nPulling Up Subqueries\n---------------------\n\nAs we described above, a subquery appearing in the range table is planned\nindependently and treated as a \"black box\" during planning of the outer\nquery.  This is necessary when the subquery uses features such as\naggregates, GROUP, or DISTINCT.  But if the subquery is just a simple\nscan or join, treating the subquery as a black box may produce a poor plan\ncompared to considering it as part of the entire plan search space.\nTherefore, at the start of the planning process the planner looks for\nsimple subqueries and pulls them up into the main query's jointree.\n\nPulling up a subquery may result in FROM-list joins appearing below the top\nof the join tree.  Each FROM-list is planned using the dynamic-programming\nsearch method described above.\n\nIf pulling up a subquery produces a FROM-list as a direct child of another\nFROM-list, then we can merge the two FROM-lists together.  Once that's\ndone, the subquery is an absolutely integral part of the outer query and\nwill not constrain the join tree search space at all.  However, that could\nresult in unpleasant growth of planning time, since the dynamic-programming\nsearch has runtime exponential in the number of FROM-items considered.\nTherefore, we don't merge FROM-lists if the result would have too many\nFROM-items in one list.\n\n\nVars and PlaceHolderVars\n------------------------\n\nA Var node is simply the parse-tree representation of a table column\nreference.  However, in the presence of outer joins, that concept is\nmore subtle than it might seem.  We need to distinguish the values of\na Var \"above\" and \"below\" any outer join that could force the Var to\nnull.  As an example, consider\n\n\tSELECT * FROM t1 LEFT JOIN t2 ON (t1.x = t2.y) WHERE foo(t2.z)\n\n(Assume foo() is not strict, so that we can't reduce the left join to\na plain join.)  A naive implementation might try to push the foo(t2.z)\ncall down to the scan of t2, but that is not correct because\n(a) what foo() should actually see for a null-extended join row is NULL,\nand (b) if foo() returns false, we should suppress the t1 row from the\njoin altogether, not emit it with a null-extended t2 row.  On the other\nhand, it *would* be correct (and desirable) to push that call down to\nthe scan level if the query were\n\n\tSELECT * FROM t1 LEFT JOIN t2 ON (t1.x = t2.y AND foo(t2.z))\n\nThis motivates considering \"t2.z\" within the left join's ON clause\nto be a different value from \"t2.z\" outside the JOIN clause.  The\nformer can be identified with t2.z as seen at the relation scan level,\nbut the latter can't.\n\nAnother example occurs in connection with EquivalenceClasses (discussed\nbelow).  Given\n\n\tSELECT * FROM t1 LEFT JOIN t2 ON (t1.x = t2.y) WHERE t1.x = 42\n\nwe would like to use the EquivalenceClass mechanisms to derive \"t2.y = 42\"\nto use as a restriction clause for the scan of t2.  (That works, because t2\nrows having y different from 42 cannot affect the query result.)  However,\nit'd be wrong to conclude that t2.y will be equal to t1.x in every joined\nrow.  Part of the solution to this problem is to deem that \"t2.y\" in the\nON clause refers to the relation-scan-level value of t2.y, but not to the\nvalue that y will have in joined rows, where it might be NULL rather than\nequal to t1.x.\n\nTherefore, Var nodes are decorated with \"varnullingrels\", which are sets\nof the rangetable indexes of outer joins that potentially null the Var\nat the point where it appears in the query.  (Using a set, not an ordered\nlist, is fine since it doesn't matter which join forced the value to null;\nand that avoids having to change the representation when we consider\ndifferent outer-join orders.)  In the examples above, all occurrences of\nt1.x would have empty varnullingrels, since the left join doesn't null t1.\nThe t2 references within the JOIN ON clauses would also have empty\nvarnullingrels.  But outside the JOIN clauses, any Vars referencing t2\nwould have varnullingrels containing the index of the JOIN's rangetable\nentry (RTE), so that they'd be understood as potentially different from\nthe t2 values seen at scan level.  Labeling t2.z in the WHERE clause with\nthe JOIN's RT index lets us recognize that that occurrence of foo(t2.z)\ncannot be pushed down to the t2 scan level: we cannot evaluate that value\nat the scan level, but only after the join has been done.\n\nFor LEFT and RIGHT outer joins, only Vars coming from the nullable side\nof the join are marked with that join's RT index.  For FULL joins, Vars\nfrom both inputs are marked.  (Such marking doesn't let us tell which\nside of the full join a Var came from; but that information can be found\nelsewhere at need.)\n\nNotionally, a Var having nonempty varnullingrels can be thought of as\n\tCASE WHEN any-of-these-outer-joins-produced-a-null-extended-row\n\t  THEN NULL\n\t  ELSE the-scan-level-value-of-the-column\n\t  END\nIt's only notional, because no such calculation is ever done explicitly.\nIn a finished plan, Vars occurring in scan-level plan nodes represent\nthe actual table column values, but upper-level Vars are always\nreferences to outputs of lower-level plan nodes.  When a join node emits\na null-extended row, it just returns nulls for the relevant output\ncolumns rather than copying up values from its input.  Because we don't\never have to do this calculation explicitly, it's not necessary to\ndistinguish which side of an outer join got null-extended, which'd\notherwise be essential information for FULL JOIN cases.\n\nOuter join identity 3 (discussed above) complicates this picture\na bit.  In the form\n\tA leftjoin (B leftjoin C on (Pbc)) on (Pab)\nall of the Vars in clauses Pbc and Pab will have empty varnullingrels,\nbut if we start with\n\t(A leftjoin B on (Pab)) leftjoin C on (Pbc)\nthen the parser will have marked Pbc's B Vars with the A/B join's\nRT index, making this form artificially different from the first.\nFor discussion's sake, let's denote this marking with a star:\n\t(A leftjoin B on (Pab)) leftjoin C on (Pb*c)\nTo cope with this, once we have detected that commuting these joins\nis legal, we generate both the Pbc and Pb*c forms of that ON clause,\nby either removing or adding the first join's RT index in the B Vars\nthat the parser created.  While generating paths for a plan step that\njoins B and C, we include as a relevant join qual only the form that\nis appropriate depending on whether A has already been joined to B.\n\nIt's also worth noting that identity 3 makes \"the left join's RT index\"\nitself a bit of a fuzzy concept, since the syntactic scope of each join\nRTE will depend on which form was produced by the parser.  We resolve\nthis by considering that a left join's identity is determined by its\nminimum set of right-hand-side input relations.  In both forms allowed\nby identity 3, we can identify the first join as having minimum RHS B\nand the second join as having minimum RHS C.\n\nAnother thing to notice is that C Vars appearing outside the nested\nJOIN clauses will be marked as nulled by both left joins if the\noriginal parser input was in the first form of identity 3, but if the\nparser input was in the second form, such Vars will only be marked as\nnulled by the second join.  This is not really a semantic problem:\nsuch Vars will be marked the same way throughout the upper part of the\nquery, so they will all look equal() which is correct; and they will not\nlook equal() to any C Var appearing in the JOIN ON clause or below these\njoins.  However, when building Vars representing the outputs of join\nrelations, we need to ensure that their varnullingrels are set to\nvalues consistent with the syntactic join order, so that they will\nappear equal() to pre-existing Vars in the upper part of the query.\n\nOuter joins also complicate handling of subquery pull-up.  Consider\n\n\tSELECT ..., ss.x FROM tab1\n\t  LEFT JOIN (SELECT *, 42 AS x FROM tab2) ss ON ...\n\nWe want to be able to pull up the subquery as discussed previously,\nbut we can't just replace the \"ss.x\" Var in the top-level SELECT list\nwith the constant 42.  That'd result in always emitting 42, rather\nthan emitting NULL in null-extended join rows.\n\nTo solve this, we introduce the concept of PlaceHolderVars.\nA PlaceHolderVar is somewhat like a Var, in that its value originates\nat a relation scan level and can then be forced to null by higher-level\nouter joins; hence PlaceHolderVars carry a set of nulling rel IDs just\nlike Vars.  Unlike a Var, whose original value comes from a table,\na PlaceHolderVar's original value is defined by a query-determined\nexpression (\"42\" in this example); so we represent the PlaceHolderVar\nas a node with that expression as child.  We insert a PlaceHolderVar\nwhenever subquery pullup needs to replace a subquery-referencing Var\nthat has nonempty varnullingrels with an expression that is not simply a\nVar.  (When the replacement expression is a pulled-up Var, we can just\nadd the replaced Var's varnullingrels to its set.  Also, if the replaced\nVar has empty varnullingrels, we don't need a PlaceHolderVar: there is\nnothing that'd force the value to null, so the pulled-up expression is\nfine to use as-is.)  In a finished plan, a PlaceHolderVar becomes just\nthe contained expression at whatever plan level it's supposed to be\nevaluated at, and then upper-level occurrences are replaced by Var\nreferences to that output column of the lower plan level.  That causes\nthe value to go to null when appropriate at an outer join, in the same\nway as for normal Vars.  Thus, PlaceHolderVars are never seen outside\nthe planner.\n\nPlaceHolderVars (PHVs) are more complicated than Vars in another way:\ntheir original value might need to be calculated at a join, not a\nbase-level relation scan.  This can happen when a pulled-up subquery\ncontains a join.  Because of this, a PHV can create a join order\nconstraint that wouldn't otherwise exist, to ensure that it can\nbe calculated before it is used.  A PHV's expression can also contain\nLATERAL references, adding complications that are discussed below.\n\n\nRelation Identification and Qual Clause Placement\n-------------------------------------------------\n\nA qual clause obtained from WHERE or JOIN/ON can be enforced at the lowest\nscan or join level that includes all relations used in the clause.  For\nthis purpose we consider that outer joins listed in varnullingrels or\nphnullingrels are used in the clause, since we can't compute the qual's\nresult correctly until we know whether such Vars have gone to null.\n\nThe one exception to this general rule is that a non-degenerate outer\nJOIN/ON qual (one that references the non-nullable side of the join)\ncannot be enforced below that join, even if it doesn't reference the\nnullable side.  Pushing it down into the non-nullable side would result\nin rows disappearing from the join's result, rather than appearing as\nnull-extended rows.  To handle that, when we identify such a qual we\nartificially add the join's minimum input relid set to the set of\nrelations it is considered to use, forcing it to be evaluated exactly at\nthat join level.  The same happens for outer-join quals that mention no\nrelations at all.\n\nWhen attaching a qual clause to a join plan node that is performing an\nouter join, the qual clause is considered a \"join clause\" (that is, it is\napplied before the join performs null-extension) if it does not reference\nthat outer join in any varnullingrels or phnullingrels set, or a \"filter\nclause\" (applied after null-extension) if it does reference that outer\njoin.  A qual clause that originally appeared in that outer join's JOIN/ON\nwill fall into the first category, since the parser would not have marked\nany of its Vars as referencing the outer join.  A qual clause that\noriginally came from some upper ON clause or WHERE clause will be seen as\nreferencing the outer join if it references any of the nullable side's\nVars, since those Vars will be so marked by the parser.  But, if such a\nqual does not reference any nullable-side Vars, it's okay to push it down\ninto the non-nullable side, so it won't get attached to the join node in\nthe first place.\n\nThese things lead us to identify join relations within the planner\nby the sets of base relation RT indexes plus outer join RT indexes\nthat they include.  In that way, the sets of relations used by qual\nclauses can be directly compared to join relations' relid sets to\nsee where to place the clauses.  These identifying sets are unique\nbecause, for any given collection of base relations, there is only\none valid set of outer joins to have performed along the way to\njoining that set of base relations (although the order of applying\nthem could vary, as discussed above).\n\nSEMI joins do not have RT indexes, because they are artifacts made by\nthe planner rather than the parser.  (We could create rangetable\nentries for them, but there seems no need at present.)  This does not\ncause a problem for qual placement, because the nullable side of a\nsemijoin is not referenceable from above the join, so there is never a\nneed to cite it in varnullingrels or phnullingrels.  It does not cause a\nproblem for join relation identification either, since whether a semijoin\nhas been completed is again implicit in the set of base relations\nincluded in the join.\n\nAs usual, outer join identity 3 complicates matters.  If we start with\n\t(A leftjoin B on (Pab)) leftjoin C on (Pbc)\nthen the parser will have marked any C Vars appearing above these joins\nwith the RT index of the B/C join.  If we now transform to\n\tA leftjoin (B leftjoin C on (Pbc)) on (Pab)\nthen it would appear that a clause using only such Vars could be pushed\ndown and applied as a filter clause (not a join clause) at the lower\nB/C join.  But *this might not give the right answer* since the clause\nmight see a non-null value for the C Var that will be replaced by null\nonce the A/B join is performed.  We handle this by saying that the\npushed-down join hasn't completely performed the work of the B/C join\nand hence is not entitled to include that outer join relid in its\nrelid set.  When we form the A/B join, both outer joins' relids will\nbe added to its relid set, and then the upper clause will be applied\nat the correct join level.  (Note there is no problem when identity 3\nis applied in the other direction: if we started with the second form\nthen upper C Vars are marked with both outer join relids, so they\ncannot drop below whichever join is applied second.)  Similarly,\nVars representing the output of a pushed-down join do not acquire\nnullingrel bits for that join until after the upper join is performed.\n\nThere is one additional complication for qual clause placement, which\noccurs when we have made multiple versions of an outer-join clause as\ndescribed previously (that is, we have both \"Pbc\" and \"Pb*c\" forms of\nthe same clause seen in outer join identity 3).  When forming an outer\njoin we only want to apply one of the redundant versions of the clause.\nIf we are forming the B/C join without having yet computed the A/B\njoin, it's easy to reject the \"Pb*c\" form since its required relid\nset includes the A/B join relid which is not in the input.  However,\nif we form B/C after A/B, then both forms of the clause are applicable\nso far as that test can tell.  We have to look more closely to notice\nthat the \"Pbc\" clause form refers to relation B which is no longer\ndirectly accessible.  While such a check could be performed using the\nper-relation RelOptInfo.nulling_relids data, it would be annoyingly\nexpensive to do over and over as we consider different join paths.\nTo make this simple and reliable, we compute an \"incompatible_relids\"\nset for each variant version (clone) of a redundant clause.  A clone\nclause should not be applied if any of the outer-join relids listed in\nincompatible_relids has already been computed below the current join.\n\n\nOptimizer Functions\n-------------------\n\nThe primary entry point is planner().\n\nplanner()\nset up for recursive handling of subqueries\n-subquery_planner()\n pull up sublinks and subqueries from rangetable, if possible\n canonicalize qual\n     Attempt to simplify WHERE clause to the most useful form; this includes\n     flattening nested AND/ORs and detecting clauses that are duplicated in\n     different branches of an OR.\n simplify constant expressions\n process sublinks\n convert Vars of outer query levels into Params\n--grouping_planner()\n  preprocess target list for non-SELECT queries\n  handle UNION/INTERSECT/EXCEPT, GROUP BY, HAVING, aggregates,\n\tORDER BY, DISTINCT, LIMIT\n---query_planner()\n   make list of base relations used in query\n   split up the qual into restrictions (a=1) and joins (b=c)\n   find qual clauses that enable merge and hash joins\n----make_one_rel()\n     set_base_rel_pathlists()\n      find seqscan and all index paths for each base relation\n      find selectivity of columns used in joins\n     make_rel_from_joinlist()\n      hand off join subproblems to a plugin, GEQO, or standard_join_search()\n------standard_join_search()\n      call join_search_one_level() for each level of join tree needed\n      join_search_one_level():\n        For each joinrel of the prior level, do make_rels_by_clause_joins()\n        if it has join clauses, or make_rels_by_clauseless_joins() if not.\n        Also generate \"bushy plan\" joins between joinrels of lower levels.\n      Back at standard_join_search(), generate gather paths if needed for\n      each newly constructed joinrel, then apply set_cheapest() to extract\n      the cheapest path for it.\n      Loop back if this wasn't the top join level.\n  Back at grouping_planner:\n  do grouping (GROUP BY) and aggregation\n  do window functions\n  make unique (DISTINCT)\n  do sorting (ORDER BY)\n  do limit (LIMIT/OFFSET)\nBack at planner():\nconvert finished Path tree into a Plan tree\ndo final cleanup after planning\n\n\nOptimizer Data Structures\n-------------------------\n\nPlannerGlobal   - global information for a single planner invocation\n\nPlannerInfo     - information for planning a particular Query (we make\n                  a separate PlannerInfo node for each sub-Query)\n\nRelOptInfo      - a relation or joined relations\n\n RestrictInfo   - WHERE clauses, like \"x = 3\" or \"y = z\"\n                  (note the same structure is used for restriction and\n                   join clauses)\n\n Path           - every way to generate a RelOptInfo(sequential,index,joins)\n  A plain Path node can represent several simple plans, per its pathtype:\n    T_SeqScan   - sequential scan\n    T_SampleScan - tablesample scan\n    T_FunctionScan - function-in-FROM scan\n    T_TableFuncScan - table function scan\n    T_ValuesScan - VALUES scan\n    T_CteScan   - CTE (WITH) scan\n    T_NamedTuplestoreScan - ENR scan\n    T_WorkTableScan - scan worktable of a recursive CTE\n    T_Result    - childless Result plan node (used for FROM-less SELECT)\n  IndexPath     - index scan\n  BitmapHeapPath - top of a bitmapped index scan\n  TidPath       - scan by CTID\n  TidRangePath  - scan a contiguous range of CTIDs\n  SubqueryScanPath - scan a subquery-in-FROM\n  ForeignPath   - scan a foreign table, foreign join or foreign upper-relation\n  CustomPath    - for custom scan providers\n  AppendPath    - append multiple subpaths together\n  MergeAppendPath - merge multiple subpaths, preserving their common sort order\n  GroupResultPath - childless Result plan node (used for degenerate grouping)\n  MaterialPath  - a Material plan node\n  MemoizePath   - a Memoize plan node for caching tuples from sub-paths\n  UniquePath    - remove duplicate rows (either by hashing or sorting)\n  GatherPath    - collect the results of parallel workers\n  GatherMergePath - collect parallel results, preserving their common sort order\n  ProjectionPath - a Result plan node with child (used for projection)\n  ProjectSetPath - a ProjectSet plan node applied to some sub-path\n  SortPath      - a Sort plan node applied to some sub-path\n  IncrementalSortPath - an IncrementalSort plan node applied to some sub-path\n  GroupPath     - a Group plan node applied to some sub-path\n  UpperUniquePath - a Unique plan node applied to some sub-path\n  AggPath       - an Agg plan node applied to some sub-path\n  GroupingSetsPath - an Agg plan node used to implement GROUPING SETS\n  MinMaxAggPath - a Result plan node with subplans performing MIN/MAX\n  WindowAggPath - a WindowAgg plan node applied to some sub-path\n  SetOpPath     - a SetOp plan node applied to some sub-path\n  RecursiveUnionPath - a RecursiveUnion plan node applied to two sub-paths\n  LockRowsPath  - a LockRows plan node applied to some sub-path\n  ModifyTablePath - a ModifyTable plan node applied to some sub-path(s)\n  LimitPath     - a Limit plan node applied to some sub-path\n  NestPath      - nested-loop joins\n  MergePath     - merge joins\n  HashPath      - hash joins\n\n EquivalenceClass - a data structure representing a set of values known equal\n\n PathKey        - a data structure representing the sort ordering of a path\n\nThe optimizer spends a good deal of its time worrying about the ordering\nof the tuples returned by a path.  The reason this is useful is that by\nknowing the sort ordering of a path, we may be able to use that path as\nthe left or right input of a mergejoin and avoid an explicit sort step.\nNestloops and hash joins don't really care what the order of their inputs\nis, but mergejoin needs suitably ordered inputs.  Therefore, all paths\ngenerated during the optimization process are marked with their sort order\n(to the extent that it is known) for possible use by a higher-level merge.\n\nIt is also possible to avoid an explicit sort step to implement a user's\nORDER BY clause if the final path has the right ordering already, so the\nsort ordering is of interest even at the top level.  grouping_planner() will\nlook for the cheapest path with a sort order matching the desired order,\nthen compare its cost to the cost of using the cheapest-overall path and\ndoing an explicit sort on that.\n\nWhen we are generating paths for a particular RelOptInfo, we discard a path\nif it is more expensive than another known path that has the same or better\nsort order.  We will never discard a path that is the only known way to\nachieve a given sort order (without an explicit sort, that is).  In this\nway, the next level up will have the maximum freedom to build mergejoins\nwithout sorting, since it can pick from any of the paths retained for its\ninputs.\n\n\nEquivalenceClasses\n------------------\n\nDuring the deconstruct_jointree() scan of the query's qual clauses, we\nlook for mergejoinable equality clauses A = B.  When we find one, we\ncreate an EquivalenceClass containing the expressions A and B to record\nthat they are equal.  If we later find another equivalence clause B = C,\nwe add C to the existing EquivalenceClass for {A B}; this may require\nmerging two existing EquivalenceClasses.  At the end of the scan, we have\nsets of values that are known all transitively equal to each other.  We can\ntherefore use a comparison of any pair of the values as a restriction or\njoin clause (when these values are available at the scan or join, of\ncourse); furthermore, we need test only one such comparison, not all of\nthem.  Therefore, equivalence clauses are removed from the standard qual\ndistribution process.  Instead, when preparing a restriction or join clause\nlist, we examine each EquivalenceClass to see if it can contribute a\nclause, and if so we select an appropriate pair of values to compare.  For\nexample, if we are trying to join A's relation to C's, we can generate the\nclause A = C, even though this appeared nowhere explicitly in the original\nquery.  This may allow us to explore join paths that otherwise would have\nbeen rejected as requiring Cartesian-product joins.\n\nSometimes an EquivalenceClass may contain a pseudo-constant expression\n(i.e., one not containing Vars or Aggs of the current query level, nor\nvolatile functions).  In this case we do not follow the policy of\ndynamically generating join clauses: instead, we dynamically generate\nrestriction clauses \"var = const\" wherever one of the variable members of\nthe class can first be computed.  For example, if we have A = B and B = 42,\nwe effectively generate the restriction clauses A = 42 and B = 42, and then\nwe need not bother with explicitly testing the join clause A = B when the\nrelations are joined.  In effect, all the class members can be tested at\nrelation-scan level and there's never a need for join tests.\n\nThe precise technical interpretation of an EquivalenceClass is that it\nasserts that at any plan node where more than one of its member values\ncan be computed, output rows in which the values are not all equal may\nbe discarded without affecting the query result.  (We require all levels\nof the plan to enforce EquivalenceClasses, hence a join need not recheck\nequality of values that were computable by one of its children.)\n\nOuter joins complicate this picture quite a bit, however.  While we could\ntheoretically use mergejoinable equality clauses that appear in outer-join\nconditions as sources of EquivalenceClasses, there's a serious difficulty:\nthe resulting deductions are not valid everywhere.  For example, given\n\n\tSELECT * FROM a LEFT JOIN b ON (a.x = b.y AND a.x = 42);\n\nwe can safely derive b.y = 42 and use that in the scan of B, because B\nrows not having b.y = 42 will not contribute to the join result.  However,\nwe cannot apply a.x = 42 at the scan of A, or we will remove rows that\nshould appear in the join result.  We could apply a.x = 42 as an outer join\ncondition (and then it would be unnecessary to also check a.x = b.y).\nThis is not yet implemented, however.\n\nA related issue is that constants appearing below an outer join are\nless constant than they appear.  Ordinarily, if we find \"A = 1\" and\n\"B = 1\", it's okay to put A and B into the same EquivalenceClass.\nBut consider\n\n\tSELECT * FROM a\n\t  LEFT JOIN (SELECT * FROM b WHERE b.z = 1) b ON (a.x = b.y)\n\tWHERE a.x = 1;\n\nIt would be a serious error to conclude that a.x = b.z, so we cannot\nform a single EquivalenceClass {a.x b.z 1}.\n\nThis leads to considering EquivalenceClasses as applying within \"join\ndomains\", which are sets of relations that are inner-joined to each other.\n(We can treat semijoins as if they were inner joins for this purpose.)\nThere is a top-level join domain, and then each outer join in the query\ncreates a new join domain comprising its nullable side.  Full joins create\ntwo join domains, one for each side.  EquivalenceClasses generated from\nWHERE are associated with the top-level join domain.  EquivalenceClasses\ngenerated from the ON clause of an outer join are associated with the\ndomain created by that outer join.  EquivalenceClasses generated from the\nON clause of an inner or semi join are associated with the syntactically\nmost closely nested join domain.\n\nHaving defined these domains, we can fix the not-so-constant-constants\nproblem by considering that constants only match EquivalenceClass members\nwhen they come from clauses within the same join domain.  In the above\nexample, this means we keep {a.x 1} and {b.z 1} as separate\nEquivalenceClasses and don't erroneously merge them.  We don't have to\nworry about this for Vars (or expressions containing Vars), because\nreferences to the \"same\" column from different join domains will have\ndifferent varnullingrels and thus won't be equal() anyway.\n\nIn the future, the join-domain concept may allow us to treat mergejoinable\nouter-join conditions as sources of EquivalenceClasses.  The idea would be\nthat conditions derived from such classes could only be enforced at scans\nor joins that are within the appropriate join domain.  This is not\nimplemented yet, however, as the details are trickier than they appear.\n\nAnother instructive example is:\n\n\tSELECT *\n\t  FROM a LEFT JOIN\n\t       (SELECT * FROM b JOIN c ON b.y = c.z WHERE b.y = 10) ss\n\t       ON a.x = ss.y\n\t  ORDER BY ss.y;\n\nWe can form the EquivalenceClass {b.y c.z 10} and thereby apply c.z = 10\nwhile scanning C, as well as b.y = 10 while scanning B, so that no clause\nneeds to be checked at the inner join.  The left-join clause \"a.x = ss.y\"\n(really \"a.x = b.y\") is not considered an equivalence clause, so we do\nnot insert a.x into that same EquivalenceClass; if we did, we'd falsely\nconclude a.x = 10.  In the future though we might be able to do that,\nif we can keep from applying a.x = 10 at the scan of A, which in principle\nwe could do by noting that the EquivalenceClass only applies within the\n{B,C} join domain.\n\nAlso notice that ss.y in the ORDER BY is really b.y* (that is, the\npossibly-nulled form of b.y), so we will not confuse it with the b.y member\nof the lower EquivalenceClass.  Thus, we won't mistakenly conclude that\nthat ss.y is equal to a constant, which if true would lead us to think that\nsorting for the ORDER BY is unnecessary (see discussion of PathKeys below).\nInstead, there will be a separate EquivalenceClass containing only b.y*,\nwhich will form the basis for the PathKey describing the required sort\norder.\n\nAlso consider this variant:\n\n\tSELECT *\n\t  FROM a LEFT JOIN\n\t       (SELECT * FROM b JOIN c ON b.y = c.z WHERE b.y = 10) ss\n\t       ON a.x = ss.y\n\t  WHERE a.x = 42;\n\nWe still form the EquivalenceClass {b.y c.z 10}, and additionally\nwe have an EquivalenceClass {a.x 42} belonging to a different join domain.\nWe cannot use \"a.x = b.y\" to merge these classes.  However, we can compare\nthat outer join clause to the existing EquivalenceClasses and form the\nderived clause \"b.y = 42\", which we can treat as a valid equivalence\nwithin the lower join domain (since no row of that domain not having\nb.y = 42 can contribute to the outer-join result).  That makes the lower\nEquivalenceClass {42 b.y c.z 10}, resulting in the contradiction 10 = 42,\nwhich lets the planner deduce that the B/C join need not be computed at\nall: the result of that whole join domain can be forced to empty.\n(This gets implemented as a gating Result filter, since more usually the\npotential contradiction involves Param values rather than just Consts, and\nthus it has to be checked at runtime.  We can use the join domain to\ndetermine the join level at which to place the gating condition.)\n\nThere is an additional complication when re-ordering outer joins according\nto identity 3.  Recall that the two choices we consider for such joins are\n\n\tA leftjoin (B leftjoin C on (Pbc)) on (Pab)\n\t(A leftjoin B on (Pab)) leftjoin C on (Pb*c)\n\nwhere the star denotes varnullingrels markers on B's Vars.  When Pbc\nis (or includes) a mergejoinable clause, we have something like\n\n\tA leftjoin (B leftjoin C on (b.b = c.c)) on (Pab)\n\t(A leftjoin B on (Pab)) leftjoin C on (b.b* = c.c)\n\nWe could generate an EquivalenceClause linking b.b and c.c, but if we\nthen also try to link b.b* and c.c, we end with a nonsensical conclusion\nthat b.b and b.b* are equal (at least in some parts of the plan tree).\nIn any case, the conclusions we could derive from such a thing would be\nlargely duplicative.  Conditions involving b.b* can't be computed below\nthis join nest, while any conditions that can be computed would be\nduplicative of what we'd get from the b.b/c.c combination.  Therefore,\nwe choose to generate an EquivalenceClause linking b.b and c.c, but\n\"b.b* = c.c\" is handled as just an ordinary clause.\n\nTo aid in determining the sort ordering(s) that can work with a mergejoin,\nwe mark each mergejoinable clause with the EquivalenceClasses of its left\nand right inputs.  For an equivalence clause, these are of course the same\nEquivalenceClass.  For a non-equivalence mergejoinable clause (such as an\nouter-join qualification), we generate two separate EquivalenceClasses for\nthe left and right inputs.  This may result in creating single-item\nequivalence \"classes\", though of course these are still subject to merging\nif other equivalence clauses are later found to bear on the same\nexpressions.\n\nAnother way that we may form a single-item EquivalenceClass is in creation\nof a PathKey to represent a desired sort order (see below).  This happens\nif an ORDER BY or GROUP BY key is not mentioned in any equivalence\nclause.  We need to reason about sort orders in such queries, and our\nrepresentation of sort ordering is a PathKey which depends on an\nEquivalenceClass, so we have to make an EquivalenceClass.  This is a bit\ndifferent from the above cases because such an EquivalenceClass might\ncontain an aggregate function or volatile expression.  (A clause containing\na volatile function will never be considered mergejoinable, even if its top\noperator is mergejoinable, so there is no way for a volatile expression to\nget into EquivalenceClasses otherwise.  Aggregates are disallowed in WHERE\naltogether, so will never be found in a mergejoinable clause.)  This is just\na convenience to maintain a uniform PathKey representation: such an\nEquivalenceClass will never be merged with any other.  Note in particular\nthat a single-item EquivalenceClass {a.x} is *not* meant to imply an\nassertion that a.x = a.x; the practical effect of this is that a.x could\nbe NULL.\n\nAn EquivalenceClass also contains a list of btree opfamily OIDs, which\ndetermines what the equalities it represents actually \"mean\".  All the\nequivalence clauses that contribute to an EquivalenceClass must have\nequality operators that belong to the same set of opfamilies.  (Note: most\nof the time, a particular equality operator belongs to only one family, but\nit's possible that it belongs to more than one.  We keep track of all the\nfamilies to ensure that we can make use of an index belonging to any one of\nthe families for mergejoin purposes.)\n\nFor the same sort of reason, an EquivalenceClass is also associated\nwith a particular collation, if its datatype(s) care about collation.\n\nAn EquivalenceClass can contain \"em_is_child\" members, which are copies\nof members that contain appendrel parent relation Vars, transposed to\ncontain the equivalent child-relation variables or expressions.  These\nmembers are *not* full-fledged members of the EquivalenceClass and do not\naffect the class's overall properties at all.  They are kept only to\nsimplify matching of child-relation expressions to EquivalenceClasses.\nMost operations on EquivalenceClasses should ignore child members.\n\n\nPathKeys\n--------\n\nThe PathKeys data structure represents what is known about the sort order\nof the tuples generated by a particular Path.  A path's pathkeys field is a\nlist of PathKey nodes, where the n'th item represents the n'th sort key of\nthe result.  Each PathKey contains these fields:\n\n\t* a reference to an EquivalenceClass\n\t* a btree opfamily OID (must match one of those in the EC)\n\t* a sort direction (ascending or descending)\n\t* a nulls-first-or-last flag\n\nThe EquivalenceClass represents the value being sorted on.  Since the\nvarious members of an EquivalenceClass are known equal according to the\nopfamily, we can consider a path sorted by any one of them to be sorted by\nany other too; this is what justifies referencing the whole\nEquivalenceClass rather than just one member of it.\n\nIn single/base relation RelOptInfo's, the Paths represent various ways\nof scanning the relation and the resulting ordering of the tuples.\nSequential scan Paths have NIL pathkeys, indicating no known ordering.\nIndex scans have Path.pathkeys that represent the chosen index's ordering,\nif any.  A single-key index would create a single-PathKey list, while a\nmulti-column index generates a list with one element per key index column.\nNon-key columns specified in the INCLUDE clause of covering indexes don't\nhave corresponding PathKeys in the list, because they have no influence on\nindex ordering.  (Actually, since an index can be scanned either forward or\nbackward, there are two possible sort orders and two possible PathKey lists\nit can generate.)\n\nNote that a bitmap scan has NIL pathkeys since we can say nothing about\nthe overall order of its result.  Also, an indexscan on an unordered type\nof index generates NIL pathkeys.  However, we can always create a pathkey\nby doing an explicit sort.  The pathkeys for a Sort plan's output just\nrepresent the sort key fields and the ordering operators used.\n\nThings get more interesting when we consider joins.  Suppose we do a\nmergejoin between A and B using the mergeclause A.X = B.Y.  The output\nof the mergejoin is sorted by X --- but it is also sorted by Y.  Again,\nthis can be represented by a PathKey referencing an EquivalenceClass\ncontaining both X and Y.\n\nWith a little further thought, it becomes apparent that nestloop joins\ncan also produce sorted output.  For example, if we do a nestloop join\nbetween outer relation A and inner relation B, then any pathkeys relevant\nto A are still valid for the join result: we have not altered the order of\nthe tuples from A.  Even more interesting, if there was an equivalence clause\nA.X=B.Y, and A.X was a pathkey for the outer relation A, then we can assert\nthat B.Y is a pathkey for the join result; X was ordered before and still\nis, and the joined values of Y are equal to the joined values of X, so Y\nmust now be ordered too.  This is true even though we used neither an\nexplicit sort nor a mergejoin on Y.  (Note: hash joins cannot be counted\non to preserve the order of their outer relation, because the executor\nmight decide to \"batch\" the join, so we always set pathkeys to NIL for\na hashjoin path.)\n\nAn outer join doesn't preserve the ordering of its nullable input\nrelation(s), because it might insert nulls at random points in the\nordering.  We don't need to think about this explicitly in the PathKey\nrepresentation, because a PathKey representing a post-join variable\nwill contain varnullingrel bits, making it not equal to a PathKey\nrepresenting the pre-join value.\n\nIn general, we can justify using EquivalenceClasses as the basis for\npathkeys because, whenever we scan a relation containing multiple\nEquivalenceClass members or join two relations each containing\nEquivalenceClass members, we apply restriction or join clauses derived from\nthe EquivalenceClass.  This guarantees that any two values listed in the\nEquivalenceClass are in fact equal in all tuples emitted by the scan or\njoin, and therefore that if the tuples are sorted by one of the values,\nthey can be considered sorted by any other as well.  It does not matter\nwhether the test clause is used as a mergeclause, or merely enforced\nafter-the-fact as a qpqual filter.\n\nNote that there is no particular difficulty in labeling a path's sort\norder with a PathKey referencing an EquivalenceClass that contains\nvariables not yet joined into the path's output.  We can simply ignore\nsuch entries as not being relevant (yet).  This makes it possible to\nuse the same EquivalenceClasses throughout the join planning process.\nIn fact, by being careful not to generate multiple identical PathKey\nobjects, we can reduce comparison of EquivalenceClasses and PathKeys\nto simple pointer comparison, which is a huge savings because add_path\nhas to make a large number of PathKey comparisons in deciding whether\ncompeting Paths are equivalently sorted.\n\nPathkeys are also useful to represent an ordering that we wish to achieve,\nsince they are easily compared to the pathkeys of a potential candidate\npath.  So, SortGroupClause lists are turned into pathkeys lists for use\ninside the optimizer.\n\nAn additional refinement we can make is to insist that canonical pathkey\nlists (sort orderings) do not mention the same EquivalenceClass more than\nonce.  For example, in all these cases the second sort column is redundant,\nbecause it cannot distinguish values that are the same according to the\nfirst sort column:\n\tSELECT ... ORDER BY x, x\n\tSELECT ... ORDER BY x, x DESC\n\tSELECT ... WHERE x = y ORDER BY x, y\nAlthough a user probably wouldn't write \"ORDER BY x,x\" directly, such\nredundancies are more probable once equivalence classes have been\nconsidered.  Also, the system may generate redundant pathkey lists when\ncomputing the sort ordering needed for a mergejoin.  By eliminating the\nredundancy, we save time and improve planning, since the planner will more\neasily recognize equivalent orderings as being equivalent.\n\nAnother interesting property is that if the underlying EquivalenceClass\ncontains a constant, then the pathkey is completely redundant and need not\nbe sorted by at all!  Every interesting row must contain the same value,\nso there's no need to sort.  This might seem pointless because users\nare unlikely to write \"... WHERE x = 42 ORDER BY x\", but it allows us to\nrecognize when particular index columns are irrelevant to the sort order:\nif we have \"... WHERE x = 42 ORDER BY y\", scanning an index on (x,y)\nproduces correctly ordered data without a sort step.  We used to have very\nugly ad-hoc code to recognize that in limited contexts, but discarding\nconstant ECs from pathkeys makes it happen cleanly and automatically.\n\n\nOrder of processing for EquivalenceClasses and PathKeys\n-------------------------------------------------------\n\nAs alluded to above, there is a specific sequence of phases in the\nprocessing of EquivalenceClasses and PathKeys during planning.  During the\ninitial scanning of the query's quals (deconstruct_jointree followed by\nreconsider_outer_join_clauses), we construct EquivalenceClasses based on\nmergejoinable clauses found in the quals.  At the end of this process,\nwe know all we can know about equivalence of different variables, so\nsubsequently there will be no further merging of EquivalenceClasses.\nAt that point it is possible to consider the EquivalenceClasses as\n\"canonical\" and build canonical PathKeys that reference them.  At this\ntime we construct PathKeys for the query's ORDER BY and related clauses.\n(Any ordering expressions that do not appear elsewhere will result in\nthe creation of new EquivalenceClasses, but this cannot result in merging\nexisting classes, so canonical-ness is not lost.)\n\nBecause all the EquivalenceClasses are known before we begin path\ngeneration, we can use them as a guide to which indexes are of interest:\nif an index's column is not mentioned in any EquivalenceClass then that\nindex's sort order cannot possibly be helpful for the query.  This allows\nshort-circuiting of much of the processing of create_index_paths() for\nirrelevant indexes.\n\nThere are some cases where planner.c constructs additional\nEquivalenceClasses and PathKeys after query_planner has completed.\nIn these cases, the extra ECs/PKs are needed to represent sort orders\nthat were not considered during query_planner.  Such situations should be\nminimized since it is impossible for query_planner to return a plan\nproducing such a sort order, meaning an explicit sort will always be needed.\nCurrently this happens only for queries involving multiple window functions\nwith different orderings, for which extra sorts are needed anyway.\n\n\nParameterized Paths\n-------------------\n\nThe naive way to join two relations using a clause like WHERE A.X = B.Y\nis to generate a nestloop plan like this:\n\n\tNestLoop\n\t\tFilter: A.X = B.Y\n\t\t-> Seq Scan on A\n\t\t-> Seq Scan on B\n\nWe can make this better by using a merge or hash join, but it still\nrequires scanning all of both input relations.  If A is very small and B is\nvery large, but there is an index on B.Y, it can be enormously better to do\nsomething like this:\n\n\tNestLoop\n\t\t-> Seq Scan on A\n\t\t-> Index Scan using B_Y_IDX on B\n\t\t\tIndex Condition: B.Y = A.X\n\nHere, we are expecting that for each row scanned from A, the nestloop\nplan node will pass down the current value of A.X into the scan of B.\nThat allows the indexscan to treat A.X as a constant for any one\ninvocation, and thereby use it as an index key.  This is the only plan type\nthat can avoid fetching all of B, and for small numbers of rows coming from\nA, that will dominate every other consideration.  (As A gets larger, this\ngets less attractive, and eventually a merge or hash join will win instead.\nSo we have to cost out all the alternatives to decide what to do.)\n\nIt can be useful for the parameter value to be passed down through\nintermediate layers of joins, for example:\n\n\tNestLoop\n\t\t-> Seq Scan on A\n\t\tHash Join\n\t\t\tJoin Condition: B.Y = C.W\n\t\t\t-> Seq Scan on B\n\t\t\t-> Index Scan using C_Z_IDX on C\n\t\t\t\tIndex Condition: C.Z = A.X\n\nIf all joins are plain inner joins then this is usually unnecessary,\nbecause it's possible to reorder the joins so that a parameter is used\nimmediately below the nestloop node that provides it.  But in the\npresence of outer joins, such join reordering may not be possible.\n\nAlso, the bottom-level scan might require parameters from more than one\nother relation.  In principle we could join the other relations first\nso that all the parameters are supplied from a single nestloop level.\nBut if those other relations have no join clause in common (which is\ncommon in star-schema queries for instance), the planner won't consider\njoining them directly to each other.  In such a case we need to be able\nto create a plan like\n\n    NestLoop\n        -> Seq Scan on SmallTable1 A\n        NestLoop\n            -> Seq Scan on SmallTable2 B\n            -> Index Scan using XYIndex on LargeTable C\n                 Index Condition: C.X = A.AID and C.Y = B.BID\n\nso we should be willing to pass down A.AID through a join even though\nthere is no join order constraint forcing the plan to look like this.\n\nBefore version 9.2, Postgres used ad-hoc methods for planning and\nexecuting nestloop queries of this kind, and those methods could not\nhandle passing parameters down through multiple join levels.\n\nTo plan such queries, we now use a notion of a \"parameterized path\",\nwhich is a path that makes use of a join clause to a relation that's not\nscanned by the path.  In the example two above, we would construct a\npath representing the possibility of doing this:\n\n\t-> Index Scan using C_Z_IDX on C\n\t\tIndex Condition: C.Z = A.X\n\nThis path will be marked as being parameterized by relation A.  (Note that\nthis is only one of the possible access paths for C; we'd still have a\nplain unparameterized seqscan, and perhaps other possibilities.)  The\nparameterization marker does not prevent joining the path to B, so one of\nthe paths generated for the joinrel {B C} will represent\n\n\tHash Join\n\t\tJoin Condition: B.Y = C.W\n\t\t-> Seq Scan on B\n\t\t-> Index Scan using C_Z_IDX on C\n\t\t\tIndex Condition: C.Z = A.X\n\nThis path is still marked as being parameterized by A.  When we attempt to\njoin {B C} to A to form the complete join tree, such a path can only be\nused as the inner side of a nestloop join: it will be ignored for other\npossible join types.  So we will form a join path representing the query\nplan shown above, and it will compete in the usual way with paths built\nfrom non-parameterized scans.\n\nWhile all ordinary paths for a particular relation generate the same set\nof rows (since they must all apply the same set of restriction clauses),\nparameterized paths typically generate fewer rows than less-parameterized\npaths, since they have additional clauses to work with.  This means we\nmust consider the number of rows generated as an additional figure of\nmerit.  A path that costs more than another, but generates fewer rows,\nmust be kept since the smaller number of rows might save work at some\nintermediate join level.  (It would not save anything if joined\nimmediately to the source of the parameters.)\n\nTo keep cost estimation rules relatively simple, we make an implementation\nrestriction that all paths for a given relation of the same parameterization\n(i.e., the same set of outer relations supplying parameters) must have the\nsame rowcount estimate.  This is justified by insisting that each such path\napply *all* join clauses that are available with the named outer relations.\nDifferent paths might, for instance, choose different join clauses to use\nas index clauses; but they must then apply any other join clauses available\nfrom the same outer relations as filter conditions, so that the set of rows\nreturned is held constant.  This restriction doesn't degrade the quality of\nthe finished plan: it amounts to saying that we should always push down\nmovable join clauses to the lowest possible evaluation level, which is a\ngood thing anyway.  The restriction is useful in particular to support\npre-filtering of join paths in add_path_precheck.  Without this rule we\ncould never reject a parameterized path in advance of computing its rowcount\nestimate, which would greatly reduce the value of the pre-filter mechanism.\n\nTo limit planning time, we have to avoid generating an unreasonably large\nnumber of parameterized paths.  We do this by only generating parameterized\nrelation scan paths for index scans, and then only for indexes for which\nsuitable join clauses are available.  There are also heuristics in join\nplanning that try to limit the number of parameterized paths considered.\n\nIn particular, there's been a deliberate policy decision to favor hash\njoins over merge joins for parameterized join steps (those occurring below\na nestloop that provides parameters to the lower join's inputs).  While we\ndo not ignore merge joins entirely, joinpath.c does not fully explore the\nspace of potential merge joins with parameterized inputs.  Also, add_path\ntreats parameterized paths as having no pathkeys, so that they compete\nonly on cost and rowcount; they don't get preference for producing a\nspecial sort order.  This creates additional bias against merge joins,\nsince we might discard a path that could have been useful for performing\na merge without an explicit sort step.  Since a parameterized path must\nultimately be used on the inside of a nestloop, where its sort order is\nuninteresting, these choices do not affect any requirement for the final\noutput order of a query --- they only make it harder to use a merge join\nat a lower level.  The savings in planning work justifies that.\n\nSimilarly, parameterized paths do not normally get preference in add_path\nfor having cheap startup cost; that's seldom of much value when on the\ninside of a nestloop, so it seems not worth keeping extra paths solely for\nthat.  An exception occurs for parameterized paths for the RHS relation of\na SEMI or ANTI join: in those cases, we can stop the inner scan after the\nfirst match, so it's primarily startup not total cost that we care about.\n\n\nLATERAL subqueries\n------------------\n\nAs of 9.3 we support SQL-standard LATERAL references from subqueries in\nFROM (and also functions in FROM).  The planner implements these by\ngenerating parameterized paths for any RTE that contains lateral\nreferences.  In such cases, *all* paths for that relation will be\nparameterized by at least the set of relations used in its lateral\nreferences.  (And in turn, join relations including such a subquery might\nnot have any unparameterized paths.)  All the other comments made above for\nparameterized paths still apply, though; in particular, each such path is\nstill expected to enforce any join clauses that can be pushed down to it,\nso that all paths of the same parameterization have the same rowcount.\n\nWe also allow LATERAL subqueries to be flattened (pulled up into the parent\nquery) by the optimizer, but only when this does not introduce lateral\nreferences into JOIN/ON quals that would refer to relations outside the\nlowest outer join at/above that qual.  The semantics of such a qual would\nbe unclear.  Note that even with this restriction, pullup of a LATERAL\nsubquery can result in creating PlaceHolderVars that contain lateral\nreferences to relations outside their syntactic scope.  We still evaluate\nsuch PHVs at their syntactic location or lower, but the presence of such a\nPHV in the quals or targetlist of a plan node requires that node to appear\non the inside of a nestloop join relative to the rel(s) supplying the\nlateral reference.  (Perhaps now that that stuff works, we could relax the\npullup restriction?)\n\n\nSecurity-level constraints on qual clauses\n------------------------------------------\n\nTo support row-level security and security-barrier views efficiently,\nwe mark qual clauses (RestrictInfo nodes) with a \"security_level\" field.\nThe basic concept is that a qual with a lower security_level must be\nevaluated before one with a higher security_level.  This ensures that\n\"leaky\" quals that might expose sensitive data are not evaluated until\nafter the security barrier quals that are supposed to filter out\nsecurity-sensitive rows.  However, many qual conditions are \"leakproof\",\nthat is we trust the functions they use to not expose data.  To avoid\nunnecessarily inefficient plans, a leakproof qual is not delayed by\nsecurity-level considerations, even if it has a higher syntactic\nsecurity_level than another qual.\n\nIn a query that contains no use of RLS or security-barrier views, all\nquals will have security_level zero, so that none of these restrictions\nkick in; we don't even need to check leakproofness of qual conditions.\n\nIf there are security-barrier quals, they get security_level zero (and\npossibly higher, if there are multiple layers of barriers).  Regular quals\ncoming from the query text get a security_level one more than the highest\nlevel used for barrier quals.\n\nWhen new qual clauses are generated by EquivalenceClass processing,\nthey must be assigned a security_level.  This is trickier than it seems.\nOne's first instinct is that it would be safe to use the largest level\nfound among the source quals for the EquivalenceClass, but that isn't\nsafe at all, because it allows unwanted delays of security-barrier quals.\nConsider a barrier qual \"t.x = t.y\" plus a query qual \"t.x = constant\",\nand suppose there is another query qual \"leaky_function(t.z)\" that\nwe mustn't evaluate before the barrier qual has been checked.\nWe will have an EC {t.x, t.y, constant} which will lead us to replace\nthe EC quals with \"t.x = constant AND t.y = constant\".  (We do not want\nto give up that behavior, either, since the latter condition could allow\nuse of an index on t.y, which we would never discover from the original\nquals.)  If these generated quals are assigned the same security_level as\nthe query quals, then it's possible for the leaky_function qual to be\nevaluated first, allowing leaky_function to see data from rows that\npossibly don't pass the barrier condition.\n\nInstead, our handling of security levels with ECs works like this:\n* Quals are not accepted as source clauses for ECs in the first place\nunless they are leakproof or have security_level zero.\n* EC-derived quals are assigned the minimum (not maximum) security_level\nfound among the EC's source clauses.\n* If the maximum security_level found among the EC's source clauses is\nabove zero, then the equality operators selected for derived quals must\nbe leakproof.  When no such operator can be found, the EC is treated as\n\"broken\" and we fall back to emitting its source clauses without any\nadditional derived quals.\n\nThese rules together ensure that an untrusted qual clause (one with\nsecurity_level above zero) cannot cause an EC to generate a leaky derived\nclause.  This makes it safe to use the minimum not maximum security_level\nfor derived clauses.  The rules could result in poor plans due to not\nbeing able to generate derived clauses at all, but the risk of that is\nsmall in practice because most btree equality operators are leakproof.\nAlso, by making exceptions for level-zero quals, we ensure that there is\nno plan degradation when no barrier quals are present.\n\nOnce we have security levels assigned to all clauses, enforcement\nof barrier-qual ordering restrictions boils down to two rules:\n\n* Table scan plan nodes must not select quals for early execution\n(for example, use them as index qualifiers in an indexscan) unless\nthey are leakproof or have security_level no higher than any other\nqual that is due to be executed at the same plan node.  (Use the\nutility function restriction_is_securely_promotable() to check\nwhether it's okay to select a qual for early execution.)\n\n* Normal execution of a list of quals must execute them in an order\nthat satisfies the same security rule, ie higher security_levels must\nbe evaluated later unless leakproof.  (This is handled in a single place\nby order_qual_clauses() in createplan.c.)\n\norder_qual_clauses() uses a heuristic to decide exactly what to do with\nleakproof clauses.  Normally it sorts clauses by security_level then cost,\nbeing careful that the sort is stable so that we don't reorder clauses\nwithout a clear reason.  But this could result in a very expensive qual\nbeing done before a cheaper one that is of higher security_level.\nIf the cheaper qual is leaky we have no choice, but if it is leakproof\nwe could put it first.  We choose to sort leakproof quals as if they\nhave security_level zero, but only when their cost is less than 10X\ncpu_operator_cost; that restriction alleviates the opposite problem of\ndoing expensive quals first just because they're leakproof.\n\nAdditional rules will be needed to support safe handling of join quals\nwhen there is a mix of security levels among join quals; for example, it\nwill be necessary to prevent leaky higher-security-level quals from being\nevaluated at a lower join level than other quals of lower security level.\nCurrently there is no need to consider that since security-prioritized\nquals can only be single-table restriction quals coming from RLS policies\nor security-barrier views, and security-barrier view subqueries are never\nflattened into the parent query.  Hence enforcement of security-prioritized\nquals only happens at the table scan level.  With extra rules for safe\nhandling of security levels among join quals, it should be possible to let\nsecurity-barrier views be flattened into the parent query, allowing more\nflexibility of planning while still preserving required ordering of qual\nevaluation.  But that will come later.\n\n\nPost scan/join planning\n-----------------------\n\nSo far we have discussed only scan/join planning, that is, implementation\nof the FROM and WHERE clauses of a SQL query.  But the planner must also\ndetermine how to deal with GROUP BY, aggregation, and other higher-level\nfeatures of queries; and in many cases there are multiple ways to do these\nsteps and thus opportunities for optimization choices.  These steps, like\nscan/join planning, are handled by constructing Paths representing the\ndifferent ways to do a step, then choosing the cheapest Path.\n\nSince all Paths require a RelOptInfo as \"parent\", we create RelOptInfos\nrepresenting the outputs of these upper-level processing steps.  These\nRelOptInfos are mostly dummy, but their pathlist lists hold all the Paths\nconsidered useful for each step.  Currently, we may create these types of\nadditional RelOptInfos during upper-level planning:\n\nUPPERREL_SETOP\t\tresult of UNION/INTERSECT/EXCEPT, if any\nUPPERREL_PARTIAL_GROUP_AGG\tresult of partial grouping/aggregation, if any\nUPPERREL_GROUP_AGG\tresult of grouping/aggregation, if any\nUPPERREL_WINDOW\t\tresult of window functions, if any\nUPPERREL_PARTIAL_DISTINCT\tresult of partial \"SELECT DISTINCT\", if any\nUPPERREL_DISTINCT\tresult of \"SELECT DISTINCT\", if any\nUPPERREL_ORDERED\tresult of ORDER BY, if any\nUPPERREL_FINAL\t\tresult of any remaining top-level actions\n\nUPPERREL_FINAL is used to represent any final processing steps, currently\nLockRows (SELECT FOR UPDATE), LIMIT/OFFSET, and ModifyTable.  There is no\nflexibility about the order in which these steps are done, and thus no need\nto subdivide this stage more finely.\n\nThese \"upper relations\" are identified by the UPPERREL enum values shown\nabove, plus a relids set, which allows there to be more than one upperrel\nof the same kind.  We use NULL for the relids if there's no need for more\nthan one upperrel of the same kind.  Currently, in fact, the relids set\nis vestigial because it's always NULL, but that's expected to change in\nthe future.  For example, in planning set operations, we might need the\nrelids to denote which subset of the leaf SELECTs has been combined in a\nparticular group of Paths that are competing with each other.\n\nThe result of subquery_planner() is always returned as a set of Paths\nstored in the UPPERREL_FINAL rel with NULL relids.  The other types of\nupperrels are created only if needed for the particular query.\n\n\nParallel Query and Partial Paths\n--------------------------------\n\nParallel query involves dividing up the work that needs to be performed\neither by an entire query or some portion of the query in such a way that\nsome of that work can be done by one or more worker processes, which are\ncalled parallel workers.  Parallel workers are a subtype of dynamic\nbackground workers; see src/backend/access/transam/README.parallel for a\nfuller description.  The academic literature on parallel query suggests\nthat parallel execution strategies can be divided into essentially two\ncategories: pipelined parallelism, where the execution of the query is\ndivided into multiple stages and each stage is handled by a separate\nprocess; and partitioning parallelism, where the data is split between\nmultiple processes and each process handles a subset of it.  The\nliterature, however, suggests that gains from pipeline parallelism are\noften very limited due to the difficulty of avoiding pipeline stalls.\nConsequently, we do not currently attempt to generate query plans that\nuse this technique.\n\nInstead, we focus on partitioning parallelism, which does not require\nthat the underlying table be partitioned.  It only requires that (1)\nthere is some method of dividing the data from at least one of the base\ntables involved in the relation across multiple processes, (2) allowing\neach process to handle its own portion of the data, and then (3)\ncollecting the results.  Requirements (2) and (3) are satisfied by the\nexecutor node Gather (or GatherMerge), which launches any number of worker\nprocesses and executes its single child plan in all of them, and perhaps\nin the leader also, if the children aren't generating enough data to keep\nthe leader busy.  Requirement (1) is handled by the table scan node: when\ninvoked with parallel_aware = true, this node will, in effect, partition\nthe table on a block by block basis, returning a subset of the tuples from\nthe relation in each worker where that scan node is executed.\n\nJust as we do for non-parallel access methods, we build Paths to\nrepresent access strategies that can be used in a parallel plan.  These\nare, in essence, the same strategies that are available in the\nnon-parallel plan, but there is an important difference: a path that\nwill run beneath a Gather node returns only a subset of the query\nresults in each worker, not all of them.  To form a path that can\nactually be executed, the (rather large) cost of the Gather node must be\naccounted for.  For this reason among others, paths intended to run\nbeneath a Gather node - which we call \"partial\" paths since they return\nonly a subset of the results in each worker - must be kept separate from\nordinary paths (see RelOptInfo's partial_pathlist and the function\nadd_partial_path).\n\nOne of the keys to making parallel query effective is to run as much of\nthe query in parallel as possible.  Therefore, we expect it to generally\nbe desirable to postpone the Gather stage until as near to the top of the\nplan as possible.  Expanding the range of cases in which more work can be\npushed below the Gather (and costing them accurately) is likely to keep us\nbusy for a long time to come.\n\nPartitionwise joins\n-------------------\n\nA join between two similarly partitioned tables can be broken down into joins\nbetween their matching partitions if there exists an equi-join condition\nbetween the partition keys of the joining tables. The equi-join between\npartition keys implies that all join partners for a given row in one\npartitioned table must be in the corresponding partition of the other\npartitioned table. Because of this the join between partitioned tables to be\nbroken into joins between the matching partitions. The resultant join is\npartitioned in the same way as the joining relations, thus allowing an N-way\njoin between similarly partitioned tables having equi-join condition between\ntheir partition keys to be broken down into N-way joins between their matching\npartitions. This technique of breaking down a join between partitioned tables\ninto joins between their partitions is called partitionwise join. We will use\nterm \"partitioned relation\" for either a partitioned table or a join between\ncompatibly partitioned tables.\n\nEven if the joining relations don't have exactly the same partition bounds,\npartitionwise join can still be applied by using an advanced\npartition-matching algorithm.  For both the joining relations, the algorithm\nchecks whether every partition of one joining relation only matches one\npartition of the other joining relation at most.  In such a case the join\nbetween the joining relations can be broken down into joins between the\nmatching partitions.  The join relation can then be considered partitioned.\nThe algorithm produces the pairs of the matching partitions, plus the\npartition bounds for the join relation, to allow partitionwise join for\ncomputing the join.  The algorithm is implemented in partition_bounds_merge().\nFor an N-way join relation considered partitioned this way, not every pair of\njoining relations can use partitionwise join.  For example:\n\n\t(A leftjoin B on (Pab)) innerjoin C on (Pac)\n\nwhere A, B, and C are partitioned tables, and A has an extra partition\ncompared to B and C.  When considering partitionwise join for the join {A B},\nthe extra partition of A doesn't have a matching partition on the nullable\nside, which is the case that the current implementation of partitionwise join\ncan't handle.  So {A B} is not considered partitioned, and the pair of {A B}\nand C considered for the 3-way join can't use partitionwise join.  On the\nother hand, the pair of {A C} and B can use partitionwise join because {A C}\nis considered partitioned by eliminating the extra partition (see identity 1\non outer join reordering).  Whether an N-way join can use partitionwise join\nis determined based on the first pair of joining relations that are both\npartitioned and can use partitionwise join.\n\nThe partitioning properties of a partitioned relation are stored in its\nRelOptInfo.  The information about data types of partition keys are stored in\nPartitionSchemeData structure. The planner maintains a list of canonical\npartition schemes (distinct PartitionSchemeData objects) so that RelOptInfo of\nany two partitioned relations with same partitioning scheme point to the same\nPartitionSchemeData object.  This reduces memory consumed by\nPartitionSchemeData objects and makes it easy to compare the partition schemes\nof joining relations.\n\nPartitionwise aggregates/grouping\n---------------------------------\n\nIf the GROUP BY clause contains all of the partition keys, all the rows\nthat belong to a given group must come from a single partition; therefore,\naggregation can be done completely separately for each partition. Otherwise,\npartial aggregates can be computed for each partition, and then finalized\nafter appending the results from the individual partitions.  This technique of\nbreaking down aggregation or grouping over a partitioned relation into\naggregation or grouping over its partitions is called partitionwise\naggregation.  Especially when the partition keys match the GROUP BY clause,\nthis can be significantly faster than the regular method.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\optimizer\\README",
      "directory": "backend\\optimizer"
    }
  },
  {
    "title": "README: backend\\optimizer\\plan",
    "url": "backend\\optimizer\\plan\\README",
    "content": "src/backend/optimizer/plan/README\n\nSubselects\n==========\n\nVadim B. Mikheev\n\n\nFrom owner-pgsql-hackers@hub.org Fri Feb 13 09:01:19 1998\nReceived: from renoir.op.net (root@renoir.op.net [209.152.193.4])\n\tby candle.pha.pa.us (8.8.5/8.8.5) with ESMTP id JAA11576\n\tfor <maillist@candle.pha.pa.us>; Fri, 13 Feb 1998 09:01:17 -0500 (EST)\nReceived: from hub.org (hub.org [209.47.148.200]) by renoir.op.net (o1/$Revision: 1.14 $) with ESMTP id IAA09761 for <maillist@candle.pha.pa.us>; Fri, 13 Feb 1998 08:41:22 -0500 (EST)\nReceived: from localhost (majordom@localhost) by hub.org (8.8.8/8.7.5) with SMTP id IAA08135; Fri, 13 Feb 1998 08:40:17 -0500 (EST)\nReceived: by hub.org (TLB v0.10a (1.23 tibbs 1997/01/09 00:29:32)); Fri, 13 Feb 1998 08:38:42 -0500 (EST)\nReceived: (from majordom@localhost) by hub.org (8.8.8/8.7.5) id IAA06646 for pgsql-hackers-outgoing; Fri, 13 Feb 1998 08:38:35 -0500 (EST)\nReceived: from dune.krasnet.ru (dune.krasnet.ru [193.125.44.86]) by hub.org (8.8.8/8.7.5) with ESMTP id IAA04568 for <hackers@postgreSQL.org>; Fri, 13 Feb 1998 08:37:16 -0500 (EST)\nReceived: from sable.krasnoyarsk.su (dune.krasnet.ru [193.125.44.86])\n\tby dune.krasnet.ru (8.8.7/8.8.7) with ESMTP id UAA13717\n\tfor <hackers@postgreSQL.org>; Fri, 13 Feb 1998 20:51:03 +0700 (KRS)\n\t(envelope-from vadim@sable.krasnoyarsk.su)\nMessage-ID: <34E44FBA.D64E7997@sable.krasnoyarsk.su>\nDate: Fri, 13 Feb 1998 20:50:50 +0700\nFrom: \"Vadim B. Mikheev\" <vadim@sable.krasnoyarsk.su>\nOrganization: ITTS (Krasnoyarsk)\nX-Mailer: Mozilla 4.04 [en] (X11; I; FreeBSD 2.2.5-RELEASE i386)\nMIME-Version: 1.0\nTo: PostgreSQL Developers List <hackers@postgreSQL.org>\nSubject: [HACKERS] Subselects are in CVS...\nContent-Type: text/plain; charset=us-ascii\nContent-Transfer-Encoding: 7bit\nSender: owner-pgsql-hackers@hub.org\nPrecedence: bulk\nStatus: OR\n\nThis is some implementation notes and opened issues...\n\nFirst, implementation uses new type of parameters - PARAM_EXEC - to deal\nwith correlation Vars. When query_planner() is called, it first tries to\nreplace all upper queries Var referenced in current query with Param of\nthis type. Some global variables are used to keep mapping of Vars to\nParams and Params to Vars.\n\nAfter this, all current query' SubLinks are processed: for each SubLink\nfound in query' qual union_planner() (old planner() function) will be\ncalled to plan corresponding subselect (union_planner() calls\nquery_planner() for \"simple\" query and supports UNIONs). After subselect\nare planned, optimizer knows about is this correlated, un-correlated or\n_undirect_ correlated (references some grand-parent Vars but no parent\nones: uncorrelated from the parent' point of view) query.\n\nFor uncorrelated and undirect correlated subqueries of EXPRession or\nEXISTS type SubLinks will be replaced with \"normal\" clauses from\nSubLink->Oper list (I changed this list to be list of EXPR nodes,\nnot just Oper ones). Right sides of these nodes are replaced with\nPARAM_EXEC parameters. This is second use of new parameter type.\nAt run-time these parameters get value from result of subquery\nevaluation (i.e. - from target list of subquery). Execution plan of\nsubquery itself becomes init plan of parent query. InitPlan knows\nwhat parameters are to get values from subquery' results and will be\nexecuted \"on-demand\" (for query select * from table where x > 0 and\ny > (select max(a) from table_a) subquery will not be executed at all\nif there are no tuples with x > 0 _and_ y is not used in index scan).\n\nSubLinks for subqueries of all other types are transformed into\nnew type of Expr node - SUBPLAN_EXPR. Expr->args are just correlation\nvariables from _parent_ query. Expr->oper is new SubPlan node.\n\nThis node is used for InitPlan too. It keeps subquery range table,\nindices of Params which are to get value from _parent_ query Vars\n(i.e. - from Expr->args), indices of Params into which subquery'\nresults are to be substituted (this is for InitPlans), SubLink\nand subquery' execution plan.\n\nPlan node was changed to know about dependencies on Params from\nparent queries and InitPlans, to keep list of changed Params\n(from the above) and so be re-scanned if this list is not NULL.\nAlso, added list of InitPlans (actually, all of them for current\nquery are in topmost plan node now) and other SubPlans (from\nplan->qual) - to initialize them and let them know about changed\nParams (from the list of their \"interests\").\n\nAfter all SubLinks are processed, query_planner() calls qual'\ncanonificator and does \"normal\" work. By using Params optimizer\nis mostly unchanged.\n\nWell, Executor. To get subplans re-evaluated without ExecutorStart()\nand ExecutorEnd() (without opening and closing relations and indices\nand without many palloc() and pfree() - this is what SQL-funcs does\non each call) ExecReScan() now supports most of Plan types...\n\nExplanation of EXPLAIN.\n\nvac=> explain select * from tmp where x >= (select max(x2) from test2\nwhere y2 = y and exists (select * from tempx where tx = x));\nNOTICE:  QUERY PLAN:\n\nSeq Scan on tmp  (cost=40.03 size=101 width=8)\n  SubPlan\n  ^^^^^^^ subquery is in Seq Scan' qual, its plan is below\n    ->  Aggregate  (cost=2.05 size=0 width=0)\n          InitPlan\n          ^^^^^^^^ EXISTS subsubquery is InitPlan of subquery\n            ->  Seq Scan on tempx  (cost=4.33 size=1 width=4)\n          ->  Result  (cost=2.05 size=0 width=0)\n              ^^^^^^ EXISTS subsubquery was transformed into Param\n                     and so we have Result node here\n                ->  Index Scan on test2  (cost=2.05 size=1 width=4)\n\n\nOpened issues.\n\n1. No read permissions checking (easy, just not done yet).\n2. readfuncs.c can't read subplan-s (easy, not critical, because of\n   we currently nowhere use ascii representation of execution plans).\n3. ExecReScan() doesn't support all plan types. At least support for\n   MergeJoin has to be implemented.\n4. Memory leaks in ExecReScan().\n5. I need in advice: if subquery introduced with NOT IN doesn't return\n   any tuples then qualification is failed, yes ?\n6. Regression tests !!!!!!!!!!!!!!!!!!!!\n   (Could we use data/queries from MySQL' crash.me ?\n    Copyright-ed ? Could they give us rights ?)\n7. Performance.\n   - Should be good when subquery is transformed into InitPlan.\n   - Something should be done for uncorrelated subqueries introduced\n     with ANY/ALL - keep thinking. Currently, subplan will be re-scanned\n     for each parent tuple - very slow...\n\nResults of some test. TMP is table with x,y (int4-s), x in 0-9,\ny = 100 - x, 1000 tuples (10 duplicates of each tuple). TEST2 is table\nwith x2, y2 (int4-s), x2 in 1-99, y2 = 100 -x2, 10000 tuples (100 dups).\n\n   Trying\n\nselect * from tmp where x >= (select max(x2) from test2 where y2 = y);\n\n   and\n\nbegin;\nselect y as ty, max(x2) as mx into table tsub from test2, tmp\nwhere y2 = y group by ty;\nvacuum tsub;\nselect x, y from tmp, tsub where x >= mx and y = ty;\ndrop table tsub;\nend;\n\n   Without index on test2(y2):\n\nSubSelect         -> 320 sec\nUsing temp table  -> 32 sec\n\n   Having index\n\nSubSelect         -> 17 sec (2M of memory)\nUsing temp table  -> 32 sec (12M of memory: -S 8192)\n\nVadim",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\optimizer\\plan\\README",
      "directory": "backend\\optimizer\\plan"
    }
  },
  {
    "title": "README: backend\\parser",
    "url": "backend\\parser\\README",
    "content": "src/backend/parser/README\n\nParser\n======\n\nThis directory does more than tokenize and parse SQL queries.  It also\ncreates Query structures for the various complex queries that are passed\nto the optimizer and then executor.\n\nparser.c\tthings start here\nscan.l\t\tbreak query into tokens\nscansup.c\thandle escapes in input strings\ngram.y\t\tparse the tokens and produce a \"raw\" parse tree\nanalyze.c\ttop level of parse analysis for optimizable queries\nparse_agg.c\thandle aggregates, like SUM(col1),  AVG(col2), ...\nparse_clause.c\thandle clauses like WHERE, ORDER BY, GROUP BY, ...\nparse_coerce.c\thandle coercing expressions to different data types\nparse_collate.c\tassign collation information in completed expressions\nparse_cte.c\thandle Common Table Expressions (WITH clauses)\nparse_expr.c\thandle expressions like col, col + 3, x = 3 or x = 4\nparse_enr.c\thandle ephemeral named rels (trigger transition tables, ...)\nparse_func.c\thandle functions, table.column and column identifiers\nparse_merge.c\thandle MERGE\nparse_node.c\tcreate nodes for various structures\nparse_oper.c\thandle operators in expressions\nparse_param.c\thandle Params (for the cases used in the core backend)\nparse_relation.c support routines for tables and column handling\nparse_target.c\thandle the result list of the query\nparse_type.c\tsupport routines for data type handling\nparse_utilcmd.c\tparse analysis for utility commands (done at execution time)\n\nSee also src/common/keywords.c, which contains the table of standard\nkeywords and the keyword lookup function.  We separated that out because\nvarious frontend code wants to use it too.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\parser\\README",
      "directory": "backend\\parser"
    }
  },
  {
    "title": "README: backend\\regex",
    "url": "backend\\regex\\README",
    "content": "Implementation notes about Henry Spencer's regex library\n========================================================\n\nIf Henry ever had any internals documentation, he didn't publish it.\nSo this file is an attempt to reverse-engineer some docs.\n\nGeneral source-file layout\n--------------------------\n\nThere are six separately-compilable source files, five of which expose\nexactly one exported function apiece:\n\tregcomp.c: pg_regcomp\n\tregexec.c: pg_regexec\n\tregerror.c: pg_regerror\n\tregfree.c: pg_regfree\n\tregprefix.c: pg_regprefix\n(The pg_ prefixes were added by the Postgres project to distinguish this\nlibrary version from any similar one that might be present on a particular\nsystem.  They'd need to be removed or replaced in any standalone version\nof the library.)\n\nThe sixth file, regexport.c, exposes multiple functions that allow extraction\nof info about a compiled regex (see regexport.h).\n\nThere are additional source files regc_*.c that are #include'd in regcomp,\nand similarly additional source files rege_*.c that are #include'd in\nregexec.  This was done to avoid exposing internal symbols globally;\nall functions not meant to be part of the library API are static.\n\n(Actually the above is a lie in one respect: there are two more global\nsymbols, pg_set_regex_collation and pg_reg_getcolor in regcomp.  These are\nnot meant to be part of the API, but they have to be global because both\nregcomp and regexec call them.  It'd be better to get rid of\npg_set_regex_collation, as well as the static variables it sets, in favor of\nkeeping the needed locale state in the regex structs.  We have not done this\nyet for lack of a design for how to add application-specific state to the\nstructs.)\n\nWhat's where in src/backend/regex/:\n\nregcomp.c\t\tTop-level regex compilation code\nregc_color.c\t\tColor map management\nregc_cvec.c\t\tCharacter vector (cvec) management\nregc_lex.c\t\tLexer\nregc_nfa.c\t\tNFA handling\nregc_locale.c\t\tApplication-specific locale code from Tcl project\nregc_pg_locale.c\tPostgres-added application-specific locale code\nregexec.c\t\tTop-level regex execution code\nrege_dfa.c\t\tDFA creation and execution\nregerror.c\t\tpg_regerror: generate text for a regex error code\nregfree.c\t\tpg_regfree: API to free a no-longer-needed regex_t\nregexport.c\t\tFunctions for extracting info from a regex_t\nregprefix.c\t\tCode for extracting a common prefix from a regex_t\n\nThe locale-specific code is concerned primarily with case-folding and with\nexpanding locale-specific character classes, such as [[:alnum:]].  It\nreally needs refactoring if this is ever to become a standalone library.\n\nThe header files for the library are in src/include/regex/:\n\nregcustom.h\t\tCustomizes library for particular application\nregerrs.h\t\tError message list\nregex.h\t\t\tExported API\nregexport.h\t\tExported API for regexport.c\nregguts.h\t\tInternals declarations\n\n\nDFAs, NFAs, and all that\n------------------------\n\nThis library is a hybrid DFA/NFA regex implementation.  (If you've never\nheard either of those terms, get thee to a first-year comp sci textbook.)\nIt might not be clear at first glance what that really means and how it\nrelates to what you'll see in the code.  Here's what really happens:\n\n* Initial parsing of a regex generates an NFA representation, with number\nof states approximately proportional to the length of the regexp.\n\n* The NFA is then optimized into a \"compact NFA\" representation, which is\nbasically the same idea but without fields that are not going to be needed\nat runtime.  It is simplified too: the compact format only allows \"plain\"\nand \"LACON\" arc types.  The cNFA representation is what is passed from\nregcomp to regexec.\n\n* Unlike traditional NFA-based regex engines, we do not execute directly\nfrom the NFA representation, as that would require backtracking and so be\nvery slow in some cases.  Rather, we execute a DFA, which ideally can\nprocess an input string in linear time (O(M) for M characters of input)\nwithout backtracking.  Each state of the DFA corresponds to a set of\nstates of the NFA, that is all the states that the NFA might have been in\nupon reaching the current point in the input string.  Therefore, an NFA\nwith N states might require as many as 2^N states in the corresponding\nDFA, which could easily require unreasonable amounts of memory.  We deal\nwith this by materializing states of the DFA lazily (only when needed) and\nkeeping them in a limited-size cache.  The possible need to build the same\nstate of the DFA repeatedly makes this approach not truly O(M) time, but\nin the worst case as much as O(M*N).  That's still far better than the\nworst case for a backtracking NFA engine.\n\nIf that were the end of it, we'd just say this is a DFA engine, with the\nuse of NFAs being merely an implementation detail.  However, a DFA engine\ncannot handle some important regex features such as capturing parens and\nback-references.  If the parser finds that a regex uses these features\n(collectively called \"messy cases\" in the code), then we have to use\nNFA-style backtracking search after all.\n\nWhen using the NFA mode, the representation constructed by the parser\nconsists of a tree of sub-expressions (\"subre\"s).  Leaf tree nodes are\neither plain regular expressions (which are executed as DFAs in the manner\ndescribed above) or back-references (which try to match the input to some\nprevious substring).  Non-leaf nodes are capture nodes (which save the\nlocation of the substring currently matching their child node),\nconcatenation, alternation, or iteration nodes.  At execution time, the\nexecutor recursively scans the tree.  At concatenation, alternation, or\niteration nodes, it considers each possible alternative way of matching the\ninput string, that is each place where the string could be split for a\nconcatenation or iteration, or each child node for an alternation.  It\ntries the next alternative if the match fails according to the child nodes.\nThis is exactly the sort of backtracking search done by a traditional NFA\nregex engine.  If there are many tree levels it can get very slow.\n\nBut all is not lost: we can still be smarter than the average pure NFA\nengine.  To do this, each subre node has an associated DFA, which\nrepresents what the node could possibly match insofar as a mathematically\npure regex can describe that, which basically means \"no backrefs\".\nBefore we perform any search of possible alternative sub-matches, we run\nthe DFA to see if it thinks the proposed substring could possibly match.\nIf not, we can reject the match immediately without iterating through many\npossibilities.\n\nAs an example, consider the regex \"(a[bc]+)\\1\".  The compiled\nrepresentation will have a top-level concatenation subre node.  Its first\nchild is a plain DFA node for \"a[bc]+\" (which is marked as being a capture\nnode).  The concatenation's second child is a backref node for \\1.\nThe DFA associated with the concatenation node will be \"a[bc]+a[bc]+\",\nwhere the backref has been replaced by a copy of the DFA for its referent\nexpression.  When executed, the concatenation node will have to search for\na possible division of the input string that allows its two child nodes to\neach match their part of the string (and although this specific case can\nonly succeed when the division is at the middle, the code does not know\nthat, nor would it be true in general).  However, we can first run the DFA\nand quickly reject any input that doesn't start with an \"a\" and contain\none more \"a\" plus some number of b's and c's.  If the DFA doesn't match,\nthere is no need to recurse to the two child nodes for each possible\nstring division point.  In many cases, this prefiltering makes the search\nrun much faster than a pure NFA engine could do.  It is this behavior that\njustifies using the phrase \"hybrid DFA/NFA engine\" to describe Spencer's\nlibrary.\n\nIt's perhaps worth noting that separate capture subre nodes are a rarity:\nnormally, we just mark a subre as capturing and that's it.  However, it's\nlegal to write a regex like \"((x))\" in which the same substring has to be\ncaptured by multiple sets of parentheses.  Since a subre has room for only\none \"capno\" field, a single subre can't handle that.  We handle such cases\nby wrapping the base subre (which captures the innermost parens) in a\nno-op capture node, or even more than one for \"(((x)))\" etc.  This is a\nlittle bit inefficient because we end up with multiple identical NFAs,\nbut since the case is pointless and infrequent, it's not worth working\nharder.\n\n\nColors and colormapping\n-----------------------\n\nIn many common regex patterns, there are large numbers of characters that\ncan be treated alike by the execution engine.  A simple example is the\npattern \"[[:alpha:]][[:alnum:]]*\" for an identifier.  Basically the engine\nonly needs to care whether an input symbol is a letter, a digit, or other.\nWe could build the NFA or DFA with a separate arc for each possible letter\nand digit, but that's very wasteful of space and not so cheap to execute\neither, especially when dealing with Unicode which can have thousands of\nletters.  Instead, the parser builds a \"color map\" that maps each possible\ninput symbol to a \"color\", or equivalence class.  The NFA or DFA\nrepresentation then has arcs labeled with colors, not specific input\nsymbols.  At execution, the first thing the executor does with each input\nsymbol is to look up its color in the color map, and then everything else\nworks from the color only.\n\nTo build the colormap, we start by assigning every possible input symbol\nthe color WHITE, which means \"other\" (that is, at the end of parsing, the\nsymbols that are still WHITE are those not explicitly referenced anywhere\nin the regex).  When we see a simple literal character or a bracket\nexpression in the regex, we want to assign that character, or all the\ncharacters represented by the bracket expression, a unique new color that\ncan be used to label the NFA arc corresponding to the state transition for\nmatching this character or bracket expression.  The basic idea is:\nfirst, change the color assigned to a character to some new value;\nsecond, run through all the existing arcs in the partially-built NFA,\nand for each one referencing the character's old color, add a parallel\narc referencing its new color (this keeps the reassignment from changing\nthe semantics of what we already built); and third, add a new arc with\nthe character's new color to the current pair of NFA states, denoting\nthat seeing this character allows the state transition to be made.\n\nThis is complicated a bit by not wanting to create more colors\n(equivalence classes) than absolutely necessary.  In particular, if a\nbracket expression mentions two characters that had the same color before,\nthey should still share the same color after we process the bracket, since\nthere is still not a need to distinguish them.  But we do need to\ndistinguish them from other characters that previously had the same color\nyet are not listed in the bracket expression.  To mechanize this, the code\nhas a concept of \"parent colors\" and \"subcolors\", where a color's subcolor\nis the new color that we are giving to any characters of that color while\nparsing the current atom.  (The word \"parent\" is a bit unfortunate here,\nbecause it suggests a long-lived relationship, but a subcolor link really\nonly lasts for the duration of parsing a single atom.)  In other words,\na subcolor link means that we are in process of splitting the parent color\ninto two colors (equivalence classes), depending on whether or not each\nmember character should be included by the current regex atom.\n\nAs an example, suppose we have the regex \"a\\d\\wx\".  Initially all possible\ncharacter codes are labeled WHITE (color 0).  To parse the atom \"a\", we\ncreate a new color (1), update \"a\"'s color map entry to 1, and create an\narc labeled 1 between the first two states of the NFA.  Now we see \\d,\nwhich is really a bracket expression containing the digits \"0\"-\"9\".\nFirst we process \"0\", which is currently WHITE, so we create a new color\n(2), update \"0\"'s color map entry to 2, and create an arc labeled 2\nbetween the second and third states of the NFA.  We also mark color WHITE\nas having the subcolor 2, which means that future relabelings of WHITE\ncharacters should also select 2 as the new color.  Thus, when we process\n\"1\", we won't create a new color but re-use 2.  We update \"1\"'s color map\nentry to 2, and then find that we don't need a new arc because there is\nalready one labeled 2 between the second and third states of the NFA.\nSimilarly for the other 8 digits, so there will be only one arc labeled 2\nbetween NFA states 2 and 3 for all members of this bracket expression.\nAt completion of processing of the bracket expression, we call okcolors()\nwhich breaks all the existing parent/subcolor links; there is no longer a\nmarker saying that WHITE characters should be relabeled 2.  (Note:\nactually, we did the same creation and clearing of a subcolor link for the\nprimitive atom \"a\", but it didn't do anything very interesting.)  Now we\ncome to the \"\\w\" bracket expression, which for simplicity assume expands\nto just \"[a-z0-9]\".  We process \"a\", but observe that it is already the\nsole member of its color 1.  This means there is no need to subdivide that\nequivalence class more finely, so we do not create any new color.  We just\nmake an arc labeled 1 between the third and fourth NFA states.  Next we\nprocess \"b\", which is WHITE and far from the only WHITE character, so we\ncreate a new color (3), link that as WHITE's subcolor, relabel \"b\" as\ncolor 3, and make an arc labeled 3.  As we process \"c\" through \"z\", each\nis relabeled from WHITE to 3, but no new arc is needed.  Now we come to\n\"0\", which is not the only member of its color 2, so we suppose that a new\ncolor is needed and create color 4.  We link 4 as subcolor of 2, relabel\n\"0\" as color 4 in the map, and add an arc for color 4.  Next \"1\" through\n\"9\" are similarly relabeled as color 4, with no additional arcs needed.\nHaving finished the bracket expression, we call okcolors(), which breaks\nthe subcolor links.  okcolors() further observes that we have removed\nevery member of color 2 (the previous color of the digit characters).\nTherefore, it runs through the partial NFA built so far and relabels arcs\nlabeled 2 to color 4; in particular the arc from NFA state 2 to state 3 is\nrelabeled color 4.  Then it frees up color 2, since we have no more use\nfor that color.  We now have an NFA in which transitions for digits are\nconsistently labeled with color 4.  Last, we come to the atom \"x\".\n\"x\" is currently labeled with color 3, and it's not the only member of\nthat color, so we realize that we now need to distinguish \"x\" from other\nletters when we did not before.  We create a new color, which might have\nbeen 5 but instead we recycle the unused color 2.  \"x\" is relabeled 2 in\nthe color map and 2 is linked as the subcolor of 3, and we add an arc for\n2 between states 4 and 5 of the NFA.  Now we call okcolors(), which breaks\nthe subcolor link between colors 3 and 2 and notices that both colors are\nnonempty.  Therefore, it also runs through the existing NFA arcs and adds\nan additional arc labeled 2 wherever there is an arc labeled 3; this\naction ensures that characters of color 2 (i.e., \"x\") will still be\nconsidered as allowing any transitions they did before.  We are now done\nparsing the regex, and we have these final color assignments:\n\tcolor 1: \"a\"\n\tcolor 2: \"x\"\n\tcolor 3: other letters\n\tcolor 4: digits\nand the NFA has these arcs:\n\tstates 1 -> 2 on color 1 (hence, \"a\" only)\n\tstates 2 -> 3 on color 4 (digits)\n\tstates 3 -> 4 on colors 1, 3, 4, and 2 (covering all \\w characters)\n\tstates 4 -> 5 on color 2 (\"x\" only)\nwhich can be seen to be a correct representation of the regex.\n\nThere is one more complexity, which is how to handle \".\", that is a\nmatch-anything atom.  We used to do that by generating a \"rainbow\"\nof arcs of all live colors between the two NFA states before and after\nthe dot.  That's expensive in itself when there are lots of colors,\nand it also typically adds lots of follow-on arc-splitting work for the\ncolor splitting logic.  Now we handle this case by generating a single arc\nlabeled with the special color RAINBOW, meaning all colors.  Such arcs\nnever need to be split, so they help keep NFAs small in this common case.\n(Note: this optimization doesn't help in REG_NLSTOP mode, where \".\" is\nnot supposed to match newline.  In that case we still handle \".\" by\ngenerating an almost-rainbow of all colors except newline's color.)\n\nGiven this summary, we can see we need the following operations for\ncolors:\n\n* A fast way to look up the current color assignment for any character\n  code.  (This is needed during both parsing and execution, while the\n  remaining operations are needed only during parsing.)\n* A way to alter the color assignment for any given character code.\n* We must track the number of characters currently assigned to each\n  color, so that we can detect empty and singleton colors.\n* We must track all existing NFA arcs of a given color, so that we\n  can relabel them at need, or add parallel arcs of a new color when\n  an existing color has to be subdivided.\n\nThe last two of these are handled with the \"struct colordesc\" array and\nthe \"colorchain\" links in NFA arc structs.\n\nIdeally, we'd do the first two operations using a simple linear array\nstoring the current color assignment for each character code.\nUnfortunately, that's not terribly workable for large charsets such as\nUnicode.  Our solution is to divide the color map into two parts.  A simple\nlinear array is used for character codes up to MAX_SIMPLE_CHR, which can be\nchosen large enough to include all popular characters (so that the\nsignificantly-slower code paths about to be described are seldom invoked).\nCharacters above that need be considered at compile time only if they\nappear explicitly in the regex pattern.  We store each such mentioned\ncharacter or character range as an entry in the \"colormaprange\" array in\nthe colormap.  (Overlapping ranges are split into unique subranges, so that\neach range in the finished list needs only a single color that describes\nall its characters.)  When mapping a character above MAX_SIMPLE_CHR to a\ncolor at runtime, we search this list of ranges explicitly.\n\nThat's still not quite enough, though, because of locale-dependent\ncharacter classes such as [[:alpha:]].  In Unicode locales these classes\nmay have thousands of entries that are above MAX_SIMPLE_CHR, and we\ncertainly don't want to be searching large colormaprange arrays at runtime.\nNor do we even want to spend the time to initialize cvec structures that\nexhaustively describe all of those characters.  Our solution is to compute\nexact per-character colors at regex compile time only up to MAX_SIMPLE_CHR.\nFor characters above that, we apply the <ctype.h> or <wctype.h> lookup\nfunctions at runtime for each locale-dependent character class used in the\nregex pattern, constructing a bitmap that describes which classes the\nruntime character belongs to.  The per-character-range data structure\nmentioned above actually holds, for each range, a separate color entry\nfor each possible combination of character class properties.  That is,\nthe color map for characters above MAX_SIMPLE_CHR is really a 2-D array,\nwhose rows correspond to high characters or character ranges that are\nexplicitly mentioned in the regex pattern, and whose columns correspond\nto sets of the locale-dependent character classes that are used in the\nregex.\n\nAs an example, given the pattern '\\w\\u1234[\\U0001D100-\\U0001D1FF]'\n(and supposing that MAX_SIMPLE_CHR is less than 0x1234), we will need\na high color map with three rows.  One row is for the single character\nU+1234 (represented as a single-element range), one is for the range\nU+1D100..U+1D1FF, and the other row represents all remaining high\ncharacters.  The color map has two columns, one for characters that\nsatisfy iswalnum() and one for those that don't.\n\nWe build this color map in parallel with scanning the regex.  Each time\nwe detect a new explicit high character (or range) or a locale-dependent\ncharacter class, we split existing entry(s) in the high color map so that\ncharacters we need to be able to distinguish will have distinct entries\nthat can be given separate colors.  Often, though, single entries in the\nhigh color map will represent very large sets of characters.\n\nIf there are both explicit high characters/ranges and locale-dependent\ncharacter classes, we may have entries in the high color map array that\nhave non-WHITE colors but don't actually represent any real characters.\n(For example, in a row representing a singleton range, only one of the\ncolumns could possibly be a live entry; it's the one matching the actual\nlocale properties for that single character.)  We don't currently make\nany effort to reclaim such colors.  In principle it could be done, but\nit's not clear that it's worth the trouble.\n\n\nDetailed semantics of an NFA\n----------------------------\n\nWhen trying to read dumped-out NFAs, it's helpful to know these facts:\n\nState 0 (additionally marked with \"@\" in dumpnfa's output) is always the\ngoal state, and state 1 (additionally marked with \">\") is the start state.\n(The code refers to these as the post state and pre state respectively.)\n\nThe possible arc types are:\n\n    PLAIN arcs, which specify matching of any character of a given \"color\"\n    (see above).  These are dumped as \"[color_number]->to_state\".\n    In addition there can be \"rainbow\" PLAIN arcs, which are dumped as\n    \"[*]->to_state\".\n\n    EMPTY arcs, which specify a no-op transition to another state.  These\n    are dumped as \"->to_state\".\n\n    AHEAD constraints, which represent a \"next character must be of this\n    color\" constraint.  AHEAD differs from a PLAIN arc in that the input\n    character is not consumed when crossing the arc.  These are dumped as\n    \">color_number>->to_state\", or possibly \">*>->to_state\".\n\n    BEHIND constraints, which represent a \"previous character must be of\n    this color\" constraint, which likewise consumes no input.  These are\n    dumped as \"<color_number<->to_state\", or possibly \"<*<->to_state\".\n\n    '^' arcs, which specify a beginning-of-input constraint.  These are\n    dumped as \"^0->to_state\" or \"^1->to_state\" for beginning-of-string and\n    beginning-of-line constraints respectively.\n\n    '$' arcs, which specify an end-of-input constraint.  These are dumped\n    as \"$0->to_state\" or \"$1->to_state\" for end-of-string and end-of-line\n    constraints respectively.\n\n    LACON constraints, which represent \"(?=re)\", \"(?!re)\", \"(?<=re)\", and\n    \"(?<!re)\" constraints, i.e. the input starting/ending at this point must\n    match (or not match) a given sub-RE, but the matching input is not\n    consumed.  These are dumped as \":subtree_number:->to_state\".\n\nIf you see anything else (especially any question marks) in the display of\nan arc, it's dumpnfa() trying to tell you that there's something fishy\nabout the arc; see the source code.\n\nThe regex executor can only handle PLAIN and LACON transitions.  The regex\noptimize() function is responsible for transforming the parser's output\nto get rid of all the other arc types.  In particular, ^ and $ arcs that\nare not dropped as impossible will always end up adjacent to the pre or\npost state respectively, and then will be converted into PLAIN arcs that\nmention the special \"colors\" for BOS, BOL, EOS, or EOL.\n\nTo decide whether a thus-transformed NFA matches a given substring of the\ninput string, the executor essentially follows these rules:\n1. Start the NFA \"looking at\" the character *before* the given substring,\nor if the substring is at the start of the input, prepend an imaginary BOS\ncharacter instead.\n2. Run the NFA until it has consumed the character *after* the given\nsubstring, or an imaginary following EOS character if the substring is at\nthe end of the input.\n3. If the NFA is (or can be) in the goal state at this point, it matches.\n\nThis definition is necessary to support regexes that begin or end with\nconstraints such as \\m and \\M, which imply requirements on the adjacent\ncharacter if any.  The executor implements that by checking if the\nadjacent character (or BOS/BOL/EOS/EOL pseudo-character) is of the\nright color, and it does that in the same loop that checks characters\nwithin the match.\n\nSo one can mentally execute an untransformed NFA by taking ^ and $ as\nordinary constraints that match at start and end of input; but plain\narcs out of the start state should be taken as matches for the character\nbefore the target substring, and similarly, plain arcs leading to the\npost state are matches for the character after the target substring.\nAfter the optimize() transformation, there are explicit arcs mentioning\nBOS/BOL/EOS/EOL adjacent to the pre-state and post-state.  So a finished\nNFA for a pattern without anchors or adjacent-character constraints will\nhave pre-state outarcs for RAINBOW (all possible character colors) as well\nas BOS and BOL, and likewise post-state inarcs for RAINBOW, EOS, and EOL.\nAlso note that LACON arcs will never connect to the pre-state\nor post-state.\n\n\nLook-around constraints (LACONs)\n--------------------------------\n\nThe regex compiler doesn't have much intelligence about LACONs; it just\nconstructs a sub-NFA representing the pattern that the constraint says to\nmatch or not match, and puts a LACON arc referencing that sub-NFA into the\nmain NFA.  At runtime, the executor applies the sub-NFA at each point in\nthe string where the constraint is relevant, and then traverses or doesn't\ntraverse the arc.  (\"Traversal\" means including the arc's to-state in the\nset of NFA states that are considered active at the next character.)\n\nThe actual basic matching cycle of the executor is\n1.  Identify the color of the next input character, then advance over it.\n2.  Apply the DFA to follow all the matching \"plain\" arcs of the NFA.\n    (Notionally, the previous DFA state represents the set of states the\n    NFA could have been in before the character, and the new DFA state\n    represents the set of states the NFA could be in after the character.)\n3.  If there are any LACON arcs leading out of any of the new NFA states,\n    apply each LACON constraint starting from the new next input character\n    (while not actually consuming any input).  For each successful LACON,\n    add its to-state to the current set of NFA states.  If any such\n    to-state has outgoing LACON arcs, process those in the same way.\n    (Mathematically speaking, we compute the transitive closure of the\n    set of states reachable by successful LACONs.)\n\nThus, LACONs are always checked immediately after consuming a character\nvia a plain arc.  This is okay because the NFA's \"pre\" state only has\nplain out-arcs, so we can always consume a character (possibly a BOS\npseudo-character as described above) before we need to worry about LACONs.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\regex\\README",
      "directory": "backend\\regex"
    }
  },
  {
    "title": "README: backend\\replication",
    "url": "backend\\replication\\README",
    "content": "src/backend/replication/README\n\nWalreceiver - libpqwalreceiver API\n----------------------------------\n\nThe transport-specific part of walreceiver, responsible for connecting to\nthe primary server, receiving WAL files and sending messages, is loaded\ndynamically to avoid having to link the main server binary with libpq.\nThe dynamically loaded module is in libpqwalreceiver subdirectory.\n\nThe dynamically loaded module implements a set of functions with details\nabout each one of them provided in src/include/replication/walreceiver.h.\n\nThis API should be considered internal at the moment, but we could open it\nup for 3rd party replacements of libpqwalreceiver in the future, allowing\npluggable methods for receiving WAL.\n\nWalreceiver IPC\n---------------\n\nWhen the WAL replay in startup process has reached the end of archived WAL,\nrestorable using restore_command, it starts up the walreceiver process\nto fetch more WAL (if streaming replication is configured).\n\nWalreceiver is a postmaster subprocess, so the startup process can't fork it\ndirectly. Instead, it sends a signal to postmaster, asking postmaster to launch\nit. Before that, however, startup process fills in WalRcvData->conninfo\nand WalRcvData->slotname, and initializes the starting point in\nWalRcvData->receiveStart.\n\nAs walreceiver receives WAL from the primary server, and writes and flushes\nit to disk (in pg_wal), it updates WalRcvData->flushedUpto and signals\nthe startup process to know how far WAL replay can advance.\n\nWalreceiver sends information about replication progress to the primary server\nwhenever it either writes or flushes new WAL, or the specified interval elapses.\nThis is used for reporting purpose.\n\nWalsender IPC\n-------------\n\nAt shutdown, postmaster handles walsender processes differently from regular\nbackends. It waits for regular backends to die before writing the\nshutdown checkpoint and terminating pgarch and other auxiliary processes, but\nthat's not desirable for walsenders, because we want the standby servers to\nreceive all the WAL, including the shutdown checkpoint, before the primary\nis shut down. Therefore postmaster treats walsenders like the pgarch process,\nand instructs them to terminate at PM_SHUTDOWN_2 phase, after all regular\nbackends have died and checkpointer has issued the shutdown checkpoint.\n\nWhen postmaster accepts a connection, it immediately forks a new process\nto handle the handshake and authentication, and the process initializes to\nbecome a backend. Postmaster doesn't know if the process becomes a regular\nbackend or a walsender process at that time - that's indicated in the\nconnection handshake - so we need some extra signaling to let postmaster\nidentify walsender processes.\n\nWhen walsender process starts up, it marks itself as a walsender process in\nthe PMSignal array. That way postmaster can tell it apart from regular\nbackends.\n\nNote that no big harm is done if postmaster thinks that a walsender is a\nregular backend; it will just terminate the walsender earlier in the shutdown\nphase. A walsender will look like a regular backend until it's done with the\ninitialization and has marked itself in PMSignal array, and at process\ntermination, after unmarking the PMSignal slot.\n\nEach walsender allocates an entry from the WalSndCtl array, and tracks\ninformation about replication progress. User can monitor them via\nstatistics views.\n\n\nWalsender - walreceiver protocol\n--------------------------------\n\nSee manual.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\replication\\README",
      "directory": "backend\\replication"
    }
  },
  {
    "title": "README: backend\\snowball",
    "url": "backend\\snowball\\README",
    "content": "src/backend/snowball/README\n\nSnowball-Based Stemming\n=======================\n\nThis module uses the word stemming code developed by the Snowball project,\nhttp://snowballstem.org (formerly http://snowball.tartarus.org)\nwhich is released by them under a BSD-style license.\n\nThe Snowball project does not often make formal releases; it's best\nto pull from their git repository\n\ngit clone https://github.com/snowballstem/snowball.git\n\nand then building the derived files is as simple as\n\ncd snowball\nmake\n\nAt least on Linux, no platform-specific adjustment is needed.\n\nPostgres' files under src/backend/snowball/libstemmer/ and\nsrc/include/snowball/libstemmer/ are taken directly from the Snowball\nfiles, with only some minor adjustments of file inclusions.  Note\nthat most of these files are in fact derived files, not original source.\nThe original sources are in the Snowball language, and are built using\nthe Snowball-to-C compiler that is also part of the Snowball project.\nWe choose to include the derived files in the PostgreSQL distribution\nbecause most installations will not have the Snowball compiler available.\n\nWe are currently synced with the Snowball git commit\n48a67a2831005f49c48ec29a5837640e23e54e6b (tag v2.2.0)\nof 2021-11-10.\n\nTo update the PostgreSQL sources from a new Snowball version:\n\n0. If you didn't do it already, \"make -C snowball\".\n\n1. Copy the *.c files in snowball/src_c/ to src/backend/snowball/libstemmer\nwith replacement of \"../runtime/header.h\" by \"header.h\", for example\n\nfor f in .../snowball/src_c/*.c\ndo\n    sed 's|\\.\\./runtime/header\\.h|header.h|' $f >libstemmer/`basename $f`\ndone\n\nDo not copy stemmers that are listed in libstemmer/modules.txt as\nnonstandard, such as \"german2\" or \"lovins\".\n\n2. Copy the *.c files in snowball/runtime/ to\nsrc/backend/snowball/libstemmer, and edit them to remove direct inclusions\nof system headers such as <stdio.h> --- they should only include \"header.h\".\n(This removal avoids portability problems on some platforms where <stdio.h>\nis sensitive to largefile compilation options.)\n\n3. Copy the *.h files in snowball/src_c/ and snowball/runtime/\nto src/include/snowball/libstemmer.  At this writing the header files\ndo not require any changes.\n\n4. Check whether any stemmer modules have been added or removed.  If so, edit\nthe OBJS list in Makefile, the list of #include's in dict_snowball.c, and the\nstemmer_modules[] table in dict_snowball.c, as well as the list in the\ndocumentation in textsearch.sgml.  You might also need to change\nthe LANGUAGES list in Makefile and tsearch_config_languages in initdb.c.\n\n5. The various stopword files in stopwords/ must be downloaded\nindividually from pages on the snowballstem.org website.\nBe careful that these files must be stored in UTF-8 encoding.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\snowball\\README",
      "directory": "backend\\snowball"
    }
  },
  {
    "title": "README: backend\\statistics",
    "url": "backend\\statistics\\README",
    "content": "Extended statistics\n===================\n\nWhen estimating various quantities (e.g. condition selectivities) the default\napproach relies on the assumption of independence. In practice that's often\nnot true, resulting in estimation errors.\n\nExtended statistics track different types of dependencies between the columns,\nhopefully improving the estimates and producing better plans.\n\n\nTypes of statistics\n-------------------\n\nThere are currently several kinds of extended statistics:\n\n    (a) ndistinct coefficients\n\n    (b) soft functional dependencies (README.dependencies)\n\n    (c) MCV lists (README.mcv)\n\n\nCompatible clause types\n-----------------------\n\nEach type of statistics may be used to estimate some subset of clause types.\n\n    (a) functional dependencies - equality clauses (AND), possibly IS NULL\n\n    (b) MCV lists - equality and inequality clauses (AND, OR, NOT), IS [NOT] NULL\n\nCurrently, only OpExprs in the form Var op Const, or Const op Var are\nsupported, however it's feasible to expand the code later to also estimate the\nselectivities on clauses such as Var op Var.\n\n\nComplex clauses\n---------------\n\nWe also support estimating more complex clauses - essentially AND/OR clauses\nwith (Var op Const) as leaves, as long as all the referenced attributes are\ncovered by a single statistics object.\n\nFor example this condition\n\n    (a=1) AND ((b=2) OR ((c=3) AND (d=4)))\n\nmay be estimated using statistics on (a,b,c,d). If we only have statistics on\n(b,c,d) we may estimate the second part, and estimate (a=1) using simple stats.\n\nIf we only have statistics on (a,b,c) we can't apply it at all at this point,\nbut it's worth pointing out clauselist_selectivity() works recursively and when\nhandling the second part (the OR-clause), we'll be able to apply the statistics.\n\nNote: The multi-statistics estimation patch also makes it possible to pass some\nclauses as 'conditions' into the deeper parts of the expression tree.\n\n\nSelectivity estimation\n----------------------\n\nThroughout the planner clauselist_selectivity() still remains in charge of\nmost selectivity estimate requests. clauselist_selectivity() can be instructed\nto try to make use of any extended statistics on the given RelOptInfo, which\nit will do if:\n\n    (a) An actual valid RelOptInfo was given. Join relations are passed in as\n        NULL, therefore are invalid.\n\n    (b) The relation given actually has any extended statistics defined which\n        are actually built.\n\nWhen the above conditions are met, clauselist_selectivity() first attempts to\npass the clause list off to the extended statistics selectivity estimation\nfunction. This function may not find any clauses which it can perform any\nestimations on. In such cases, these clauses are simply ignored. When actual\nestimation work is performed in these functions they're expected to mark which\nclauses they've performed estimations for so that any other function\nperforming estimations knows which clauses are to be skipped.\n\nSize of sample in ANALYZE\n-------------------------\n\nWhen performing ANALYZE, the number of rows to sample is determined as\n\n    (300 * statistics_target)\n\nThat works reasonably well for statistics on individual columns, but perhaps\nit's not enough for extended statistics. Papers analyzing estimation errors\nall use samples proportional to the table (usually finding that 1-3% of the\ntable is enough to build accurate stats).\n\nThe requested accuracy (number of MCV items or histogram bins) should also\nbe considered when determining the sample size, and in extended statistics\nthose are not necessarily limited by statistics_target.\n\nThis however merits further discussion, because collecting the sample is quite\nexpensive and increasing it further would make ANALYZE even more painful.\nJudging by the experiments with the current implementation, the fixed size\nseems to work reasonably well for now, so we leave this as future work.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\statistics\\README",
      "directory": "backend\\statistics"
    }
  },
  {
    "title": "README: backend\\statistics",
    "url": "backend\\statistics\\README.dependencies",
    "content": "Soft functional dependencies\n============================\n\nFunctional dependencies are a concept well described in relational theory,\nparticularly in the definition of normalization and \"normal forms\". Wikipedia\nhas a nice definition of a functional dependency [1]:\n\n    In a given table, an attribute Y is said to have a functional dependency\n    on a set of attributes X (written X -> Y) if and only if each X value is\n    associated with precisely one Y value. For example, in an \"Employee\"\n    table that includes the attributes \"Employee ID\" and \"Employee Date of\n    Birth\", the functional dependency\n\n        {Employee ID} -> {Employee Date of Birth}\n\n    would hold. It follows from the previous two sentences that each\n    {Employee ID} is associated with precisely one {Employee Date of Birth}.\n\n    [1] https://en.wikipedia.org/wiki/Functional_dependency\n\nIn practical terms, functional dependencies mean that a value in one column\ndetermines values in some other column. Consider for example this trivial\ntable with two integer columns:\n\n    CREATE TABLE t (a INT, b INT)\n        AS SELECT i, i/10 FROM generate_series(1,100000) s(i);\n\nClearly, knowledge of the value in column 'a' is sufficient to determine the\nvalue in column 'b', as it's simply (a/10). A more practical example may be\naddresses, where the knowledge of a ZIP code (usually) determines city. Larger\ncities may have multiple ZIP codes, so the dependency can't be reversed.\n\nMany datasets might be normalized not to contain such dependencies, but often\nit's not practical for various reasons. In some cases, it's actually a conscious\ndesign choice to model the dataset in a denormalized way, either because of\nperformance or to make querying easier.\n\n\nSoft dependencies\n-----------------\n\nReal-world data sets often contain data errors, either because of data entry\nmistakes (user mistyping the ZIP code) or perhaps issues in generating the\ndata (e.g. a ZIP code mistakenly assigned to two cities in different states).\n\nA strict implementation would either ignore dependencies in such cases,\nrendering the approach mostly useless even for slightly noisy data sets, or\nresult in sudden changes in behavior depending on minor differences between\nsamples provided to ANALYZE.\n\nFor this reason, extended statistics implement \"soft\" functional dependencies,\nassociating each functional dependency with a degree of validity (a number\nbetween 0 and 1). This degree is then used to combine selectivities in a\nsmooth manner.\n\n\nMining dependencies (ANALYZE)\n-----------------------------\n\nThe current algorithm is fairly simple - generate all possible functional\ndependencies, and for each one count the number of rows consistent with it.\nThen use the fraction of rows (supporting/total) as the degree.\n\nTo count the rows consistent with the dependency (a => b):\n\n (a) Sort the data lexicographically, i.e. first by 'a' then 'b'.\n\n (b) For each group of rows with the same 'a' value, count the number of\n     distinct values in 'b'.\n\n (c) If there's a single distinct value in 'b', the rows are consistent with\n     the functional dependency, otherwise they contradict it.\n\n\nClause reduction (planner/optimizer)\n------------------------------------\n\nApplying the functional dependencies is fairly simple: given a list of\nequality clauses, we compute selectivities of each clause and then use the\ndegree to combine them using this formula\n\n    P(a=?,b=?) = P(a=?) * (d + (1-d) * P(b=?))\n\nWhere 'd' is the degree of functional dependency (a => b).\n\nWith more than two equality clauses, this process happens recursively. For\nexample for (a,b,c) we first use (a,b => c) to break the computation into\n\n    P(a=?,b=?,c=?) = P(a=?,b=?) * (e + (1-e) * P(c=?))\n\nwhere 'e' is the degree of functional dependency (a,b => c); then we can\napply (a=>b) the same way on P(a=?,b=?).\n\n\nConsistency of clauses\n----------------------\n\nFunctional dependencies only express general dependencies between columns,\nwithout referencing particular values. This assumes that the equality clauses\nare in fact consistent with the functional dependency, i.e. that given a\ndependency (a=>b), the value in (b=?) clause is the value determined by (a=?).\nIf that's not the case, the clauses are \"inconsistent\" with the functional\ndependency and the result will be over-estimation.\n\nThis may happen, for example, when using conditions on the ZIP code and city\nname with mismatching values (ZIP code for a different city), etc. In such a\ncase, the result set will be empty, but we'll estimate the selectivity using\nthe ZIP code condition.\n\nIn this case, the default estimation based on AVIA principle happens to work\nbetter, but mostly by chance.\n\nThis issue is the price for the simplicity of functional dependencies. If the\napplication frequently constructs queries with clauses inconsistent with\nfunctional dependencies present in the data, the best solution is not to\nuse functional dependencies, but one of the more complex types of statistics.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\statistics\\README.dependencies",
      "directory": "backend\\statistics"
    }
  },
  {
    "title": "README: backend\\statistics",
    "url": "backend\\statistics\\README.mcv",
    "content": "MCV lists\n=========\n\nMultivariate MCV (most-common values) lists are a straightforward extension of\nregular MCV lists, tracking most frequent combinations of values for a group of\nattributes.\n\nThis works particularly well for columns with a small number of distinct values,\nas the list may include all the combinations and approximate the distribution\nvery accurately.\n\nFor columns with a large number of distinct values (e.g. those with continuous\ndomains), the list will only track the most frequent combinations. If the\ndistribution is mostly uniform (all combinations about equally frequent), the\nMCV list will be empty.\n\nEstimates of some clauses (e.g. equality) based on MCV lists are more accurate\nthan when using histograms.\n\nAlso, MCV lists don't necessarily require sorting of the values (the fact that\nwe use sorting when building them is an implementation detail), but even more\nimportantly the ordering is not built into the approximation (while histograms\nare built on ordering). So MCV lists work well even for attributes where the\nordering of the data type is disconnected from the meaning of the data. For\nexample we know how to sort strings, but it's unlikely to make much sense for\ncity names (or other label-like attributes).\n\n\nSelectivity estimation\n----------------------\n\nThe estimation, implemented in mcv_clauselist_selectivity(), is quite simple\nin principle - we need to identify MCV items matching all the clauses and sum\nfrequencies of all those items.\n\nCurrently MCV lists support estimation of the following clause types:\n\n    (a) equality clauses      WHERE (a = 1) AND (b = 2)\n    (b) inequality clauses    WHERE (a < 1) AND (b >= 2)\n    (c) NULL clauses          WHERE (a IS NULL) AND (b IS NOT NULL)\n    (d) OR clauses            WHERE (a < 1) OR (b >= 2)\n\nIt's possible to add support for additional clauses, for example:\n\n    (e) multi-var clauses     WHERE (a > b)\n\nand possibly others. These are tasks for the future, not yet implemented.\n\n\nHashed MCV (not yet implemented)\n--------------------------------\n\nRegular MCV lists have to include actual values for each item, so if those items\nare large the list may be quite large. This is especially true for multivariate\nMCV lists, although the current implementation partially mitigates this by\nde-duplicating the values before storing them on disk.\n\nIt's possible to only store hashes (32-bit values) instead of the actual values,\nsignificantly reducing the space requirements. Obviously, this would only make\nthe MCV lists useful for estimating equality conditions (assuming the 32-bit\nhashes make the collisions rare enough).\n\nThis might also complicate matching the columns to available stats.\n\n\nTODO Consider implementing hashed MCV list, storing just 32-bit hashes instead\n     of the actual values. This type of MCV list will be useful only for\n     estimating equality clauses, and will reduce space requirements for large\n     varlena types (in such cases we usually only want equality anyway).\n\n\nInspecting the MCV list\n-----------------------\n\nInspecting the regular (per-attribute) MCV lists is trivial, as it's enough\nto select the columns from pg_stats. The data is encoded as anyarrays, and\nall the items have the same data type, so anyarray provides a simple way to\nget a text representation.\n\nWith multivariate MCV lists, the columns may use different data types, making\nit impossible to use anyarrays. It might be possible to produce a similar\narray-like representation, but that would complicate further processing and\nanalysis of the MCV list.\n\nSo instead the MCV lists are stored in a custom data type (pg_mcv_list),\nwhich however makes it more difficult to inspect the contents. To make that\neasier, there's a SRF returning detailed information about the MCV lists.\n\n    SELECT m.* FROM pg_statistic_ext s,\n                    pg_statistic_ext_data d,\n                    pg_mcv_list_items(stxdmcv) m\n              WHERE s.stxname = 'stts2'\n                AND d.stxoid = s.oid;\n\nIt accepts one parameter - a pg_mcv_list value (which can only be obtained\nfrom pg_statistic_ext_data catalog, to defend against malicious input), and\nreturns these columns:\n\n    - item index (0, ..., (nitems-1))\n    - values (string array)\n    - nulls only (boolean array)\n    - frequency (double precision)\n    - base_frequency (double precision)",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\statistics\\README.mcv",
      "directory": "backend\\statistics"
    }
  },
  {
    "title": "README: backend\\storage\\buffer",
    "url": "backend\\storage\\buffer\\README",
    "content": "src/backend/storage/buffer/README\n\nNotes About Shared Buffer Access Rules\n======================================\n\nThere are two separate access control mechanisms for shared disk buffers:\nreference counts (a/k/a pin counts) and buffer content locks.  (Actually,\nthere's a third level of access control: one must hold the appropriate kind\nof lock on a relation before one can legally access any page belonging to\nthe relation.  Relation-level locks are not discussed here.)\n\nPins: one must \"hold a pin on\" a buffer (increment its reference count)\nbefore being allowed to do anything at all with it.  An unpinned buffer is\nsubject to being reclaimed and reused for a different page at any instant,\nso touching it is unsafe.  Normally a pin is acquired via ReadBuffer and\nreleased via ReleaseBuffer.  It is OK and indeed common for a single\nbackend to pin a page more than once concurrently; the buffer manager\nhandles this efficiently.  It is considered OK to hold a pin for long\nintervals --- for example, sequential scans hold a pin on the current page\nuntil done processing all the tuples on the page, which could be quite a\nwhile if the scan is the outer scan of a join.  Similarly, a btree index\nscan may hold a pin on the current index page.  This is OK because normal\noperations never wait for a page's pin count to drop to zero.  (Anything\nthat might need to do such a wait is instead handled by waiting to obtain\nthe relation-level lock, which is why you'd better hold one first.)  Pins\nmay not be held across transaction boundaries, however.\n\nBuffer content locks: there are two kinds of buffer lock, shared and exclusive,\nwhich act just as you'd expect: multiple backends can hold shared locks on\nthe same buffer, but an exclusive lock prevents anyone else from holding\neither shared or exclusive lock.  (These can alternatively be called READ\nand WRITE locks.)  These locks are intended to be short-term: they should not\nbe held for long.  Buffer locks are acquired and released by LockBuffer().\nIt will *not* work for a single backend to try to acquire multiple locks on\nthe same buffer.  One must pin a buffer before trying to lock it.\n\nBuffer access rules:\n\n1. To scan a page for tuples, one must hold a pin and either shared or\nexclusive content lock.  To examine the commit status (XIDs and status bits)\nof a tuple in a shared buffer, one must likewise hold a pin and either shared\nor exclusive lock.\n\n2. Once one has determined that a tuple is interesting (visible to the\ncurrent transaction) one may drop the content lock, yet continue to access\nthe tuple's data for as long as one holds the buffer pin.  This is what is\ntypically done by heap scans, since the tuple returned by heap_fetch\ncontains a pointer to tuple data in the shared buffer.  Therefore the\ntuple cannot go away while the pin is held (see rule #5).  Its state could\nchange, but that is assumed not to matter after the initial determination\nof visibility is made.\n\n3. To add a tuple or change the xmin/xmax fields of an existing tuple,\none must hold a pin and an exclusive content lock on the containing buffer.\nThis ensures that no one else might see a partially-updated state of the\ntuple while they are doing visibility checks.\n\n4. It is considered OK to update tuple commit status bits (ie, OR the\nvalues HEAP_XMIN_COMMITTED, HEAP_XMIN_INVALID, HEAP_XMAX_COMMITTED, or\nHEAP_XMAX_INVALID into t_infomask) while holding only a shared lock and\npin on a buffer.  This is OK because another backend looking at the tuple\nat about the same time would OR the same bits into the field, so there\nis little or no risk of conflicting update; what's more, if there did\nmanage to be a conflict it would merely mean that one bit-update would\nbe lost and need to be done again later.  These four bits are only hints\n(they cache the results of transaction status lookups in pg_xact), so no\ngreat harm is done if they get reset to zero by conflicting updates.\nNote, however, that a tuple is frozen by setting both HEAP_XMIN_INVALID\nand HEAP_XMIN_COMMITTED; this is a critical update and accordingly requires\nan exclusive buffer lock (and it must also be WAL-logged).\n\n5. To physically remove a tuple or compact free space on a page, one\nmust hold a pin and an exclusive lock, *and* observe while holding the\nexclusive lock that the buffer's shared reference count is one (ie,\nno other backend holds a pin).  If these conditions are met then no other\nbackend can perform a page scan until the exclusive lock is dropped, and\nno other backend can be holding a reference to an existing tuple that it\nmight expect to examine again.  Note that another backend might pin the\nbuffer (increment the refcount) while one is performing the cleanup, but\nit won't be able to actually examine the page until it acquires shared\nor exclusive content lock.\n\n\nObtaining the lock needed under rule #5 is done by the bufmgr routines\nLockBufferForCleanup() or ConditionalLockBufferForCleanup().  They first get\nan exclusive lock and then check to see if the shared pin count is currently\n1.  If not, ConditionalLockBufferForCleanup() releases the exclusive lock and\nthen returns false, while LockBufferForCleanup() releases the exclusive lock\n(but not the caller's pin) and waits until signaled by another backend,\nwhereupon it tries again.  The signal will occur when UnpinBuffer decrements\nthe shared pin count to 1.  As indicated above, this operation might have to\nwait a good while before it acquires the lock, but that shouldn't matter much\nfor concurrent VACUUM.  The current implementation only supports a single\nwaiter for pin-count-1 on any particular shared buffer.  This is enough for\nVACUUM's use, since we don't allow multiple VACUUMs concurrently on a single\nrelation anyway.  Anyone wishing to obtain a cleanup lock outside of recovery\nor a VACUUM must use the conditional variant of the function.\n\n\nBuffer Manager's Internal Locking\n---------------------------------\n\nBefore PostgreSQL 8.1, all operations of the shared buffer manager itself\nwere protected by a single system-wide lock, the BufMgrLock, which\nunsurprisingly proved to be a source of contention.  The new locking scheme\navoids grabbing system-wide exclusive locks in common code paths.  It works\nlike this:\n\n* There is a system-wide LWLock, the BufMappingLock, that notionally\nprotects the mapping from buffer tags (page identifiers) to buffers.\n(Physically, it can be thought of as protecting the hash table maintained\nby buf_table.c.)  To look up whether a buffer exists for a tag, it is\nsufficient to obtain share lock on the BufMappingLock.  Note that one\nmust pin the found buffer, if any, before releasing the BufMappingLock.\nTo alter the page assignment of any buffer, one must hold exclusive lock\non the BufMappingLock.  This lock must be held across adjusting the buffer's\nheader fields and changing the buf_table hash table.  The only common\noperation that needs exclusive lock is reading in a page that was not\nin shared buffers already, which will require at least a kernel call\nand usually a wait for I/O, so it will be slow anyway.\n\n* As of PG 8.2, the BufMappingLock has been split into NUM_BUFFER_PARTITIONS\nseparate locks, each guarding a portion of the buffer tag space.  This allows\nfurther reduction of contention in the normal code paths.  The partition\nthat a particular buffer tag belongs to is determined from the low-order\nbits of the tag's hash value.  The rules stated above apply to each partition\nindependently.  If it is necessary to lock more than one partition at a time,\nthey must be locked in partition-number order to avoid risk of deadlock.\n\n* A separate system-wide spinlock, buffer_strategy_lock, provides mutual\nexclusion for operations that access the buffer free list or select\nbuffers for replacement.  A spinlock is used here rather than a lightweight\nlock for efficiency; no other locks of any sort should be acquired while\nbuffer_strategy_lock is held.  This is essential to allow buffer replacement\nto happen in multiple backends with reasonable concurrency.\n\n* Each buffer header contains a spinlock that must be taken when examining\nor changing fields of that buffer header.  This allows operations such as\nReleaseBuffer to make local state changes without taking any system-wide\nlock.  We use a spinlock, not an LWLock, since there are no cases where\nthe lock needs to be held for more than a few instructions.\n\nNote that a buffer header's spinlock does not control access to the data\nheld within the buffer.  Each buffer header also contains an LWLock, the\n\"buffer content lock\", that *does* represent the right to access the data\nin the buffer.  It is used per the rules above.\n\n* The BM_IO_IN_PROGRESS flag acts as a kind of lock, used to wait for I/O on a\nbuffer to complete (and in releases before 14, it was accompanied by a\nper-buffer LWLock).  The process doing a read or write sets the flag for the\nduration, and processes that need to wait for it to be cleared sleep on a\ncondition variable.\n\n\nNormal Buffer Replacement Strategy\n----------------------------------\n\nThere is a \"free list\" of buffers that are prime candidates for replacement.\nIn particular, buffers that are completely free (contain no valid page) are\nalways in this list.  We could also throw buffers into this list if we\nconsider their pages unlikely to be needed soon; however, the current\nalgorithm never does that.  The list is singly-linked using fields in the\nbuffer headers; we maintain head and tail pointers in global variables.\n(Note: although the list links are in the buffer headers, they are\nconsidered to be protected by the buffer_strategy_lock, not the buffer-header\nspinlocks.)  To choose a victim buffer to recycle when there are no free\nbuffers available, we use a simple clock-sweep algorithm, which avoids the\nneed to take system-wide locks during common operations.  It works like\nthis:\n\nEach buffer header contains a usage counter, which is incremented (up to a\nsmall limit value) whenever the buffer is pinned.  (This requires only the\nbuffer header spinlock, which would have to be taken anyway to increment the\nbuffer reference count, so it's nearly free.)\n\nThe \"clock hand\" is a buffer index, nextVictimBuffer, that moves circularly\nthrough all the available buffers.  nextVictimBuffer is protected by the\nbuffer_strategy_lock.\n\nThe algorithm for a process that needs to obtain a victim buffer is:\n\n1. Obtain buffer_strategy_lock.\n\n2. If buffer free list is nonempty, remove its head buffer.  Release\nbuffer_strategy_lock.  If the buffer is pinned or has a nonzero usage count,\nit cannot be used; ignore it go back to step 1.  Otherwise, pin the buffer,\nand return it.\n\n3. Otherwise, the buffer free list is empty.  Select the buffer pointed to by\nnextVictimBuffer, and circularly advance nextVictimBuffer for next time.\nRelease buffer_strategy_lock.\n\n4. If the selected buffer is pinned or has a nonzero usage count, it cannot\nbe used.  Decrement its usage count (if nonzero), reacquire\nbuffer_strategy_lock, and return to step 3 to examine the next buffer.\n\n5. Pin the selected buffer, and return.\n\n(Note that if the selected buffer is dirty, we will have to write it out\nbefore we can recycle it; if someone else pins the buffer meanwhile we will\nhave to give up and try another buffer.  This however is not a concern\nof the basic select-a-victim-buffer algorithm.)\n\n\nBuffer Ring Replacement Strategy\n---------------------------------\n\nWhen running a query that needs to access a large number of pages just once,\nsuch as VACUUM or a large sequential scan, a different strategy is used.\nA page that has been touched only by such a scan is unlikely to be needed\nagain soon, so instead of running the normal clock sweep algorithm and\nblowing out the entire buffer cache, a small ring of buffers is allocated\nusing the normal clock sweep algorithm and those buffers are reused for the\nwhole scan.  This also implies that much of the write traffic caused by such\na statement will be done by the backend itself and not pushed off onto other\nprocesses.\n\nFor sequential scans, a 256KB ring is used. That's small enough to fit in L2\ncache, which makes transferring pages from OS cache to shared buffer cache\nefficient.  Even less would often be enough, but the ring must be big enough\nto accommodate all pages in the scan that are pinned concurrently.  256KB\nshould also be enough to leave a small cache trail for other backends to\njoin in a synchronized seq scan.  If a ring buffer is dirtied and its LSN\nupdated, we would normally have to write and flush WAL before we could\nre-use the buffer; in this case we instead discard the buffer from the ring\nand (later) choose a replacement using the normal clock-sweep algorithm.\nHence this strategy works best for scans that are read-only (or at worst\nupdate hint bits).  In a scan that modifies every page in the scan, like a\nbulk UPDATE or DELETE, the buffers in the ring will always be dirtied and\nthe ring strategy effectively degrades to the normal strategy.\n\nVACUUM uses a ring like sequential scans, however, the size of this ring is\ncontrolled by the vacuum_buffer_usage_limit GUC.  Dirty pages are not removed\nfrom the ring.  Instead, WAL is flushed if needed to allow reuse of the\nbuffers.  Before introducing the buffer ring strategy in 8.3, VACUUM's buffers\nwere sent to the freelist, which was effectively a buffer ring of 1 buffer,\nresulting in excessive WAL flushing.\n\nBulk writes work similarly to VACUUM.  Currently this applies only to\nCOPY IN and CREATE TABLE AS SELECT.  (Might it be interesting to make\nseqscan UPDATE and DELETE use the bulkwrite strategy?)  For bulk writes\nwe use a ring size of 16MB (but not more than 1/8th of shared_buffers).\nSmaller sizes have been shown to result in the COPY blocking too often\nfor WAL flushes.  While it's okay for a background vacuum to be slowed by\ndoing its own WAL flushing, we'd prefer that COPY not be subject to that,\nso we let it use up a bit more of the buffer arena.\n\n\nBackground Writer's Processing\n------------------------------\n\nThe background writer is designed to write out pages that are likely to be\nrecycled soon, thereby offloading the writing work from active backends.\nTo do this, it scans forward circularly from the current position of\nnextVictimBuffer (which it does not change!), looking for buffers that are\ndirty and not pinned nor marked with a positive usage count.  It pins,\nwrites, and releases any such buffer.\n\nIf we can assume that reading nextVictimBuffer is an atomic action, then\nthe writer doesn't even need to take buffer_strategy_lock in order to look\nfor buffers to write; it needs only to spinlock each buffer header for long\nenough to check the dirtybit.  Even without that assumption, the writer\nonly needs to take the lock long enough to read the variable value, not\nwhile scanning the buffers.  (This is a very substantial improvement in\nthe contention cost of the writer compared to PG 8.0.)\n\nThe background writer takes shared content lock on a buffer while writing it\nout (and anyone else who flushes buffer contents to disk must do so too).\nThis ensures that the page image transferred to disk is reasonably consistent.\nWe might miss a hint-bit update or two but that isn't a problem, for the same\nreasons mentioned under buffer access rules.\n\nAs of 8.4, background writer starts during recovery mode when there is\nsome form of potentially extended recovery to perform. It performs an\nidentical service to normal processing, except that checkpoints it\nwrites are technically restartpoints.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\storage\\buffer\\README",
      "directory": "backend\\storage\\buffer"
    }
  },
  {
    "title": "README: backend\\storage\\freespace",
    "url": "backend\\storage\\freespace\\README",
    "content": "src/backend/storage/freespace/README\n\nFree Space Map\n--------------\n\nThe purpose of the free space map is to quickly locate a page with enough\nfree space to hold a tuple to be stored; or to determine that no such page\nexists and the relation must be extended by one page.  As of PostgreSQL 8.4\neach relation has its own, extensible free space map stored in a separate\n\"fork\" of its relation.  This eliminates the disadvantages of the former\nfixed-size FSM.\n\nIt is important to keep the map small so that it can be searched rapidly.\nTherefore, we don't attempt to record the exact free space on a page.\nWe allocate one map byte to each page, allowing us to record free space\nat a granularity of 1/256th of a page.  Another way to say it is that\nthe stored value is the free space divided by BLCKSZ/256 (rounding down).\nWe assume that the free space must always be less than BLCKSZ, since\nall pages have some overhead; so the maximum map value is 255.\n\nTo assist in fast searching, the map isn't simply an array of per-page\nentries, but has a tree structure above those entries.  There is a tree\nstructure of pages, and a tree structure within each page, as described\nbelow.\n\nFSM page structure\n------------------\n\nWithin each FSM page, we use a binary tree structure where leaf nodes store\nthe amount of free space on heap pages (or lower level FSM pages, see\n\"Higher-level structure\" below), with one leaf node per heap page. A non-leaf\nnode stores the max amount of free space on any of its children.\n\nFor example:\n\n    4\n 4     2\n3 4   0 2    <- This level represents heap pages\n\nWe need two basic operations: search and update.\n\nTo search for a page with X amount of free space, traverse down the tree\nalong a path where n >= X, until you hit the bottom. If both children of a\nnode satisfy the condition, you can pick either one arbitrarily.\n\nTo update the amount of free space on a page to X, first update the leaf node\ncorresponding to the heap page, then \"bubble up\" the change to upper nodes,\nby walking up to each parent and recomputing its value as the max of its\ntwo children.  Repeat until reaching the root or a parent whose value\ndoesn't change.\n\nThis data structure has a couple of nice properties:\n- to discover that there is no page with X bytes of free space, you only\n  need to look at the root node\n- by varying which child to traverse to in the search algorithm, when you have\n  a choice, we can implement various strategies, like preferring pages closer\n  to a given page, or spreading the load across the table.\n\nHigher-level routines that use FSM pages access them through the fsm_set_avail()\nand fsm_search_avail() functions. The interface to those functions hides the\npage's internal tree structure, treating the FSM page as a black box that has\na certain number of \"slots\" for storing free space information.  (However,\nthe higher routines have to be aware of the tree structure of the whole map.)\n\nThe binary tree is stored on each FSM page as an array. Because the page\nheader takes some space on a page, the binary tree isn't perfect. That is,\na few right-most leaf nodes are missing, and there are some useless non-leaf\nnodes at the right. So the tree looks something like this:\n\n       0\n   1       2\n 3   4   5   6\n7 8 9 A B\n\nwhere the numbers denote each node's position in the array.  Note that the\ntree is guaranteed complete above the leaf level; only some leaf nodes are\nmissing.  This is reflected in the number of usable \"slots\" per page not\nbeing an exact power of 2.\n\nA FSM page also has a next slot pointer, fp_next_slot, that determines where\nto start the next search for free space within that page.  The reason for that\nis to spread out the pages that are returned by FSM searches.  When several\nbackends are concurrently inserting into a relation, contention can be avoided\nby having them insert into different pages.  But it is also desirable to fill\nup pages in sequential order, to get the benefit of OS prefetching and batched\nwrites.  The FSM is responsible for making that happen, and the next slot\npointer helps provide the desired behavior.\n\nHigher-level structure\n----------------------\n\nTo scale up the data structure described above beyond a single page, we\nmaintain a similar tree-structure across pages. Leaf nodes in higher level\npages correspond to lower level FSM pages. The root node within each page\nhas the same value as the corresponding leaf node on its parent page.\n\nThe root page is always stored at physical block 0.\n\nFor example, assuming each FSM page can hold information about 4 pages (in\nreality, it holds (BLCKSZ - headers) / 2, or ~4000 with default BLCKSZ),\nwe get a disk layout like this:\n\n 0     <-- page 0 at level 2 (root page)\n  0     <-- page 0 at level 1\n   0     <-- page 0 at level 0\n   1     <-- page 1 at level 0\n   2     <-- ...\n   3\n  1     <-- page 1 at level 1\n   4\n   5\n   6\n   7\n  2\n   8\n   9\n   10\n   11\n  3\n   12\n   13\n   14\n   15\n\nwhere the numbers are page numbers *at that level*, starting from 0.\n\nTo find the physical block # corresponding to leaf page n, we need to\ncount the number of leaf and upper-level pages preceding page n.\nThis turns out to be\n\ny = n + (n / F + 1) + (n / F^2 + 1) + ... + 1\n\nwhere F is the fanout (4 in the above example). The first term n is the number\nof preceding leaf pages, the second term is the number of pages at level 1,\nand so forth.\n\nTo keep things simple, the tree is always constant height. To cover the\nmaximum relation size of 2^32-1 blocks, three levels is enough with the default\nBLCKSZ (4000^3 > 2^32).\n\nAddressing\n----------\n\nThe higher-level routines operate on \"logical\" addresses, consisting of\n- level,\n- logical page number, and\n- slot (if applicable)\n\nBottom level FSM pages have level of 0, the level above that 1, and root 2.\nAs in the diagram above, logical page number is the page number at that level,\nstarting from 0.\n\nLocking\n-------\n\nWhen traversing down to search for free space, only one page is locked at a\ntime: the parent page is released before locking the child. If the child page\nis concurrently modified, and there no longer is free space on the child page\nwhen you land on it, you need to start from scratch (after correcting the\nparent page, so that you don't get into an infinite loop).\n\nWe use shared buffer locks when searching, but exclusive buffer lock when\nupdating a page.  However, the next slot search pointer is updated during\nsearches even though we have only a shared lock.  fp_next_slot is just a hint\nand we can easily reset it if it gets corrupted; so it seems better to accept\nsome risk of that type than to pay the overhead of exclusive locking.\n\nRecovery\n--------\n\nThe FSM is not explicitly WAL-logged. Instead, we rely on a bunch of\nself-correcting measures to repair possible corruption.\n\nFirst of all, whenever a value is set on an FSM page, the root node of the\npage is compared against the new value after bubbling up the change is\nfinished. It should be greater than or equal to the value just set, or we\nhave a corrupted page, with a parent somewhere with too small a value.\nSecondly, if we detect corrupted pages while we search, traversing down\nthe tree. That check will notice if a parent node is set to too high a value.\nIn both cases, the upper nodes on the page are immediately rebuilt, fixing\nthe corruption so far as that page is concerned.\n\nVACUUM updates all the bottom-level FSM pages with the correct amount of free\nspace on corresponding heap pages, as it proceeds through the heap.  This\ngoes through fsm_set_avail(), so that the upper nodes on those pages are\nimmediately updated.  Periodically, VACUUM calls FreeSpaceMapVacuum[Range]\nto propagate the new free-space info into the upper pages of the FSM tree.\n\nAs a result when we write to the FSM we treat that as a hint and thus use\nMarkBufferDirtyHint() rather than MarkBufferDirty().  Every read here uses\nRBM_ZERO_ON_ERROR to bypass checksum mismatches and other verification\nfailures.  We'd operate correctly without the full page images that\nMarkBufferDirtyHint() provides, but they do decrease the chance of losing slot\nknowledge to RBM_ZERO_ON_ERROR.\n\nRelation extension is not WAL-logged.  Hence, after WAL replay, an on-disk FSM\nslot may indicate free space in PageIsNew() blocks that never reached disk.\nWe detect this case by comparing against the actual relation size, and we mark\nthe block as full in that case.\n\nTODO\n----\n\n- fastroot to avoid traversing upper nodes with just 1 child\n- use a different system for tables that fit into one FSM page, with a\n  mechanism to switch to the real thing as it grows.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\storage\\freespace\\README",
      "directory": "backend\\storage\\freespace"
    }
  },
  {
    "title": "README: backend\\storage\\lmgr",
    "url": "backend\\storage\\lmgr\\README",
    "content": "src/backend/storage/lmgr/README\n\nLocking Overview\n================\n\nPostgres uses four types of interprocess locks:\n\n* Spinlocks.  These are intended for *very* short-term locks.  If a lock\nis to be held more than a few dozen instructions, or across any sort of\nkernel call (or even a call to a nontrivial subroutine), don't use a\nspinlock. Spinlocks are primarily used as infrastructure for lightweight\nlocks. They are implemented using a hardware atomic-test-and-set\ninstruction, if available.  Waiting processes busy-loop until they can\nget the lock. There is no provision for deadlock detection, automatic\nrelease on error, or any other nicety.  There is a timeout if the lock\ncannot be gotten after a minute or so (which is approximately forever in\ncomparison to the intended lock hold time, so this is certainly an error\ncondition).\n\n* Lightweight locks (LWLocks).  These locks are typically used to\ninterlock access to datastructures in shared memory.  LWLocks support\nboth exclusive and shared lock modes (for read/write and read-only\naccess to a shared object). There is no provision for deadlock\ndetection, but the LWLock manager will automatically release held\nLWLocks during elog() recovery, so it is safe to raise an error while\nholding LWLocks.  Obtaining or releasing an LWLock is quite fast (a few\ndozen instructions) when there is no contention for the lock.  When a\nprocess has to wait for an LWLock, it blocks on a SysV semaphore so as\nto not consume CPU time.  Waiting processes will be granted the lock in\narrival order.  There is no timeout.\n\n* Regular locks (a/k/a heavyweight locks).  The regular lock manager\nsupports a variety of lock modes with table-driven semantics, and it has\nfull deadlock detection and automatic release at transaction end.\nRegular locks should be used for all user-driven lock requests.\n\n* SIReadLock predicate locks.  See separate README-SSI file for details.\n\nAcquisition of either a spinlock or a lightweight lock causes query\ncancel and die() interrupts to be held off until all such locks are\nreleased. No such restriction exists for regular locks, however.  Also\nnote that we can accept query cancel and die() interrupts while waiting\nfor a regular lock, but we will not accept them while waiting for\nspinlocks or LW locks. It is therefore not a good idea to use LW locks\nwhen the wait time might exceed a few seconds.\n\nThe rest of this README file discusses the regular lock manager in detail.\n\n\nLock Data Structures\n--------------------\n\nLock methods describe the overall locking behavior.  Currently there are\ntwo lock methods: DEFAULT and USER.\n\nLock modes describe the type of the lock (read/write or shared/exclusive).\nIn principle, each lock method can have its own set of lock modes with\ndifferent conflict rules, but currently DEFAULT and USER methods use\nidentical lock mode sets. See src/include/storage/lock.h for more details.\n(Lock modes are also called lock types in some places in the code and\ndocumentation.)\n\nThere are two main methods for recording locks in shared memory.  The primary\nmechanism uses two main structures: the per-lockable-object LOCK struct, and\nthe per-lock-and-requestor PROCLOCK struct.  A LOCK object exists for each\nlockable object that currently has locks held or requested on it.  A PROCLOCK\nstruct exists for each backend that is holding or requesting lock(s) on each\nLOCK object.\n\nThere is also a special \"fast path\" mechanism which backends may use to\nrecord a limited number of locks with very specific characteristics: they must\nuse the DEFAULT lockmethod; they must represent a lock on a database relation\n(not a shared relation), they must be a \"weak\" lock which is unlikely to\nconflict (AccessShareLock, RowShareLock, or RowExclusiveLock); and the system\nmust be able to quickly verify that no conflicting locks could possibly be\npresent.  See \"Fast Path Locking\", below, for more details.\n\nEach backend also maintains an unshared LOCALLOCK structure for each lockable\nobject and lock mode that it is currently holding or requesting.  The shared\nlock structures only allow a single lock grant to be made per lockable\nobject/lock mode/backend.  Internally to a backend, however, the same lock may\nbe requested and perhaps released multiple times in a transaction, and it can\nalso be held both transactionally and session-wide.  The internal request\ncounts are held in LOCALLOCK so that the shared data structures need not be\naccessed to alter them.\n\n---------------------------------------------------------------------------\n\nThe lock manager's LOCK objects contain:\n\ntag -\n    The key fields that are used for hashing locks in the shared memory\n    lock hash table.  The contents of the tag essentially define an\n    individual lockable object.  See include/storage/lock.h for details\n    about the supported types of lockable objects.  This is declared as\n    a separate struct to ensure that we always zero out the correct number\n    of bytes.  It is critical that any alignment-padding bytes the compiler\n    might insert in the struct be zeroed out, else the hash computation\n    will be random.  (Currently, we are careful to define struct LOCKTAG\n    so that there are no padding bytes.)\n\ngrantMask -\n    This bitmask indicates what types of locks are currently held on the\n    given lockable object.  It is used (against the lock table's conflict\n    table) to determine if a new lock request will conflict with existing\n    lock types held.  Conflicts are determined by bitwise AND operations\n    between the grantMask and the conflict table entry for the requested\n    lock type.  Bit i of grantMask is 1 if and only if granted[i] > 0.\n\nwaitMask -\n    This bitmask shows the types of locks being waited for.  Bit i of waitMask\n    is 1 if and only if requested[i] > granted[i].\n\nprocLocks -\n    This is a shared memory queue of all the PROCLOCK structs associated with\n    the lock object.  Note that both granted and waiting PROCLOCKs are in this\n    list (indeed, the same PROCLOCK might have some already-granted locks and\n    be waiting for more!).\n\nwaitProcs -\n    This is a shared memory queue of all PGPROC structures corresponding to\n    backends that are waiting (sleeping) until another backend releases this\n    lock.  The process structure holds the information needed to determine\n    if it should be woken up when the lock is released.\n\nnRequested -\n    Keeps a count of how many times this lock has been attempted to be\n    acquired.  The count includes attempts by processes which were put\n    to sleep due to conflicts.  It also counts the same backend twice\n    if, for example, a backend process first acquires a read and then\n    acquires a write.  (But multiple acquisitions of the same lock/lock mode\n    within a backend are not multiply counted here; they are recorded\n    only in the backend's LOCALLOCK structure.)\n\nrequested -\n    Keeps a count of how many locks of each type have been attempted.  Only\n    elements 1 through MAX_LOCKMODES-1 are used as they correspond to the lock\n    type defined constants.  Summing the values of requested[] should come out\n    equal to nRequested.\n\nnGranted -\n    Keeps count of how many times this lock has been successfully acquired.\n    This count does not include attempts that are waiting due to conflicts.\n    Otherwise the counting rules are the same as for nRequested.\n\ngranted -\n    Keeps count of how many locks of each type are currently held.  Once again\n    only elements 1 through MAX_LOCKMODES-1 are used (0 is not).  Also, like\n    requested[], summing the values of granted[] should total to the value\n    of nGranted.\n\nWe should always have 0 <= nGranted <= nRequested, and\n0 <= granted[i] <= requested[i] for each i.  When all the request counts\ngo to zero, the LOCK object is no longer needed and can be freed.\n\n---------------------------------------------------------------------------\n\nThe lock manager's PROCLOCK objects contain:\n\ntag -\n    The key fields that are used for hashing entries in the shared memory\n    PROCLOCK hash table.  This is declared as a separate struct to ensure that\n    we always zero out the correct number of bytes.  It is critical that any\n    alignment-padding bytes the compiler might insert in the struct be zeroed\n    out, else the hash computation will be random.  (Currently, we are careful\n    to define struct PROCLOCKTAG so that there are no padding bytes.)\n\n    tag.myLock\n        Pointer to the shared LOCK object this PROCLOCK is for.\n\n    tag.myProc\n        Pointer to the PGPROC of backend process that owns this PROCLOCK.\n\n    Note: it's OK to use pointers here because a PROCLOCK never outlives\n    either its lock or its proc.  The tag is therefore unique for as long\n    as it needs to be, even though the same tag values might mean something\n    else at other times.\n\nholdMask -\n    A bitmask for the lock modes successfully acquired by this PROCLOCK.\n    This should be a subset of the LOCK object's grantMask, and also a\n    subset of the PGPROC object's heldLocks mask (if the PGPROC is\n    currently waiting for another lock mode on this lock).\n\nreleaseMask -\n    A bitmask for the lock modes due to be released during LockReleaseAll.\n    This must be a subset of the holdMask.  Note that it is modified without\n    taking the partition LWLock, and therefore it is unsafe for any\n    backend except the one owning the PROCLOCK to examine/change it.\n\nlockLink -\n    List link for shared memory queue of all the PROCLOCK objects for the\n    same LOCK.\n\nprocLink -\n    List link for shared memory queue of all the PROCLOCK objects for the\n    same backend.\n\n---------------------------------------------------------------------------\n\n\nLock Manager Internal Locking\n-----------------------------\n\nBefore PostgreSQL 8.2, all of the shared-memory data structures used by\nthe lock manager were protected by a single LWLock, the LockMgrLock;\nany operation involving these data structures had to exclusively lock\nLockMgrLock.  Not too surprisingly, this became a contention bottleneck.\nTo reduce contention, the lock manager's data structures have been split\ninto multiple \"partitions\", each protected by an independent LWLock.\nMost operations only need to lock the single partition they are working in.\nHere are the details:\n\n* Each possible lock is assigned to one partition according to a hash of\nits LOCKTAG value.  The partition's LWLock is considered to protect all the\nLOCK objects of that partition as well as their subsidiary PROCLOCKs.\n\n* The shared-memory hash tables for LOCKs and PROCLOCKs are organized\nso that different partitions use different hash chains, and thus there\nis no conflict in working with objects in different partitions.  This\nis supported directly by dynahash.c's \"partitioned table\" mechanism\nfor the LOCK table: we need only ensure that the partition number is\ntaken from the low-order bits of the dynahash hash value for the LOCKTAG.\nTo make it work for PROCLOCKs, we have to ensure that a PROCLOCK's hash\nvalue has the same low-order bits as its associated LOCK.  This requires\na specialized hash function (see proclock_hash).\n\n* Formerly, each PGPROC had a single list of PROCLOCKs belonging to it.\nThis has now been split into per-partition lists, so that access to a\nparticular PROCLOCK list can be protected by the associated partition's\nLWLock.  (This rule allows one backend to manipulate another backend's\nPROCLOCK lists, which was not originally necessary but is now required in\nconnection with fast-path locking; see below.)\n\n* The other lock-related fields of a PGPROC are only interesting when\nthe PGPROC is waiting for a lock, so we consider that they are protected\nby the partition LWLock of the awaited lock.\n\nFor normal lock acquisition and release, it is sufficient to lock the\npartition containing the desired lock.  Deadlock checking needs to touch\nmultiple partitions in general; for simplicity, we just make it lock all\nthe partitions in partition-number order.  (To prevent LWLock deadlock,\nwe establish the rule that any backend needing to lock more than one\npartition at once must lock them in partition-number order.)  It's\npossible that deadlock checking could be done without touching every\npartition in typical cases, but since in a properly functioning system\ndeadlock checking should not occur often enough to be performance-critical,\ntrying to make this work does not seem a productive use of effort.\n\nA backend's internal LOCALLOCK hash table is not partitioned.  We do store\na copy of the locktag hash code in LOCALLOCK table entries, from which the\npartition number can be computed, but this is a straight speed-for-space\ntradeoff: we could instead recalculate the partition number from the LOCKTAG\nwhen needed.\n\n\nFast Path Locking\n-----------------\n\nFast path locking is a special purpose mechanism designed to reduce the\noverhead of taking and releasing certain types of locks which are taken\nand released very frequently but rarely conflict.  Currently, this includes\ntwo categories of locks:\n\n(1) Weak relation locks.  SELECT, INSERT, UPDATE, and DELETE must acquire a\nlock on every relation they operate on, as well as various system catalogs\nthat can be used internally.  Many DML operations can proceed in parallel\nagainst the same table at the same time; only DDL operations such as\nCLUSTER, ALTER TABLE, or DROP -- or explicit user action such as LOCK TABLE\n-- will create lock conflicts with the \"weak\" locks (AccessShareLock,\nRowShareLock, RowExclusiveLock) acquired by DML operations.\n\n(2) VXID locks.  Every transaction takes a lock on its own virtual\ntransaction ID.  Currently, the only operations that wait for these locks\nare CREATE INDEX CONCURRENTLY and Hot Standby (in the case of a conflict),\nso most VXID locks are taken and released by the owner without anyone else\nneeding to care.\n\nThe primary locking mechanism does not cope well with this workload.  Even\nthough the lock manager locks are partitioned, the locktag for any given\nrelation still falls in one, and only one, partition.  Thus, if many short\nqueries are accessing the same relation, the lock manager partition lock for\nthat partition becomes a contention bottleneck.  This effect is measurable\neven on 2-core servers, and becomes very pronounced as core count increases.\n\nTo alleviate this bottleneck, beginning in PostgreSQL 9.2, each backend is\npermitted to record a limited number of locks on unshared relations in an\narray within its PGPROC structure, rather than using the primary lock table.\nThis mechanism can only be used when the locker can verify that no conflicting\nlocks exist at the time of taking the lock.\n\nA key point of this algorithm is that it must be possible to verify the\nabsence of possibly conflicting locks without fighting over a shared LWLock or\nspinlock.  Otherwise, this effort would simply move the contention bottleneck\nfrom one place to another.  We accomplish this using an array of 1024 integer\ncounters, which are in effect a 1024-way partitioning of the lock space.\nEach counter records the number of \"strong\" locks (that is, ShareLock,\nShareRowExclusiveLock, ExclusiveLock, and AccessExclusiveLock) on unshared\nrelations that fall into that partition.  When this counter is non-zero, the\nfast path mechanism may not be used to take new relation locks within that\npartition.  A strong locker bumps the counter and then scans each per-backend\narray for matching fast-path locks; any which are found must be transferred to\nthe primary lock table before attempting to acquire the lock, to ensure proper\nlock conflict and deadlock detection.\n\nOn an SMP system, we must guarantee proper memory synchronization.  Here we\nrely on the fact that LWLock acquisition acts as a memory sequence point: if\nA performs a store, A and B both acquire an LWLock in either order, and B\nthen performs a load on the same memory location, it is guaranteed to see\nA's store.  In this case, each backend's fast-path lock queue is protected\nby an LWLock.  A backend wishing to acquire a fast-path lock grabs this\nLWLock before examining FastPathStrongRelationLocks to check for the presence\nof a conflicting strong lock.  And the backend attempting to acquire a strong\nlock, because it must transfer any matching weak locks taken via the fast-path\nmechanism to the shared lock table, will acquire every LWLock protecting a\nbackend fast-path queue in turn.  So, if we examine\nFastPathStrongRelationLocks and see a zero, then either the value is truly\nzero, or if it is a stale value, the strong locker has yet to acquire the\nper-backend LWLock we now hold (or, indeed, even the first per-backend LWLock)\nand will notice any weak lock we take when it does.\n\nFast-path VXID locks do not use the FastPathStrongRelationLocks table.  The\nfirst lock taken on a VXID is always the ExclusiveLock taken by its owner.\nAny subsequent lockers are share lockers waiting for the VXID to terminate.\nIndeed, the only reason VXID locks use the lock manager at all (rather than\nwaiting for the VXID to terminate via some other method) is for deadlock\ndetection.  Thus, the initial VXID lock can *always* be taken via the fast\npath without checking for conflicts.  Any subsequent locker must check\nwhether the lock has been transferred to the main lock table, and if not,\ndo so.  The backend owning the VXID must be careful to clean up any entry\nmade in the main lock table at end of transaction.\n\nDeadlock detection does not need to examine the fast-path data structures,\nbecause any lock that could possibly be involved in a deadlock must have\nbeen transferred to the main tables beforehand.\n\n\nThe Deadlock Detection Algorithm\n--------------------------------\n\nSince we allow user transactions to request locks in any order, deadlock\nis possible.  We use a deadlock detection/breaking algorithm that is\nfairly standard in essence, but there are many special considerations\nneeded to deal with Postgres' generalized locking model.\n\nA key design consideration is that we want to make routine operations\n(lock grant and release) run quickly when there is no deadlock, and\navoid the overhead of deadlock handling as much as possible.  We do this\nusing an \"optimistic waiting\" approach: if a process cannot acquire the\nlock it wants immediately, it goes to sleep without any deadlock check.\nBut it also sets a delay timer, with a delay of DeadlockTimeout\nmilliseconds (typically set to one second).  If the delay expires before\nthe process is granted the lock it wants, it runs the deadlock\ndetection/breaking code. Normally this code will determine that there is\nno deadlock condition, and then the process will go back to sleep and\nwait quietly until it is granted the lock.  But if a deadlock condition\ndoes exist, it will be resolved, usually by aborting the detecting\nprocess' transaction.  In this way, we avoid deadlock handling overhead\nwhenever the wait time for a lock is less than DeadlockTimeout, while\nnot imposing an unreasonable delay of detection when there is an error.\n\nLock acquisition (routines LockAcquire and ProcSleep) follows these rules:\n\n1. A lock request is granted immediately if it does not conflict with\nany existing or waiting lock request, or if the process already holds an\ninstance of the same lock type (eg, there's no penalty to acquire a read\nlock twice).  Note that a process never conflicts with itself, eg one\ncan obtain read lock when one already holds exclusive lock.\n\n2. Otherwise the process joins the lock's wait queue.  Normally it will\nbe added to the end of the queue, but there is an exception: if the\nprocess already holds locks on this same lockable object that conflict\nwith the request of any pending waiter, then the process will be\ninserted in the wait queue just ahead of the first such waiter.  (If we\ndid not make this check, the deadlock detection code would adjust the\nqueue order to resolve the conflict, but it's relatively cheap to make\nthe check in ProcSleep and avoid a deadlock timeout delay in this case.)\nNote special case when inserting before the end of the queue: if the\nprocess's request does not conflict with any existing lock nor any\nwaiting request before its insertion point, then go ahead and grant the\nlock without waiting.\n\nWhen a lock is released, the lock release routine (ProcLockWakeup) scans\nthe lock object's wait queue.  Each waiter is awoken if (a) its request\ndoes not conflict with already-granted locks, and (b) its request does\nnot conflict with the requests of prior un-wakable waiters.  Rule (b)\nensures that conflicting requests are granted in order of arrival. There\nare cases where a later waiter must be allowed to go in front of\nconflicting earlier waiters to avoid deadlock, but it is not\nProcLockWakeup's responsibility to recognize these cases; instead, the\ndeadlock detection code will re-order the wait queue when necessary.\n\nTo perform deadlock checking, we use the standard method of viewing the\nvarious processes as nodes in a directed graph (the waits-for graph or\nWFG).  There is a graph edge leading from process A to process B if A\nwaits for B, ie, A is waiting for some lock and B holds a conflicting\nlock.  There is a deadlock condition if and only if the WFG contains a\ncycle.  We detect cycles by searching outward along waits-for edges to\nsee if we return to our starting point.  There are three possible\noutcomes:\n\n1. All outgoing paths terminate at a running process (which has no\noutgoing edge).\n\n2. A deadlock is detected by looping back to the start point.  We\nresolve such a deadlock by canceling the start point's lock request and\nreporting an error in that transaction, which normally leads to\ntransaction abort and release of that transaction's held locks.  Note\nthat it's sufficient to cancel one request to remove the cycle; we don't\nneed to kill all the transactions involved.\n\n3. Some path(s) loop back to a node other than the start point.  This\nindicates a deadlock, but one that does not involve our starting\nprocess. We ignore this condition on the grounds that resolving such a\ndeadlock is the responsibility of the processes involved --- killing our\nstart-point process would not resolve the deadlock.  So, cases 1 and 3\nboth report \"no deadlock\".\n\nPostgres' situation is a little more complex than the standard discussion\nof deadlock detection, for two reasons:\n\n1. A process can be waiting for more than one other process, since there\nmight be multiple PROCLOCKs of (non-conflicting) lock types that all\nconflict with the waiter's request.  This creates no real difficulty\nhowever; we simply need to be prepared to trace more than one outgoing\nedge.\n\n2. If a process A is behind a process B in some lock's wait queue, and\ntheir requested locks conflict, then we must say that A waits for B, since\nProcLockWakeup will never awaken A before B.  This creates additional\nedges in the WFG.  We call these \"soft\" edges, as opposed to the \"hard\"\nedges induced by locks already held.  Note that if B already holds any\nlocks conflicting with A's request, then their relationship is a hard edge\nnot a soft edge.\n\nA \"soft\" block, or wait-priority block, has the same potential for\ninducing deadlock as a hard block.  However, we may be able to resolve\na soft block without aborting the transactions involved: we can instead\nrearrange the order of the wait queue.  This rearrangement reverses the\ndirection of the soft edge between two processes with conflicting requests\nwhose queue order is reversed.  If we can find a rearrangement that\neliminates a cycle without creating new ones, then we can avoid an abort.\nChecking for such possible rearrangements is the trickiest part of the\nalgorithm.\n\nThe workhorse of the deadlock detector is a routine FindLockCycle() which\nis given a starting point process (which must be a waiting process).\nIt recursively scans outward across waits-for edges as discussed above.\nIf it finds no cycle involving the start point, it returns \"false\".\n(As discussed above, we can ignore cycles not involving the start point.)\nWhen such a cycle is found, FindLockCycle() returns \"true\", and as it\nunwinds it also builds a list of any \"soft\" edges involved in the cycle.\nIf the resulting list is empty then there is a hard deadlock and the\nconfiguration cannot succeed.  However, if the list is not empty, then\nreversing any one of the listed edges through wait-queue rearrangement\nwill eliminate that cycle.  Since such a reversal might create cycles\nelsewhere, we may need to try every possibility.  Therefore, we need to\nbe able to invoke FindLockCycle() on hypothetical configurations (wait\norders) as well as the current real order.\n\nThe easiest way to handle this seems to be to have a lookaside table that\nshows the proposed new queue order for each wait queue that we are\nconsidering rearranging.  This table is checked by FindLockCycle, and it\nbelieves the proposed queue order rather than the real order for each lock\nthat has an entry in the lookaside table.\n\nWe build a proposed new queue order by doing a \"topological sort\" of the\nexisting entries.  Each soft edge that we are currently considering\nreversing creates a property of the partial order that the topological sort\nhas to enforce.  We must use a sort method that preserves the input\nordering as much as possible, so as not to gratuitously break arrival\norder for processes not involved in a deadlock.  (This is not true of the\ntsort method shown in Knuth, for example, but it's easily done by a simple\ndoubly-nested-loop method that emits the first legal candidate at each\nstep.  Fortunately, we don't need a highly efficient sort algorithm, since\nthe number of partial order constraints is not likely to be large.)  Note\nthat failure of the topological sort tells us we have conflicting ordering\nconstraints, and therefore that the last-added soft edge reversal\nconflicts with a prior edge reversal.  We need to detect this case to\navoid an infinite loop in the case where no possible rearrangement will\nwork: otherwise, we might try a reversal, find that it still leads to\na cycle, then try to un-reverse the reversal while trying to get rid of\nthat cycle, etc etc.  Topological sort failure tells us the un-reversal\nis not a legitimate move in this context.\n\nSo, the basic step in our rearrangement method is to take a list of\nsoft edges in a cycle (as returned by FindLockCycle()) and successively\ntry the reversal of each one as a topological-sort constraint added to\nwhatever constraints we are already considering.  We recursively search\nthrough all such sets of constraints to see if any one eliminates all\nthe deadlock cycles at once.  Although this might seem impossibly\ninefficient, it shouldn't be a big problem in practice, because there\nwill normally be very few, and not very large, deadlock cycles --- if\nany at all.  So the combinatorial inefficiency isn't going to hurt us.\nBesides, it's better to spend some time to guarantee that we've checked\nall possible escape routes than to abort a transaction when we didn't\nreally have to.\n\nEach edge reversal constraint can be viewed as requesting that the waiting\nprocess A be moved to before the blocking process B in the wait queue they\nare both in.  This action will reverse the desired soft edge, as well as\nany other soft edges between A and other processes it is advanced over.\nNo other edges will be affected (note this is actually a constraint on our\ntopological sort method to not re-order the queue more than necessary.)\nTherefore, we can be sure we have not created any new deadlock cycles if\nneither FindLockCycle(A) nor FindLockCycle(B) discovers any cycle.  Given\nthe above-defined behavior of FindLockCycle, each of these searches is\nnecessary as well as sufficient, since FindLockCycle starting at the\noriginal start point will not complain about cycles that include A or B\nbut not the original start point.\n\nIn short then, a proposed rearrangement of the wait queue(s) is determined\nby one or more broken soft edges A->B, fully specified by the output of\ntopological sorts of each wait queue involved, and then tested by invoking\nFindLockCycle() starting at the original start point as well as each of\nthe mentioned processes (A's and B's).  If none of the tests detect a\ncycle, then we have a valid configuration and can implement it by\nreordering the wait queues per the sort outputs (and then applying\nProcLockWakeup on each reordered queue, in case a waiter has become wakable).\nIf any test detects a soft cycle, we can try to resolve it by adding each\nsoft link in that cycle, in turn, to the proposed rearrangement list.\nThis is repeated recursively until we either find a workable rearrangement\nor determine that none exists.  In the latter case, the outer level\nresolves the deadlock by aborting the original start-point transaction.\n\nThe particular order in which rearrangements are tried depends on the\norder FindLockCycle() happens to scan in, so if there are multiple\nworkable rearrangements of the wait queues, then it is unspecified which\none will be chosen.  What's more important is that we guarantee to try\nevery queue rearrangement that could lead to success.  (For example,\nif we have A before B before C and the needed order constraints are\nC before A and B before C, we would first discover that A before C\ndoesn't work and try the rearrangement C before A before B.  This would\neventually lead to the discovery of the additional constraint B before C.)\n\nGot that?\n\nMiscellaneous Notes\n-------------------\n\n1. It is easily proven that no deadlock will be missed due to our\nasynchronous invocation of deadlock checking.  A deadlock cycle in the WFG\nis formed when the last edge in the cycle is added; therefore the last\nprocess in the cycle to wait (the one from which that edge is outgoing) is\ncertain to detect and resolve the cycle when it later runs CheckDeadLock.\nThis holds even if that edge addition created multiple cycles; the process\nmay indeed abort without ever noticing those additional cycles, but we\ndon't particularly care.  The only other possible creation of deadlocks is\nduring deadlock resolution's rearrangement of wait queues, and we already\nsaw that that algorithm will prove that it creates no new deadlocks before\nit attempts to actually execute any rearrangement.\n\n2. It is not certain that a deadlock will be resolved by aborting the\nlast-to-wait process.  If earlier waiters in the cycle have not yet run\nCheckDeadLock, then the first one to do so will be the victim.\n\n3. No live (wakable) process can be missed by ProcLockWakeup, since it\nexamines every member of the wait queue (this was not true in the 7.0\nimplementation, BTW).  Therefore, if ProcLockWakeup is always invoked\nafter a lock is released or a wait queue is rearranged, there can be no\nfailure to wake a wakable process.  One should also note that\nLockErrorCleanup (abort a waiter due to outside factors) must run\nProcLockWakeup, in case the canceled waiter was soft-blocking other\nwaiters.\n\n4. We can minimize excess rearrangement-trial work by being careful to\nscan the wait queue from the front when looking for soft edges.  For\nexample, if we have queue order A,B,C and C has deadlock conflicts with\nboth A and B, we want to generate the \"C before A\" constraint first,\nrather than wasting time with \"C before B\", which won't move C far\nenough up.  So we look for soft edges outgoing from C starting at the\nfront of the wait queue.\n\n5. The working data structures needed by the deadlock detection code can\nbe limited to numbers of entries computed from MaxBackends.  Therefore,\nwe can allocate the worst-case space needed during backend startup. This\nseems a safer approach than trying to allocate workspace on the fly; we\ndon't want to risk having the deadlock detector run out of memory, else\nwe really have no guarantees at all that deadlock will be detected.\n\n6. We abuse the deadlock detector to implement autovacuum cancellation.\nWhen we run the detector and we find that there's an autovacuum worker\ninvolved in the waits-for graph, we store a pointer to its PGPROC, and\nreturn a special return code (unless a hard deadlock has been detected).\nThe caller can then send a cancellation signal.  This implements the\nprinciple that autovacuum has a low locking priority (eg it must not block\nDDL on the table).\n\nGroup Locking\n-------------\n\nAs if all of that weren't already complicated enough, PostgreSQL now supports\nparallelism (see src/backend/access/transam/README.parallel), which means that\nwe might need to resolve deadlocks that occur between gangs of related\nprocesses rather than individual processes.  This doesn't change the basic\ndeadlock detection algorithm very much, but it makes the bookkeeping more\ncomplicated.\n\nWe choose to regard locks held by processes in the same parallel group as\nnon-conflicting with the exception of relation extension lock.  This means that\ntwo processes in a parallel group can hold a self-exclusive lock on the same\nrelation at the same time, or one process can acquire an AccessShareLock while\nthe other already holds AccessExclusiveLock.  This might seem dangerous and\ncould be in some cases (more on that below), but if we didn't do this then\nparallel query would be extremely prone to self-deadlock.  For example, a\nparallel query against a relation on which the leader already had\nAccessExclusiveLock would hang, because the workers would try to lock the same\nrelation and be blocked by the leader; yet the leader can't finish until it\nreceives completion indications from all workers.  An undetected deadlock\nresults.  This is far from the only scenario where such a problem happens.  The\nsame thing will occur if the leader holds only AccessShareLock, the worker\nseeks AccessShareLock, but between the time the leader attempts to acquire the\nlock and the time the worker attempts to acquire it, some other process queues\nup waiting for an AccessExclusiveLock.  In this case, too, an indefinite hang\nresults.\n\nIt might seem that we could predict which locks the workers will attempt to\nacquire and ensure before going parallel that those locks would be acquired\nsuccessfully.  But this is very difficult to make work in a general way.  For\nexample, a parallel worker's portion of the query plan could involve an\nSQL-callable function which generates a query dynamically, and that query\nmight happen to hit a table on which the leader happens to hold\nAccessExclusiveLock.  By imposing enough restrictions on what workers can do,\nwe could eventually create a situation where their behavior can be adequately\nrestricted, but these restrictions would be fairly onerous, and even then, the\nsystem required to decide whether the workers will succeed at acquiring the\nnecessary locks would be complex and possibly buggy.\n\nSo, instead, we take the approach of deciding that locks within a lock group\ndo not conflict.  This eliminates the possibility of an undetected deadlock,\nbut also opens up some problem cases: if the leader and worker try to do some\noperation at the same time which would ordinarily be prevented by the\nheavyweight lock mechanism, undefined behavior might result.  In practice, the\ndangers are modest.  The leader and worker share the same transaction,\nsnapshot, and combo CID hash, and neither can perform any DDL or, indeed,\nwrite any data at all.  Thus, for either to read a table locked exclusively by\nthe other is safe enough.  Problems would occur if the leader initiated\nparallelism from a point in the code at which it had some backend-private\nstate that made table access from another process unsafe, for example after\ncalling SetReindexProcessing and before calling ResetReindexProcessing,\ncatastrophe could ensue, because the worker won't have that state.  Similarly,\nproblems could occur with certain kinds of non-relation locks, such as\nGIN page locks.  It's no safer for two related processes to perform GIN clean\nup at the same time than for unrelated processes to do the same.\nHowever, since parallel mode is strictly read-only at present, neither this\nnor most of the similar cases can arise at present.  To allow parallel writes,\nwe'll either need to (1) further enhance the deadlock detector to handle those\ntypes of locks in a different way than other types; or (2) have parallel\nworkers use some other mutual exclusion method for such cases.\n\nGroup locking adds three new members to each PGPROC: lockGroupLeader,\nlockGroupMembers, and lockGroupLink. A PGPROC's lockGroupLeader is NULL for\nprocesses not involved in parallel query. When a process wants to cooperate\nwith parallel workers, it becomes a lock group leader, which means setting\nthis field to point to its own PGPROC. When a parallel worker starts up, it\npoints this field at the leader. The lockGroupMembers field is only used in\nthe leader; it is a list of the member PGPROCs of the lock group (the leader\nand all workers). The lockGroupLink field is the list link for this list.\n\nAll three of these fields are considered to be protected by a lock manager\npartition lock.  The partition lock that protects these fields within a given\nlock group is chosen by taking the leader's pgprocno modulo the number of lock\nmanager partitions.  This unusual arrangement has a major advantage: the\ndeadlock detector can count on the fact that no lockGroupLeader field can\nchange while the deadlock detector is running, because it knows that it holds\nall the lock manager locks.  Also, holding this single lock allows safe\nmanipulation of the lockGroupMembers list for the lock group.\n\nWe need an additional interlock when setting these fields, because a newly\nstarted parallel worker has to try to join the leader's lock group, but it\nhas no guarantee that the group leader is still alive by the time it gets\nstarted.  We try to ensure that the parallel leader dies after all workers\nin normal cases, but also that the system could survive relatively intact\nif that somehow fails to happen.  This is one of the precautions against\nsuch a scenario: the leader relays its PGPROC and also its PID to the\nworker, and the worker fails to join the lock group unless the given PGPROC\nstill has the same PID and is still a lock group leader.  We assume that\nPIDs are not recycled quickly enough for this interlock to fail.\n\n\nUser Locks (Advisory Locks)\n---------------------------\n\nUser locks are handled totally on the application side as long term\ncooperative locks which may extend beyond the normal transaction boundaries.\nTheir purpose is to indicate to an application that someone is `working'\non an item.  So it is possible to put a user lock on a tuple's oid,\nretrieve the tuple, work on it for an hour and then update it and remove\nthe lock.  While the lock is active other clients can still read and write\nthe tuple but they can be aware that it has been locked at the application\nlevel by someone.\n\nUser locks and normal locks are completely orthogonal and they don't\ninterfere with each other.\n\nUser locks can be acquired either at session level or transaction level.\nA session-level lock request is not automatically released at transaction\nend, but must be explicitly released by the application.  (However, any\nremaining locks are always released at session end.)  Transaction-level\nuser lock requests behave the same as normal lock requests, in that they\nare released at transaction end and do not need explicit unlocking.\n\nLocking during Hot Standby\n--------------------------\n\nThe Startup process is the only backend that can make changes during\nrecovery, all other backends are read only.  As a result the Startup\nprocess does not acquire locks on relations or objects except when the lock\nlevel is AccessExclusiveLock.\n\nRegular backends are only allowed to take locks on relations or objects\nat RowExclusiveLock or lower. This ensures that they do not conflict with\neach other or with the Startup process, unless AccessExclusiveLocks are\nrequested by the Startup process.\n\nDeadlocks involving AccessExclusiveLocks are not possible, so we need\nnot be concerned that a user initiated deadlock can prevent recovery from\nprogressing.\n\nAccessExclusiveLocks on the primary node generate WAL records\nthat are then applied by the Startup process. Locks are released at end\nof transaction just as they are in normal processing. These locks are\nheld by the Startup process, acting as a proxy for the backends that\noriginally acquired these locks. Again, these locks cannot conflict with\none another, so the Startup process cannot deadlock itself either.\n\nAlthough deadlock is not possible, a regular backend's weak lock can\nprevent the Startup process from making progress in applying WAL, which is\nusually not something that should be tolerated for very long.  Mechanisms\nexist to forcibly cancel a regular backend's query if it blocks the\nStartup process for too long.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\storage\\lmgr\\README",
      "directory": "backend\\storage\\lmgr"
    }
  },
  {
    "title": "README: backend\\storage\\lmgr",
    "url": "backend\\storage\\lmgr\\README-SSI",
    "content": "src/backend/storage/lmgr/README-SSI\n\nSerializable Snapshot Isolation (SSI) and Predicate Locking\n===========================================================\n\nThis code is in the lmgr directory because about 90% of it is an\nimplementation of predicate locking, which is required for SSI,\nrather than being directly related to SSI itself.  When another use\nfor predicate locking justifies the effort to tease these two things\napart, this README file should probably be split.\n\n\nCredits\n-------\n\nThis feature was developed by Kevin Grittner and Dan R. K. Ports,\nwith review and suggestions from Joe Conway, Heikki Linnakangas, and\nJeff Davis.  It is based on work published in these papers:\n\n\tMichael J. Cahill, Uwe Rhm, and Alan D. Fekete. 2008.\n\tSerializable isolation for snapshot databases.\n\tIn SIGMOD '08: Proceedings of the 2008 ACM SIGMOD\n\tinternational conference on Management of data,\n\tpages 729-738, New York, NY, USA. ACM.\n\thttp://doi.acm.org/10.1145/1376616.1376690\n\n\tMichael James Cahill. 2009.\n\tSerializable Isolation for Snapshot Databases.\n\tSydney Digital Theses.\n\tUniversity of Sydney, School of Information Technologies.\n\thttp://hdl.handle.net/2123/5353\n\n\nOverview\n--------\n\nWith true serializable transactions, if you can show that your\ntransaction will do the right thing if there are no concurrent\ntransactions, it will do the right thing in any mix of serializable\ntransactions or be rolled back with a serialization failure.  This\nfeature has been implemented in PostgreSQL using SSI.\n\n\nSerializable and Snapshot Transaction Isolation Levels\n------------------------------------------------------\n\nSerializable transaction isolation is attractive for shops with\nactive development by many programmers against a complex schema\nbecause it guarantees data integrity with very little staff time --\nif a transaction can be shown to always do the right thing when it is\nrun alone (before or after any other transaction), it will always do\nthe right thing in any mix of concurrent serializable transactions.\nWhere conflicts with other transactions would result in an\ninconsistent state within the database or an inconsistent view of\nthe data, a serializable transaction will block or roll back to\nprevent the anomaly. The SQL standard provides a specific SQLSTATE\nfor errors generated when a transaction rolls back for this reason,\nso that transactions can be retried automatically.\n\nBefore version 9.1, PostgreSQL did not support a full serializable\nisolation level. A request for serializable transaction isolation\nactually provided snapshot isolation. This has well known anomalies\nwhich can allow data corruption or inconsistent views of the data\nduring concurrent transactions; although these anomalies only occur\nwhen certain patterns of read-write dependencies exist within a set\nof concurrent transactions. Where these patterns exist, the anomalies\ncan be prevented by introducing conflicts through explicitly\nprogrammed locks or otherwise unnecessary writes to the database.\nSnapshot isolation is popular because performance is better than\nserializable isolation and the integrity guarantees which it does\nprovide allow anomalies to be avoided or managed with reasonable\neffort in many environments.\n\n\nSerializable Isolation Implementation Strategies\n------------------------------------------------\n\nTechniques for implementing full serializable isolation have been\npublished and in use in many database products for decades. The\nprimary technique which has been used is Strict Two-Phase Locking\n(S2PL), which operates by blocking writes against data which has been\nread by concurrent transactions and blocking any access (read or\nwrite) against data which has been written by concurrent\ntransactions. A cycle in a graph of blocking indicates a deadlock,\nrequiring a rollback. Blocking and deadlocks under S2PL in high\ncontention workloads can be debilitating, crippling throughput and\nresponse time.\n\nA new technique for implementing full serializable isolation in an\nMVCC database appears in the literature beginning in 2008. This\ntechnique, known as Serializable Snapshot Isolation (SSI) has many of\nthe advantages of snapshot isolation. In particular, reads don't\nblock anything and writes don't block reads. Essentially, it runs\nsnapshot isolation but monitors the read-write conflicts between\ntransactions to identify dangerous structures in the transaction\ngraph which indicate that a set of concurrent transactions might\nproduce an anomaly, and rolls back transactions to ensure that no\nanomalies occur. It will produce some false positives (where a\ntransaction is rolled back even though there would not have been an\nanomaly), but will never let an anomaly occur. In the two known\nprototype implementations, performance for many workloads (even with\nthe need to restart transactions which are rolled back) is very close\nto snapshot isolation and generally far better than an S2PL\nimplementation.\n\n\nApparent Serial Order of Execution\n----------------------------------\n\nOne way to understand when snapshot anomalies can occur, and to\nvisualize the difference between the serializable implementations\ndescribed above, is to consider that among transactions executing at\nthe serializable transaction isolation level, the results are\nrequired to be consistent with some serial (one-at-a-time) execution\nof the transactions [1]. How is that order determined in each?\n\nIn S2PL, each transaction locks any data it accesses. It holds the\nlocks until committing, preventing other transactions from making\nconflicting accesses to the same data in the interim. Some\ntransactions may have to be rolled back to prevent deadlock. But\nsuccessful transactions can always be viewed as having occurred\nsequentially, in the order they committed.\n\nWith snapshot isolation, reads never block writes, nor vice versa, so\nmore concurrency is possible. The order in which transactions appear\nto have executed is determined by something more subtle than in S2PL:\nread/write dependencies. If a transaction reads data, it appears to\nexecute after the transaction that wrote the data it is reading.\nSimilarly, if it updates data, it appears to execute after the\ntransaction that wrote the previous version. These dependencies, which\nwe call \"wr-dependencies\" and \"ww-dependencies\", are consistent with\nthe commit order, because the first transaction must have committed\nbefore the second starts. However, there can also be dependencies\nbetween two *concurrent* transactions, i.e. where one was running when\nthe other acquired its snapshot.  These \"rw-conflicts\" occur when one\ntransaction attempts to read data which is not visible to it because\nthe transaction which wrote it (or will later write it) is\nconcurrent. The reading transaction appears to have executed first,\nregardless of the actual sequence of transaction starts or commits,\nbecause it sees a database state prior to that in which the other\ntransaction leaves it.\n\nAnomalies occur when a cycle is created in the graph of dependencies:\nwhen a dependency or series of dependencies causes transaction A to\nappear to have executed before transaction B, but another series of\ndependencies causes B to appear before A. If that's the case, then\nthe results can't be consistent with any serial execution of the\ntransactions.\n\n\nSSI Algorithm\n-------------\n\nAs of 9.1, serializable transactions in PostgreSQL are implemented using\nSerializable Snapshot Isolation (SSI), based on the work of Cahill\net al. Fundamentally, this allows snapshot isolation to run as it\npreviously did, while monitoring for conditions which could create a\nserialization anomaly.\n\nSSI is based on the observation [2] that each snapshot isolation\nanomaly corresponds to a cycle that contains a \"dangerous structure\"\nof two adjacent rw-conflict edges:\n\n      Tin ------> Tpivot ------> Tout\n            rw             rw\n\nSSI works by watching for this dangerous structure, and rolling\nback a transaction when needed to prevent any anomaly. This means it\nonly needs to track rw-conflicts between concurrent transactions, not\nwr- and ww-dependencies. It also means there is a risk of false\npositives, because not every dangerous structure is embedded in an\nactual cycle.  The number of false positives is low in practice, so\nthis represents an acceptable tradeoff for keeping the detection\noverhead low.\n\nThe PostgreSQL implementation uses two additional optimizations:\n\n* Tout must commit before any other transaction in the cycle\n  (see proof of Theorem 2.1 of [2]). We only roll back a transaction\n  if Tout commits before Tpivot and Tin.\n\n* if Tin is read-only, there can only be an anomaly if Tout committed\n  before Tin takes its snapshot. This optimization is an original\n  one. Proof:\n\n  - Because there is a cycle, there must be some transaction T0 that\n    precedes Tin in the cycle. (T0 might be the same as Tout.)\n\n  - The edge between T0 and Tin can't be a rw-conflict or ww-dependency,\n    because Tin was read-only, so it must be a wr-dependency.\n    Those can only occur if T0 committed before Tin took its snapshot,\n    else Tin would have ignored T0's output.\n\n  - Because Tout must commit before any other transaction in the\n    cycle, it must commit before T0 commits -- and thus before Tin\n    starts.\n\n\nPostgreSQL Implementation\n-------------------------\n\n    * Since this technique is based on Snapshot Isolation (SI), those\nareas in PostgreSQL which don't use SI can't be brought under SSI.\nThis includes system tables, temporary tables, sequences, hint bit\nrewrites, etc.  SSI can not eliminate existing anomalies in these\nareas.\n\n    * Any transaction which is run at a transaction isolation level\nother than SERIALIZABLE will not be affected by SSI.  If you want to\nenforce business rules through SSI, all transactions should be run at\nthe SERIALIZABLE transaction isolation level, and that should\nprobably be set as the default.\n\n    * If all transactions are run at the SERIALIZABLE transaction\nisolation level, business rules can be enforced in triggers or\napplication code without ever having a need to acquire an explicit\nlock or to use SELECT FOR SHARE or SELECT FOR UPDATE.\n\n    * Those who want to continue to use snapshot isolation without\nthe additional protections of SSI (and the associated costs of\nenforcing those protections), can use the REPEATABLE READ transaction\nisolation level.  This level retains its legacy behavior, which\nis identical to the old SERIALIZABLE implementation and fully\nconsistent with the standard's requirements for the REPEATABLE READ\ntransaction isolation level.\n\n    * Performance under this SSI implementation will be significantly\nimproved if transactions which don't modify permanent tables are\ndeclared to be READ ONLY before they begin reading data.\n\n    * Performance under SSI will tend to degrade more rapidly with a\nlarge number of active database transactions than under less strict\nisolation levels.  Limiting the number of active transactions through\nuse of a connection pool or similar techniques may be necessary to\nmaintain good performance.\n\n    * Any transaction which must be rolled back to prevent\nserialization anomalies will fail with SQLSTATE 40001, which has a\nstandard meaning of \"serialization failure\".\n\n    * This SSI implementation makes an effort to choose the\ntransaction to be canceled such that an immediate retry of the\ntransaction will not fail due to conflicts with exactly the same\ntransactions.  Pursuant to this goal, no transaction is canceled\nuntil one of the other transactions in the set of conflicts which\ncould generate an anomaly has successfully committed.  This is\nconceptually similar to how write conflicts are handled.  To fully\nimplement this guarantee there needs to be a way to roll back the\nactive transaction for another process with a serialization failure\nSQLSTATE, even if it is \"idle in transaction\".\n\n\nPredicate Locking\n-----------------\n\nBoth S2PL and SSI require some form of predicate locking to handle\nsituations where reads conflict with later inserts or with later\nupdates which move data into the selected range.  PostgreSQL didn't\nalready have predicate locking, so it needed to be added to support\nfull serializable transactions under either strategy. Practical\nimplementations of predicate locking generally involve acquiring\nlocks against data as it is accessed, using multiple granularities\n(tuple, page, table, etc.) with escalation as needed to keep the lock\ncount to a number which can be tracked within RAM structures.  This\napproach was used in PostgreSQL.  Coarse granularities can cause some\nfalse positive indications of conflict. The number of false positives\ncan be influenced by plan choice.\n\n\nImplementation overview\n-----------------------\n\nNew RAM structures, inspired by those used to track traditional locks\nin PostgreSQL, but tailored to the needs of SIREAD predicate locking,\nare used.  These refer to physical objects actually accessed in the\ncourse of executing the query, to model the predicates through\ninference.  Anyone interested in this subject should review the\nHellerstein, Stonebraker and Hamilton paper [3], along with the\nlocking papers referenced from that and the Cahill papers.\n\nBecause the SIREAD locks don't block, traditional locking techniques\nhave to be modified.  Intent locking (locking higher level objects\nbefore locking lower level objects) doesn't work with non-blocking\n\"locks\" (which are, in some respects, more like flags than locks).\n\nA configurable amount of shared memory is reserved at postmaster\nstart-up to track predicate locks. This size cannot be changed\nwithout a restart.\n\nTo prevent resource exhaustion, multiple fine-grained locks may\nbe promoted to a single coarser-grained lock as needed.\n\nAn attempt to acquire an SIREAD lock on a tuple when the same\ntransaction already holds an SIREAD lock on the page or the relation\nwill be ignored. Likewise, an attempt to lock a page when the\nrelation is locked will be ignored, and the acquisition of a coarser\nlock will result in the automatic release of all finer-grained locks\nit covers.\n\n\nHeap locking\n------------\n\nPredicate locks will be acquired for the heap based on the following:\n\n    * For a table scan, the entire relation will be locked.\n\n    * Each tuple read which is visible to the reading transaction\nwill be locked, whether or not it meets selection criteria; except\nthat there is no need to acquire an SIREAD lock on a tuple when the\ntransaction already holds a write lock on any tuple representing the\nrow, since a rw-conflict would also create a ww-dependency which\nhas more aggressive enforcement and thus will prevent any anomaly.\n\n    * Modifying a heap tuple creates a rw-conflict with any transaction\nthat holds a SIREAD lock on that tuple, or on the page or relation\nthat contains it.\n\n    * Inserting a new tuple creates a rw-conflict with any transaction\nholding a SIREAD lock on the entire relation. It doesn't conflict with\npage-level locks, because page-level locks are only used to aggregate\ntuple locks. Unlike index page locks, they don't lock \"gaps\" on the page.\n\n\nIndex AM implementations\n------------------------\n\nSince predicate locks only exist to detect writes which conflict with\nearlier reads, and heap tuple locks are acquired to cover all heap\ntuples actually read, including those read through indexes, the index\ntuples which were actually scanned are not of interest in themselves;\nwe only care about their \"new neighbors\" -- later inserts into the\nindex which would have been included in the scan had they existed at\nthe time.  Conceptually, we want to lock the gaps between and\nsurrounding index entries within the scanned range.\n\nCorrectness requires that any insert into an index generates a\nrw-conflict with a concurrent serializable transaction if, after that\ninsert, re-execution of any index scan of the other transaction would\naccess the heap for a row not accessed during the previous execution.\nNote that a non-HOT update which expires an old index entry covered\nby the scan and adds a new entry for the modified row's new tuple\nneed not generate a conflict, although an update which \"moves\" a row\ninto the scan must generate a conflict.  While correctness allows\nfalse positives, they should be minimized for performance reasons.\n\nSeveral optimizations are possible, though not all are implemented yet:\n\n    * An index scan which is just finding the right position for an\nindex insertion or deletion need not acquire a predicate lock.\n\n    * An index scan which is comparing for equality on the entire key\nfor a unique index need not acquire a predicate lock as long as a key\nis found corresponding to a visible tuple which has not been modified\nby another transaction -- there are no \"between or around\" gaps to\ncover.\n\n    * As long as built-in foreign key enforcement continues to use\nits current \"special tricks\" to deal with MVCC issues, predicate\nlocks should not be needed for scans done by enforcement code.\n\n    * If a search determines that no rows can be found regardless of\nindex contents because the search conditions are contradictory (e.g.,\nx = 1 AND x = 2), then no predicate lock is needed.\n\nOther index AM implementation considerations:\n\n    * For an index AM that doesn't have support for predicate locking,\nwe just acquire a predicate lock on the whole index for any search.\n\n    * B-tree index searches acquire predicate locks only on the\nindex *leaf* pages needed to lock the appropriate index range. If,\nhowever, a search discovers that no root page has yet been created, a\npredicate lock on the index relation is required.\n\n    * Like a B-tree, GIN searches acquire predicate locks only on the\nleaf pages of entry tree. When performing an equality scan, and an\nentry has a posting tree, the posting tree root is locked instead, to\nlock only that key value. However, fastupdate=on postpones the\ninsertion of tuples into index structure by temporarily storing them\ninto pending list. That makes us unable to detect r-w conflicts using\npage-level locks. To cope with that, insertions to the pending list\nconflict with all scans.\n\n    * GiST searches can determine that there are no matches at any\nlevel of the index, so we acquire predicate lock at each index\nlevel during a GiST search. An index insert at the leaf level can\nthen be trusted to ripple up to all levels and locations where\nconflicting predicate locks may exist. In case there is a page split,\nwe need to copy predicate lock from the original page to all the new\npages.\n\n    * Hash index searches acquire predicate locks on the primary\npage of a bucket. It acquires a lock on both the old and new buckets\nfor scans that happen concurrently with page splits. During a bucket\nsplit, a predicate lock is copied from the primary page of an old\nbucket to the primary page of a new bucket.\n\n    * The effects of page splits, overflows, consolidations, and\nremovals must be carefully reviewed to ensure that predicate locks\naren't \"lost\" during those operations, or kept with pages which could\nget re-used for different parts of the index.\n\n\nInnovations\n-----------\n\nThe PostgreSQL implementation of Serializable Snapshot Isolation\ndiffers from what is described in the cited papers for several\nreasons:\n\n   1. PostgreSQL didn't have any existing predicate locking. It had\nto be added from scratch.\n\n   2. The existing in-memory lock structures were not suitable for\ntracking SIREAD locks.\n          * In PostgreSQL, tuple level locks are not held in RAM for\nany length of time; lock information is written to the tuples\ninvolved in the transactions.\n          * In PostgreSQL, existing lock structures have pointers to\nmemory which is related to a session. SIREAD locks need to persist\npast the end of the originating transaction and even the session\nwhich ran it.\n          * PostgreSQL needs to be able to tolerate a large number of\ntransactions executing while one long-running transaction stays open\n-- the in-RAM techniques discussed in the papers wouldn't support\nthat.\n\n   3. Unlike the database products used for the prototypes described\nin the papers, PostgreSQL didn't already have a true serializable\nisolation level distinct from snapshot isolation.\n\n   4. PostgreSQL supports subtransactions -- an issue not mentioned\nin the papers.\n\n   5. PostgreSQL doesn't assign a transaction number to a database\ntransaction until and unless necessary (normally, when the transaction\nattempts to modify data).\n\n   6. PostgreSQL has pluggable data types with user-definable\noperators, as well as pluggable index types, not all of which are\nbased around data types which support ordering.\n\n   7. Some possible optimizations became apparent during development\nand testing.\n\nDifferences from the implementation described in the papers are\nlisted below.\n\n    * New structures needed to be created in shared memory to track\nthe proper information for serializable transactions and their SIREAD\nlocks.\n\n    * Because PostgreSQL does not have the same concept of an \"oldest\ntransaction ID\" for all serializable transactions as assumed in the\nCahill thesis, we track the oldest snapshot xmin among serializable\ntransactions, and a count of how many active transactions use that\nxmin. When the count hits zero we find the new oldest xmin and run a\nclean-up based on that.\n\n    * Because reads in a subtransaction may cause that subtransaction\nto roll back, thereby affecting what is written by the top level\ntransaction, predicate locks must survive a subtransaction rollback.\nAs a consequence, all xid usage in SSI, including predicate locking,\nis based on the top level xid.  When looking at an xid that comes\nfrom a tuple's xmin or xmax, for example, we always call\nSubTransGetTopmostTransaction() before doing much else with it.\n\n    * PostgreSQL does not use \"update in place\" with a rollback log\nfor its MVCC implementation.  Where possible it uses \"HOT\" updates on\nthe same page (if there is room and no indexed value is changed).\nFor non-HOT updates the old tuple is expired in place and a new tuple\nis inserted at a new location.  Because of this difference, a tuple\nlock in PostgreSQL doesn't automatically lock any other versions of a\nrow.  We don't try to copy or expand a tuple lock to any other\nversions of the row, based on the following proof that any additional\nserialization failures we would get from that would be false\npositives:\n\n          o If transaction T1 reads a row version (thus acquiring a\npredicate lock on it) and a second transaction T2 updates that row\nversion (thus creating a rw-conflict graph edge from T1 to T2), must a\nthird transaction T3 which re-updates the new version of the row also\nhave a rw-conflict in from T1 to prevent anomalies?  In other words,\ndoes it matter whether we recognize the edge T1 -> T3?\n\n          o If T1 has a conflict in, it certainly doesn't. Adding the\nedge T1 -> T3 would create a dangerous structure, but we already had\none from the edge T1 -> T2, so we would have aborted something anyway.\n(T2 has already committed, else T3 could not have updated its output;\nbut we would have aborted either T1 or T1's predecessor(s).  Hence\nno cycle involving T1 and T3 can survive.)\n\n          o Now let's consider the case where T1 doesn't have a\nrw-conflict in. If that's the case, for this edge T1 -> T3 to make a\ndifference, T3 must have a rw-conflict out that induces a cycle in the\ndependency graph, i.e. a conflict out to some transaction preceding T1\nin the graph. (A conflict out to T1 itself would be problematic too,\nbut that would mean T1 has a conflict in, the case we already\neliminated.)\n\n          o So now we're trying to figure out if there can be an\nrw-conflict edge T3 -> T0, where T0 is some transaction that precedes\nT1. For T0 to precede T1, there has to be some edge, or sequence of\nedges, from T0 to T1. At least the last edge has to be a wr-dependency\nor ww-dependency rather than a rw-conflict, because T1 doesn't have a\nrw-conflict in. And that gives us enough information about the order\nof transactions to see that T3 can't have a rw-conflict to T0:\n - T0 committed before T1 started (the wr/ww-dependency implies this)\n - T1 started before T2 committed (the T1->T2 rw-conflict implies this)\n - T2 committed before T3 started (otherwise, T3 would get aborted\n                                   because of an update conflict)\n\n          o That means T0 committed before T3 started, and therefore\nthere can't be a rw-conflict from T3 to T0.\n\n          o So in all cases, we don't need the T1 -> T3 edge to\nrecognize cycles.  Therefore it's not necessary for T1's SIREAD lock\non the original tuple version to cover later versions as well.\n\n    * Predicate locking in PostgreSQL starts at the tuple level\nwhen possible. Multiple fine-grained locks are promoted to a single\ncoarser-granularity lock as needed to avoid resource exhaustion.  The\namount of memory used for these structures is configurable, to balance\nRAM usage against SIREAD lock granularity.\n\n    * Each backend keeps a process-local table of the locks it holds.\nTo support granularity promotion decisions with low CPU and locking\noverhead, this table also includes the coarser covering locks and the\nnumber of finer-granularity locks they cover.\n\n    * Conflicts are identified by looking for predicate locks\nwhen tuples are written, and by looking at the MVCC information when\ntuples are read. There is no matching between two RAM-based locks.\n\n    * Because write locks are stored in the heap tuples rather than a\nRAM-based lock table, the optimization described in the Cahill thesis\nwhich eliminates an SIREAD lock where there is a write lock is\nimplemented by the following:\n         1. When checking a heap write for conflicts against existing\npredicate locks, a tuple lock on the tuple being written is removed.\n         2. When acquiring a predicate lock on a heap tuple, we\nreturn quickly without doing anything if it is a tuple written by the\nreading transaction.\n\n    * Rather than using conflictIn and conflictOut pointers which use\nNULL to indicate no conflict and a self-reference to indicate\nmultiple conflicts or conflicts with committed transactions, we use a\nlist of rw-conflicts. With the more complete information, false\npositives are reduced and we have sufficient data for more aggressive\nclean-up and other optimizations:\n\n          o We can avoid ever rolling back a transaction until and\nunless there is a pivot where a transaction on the conflict *out*\nside of the pivot committed before either of the other transactions.\n\n          o We can avoid ever rolling back a transaction when the\ntransaction on the conflict *in* side of the pivot is explicitly or\nimplicitly READ ONLY unless the transaction on the conflict *out*\nside of the pivot committed before the READ ONLY transaction acquired\nits snapshot. (An implicit READ ONLY transaction is one which\ncommitted without writing, even though it was not explicitly declared\nto be READ ONLY.)\n\n          o We can more aggressively clean up conflicts, predicate\nlocks, and SSI transaction information.\n\n    * We allow a READ ONLY transaction to \"opt out\" of SSI if there are\nno READ WRITE transactions which could cause the READ ONLY\ntransaction to ever become part of a \"dangerous structure\" of\noverlapping transaction dependencies.\n\n    * We allow the user to request that a READ ONLY transaction wait\nuntil the conditions are right for it to start in the \"opt out\" state\ndescribed above. We add a DEFERRABLE state to transactions, which is\nspecified and maintained in a way similar to READ ONLY. It is\nignored for transactions that are not SERIALIZABLE and READ ONLY.\n\n    * When a transaction must be rolled back, we pick among the\nactive transactions such that an immediate retry will not fail again\non conflicts with the same transactions.\n\n    * We use the PostgreSQL SLRU system to hold summarized\ninformation about older committed transactions to put an upper bound\non RAM used. Beyond that limit, information spills to disk.\nPerformance can degrade in a pessimal situation, but it should be\ntolerable, and transactions won't need to be canceled or blocked\nfrom starting.\n\n\nR&D Issues\n----------\n\nThis is intended to be the place to record specific issues which need\nmore detailed review or analysis.\n\n    * WAL file replay. While serializable implementations using S2PL\ncan guarantee that the write-ahead log contains commits in a sequence\nconsistent with some serial execution of serializable transactions,\nSSI cannot make that guarantee. While the WAL replay is no less\nconsistent than under snapshot isolation, it is possible that under\nPITR recovery or hot standby a database could reach a readable state\nwhere some transactions appear before other transactions which would\nhave had to precede them to maintain serializable consistency. In\nessence, if we do nothing, WAL replay will be at snapshot isolation\neven for serializable transactions. Is this OK? If not, how do we\naddress it?\n\n    * External replication. Look at how this impacts external\nreplication solutions, like Postgres-R, Slony, pgpool, HS/SR, etc.\nThis is related to the \"WAL file replay\" issue.\n\n    * UNIQUE btree search for equality on all columns. Since a search\nof a UNIQUE index using equality tests on all columns will lock the\nheap tuple if an entry is found, it appears that there is no need to\nget a predicate lock on the index in that case. A predicate lock is\nstill needed for such a search if a matching index entry which points\nto a visible tuple is not found.\n\n    * Minimize touching of shared memory. Should lists in shared\nmemory push entries which have just been returned to the front of the\navailable list, so they will be popped back off soon and some memory\nmight never be touched, or should we keep adding returned items to\nthe end of the available list?\n\n\nReferences\n----------\n\n[1] http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt\nSearch for serial execution to find the relevant section.\n\n[2] A. Fekete et al. Making Snapshot Isolation Serializable. In ACM\nTransactions on Database Systems 30:2, Jun. 2005.\nhttp://dx.doi.org/10.1145/1071610.1071615\n\n[3] Joseph M. Hellerstein, Michael Stonebraker and James Hamilton. 2007.\nArchitecture of a Database System. Foundations and Trends(R) in\nDatabases Vol. 1, No. 2 (2007) 141-259.\nhttp://db.cs.berkeley.edu/papers/fntdb07-architecture.pdf\n  Of particular interest:\n    * 6.1 A Note on ACID\n    * 6.2 A Brief Review of Serializability\n    * 6.3 Locking and Latching\n    * 6.3.1 Transaction Isolation Levels\n    * 6.5.3 Next-Key Locking: Physical Surrogates for Logical Properties",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\storage\\lmgr\\README-SSI",
      "directory": "backend\\storage\\lmgr"
    }
  },
  {
    "title": "README: backend\\storage\\lmgr",
    "url": "backend\\storage\\lmgr\\README.barrier",
    "content": "Memory Barriers\n===============\n\nModern CPUs make extensive use of pipe-lining and out-of-order execution,\nmeaning that the CPU is often executing more than one instruction at a\ntime, and not necessarily in the order that the source code would suggest.\nFurthermore, even before the CPU gets a chance to reorder operations, the\ncompiler may (and often does) reorganize the code for greater efficiency,\nparticularly at higher optimization levels.  Optimizing compilers and\nout-of-order execution are both critical for good performance, but they\ncan lead to surprising results when multiple processes access the same\nmemory space.\n\nExample\n=======\n\nSuppose x is a pointer to a structure stored in shared memory, and that the\nentire structure has been initialized to zero bytes.  One backend executes\nthe following code fragment:\n\n    x->foo = 1;\n    x->bar = 1;\n\nMeanwhile, at approximately the same time, another backend executes this\ncode fragment:\n\n    bar = x->bar;\n    foo = x->foo;\n\nThe second backend might end up with foo = 1 and bar = 1 (if it executes\nboth statements after the first backend), or with foo = 0 and bar = 0 (if\nit executes both statements before the first backend), or with foo = 1 and\nbar = 0 (if the first backend executes the first statement, the second\nbackend executes both statements, and then the first backend executes the\nsecond statement).\n\nSurprisingly, however, the second backend could also end up with foo = 0\nand bar = 1.  The compiler might swap the order of the two stores performed\nby the first backend, or the two loads performed by the second backend.\nEven if it doesn't, on a machine with weak memory ordering (such as PowerPC\nor ARM) the CPU might choose to execute either the loads or the stores\nout of order.  This surprising result can lead to bugs.\n\nA common pattern where this actually does result in a bug is when adding items\nonto a queue.  The writer does this:\n\n    q->items[q->num_items] = new_item;\n    ++q->num_items;\n\nThe reader does this:\n\n    num_items = q->num_items;\n    for (i = 0; i < num_items; ++i)\n        /* do something with q->items[i] */\n\nThis code turns out to be unsafe, because the writer might increment\nq->num_items before it finishes storing the new item into the appropriate slot.\nMore subtly, the reader might prefetch the contents of the q->items array\nbefore reading q->num_items.  Thus, there's still a bug here *even if the\nwriter does everything in the order we expect*.  We need the writer to update\nthe array before bumping the item counter, and the reader to examine the item\ncounter before examining the array.\n\nNote that these types of highly counterintuitive bugs can *only* occur when\nmultiple processes are interacting with the same memory segment.  A given\nprocess always perceives its *own* writes to memory in program order.\n\nAvoiding Memory Ordering Bugs\n=============================\n\nThe simplest (and often best) way to avoid memory ordering bugs is to\nprotect the data structures involved with an lwlock.  For more details, see\nsrc/backend/storage/lmgr/README.  For instance, in the above example, the\nwriter could acquire an lwlock in exclusive mode before appending to the\nqueue, and each reader could acquire the same lock in shared mode before\nreading it.  If the data structure is not heavily trafficked, this solution is\ngenerally entirely adequate.\n\nHowever, in some cases, it is desirable to avoid the overhead of acquiring\nand releasing locks.  In this case, memory barriers may be used to ensure\nthat the apparent order of execution is as the programmer desires.   In\nPostgreSQL backend code, the pg_memory_barrier() macro may be used to achieve\nthis result.  In the example above, we can prevent the reader from seeing a\ngarbage value by having the writer do this:\n\n    q->items[q->num_items] = new_item;\n    pg_memory_barrier();\n    ++q->num_items;\n\nAnd by having the reader do this:\n\n    num_items = q->num_items;\n    pg_memory_barrier();\n    for (i = 0; i < num_items; ++i)\n        /* do something with q->items[i] */\n\nThe pg_memory_barrier() macro will (1) prevent the compiler from rearranging\nthe code in such a way as to allow the memory accesses to occur out of order\nand (2) generate any code (often, inline assembly) that is needed to prevent\nthe CPU from executing the memory accesses out of order.  Specifically, the\nbarrier prevents loads and stores written after the barrier from being\nperformed before the barrier, and vice-versa.\n\nAlthough this code will work, it is needlessly inefficient.  On systems with\nstrong memory ordering (such as x86), the CPU never reorders loads with other\nloads, nor stores with other stores.  It can, however, allow a load to be\nperformed before a subsequent store.  To avoid emitting unnecessary memory\ninstructions, we provide two additional primitives: pg_read_barrier(), and\npg_write_barrier().  When a memory barrier is being used to separate two\nloads, use pg_read_barrier(); when it is separating two stores, use\npg_write_barrier(); when it is a separating a load and a store (in either\norder), use pg_memory_barrier().  pg_memory_barrier() can always substitute\nfor either a read or a write barrier, but is typically more expensive, and\ntherefore should be used only when needed.\n\nWith these guidelines in mind, the writer can do this:\n\n    q->items[q->num_items] = new_item;\n    pg_write_barrier();\n    ++q->num_items;\n\nAnd the reader can do this:\n\n    num_items = q->num_items;\n    pg_read_barrier();\n    for (i = 0; i < num_items; ++i)\n        /* do something with q->items[i] */\n\nOn machines with strong memory ordering, these weaker barriers will simply\nprevent compiler rearrangement, without emitting any actual machine code.\nOn machines with weak memory ordering, they will prevent compiler\nreordering and also emit whatever hardware barrier may be required.  Even\non machines with weak memory ordering, a read or write barrier may be able\nto use a less expensive instruction than a full barrier.\n\nWeaknesses of Memory Barriers\n=============================\n\nWhile memory barriers are a powerful tool, and much cheaper than locks, they\nare also much less capable than locks.  Here are some of the problems.\n\n1. Concurrent writers are unsafe.  In the above example of a queue, using\nmemory barriers doesn't make it safe for two processes to add items to the\nsame queue at the same time.  If more than one process can write to the queue,\na spinlock or lwlock must be used to synchronize access. The readers can\nperhaps proceed without any lock, but the writers may not.\n\nEven very simple write operations often require additional synchronization.\nFor example, it's not safe for multiple writers to simultaneously execute\nthis code (supposing x is a pointer into shared memory):\n\n    x->foo++;\n\nAlthough this may compile down to a single machine-language instruction,\nthe CPU will execute that instruction by reading the current value of foo,\nadding one to it, and then storing the result back to the original address.\nIf two CPUs try to do this simultaneously, both may do their reads before\neither one does their writes.  Such a case could be made safe by using an\natomic variable and an atomic add.  See port/atomics.h.\n\n2. Eight-byte loads and stores aren't necessarily atomic.  We assume in\nvarious places in the source code that an aligned four-byte load or store is\natomic, and that other processes therefore won't see a half-set value.\nSadly, the same can't be said for eight-byte value: on some platforms, an\naligned eight-byte load or store will generate two four-byte operations.  If\nyou need an atomic eight-byte read or write, you must either serialize access\nwith a lock or use an atomic variable.\n\n3. No ordering guarantees.  While memory barriers ensure that any given\nprocess performs loads and stores to shared memory in order, they don't\nguarantee synchronization.  In the queue example above, we can use memory\nbarriers to be sure that readers won't see garbage, but there's nothing to\nsay whether a given reader will run before or after a given writer.  If this\nmatters in a given situation, some other mechanism must be used instead of\nor in addition to memory barriers.\n\n4. Barrier proliferation.  Many algorithms that at first seem appealing\nrequire multiple barriers.  If the number of barriers required is more than\none or two, you may be better off just using a lock.  Keep in mind that, on\nsome platforms, a barrier may be implemented by acquiring and releasing a\nbackend-private spinlock.  This may be better than a centralized lock under\ncontention, but it may also be slower in the uncontended case.\n\nFurther Reading\n===============\n\nMuch of the documentation about memory barriers appears to be quite\nLinux-specific.  The following papers may be helpful:\n\nMemory Ordering in Modern Microprocessors, by Paul E. McKenney\n* http://www.rdrop.com/users/paulmck/scalability/paper/ordering.2007.09.19a.pdf\n\nMemory Barriers: a Hardware View for Software Hackers, by Paul E. McKenney\n* http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.06.07c.pdf\n\nThe Linux kernel also has some useful documentation on this topic.  Start\nwith Documentation/memory-barriers.txt",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\storage\\lmgr\\README.barrier",
      "directory": "backend\\storage\\lmgr"
    }
  },
  {
    "title": "README: backend\\storage\\page",
    "url": "backend\\storage\\page\\README",
    "content": "src/backend/storage/page/README\n\nChecksums\n---------\n\nChecksums on data pages are designed to detect corruption by the I/O system.\nWe do not protect buffers against uncorrectable memory errors, since these\nhave a very low measured incidence according to research on large server farms,\nhttp://www.cs.toronto.edu/~bianca/papers/sigmetrics09.pdf, discussed\n2010/12/22 on -hackers list.\n\nCurrent implementation requires this be enabled system-wide at initdb time, or\nby using the pg_checksums tool on an offline cluster.\n\nThe checksum is not valid at all times on a data page!!\nThe checksum is valid when the page leaves the shared pool and is checked\nwhen it later re-enters the shared pool as a result of I/O.\nWe set the checksum on a buffer in the shared pool immediately before we\nflush the buffer. As a result we implicitly invalidate the page's checksum\nwhen we modify the page for a data change or even a hint. This means that\nmany or even most pages in shared buffers have invalid page checksums,\nso be careful how you interpret the pd_checksum field.\n\nThat means that WAL-logged changes to a page do NOT update the page checksum,\nso full page images may not have a valid checksum. But those page images have\nthe WAL CRC covering them and so are verified separately from this\nmechanism. WAL replay should not test the checksum of a full-page image.\n\nThe best way to understand this is that WAL CRCs protect records entering the\nWAL stream, and data page verification protects blocks entering the shared\nbuffer pool. They are similar in purpose, yet completely separate.  Together\nthey ensure we are able to detect errors in data re-entering\nPostgreSQL-controlled memory. Note also that the WAL checksum is a 32-bit CRC,\nwhereas the page checksum is only 16-bits.\n\nAny write of a data block can cause a torn page if the write is unsuccessful.\nFull page writes protect us from that, which are stored in WAL.  Setting hint\nbits when a page is already dirty is OK because a full page write must already\nhave been written for it since the last checkpoint.  Setting hint bits on an\notherwise clean page can allow torn pages; this doesn't normally matter since\nthey are just hints, but when the page has checksums, then losing a few bits\nwould cause the checksum to be invalid.  So if we have full_page_writes = on\nand checksums enabled then we must write a WAL record specifically so that we\nrecord a full page image in WAL.  Hint bits updates should be protected using\nMarkBufferDirtyHint(), which is responsible for writing the full-page image\nwhen necessary.\n\nNote that when we write a page checksum we include the hopefully zeroed bytes\nthat form the hole in the centre of a standard page. Thus, when we read the\nblock back from storage we implicitly check that the hole is still all zeroes.\nWe do this to ensure that we spot errors that could have destroyed data even\nif they haven't actually done so. Full page images stored in WAL do *not*\ncheck that the hole is all zero; the data in the hole is simply skipped and\nre-zeroed if the backup block is reapplied. We do this because a failure in\nWAL is a fatal error and prevents further recovery, whereas a checksum failure\non a normal data block is a hard error but not a critical one for the server,\neven if it is a very bad thing for the user.\n\nNew WAL records cannot be written during recovery, so hint bits set during\nrecovery must not dirty the page if the buffer is not already dirty, when\nchecksums are enabled.  Systems in Hot-Standby mode may benefit from hint bits\nbeing set, but with checksums enabled, a page cannot be dirtied after setting a\nhint bit (due to the torn page risk). So, it must wait for full-page images\ncontaining the hint bit updates to arrive from the primary.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\storage\\page\\README",
      "directory": "backend\\storage\\page"
    }
  },
  {
    "title": "README: backend\\storage\\smgr",
    "url": "backend\\storage\\smgr\\README",
    "content": "src/backend/storage/smgr/README\n\nStorage Managers\n================\n\nIn the original Berkeley Postgres system, there were several storage managers,\nof which only the \"magnetic disk\" manager remains.  (At Berkeley there were\nalso managers for the Sony WORM optical disk jukebox and persistent main\nmemory, but these were never supported in any externally released Postgres,\nnor in any version of PostgreSQL.)  The \"magnetic disk\" manager is itself\nseriously misnamed, because actually it supports any kind of device for\nwhich the operating system provides standard filesystem operations; which\nthese days is pretty much everything of interest.  However, we retain the\nnotion of a storage manager switch in case anyone ever wants to reintroduce\nother kinds of storage managers.  Removing the switch layer would save\nnothing noticeable anyway, since storage-access operations are surely far\nmore expensive than one extra layer of C function calls.\n\nIn Berkeley Postgres each relation was tagged with the ID of the storage\nmanager to use for it.  This is gone.  It would be probably more reasonable\nto associate storage managers with tablespaces, should we ever re-introduce\nmultiple storage managers into the system catalogs.\n\nThe files in this directory, and their contents, are\n\n    smgr.c\tThe storage manager switch dispatch code.  The routines in\n\t\tthis file call the appropriate storage manager to do storage\n\t\taccesses requested by higher-level code.  smgr.c also manages\n\t\tthe file handle cache (SMgrRelation table).\n\n    md.c\tThe \"magnetic disk\" storage manager, which is really just\n\t\tan interface to the kernel's filesystem operations.\n\nNote that md.c in turn relies on src/backend/storage/file/fd.c.\n\n\nRelation Forks\n==============\n\nSince 8.4, a single smgr relation can be comprised of multiple physical\nfiles, called relation forks. This allows storing additional metadata like\nFree Space information in additional forks, which can be grown and truncated\nindependently of the main data file, while still treating it all as a single\nphysical relation in system catalogs.\n\nIt is assumed that the main fork, fork number 0 or MAIN_FORKNUM, always\nexists. Fork numbers are assigned in src/include/common/relpath.h.\nFunctions in smgr.c and md.c take an extra fork number argument, in addition\nto relfilelocator and block number, to identify which relation fork you want to\naccess. Since most code wants to access the main fork, a shortcut version of\nReadBuffer that accesses MAIN_FORKNUM is provided in the buffer manager for\nconvenience.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\storage\\smgr\\README",
      "directory": "backend\\storage\\smgr"
    }
  },
  {
    "title": "README: backend\\utils\\fmgr",
    "url": "backend\\utils\\fmgr\\README",
    "content": "src/backend/utils/fmgr/README\n\nFunction Manager\n================\n\n[This file originally explained the transition from the V0 to the V1\ninterface.  Now it just explains some internals and rationale for the V1\ninterface, while the V0 interface has been removed.]\n\nThe V1 Function-Manager Interface\n---------------------------------\n\nThe core of the design is data structures for representing the result of a\nfunction lookup and for representing the parameters passed to a specific\nfunction invocation.  (We want to keep function lookup separate from\nfunction call, since many parts of the system apply the same function over\nand over; the lookup overhead should be paid once per query, not once per\ntuple.)\n\n\nWhen a function is looked up in pg_proc, the result is represented as\n\ntypedef struct\n{\n    PGFunction  fn_addr;    /* pointer to function or handler to be called */\n    Oid         fn_oid;     /* OID of function (NOT of handler, if any) */\n    short       fn_nargs;   /* number of input args (0..FUNC_MAX_ARGS) */\n    bool        fn_strict;  /* function is \"strict\" (NULL in => NULL out) */\n    bool        fn_retset;  /* function returns a set (over multiple calls) */\n    unsigned char fn_stats; /* collect stats if track_functions > this */\n    void       *fn_extra;   /* extra space for use by handler */\n    MemoryContext fn_mcxt;  /* memory context to store fn_extra in */\n    Node       *fn_expr;    /* expression parse tree for call, or NULL */\n} FmgrInfo;\n\nFor an ordinary built-in function, fn_addr is just the address of the C\nroutine that implements the function.  Otherwise it is the address of a\nhandler for the class of functions that includes the target function.\nThe handler can use the function OID and perhaps also the fn_extra slot\nto find the specific code to execute.  (fn_oid = InvalidOid can be used\nto denote a not-yet-initialized FmgrInfo struct.  fn_extra will always\nbe NULL when an FmgrInfo is first filled by the function lookup code, but\na function handler could set it to avoid making repeated lookups of its\nown when the same FmgrInfo is used repeatedly during a query.)  fn_nargs\nis the number of arguments expected by the function, fn_strict is its\nstrictness flag, and fn_retset shows whether it returns a set; all of\nthese values come from the function's pg_proc entry.  fn_stats is also\nset up to control whether or not to track runtime statistics for calling\nthis function.\n\nIf the function is being called as part of a SQL expression, fn_expr will\npoint to the expression parse tree for the function call; this can be used\nto extract parse-time knowledge about the actual arguments.  Note that this\nfield really is information about the arguments rather than information\nabout the function, but it's proven to be more convenient to keep it in\nFmgrInfo than in FunctionCallInfoBaseData where it might more logically go.\n\n\nDuring a call of a function, the following data structure is created\nand passed to the function:\n\ntypedef struct\n{\n    FmgrInfo   *flinfo;         /* ptr to lookup info used for this call */\n    Node       *context;        /* pass info about context of call */\n    Node       *resultinfo;     /* pass or return extra info about result */\n    Oid         fncollation;    /* collation for function to use */\n    bool        isnull;         /* function must set true if result is NULL */\n    short       nargs;          /* # arguments actually passed */\n    NullableDatum args[];       /* Arguments passed to function */\n} FunctionCallInfoBaseData;\ntypedef FunctionCallInfoBaseData* FunctionCallInfo;\n\nflinfo points to the lookup info used to make the call.  Ordinary functions\nwill probably ignore this field, but function class handlers will need it\nto find out the OID of the specific function being called.\n\ncontext is NULL for an \"ordinary\" function call, but may point to additional\ninfo when the function is called in certain contexts.  (For example, the\ntrigger manager will pass information about the current trigger event here.)\nFurther details appear in \"Function Call Contexts\" below.\n\nresultinfo is NULL when calling any function from which a simple Datum\nresult is expected.  It may point to some subtype of Node if the function\nreturns more than a Datum.  (For example, resultinfo is used when calling a\nfunction that returns a set, as discussed below.)  Like the context field,\nresultinfo is a hook for expansion; fmgr itself doesn't constrain the use\nof the field.\n\nfncollation is the input collation derived by the parser, or InvalidOid\nwhen there are no inputs of collatable types or they don't share a common\ncollation.  This is effectively a hidden additional argument, which\ncollation-sensitive functions can use to determine their behavior.\n\nnargs and args[] hold the arguments being passed to the function.\nNotice that all the arguments passed to a function (as well as its result\nvalue) will now uniformly be of type Datum.  As discussed below, callers\nand callees should apply the standard Datum-to-and-from-whatever macros\nto convert to the actual argument types of a particular function.  The\nvalue in args[i].value is unspecified when args[i].isnull is true.\n\nIt is generally the responsibility of the caller to ensure that the\nnumber of arguments passed matches what the callee is expecting; except\nfor callees that take a variable number of arguments, the callee will\ntypically ignore the nargs field and just grab values from args[].\n\nThe isnull field will be initialized to \"false\" before the call.  On\nreturn from the function, isnull is the null flag for the function result:\nif it is true the function's result is NULL, regardless of the actual\nfunction return value.  Note that simple \"strict\" functions can ignore\nboth isnull and args[i].isnull, since they won't even get called when there\nare any TRUE values in args[].isnull.\n\nFunctionCallInfo replaces FmgrValues plus a bunch of ad-hoc parameter\nconventions, global variables (fmgr_pl_finfo and CurrentTriggerData at\nleast), and other uglinesses.\n\n\nCallees, whether they be individual functions or function handlers,\nshall always have this signature:\n\nDatum function (FunctionCallInfo fcinfo);\n\nwhich is represented by the typedef\n\ntypedef Datum (*PGFunction) (FunctionCallInfo fcinfo);\n\nThe function is responsible for setting fcinfo->isnull appropriately\nas well as returning a result represented as a Datum.  Note that since\nall callees will now have exactly the same signature, and will be called\nthrough a function pointer declared with exactly that signature, we\nshould have no portability or optimization problems.\n\n\nFunction Coding Conventions\n---------------------------\n\nHere are the proposed macros and coding conventions:\n\nThe definition of an fmgr-callable function will always look like\n\nDatum\nfunction_name(PG_FUNCTION_ARGS)\n{\n\t...\n}\n\n\"PG_FUNCTION_ARGS\" just expands to \"FunctionCallInfo fcinfo\".  The main\nreason for using this macro is to make it easy for scripts to spot function\ndefinitions.  However, if we ever decide to change the calling convention\nagain, it might come in handy to have this macro in place.\n\nA nonstrict function is responsible for checking whether each individual\nargument is null or not, which it can do with PG_ARGISNULL(n) (which is\njust \"fcinfo->args[n].isnull\").  It should avoid trying to fetch the value\nof any argument that is null.\n\nBoth strict and nonstrict functions can return NULL, if needed, with\n\tPG_RETURN_NULL();\nwhich expands to\n\t{ fcinfo->isnull = true; return (Datum) 0; }\n\nArgument values are ordinarily fetched using code like\n\tint32\tname = PG_GETARG_INT32(number);\n\nFor float4, float8, and int8, the PG_GETARG macros will hide whether the\ntypes are pass-by-value or pass-by-reference.  For example, if float8 is\npass-by-reference then PG_GETARG_FLOAT8 expands to\n\t(* (float8 *) DatumGetPointer(fcinfo->args[number].value))\nand would typically be called like this:\n\tfloat8  arg = PG_GETARG_FLOAT8(0);\nFor what are now historical reasons, the float-related typedefs and macros\nexpress the type width in bytes (4 or 8), whereas we prefer to label the\nwidths of integer types in bits.\n\nNon-null values are returned with a PG_RETURN_XXX macro of the appropriate\ntype.  For example, PG_RETURN_INT32 expands to\n\treturn Int32GetDatum(x)\nPG_RETURN_FLOAT4, PG_RETURN_FLOAT8, and PG_RETURN_INT64 hide whether their\ndata types are pass-by-value or pass-by-reference, by doing a palloc if\nneeded.\n\nfmgr.h will provide PG_GETARG and PG_RETURN macros for all the basic data\ntypes.  Modules or header files that define specialized SQL datatypes\n(eg, timestamp) should define appropriate macros for those types, so that\nfunctions manipulating the types can be coded in the standard style.\n\nFor non-primitive data types (particularly variable-length types) it won't\nbe very practical to hide the pass-by-reference nature of the data type,\nso the PG_GETARG and PG_RETURN macros for those types won't do much more\nthan DatumGetPointer/PointerGetDatum plus the appropriate typecast (but see\nTOAST discussion, below).  Functions returning such types will need to\npalloc() their result space explicitly.  I recommend naming the GETARG and\nRETURN macros for such types to end in \"_P\", as a reminder that they\nproduce or take a pointer.  For example, PG_GETARG_TEXT_P yields \"text *\".\n\nWhen a function needs to access fcinfo->flinfo or one of the other auxiliary\nfields of FunctionCallInfo, it should just do it.  I doubt that providing\nsyntactic-sugar macros for these cases is useful.\n\n\nSupport for TOAST-Able Data Types\n---------------------------------\n\nFor TOAST-able data types, the PG_GETARG macro will deliver a de-TOASTed\ndata value.  There might be a few cases where the still-toasted value is\nwanted, but the vast majority of cases want the de-toasted result, so\nthat will be the default.  To get the argument value without causing\nde-toasting, use PG_GETARG_RAW_VARLENA_P(n).\n\nSome functions require a modifiable copy of their input values.  In these\ncases, it's silly to do an extra copy step if we copied the data anyway\nto de-TOAST it.  Therefore, each toastable datatype has an additional\nfetch macro, for example PG_GETARG_TEXT_P_COPY(n), which delivers a\nguaranteed-fresh copy, combining this with the detoasting step if possible.\n\nThere is also a PG_FREE_IF_COPY(ptr,n) macro, which pfree's the given\npointer if and only if it is different from the original value of the n'th\nargument.  This can be used to free the de-toasted value of the n'th\nargument, if it was actually de-toasted.  Currently, doing this is not\nnecessary for the majority of functions because the core backend code\nreleases temporary space periodically, so that memory leaked in function\nexecution isn't a big problem.  However, as of 7.1 memory leaks in\nfunctions that are called by index searches will not be cleaned up until\nend of transaction.  Therefore, functions that are listed in pg_amop or\npg_amproc should be careful not to leak detoasted copies, and so these\nfunctions do need to use PG_FREE_IF_COPY() for toastable inputs.\n\nA function should never try to re-TOAST its result value; it should just\ndeliver an untoasted result that's been palloc'd in the current memory\ncontext.  When and if the value is actually stored into a tuple, the\ntuple toaster will decide whether toasting is needed.\n\n\nFunction Call Contexts\n----------------------\n\nIf a caller passes a non-NULL pointer in fcinfo->context, it should point\nto some subtype of Node; the particular kind of context is indicated by the\nnode type field.  (A callee should always check the node type, via IsA(),\nbefore assuming it knows what kind of context is being passed.)  fmgr\nitself puts no other restrictions on the use of this field.\n\nCurrent uses of this convention include:\n\n* Trigger functions are passed an instance of struct TriggerData,\ncontaining information about the trigger context.  (The trigger function\ndoes not receive any normal arguments.)  See commands/trigger.h for\nmore information and macros that are commonly used by trigger functions.\n\n* Aggregate functions (or to be precise, their transition and final\nfunctions) are passed an instance of struct AggState, that is the executor\nstate node for the calling Agg plan node; or if they are called as window\nfunctions, they receive an instance of struct WindowAggState.  It is\nrecommended that these pointers be used only via AggCheckCallContext()\nand sibling functions, which are declared in fmgr.h but are documented\nonly with their source code in src/backend/executor/nodeAgg.c.  Typically\nthese context nodes are only of interest when the transition and final\nfunctions wish to optimize execution based on knowing that they are being\nused within an aggregate rather than as standalone SQL functions.\n\n* True window functions receive an instance of struct WindowObject.\n(Like trigger functions, they don't receive any normal arguments.)\nSee windowapi.h for more information.\n\n* Procedures are passed an instance of struct CallContext, containing\ninformation about the context of the CALL statement, particularly\nwhether it is within an \"atomic\" execution context.\n\n* Some callers of datatype input functions (and in future perhaps\nother classes of functions) pass an instance of ErrorSaveContext.\nThis indicates that the caller wishes to handle \"soft\" errors without\na transaction-terminating exception being thrown: instead, the callee\nshould store information about the error cause in the ErrorSaveContext\nstruct and return a dummy result value.  Further details appear in\n\"Handling Soft Errors\" below.\n\n\nHandling Soft Errors\n--------------------\n\nPostgres' standard mechanism for reporting errors (ereport() or elog())\nis used for all sorts of error conditions.  This means that throwing\nan exception via ereport(ERROR) requires an expensive transaction or\nsubtransaction abort and cleanup, since the exception catcher dare not\nmake many assumptions about what has gone wrong.  There are situations\nwhere we would rather have a lighter-weight mechanism for dealing\nwith errors that are known to be safe to recover from without a full\ntransaction cleanup.  SQL-callable functions can support this need\nusing the ErrorSaveContext context mechanism.\n\nTo report a \"soft\" error, a SQL-callable function should call\n\terrsave(fcinfo->context, ...)\nwhere it would previously have done\n\tereport(ERROR, ...)\nIf the passed \"context\" is NULL or is not an ErrorSaveContext node,\nthen errsave behaves precisely as ereport(ERROR): the exception is\nthrown via longjmp, so that control does not return.  If \"context\"\nis an ErrorSaveContext node, then the error information included in\nerrsave's subsidiary reporting calls is stored into the context node\nand control returns from errsave normally.  The function should then\nreturn a dummy value to its caller.  (SQL NULL is recommendable as\nthe dummy value; but anything will do, since the caller is expected\nto ignore the function's return value once it sees that an error has\nbeen reported in the ErrorSaveContext node.)\n\nIf there is nothing to do except return after calling errsave(),\nyou can save a line or two by writing\n\tereturn(fcinfo->context, dummy_value, ...)\nto perform errsave() and then \"return dummy_value\".\n\nAn error reported \"softly\" must be safe, in the sense that there is\nno question about our ability to continue normal processing of the\ntransaction.  Error conditions that should NOT be handled this way\ninclude out-of-memory, unexpected internal errors, or anything that\ncannot easily be cleaned up after.  Such cases should still be thrown\nwith ereport, as they have been in the past.\n\nConsidering datatype input functions as examples, typical \"soft\" error\nconditions include input syntax errors and out-of-range values.  An\ninput function typically detects such cases with simple if-tests and\ncan easily change the ensuing ereport call to an errsave or ereturn.\nBecause of this restriction, it's typically not necessary to pass\nthe ErrorSaveContext pointer down very far, as errors reported by\nlow-level functions are typically reasonable to consider internal.\n(Another way to frame the distinction is that input functions should\nreport all invalid-input conditions softly, but internal problems are\nhard errors.)\n\nBecause no transaction cleanup will occur, a function that is exiting\nafter errsave() returns will bear responsibility for resource cleanup.\nIt is not necessary to be concerned about small leakages of palloc'd\nmemory, since the caller should be running the function in a short-lived\nmemory context.  However, resources such as locks, open files, or buffer\npins must be closed out cleanly, as they would be in the non-error code\npath.\n\nConventions for callers that use the ErrorSaveContext mechanism\nto trap errors are discussed with the declaration of that struct,\nin nodes/miscnodes.h.\n\n\nFunctions Accepting or Returning Sets\n-------------------------------------\n\nIf a function is marked in pg_proc as returning a set, then it is called\nwith fcinfo->resultinfo pointing to a node of type ReturnSetInfo.  A\nfunction that desires to return a set should raise an error \"called in\ncontext that does not accept a set result\" if resultinfo is NULL or does\nnot point to a ReturnSetInfo node.\n\nThere are currently two modes in which a function can return a set result:\nvalue-per-call, or materialize.  In value-per-call mode, the function returns\none value each time it is called, and finally reports \"done\" when it has no\nmore values to return.  In materialize mode, the function's output set is\ninstantiated in a Tuplestore object; all the values are returned in one call.\nAdditional modes might be added in future.\n\nReturnSetInfo contains a field \"allowedModes\" which is set (by the caller)\nto a bitmask that's the OR of the modes the caller can support.  The actual\nmode used by the function is returned in another field \"returnMode\".  For\nbackwards-compatibility reasons, returnMode is initialized to value-per-call\nand need only be changed if the function wants to use a different mode.\nThe function should ereport() if it cannot use any of the modes the caller is\nwilling to support.\n\nValue-per-call mode works like this: ReturnSetInfo contains a field\n\"isDone\", which should be set to one of these values:\n\n    ExprSingleResult             /* expression does not return a set */\n    ExprMultipleResult           /* this result is an element of a set */\n    ExprEndResult                /* there are no more elements in the set */\n\n(the caller will initialize it to ExprSingleResult).  If the function simply\nreturns a Datum without touching ReturnSetInfo, then the call is over and a\nsingle-item set has been returned.  To return a set, the function must set\nisDone to ExprMultipleResult for each set element.  After all elements have\nbeen returned, the next call should set isDone to ExprEndResult and return a\nnull result.  (Note it is possible to return an empty set by doing this on\nthe first call.)\n\nValue-per-call functions MUST NOT assume that they will be run to completion;\nthe executor might simply stop calling them, for example because of a LIMIT.\nTherefore, it's unsafe to attempt to perform any resource cleanup in the\nfinal call.  It's usually not necessary to clean up memory, anyway.  If it's\nnecessary to clean up other types of resources, such as file descriptors,\none can register a shutdown callback function in the ExprContext pointed to\nby the ReturnSetInfo node.  (But note that file descriptors are a limited\nresource, so it's generally unwise to hold those open across calls; SRFs\nthat need file access are better written to do it in a single call using\nMaterialize mode.)\n\nMaterialize mode works like this: the function creates a Tuplestore holding\nthe (possibly empty) result set, and returns it.  There are no multiple calls.\nThe function must also return a TupleDesc that indicates the tuple structure.\nThe Tuplestore and TupleDesc should be created in the context\necontext->ecxt_per_query_memory (note this will *not* be the context the\nfunction is called in).  The function stores pointers to the Tuplestore and\nTupleDesc into ReturnSetInfo, sets returnMode to indicate materialize mode,\nand returns null.  isDone is not used and should be left at ExprSingleResult.\n\nThe Tuplestore must be created with randomAccess = true if\nSFRM_Materialize_Random is set in allowedModes, but it can (and preferably\nshould) be created with randomAccess = false if not.  Callers that can support\nboth ValuePerCall and Materialize mode will set SFRM_Materialize_Preferred,\nor not, depending on which mode they prefer.\n\nIf available, the expected tuple descriptor is passed in ReturnSetInfo;\nin other contexts the expectedDesc field will be NULL.  The function need\nnot pay attention to expectedDesc, but it may be useful in special cases.\n\nInitMaterializedSRF() is a helper function able to setup the function's\nReturnSetInfo for a single call, filling in the Tuplestore and the\nTupleDesc with the proper configuration for Materialize mode.\n\nThere is no support for functions accepting sets; instead, the function will\nbe called multiple times, once for each element of the input set.\n\n\nNotes About Function Handlers\n-----------------------------\n\nHandlers for classes of functions should find life much easier and\ncleaner in this design.  The OID of the called function is directly\nreachable from the passed parameters; we don't need the global variable\nfmgr_pl_finfo anymore.  Also, by modifying fcinfo->flinfo->fn_extra,\nthe handler can cache lookup info to avoid repeat lookups when the same\nfunction is invoked many times.  (fn_extra can only be used as a hint,\nsince callers are not required to re-use an FmgrInfo struct.\nBut in performance-critical paths they normally will do so.)\n\nIf the handler wants to allocate memory to hold fn_extra data, it should\nNOT do so in CurrentMemoryContext, since the current context may well be\nmuch shorter-lived than the context where the FmgrInfo is.  Instead,\nallocate the memory in context flinfo->fn_mcxt, or in a long-lived cache\ncontext.  fn_mcxt normally points at the context that was\nCurrentMemoryContext at the time the FmgrInfo structure was created;\nin any case it is required to be a context at least as long-lived as the\nFmgrInfo itself.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\utils\\fmgr\\README",
      "directory": "backend\\utils\\fmgr"
    }
  },
  {
    "title": "README: backend\\utils\\mb",
    "url": "backend\\utils\\mb\\README",
    "content": "src/backend/utils/mb/README\n\nEncodings\n=========\n\nconv.c:\t\tstatic functions and a public table for code conversion\nmbutils.c:\tpublic functions for the backend only.\nstringinfo_mb.c: public backend-only multibyte-aware stringinfo functions\nwstrcmp.c:\tstrcmp for mb\nwstrncmp.c:\tstrncmp for mb\nwin866.c:\ta tool to generate KOI8 <--> CP866 conversion table\niso.c:\t\ta tool to generate KOI8 <--> ISO8859-5 conversion table\nwin1251.c:\ta tool to generate KOI8 <--> CP1251 conversion table\n\nSee also in src/common/:\n\nencnames.c:\tpublic functions for encoding names\nwchar.c:\tmostly static functions and a public table for mb string and\n\t\tmultibyte conversion\n\nIntroduction\n------------\n\thttp://www.cprogramming.com/tutorial/unicode.html",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\utils\\mb\\README",
      "directory": "backend\\utils\\mb"
    }
  },
  {
    "title": "README: backend\\utils\\mb\\conversion_procs",
    "url": "backend\\utils\\mb\\conversion_procs\\README.euc_jp",
    "content": "\n\n\t2006/04/15 Tatsuo Ishii\n\n\n\nPostgreSQL\n\n\nCinitdb\n/usr/local/pgsql/share/conversion_create.sql (\nmake)\n\nconvert()\n\nREADMECMakefile\n\n\no C\n\n  \n\n  conv_proc(\n\tINTEGER,\t-- source encoding id\n\tINTEGER,\t-- destination encoding id\n\tCSTRING,\t-- source string (null terminated C string)\n\tINTERNAL,\t-- destination string (null terminated C string)\n\tINTEGER\t\t-- source string length\n  ) returns VOID;\n\n  4destination string\n  pallocNULLC\n  \n  C\n  (5NULL\n  )\n\n  IDinclude/mb/pg_wchar.htypedef enum pg_enc\n  \n\no \n\n  C\n  MakefileMakefile\n  \n\n  Makefile(\n  )\n\n  (1) DIRS=\n\n  (2) @set \\ 11\n    \n\n    \n    \n    \n    \n    \n\n    1\n\no \n\n  make\n  create_conversion.sql\n  \n  \n\n  $ psql -e -f create_conversion.sql test\n\n  regression test suite\n  src/test/regress/sql/conversion.sql\n  regression test\n\no \n\n  \n  \n  2\n  \n  2\n  ",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\utils\\mb\\conversion_procs\\README.euc_jp",
      "directory": "backend\\utils\\mb\\conversion_procs"
    }
  },
  {
    "title": "README: backend\\utils\\misc",
    "url": "backend\\utils\\misc\\README",
    "content": "src/backend/utils/misc/README\n\nGUC Implementation Notes\n========================\n\nThe GUC (Grand Unified Configuration) module implements configuration\nvariables of multiple types (currently boolean, enum, int, real, and string).\nVariable settings can come from various places, with a priority ordering\ndetermining which setting is used.\n\n\nPer-Variable Hooks\n------------------\n\nEach variable known to GUC can optionally have a check_hook, an\nassign_hook, and/or a show_hook to provide customized behavior.\nCheck hooks are used to perform validity checking on variable values\n(above and beyond what GUC can do), to compute derived settings when\nnontrivial work is needed to do that, and optionally to \"canonicalize\"\nuser-supplied values.  Assign hooks are used to update any derived state\nthat needs to change when a GUC variable is set.  Show hooks are used to\nmodify the default SHOW display for a variable.\n\n\nIf a check_hook is provided, it points to a function of the signature\n\tbool check_hook(datatype *newvalue, void **extra, GucSource source)\nThe \"newvalue\" argument is of type bool *, int *, double *, or char **\nfor bool, int/enum, real, or string variables respectively.  The check\nfunction should validate the proposed new value, and return true if it is\nOK or false if not.  The function can optionally do a few other things:\n\n* When rejecting a bad proposed value, it may be useful to append some\nadditional information to the generic \"invalid value for parameter FOO\"\ncomplaint that guc.c will emit.  To do that, call\n\tvoid GUC_check_errdetail(const char *format, ...)\nwhere the format string and additional arguments follow the rules for\nerrdetail() arguments.  The resulting string will be emitted as the\nDETAIL line of guc.c's error report, so it should follow the message style\nguidelines for DETAIL messages.  There is also\n\tvoid GUC_check_errhint(const char *format, ...)\nwhich can be used in the same way to append a HINT message.\nOccasionally it may even be appropriate to override guc.c's generic primary\nmessage or error code, which can be done with\n\tvoid GUC_check_errcode(int sqlerrcode)\n\tvoid GUC_check_errmsg(const char *format, ...)\nIn general, check_hooks should avoid throwing errors directly if possible,\nthough this may be impractical to avoid for some corner cases such as\nout-of-memory.\n\n* Since the newvalue is pass-by-reference, the function can modify it.\nThis might be used for example to canonicalize the spelling of a string\nvalue, round off a buffer size to the nearest supported value, or replace\na special value such as \"-1\" with a computed default value.  If the\nfunction wishes to replace a string value, it must guc_malloc (not palloc)\nthe replacement value, and be sure to guc_free() the previous value.\n\n* Derived information, such as the role OID represented by a user name,\ncan be stored for use by the assign hook.  To do this, guc_malloc (not palloc)\nstorage space for the information, and return its address at *extra.\nguc.c will automatically guc_free() this space when the associated GUC setting\nis no longer of interest.  *extra is initialized to NULL before call, so\nit can be ignored if not needed.\n\nThe \"source\" argument indicates the source of the proposed new value,\nIf it is >= PGC_S_INTERACTIVE, then we are performing an interactive\nassignment (e.g., a SET command).  But when source < PGC_S_INTERACTIVE,\nwe are reading a non-interactive option source, such as postgresql.conf.\nThis is sometimes needed to determine whether a setting should be\nallowed.  The check_hook might also look at the current actual value of\nthe variable to determine what is allowed.\n\nNote that check hooks are sometimes called just to validate a value,\nwithout any intention of actually changing the setting.  Therefore the\ncheck hook must *not* take any action based on the assumption that an\nassignment will occur.\n\n\nIf an assign_hook is provided, it points to a function of the signature\n\tvoid assign_hook(datatype newvalue, void *extra)\nwhere the type of \"newvalue\" matches the kind of variable, and \"extra\"\nis the derived-information pointer returned by the check_hook (always\nNULL if there is no check_hook).  This function is called immediately\nbefore actually setting the variable's value (so it can look at the actual\nvariable to determine the old value, for example to avoid doing work when\nthe value isn't really changing).\n\nNote that there is no provision for a failure result code.  assign_hooks\nshould never fail except under the most dire circumstances, since a failure\nmay for example result in GUC settings not being rolled back properly during\ntransaction abort.  In general, try to do anything that could conceivably\nfail in a check_hook instead, and pass along the results in an \"extra\"\nstruct, so that the assign hook has little to do beyond copying the data to\nsomeplace.  This applies particularly to catalog lookups: any required\nlookups must be done in the check_hook, since the assign_hook may be\nexecuted during transaction rollback when lookups will be unsafe.\n\nNote that check_hooks are sometimes called outside any transaction, too.\nThis happens when processing the wired-in \"bootstrap\" value, values coming\nfrom the postmaster command line or environment, or values coming from\npostgresql.conf.  Therefore, any catalog lookups done in a check_hook\nshould be guarded with an IsTransactionState() test, and there must be a\nfallback path to allow derived values to be computed during the first\nsubsequent use of the GUC setting within a transaction.  A typical\narrangement is for the catalog values computed by the check_hook and\ninstalled by the assign_hook to be used only for the remainder of the\ntransaction in which the new setting is made.  Each subsequent transaction\nlooks up the values afresh on first use.  This arrangement is useful to\nprevent use of stale catalog values, independently of the problem of\nneeding to check GUC values outside a transaction.\n\n\nIf a show_hook is provided, it points to a function of the signature\n\tconst char *show_hook(void)\nThis hook allows variable-specific computation of the value displayed\nby SHOW (and other SQL features for showing GUC variable values).\nThe return value can point to a static buffer, since show functions are\nnot used reentrantly.\n\n\nSaving/Restoring GUC Variable Values\n------------------------------------\n\nPrior values of configuration variables must be remembered in order to deal\nwith several special cases: RESET (a/k/a SET TO DEFAULT), rollback of SET\non transaction abort, rollback of SET LOCAL at transaction end (either\ncommit or abort), and save/restore around a function that has a SET option.\nRESET is defined as selecting the value that would be effective had there\nnever been any SET commands in the current session.\n\nTo handle these cases we must keep track of many distinct values for each\nvariable.  The primary values are:\n\n* actual variable contents\talways the current effective value\n\n* reset_val\t\t\tthe value to use for RESET\n\n(Each GUC entry also has a boot_val which is the wired-in default value.\nThis is assigned to the reset_val and the actual variable during\nInitializeGUCOptions().  The boot_val is also consulted to restore the\ncorrect reset_val if SIGHUP processing discovers that a variable formerly\nspecified in postgresql.conf is no longer set there.)\n\nIn addition to the primary values, there is a stack of former effective\nvalues that might need to be restored in future.  Stacking and unstacking\nis controlled by the GUC \"nest level\", which is zero when outside any\ntransaction, one at top transaction level, and incremented for each\nopen subtransaction or function call with a SET option.  A stack entry\nis made whenever a GUC variable is first modified at a given nesting level.\n(Note: the reset_val need not be stacked because it is only changed by\nnon-transactional operations.)\n\nA stack entry has a state, a prior value of the GUC variable, a remembered\nsource of that prior value, and depending on the state may also have a\n\"masked\" value.  The masked value is needed when SET followed by SET LOCAL\noccur at the same nest level: the SET's value is masked but must be\nremembered to restore after transaction commit.\n\nDuring initialization we set the actual value and reset_val based on\nwhichever non-interactive source has the highest priority.  They will\nhave the same value.\n\nThe possible transactional operations on a GUC value are:\n\nEntry to a function with a SET option:\n\n\tPush a stack entry with the prior variable value and state SAVE,\n\tthen set the variable.\n\nPlain SET command:\n\n\tIf no stack entry of current level:\n\t\tPush new stack entry w/prior value and state SET\n\telse if stack entry's state is SAVE, SET, or LOCAL:\n\t\tchange stack state to SET, don't change saved value\n\t\t(here we are forgetting effects of prior set action)\n\telse (entry must have state SET+LOCAL):\n\t\tdiscard its masked value, change state to SET\n\t\t(here we are forgetting effects of prior SET and SET LOCAL)\n\tNow set new value.\n\nSET LOCAL command:\n\n\tIf no stack entry of current level:\n\t\tPush new stack entry w/prior value and state LOCAL\n\telse if stack entry's state is SAVE or LOCAL or SET+LOCAL:\n\t\tno change to stack entry\n\t\t(in SAVE case, SET LOCAL will be forgotten at func exit)\n\telse (entry must have state SET):\n\t\tput current active into its masked slot, set state SET+LOCAL\n\tNow set new value.\n\nTransaction or subtransaction abort:\n\n\tPop stack entries, restoring prior value, until top < subxact depth\n\nTransaction or subtransaction commit (incl. successful function exit):\n\n\tWhile stack entry level >= subxact depth\n\n\t\tif entry's state is SAVE:\n\t\t\tpop, restoring prior value\n\t\telse if level is 1 and entry's state is SET+LOCAL:\n\t\t\tpop, restoring *masked* value\n\t\telse if level is 1 and entry's state is SET:\n\t\t\tpop, discarding old value\n\t\telse if level is 1 and entry's state is LOCAL:\n\t\t\tpop, restoring prior value\n\t\telse if there is no entry of exactly level N-1:\n\t\t\tdecrement entry's level, no other state change\n\t\telse\n\t\t\tmerge entries of level N-1 and N as specified below\n\nThe merged entry will have level N-1 and prior = older prior, so easiest\nto keep older entry and free newer.  There are 12 possibilities since\nwe already handled level N state = SAVE:\n\nN-1\t\tN\n\nSAVE\t\tSET\t\tdiscard top prior, set state SET\nSAVE\t\tLOCAL\t\tdiscard top prior, no change to stack entry\nSAVE\t\tSET+LOCAL\tdiscard top prior, copy masked, state S+L\n\nSET\t\tSET\t\tdiscard top prior, no change to stack entry\nSET\t\tLOCAL\t\tcopy top prior to masked, state S+L\nSET\t\tSET+LOCAL\tdiscard top prior, copy masked, state S+L\n\nLOCAL\t\tSET\t\tdiscard top prior, set state SET\nLOCAL\t\tLOCAL\t\tdiscard top prior, no change to stack entry\nLOCAL\t\tSET+LOCAL\tdiscard top prior, copy masked, state S+L\n\nSET+LOCAL\tSET\t\tdiscard top prior and second masked, state SET\nSET+LOCAL\tLOCAL\t\tdiscard top prior, no change to stack entry\nSET+LOCAL\tSET+LOCAL\tdiscard top prior, copy masked, state S+L\n\n\nRESET is executed like a SET, but using the reset_val as the desired new\nvalue.  (We do not provide a RESET LOCAL command, but SET LOCAL TO DEFAULT\nhas the same behavior that RESET LOCAL would.)  The source associated with\nthe reset_val also becomes associated with the actual value.\n\nIf SIGHUP is received, the GUC code rereads the postgresql.conf\nconfiguration file (this does not happen in the signal handler, but at\nnext return to main loop; note that it can be executed while within a\ntransaction).  New values from postgresql.conf are assigned to actual\nvariable, reset_val, and stacked actual values, but only if each of\nthese has a current source priority <= PGC_S_FILE.  (It is thus possible\nfor reset_val to track the config-file setting even if there is\ncurrently a different interactive value of the actual variable.)\n\nThe check_hook, assign_hook and show_hook routines work only with the\nactual variable, and are not directly aware of the additional values\nmaintained by GUC.\n\n\nGUC Memory Handling\n-------------------\n\nString variable values are allocated with guc_malloc or guc_strdup,\nwhich ensure that the values are kept in a long-lived context, and provide\nmore control over handling out-of-memory failures than bare palloc.\n\nWe allow a string variable's actual value, reset_val, boot_val, and stacked\nvalues to point at the same storage.  This makes it slightly harder to free\nspace (we must test whether a value to be freed isn't equal to any of the\nother pointers in the GUC entry or associated stack items).  The main\nadvantage is that we never need to malloc during transaction commit/abort,\nso cannot cause an out-of-memory failure there.\n\n\"Extra\" structs returned by check_hook routines are managed in the same\nway as string values.  Note that we support \"extra\" structs for all types\nof GUC variables, although they are mainly useful with strings.\n\n\nGUC and Null String Variables\n-----------------------------\n\nA GUC string variable can have a boot_val of NULL.  guc.c handles this\nunsurprisingly, assigning the NULL to the underlying C variable.  Any code\nusing such a variable, as well as any hook functions for it, must then be\nprepared to deal with a NULL value.\n\nHowever, it is not possible to assign a NULL value to a GUC string\nvariable in any other way: values coming from SET, postgresql.conf, etc,\nmight be empty strings, but they'll never be NULL.  And SHOW displays\na NULL the same as an empty string.  It is therefore not appropriate to\ntreat a NULL value as a distinct user-visible setting.  A typical use\nfor a NULL boot_val is to denote that a value hasn't yet been set for\na variable that will receive a real value later in startup.\n\nIf it's undesirable for code using the underlying C variable to have to\nworry about NULL values ever, the variable can be given a non-null static\ninitializer as well as a non-null boot_val.  guc.c will overwrite the\nstatic initializer pointer with a copy of the boot_val during\nInitializeGUCOptions, but the variable will never contain a NULL.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\utils\\misc\\README",
      "directory": "backend\\utils\\misc"
    }
  },
  {
    "title": "README: backend\\utils\\mmgr",
    "url": "backend\\utils\\mmgr\\README",
    "content": "src/backend/utils/mmgr/README\n\nMemory Context System Design Overview\n=====================================\n\nBackground\n----------\n\nWe do most of our memory allocation in \"memory contexts\", which are usually\nAllocSets as implemented by src/backend/utils/mmgr/aset.c.  The key to\nsuccessful memory management without lots of overhead is to define a useful\nset of contexts with appropriate lifespans.\n\nThe basic operations on a memory context are:\n\n* create a context\n\n* allocate a chunk of memory within a context (equivalent of standard\n  C library's malloc())\n\n* delete a context (including freeing all the memory allocated therein)\n\n* reset a context (free all memory allocated in the context, but not the\n  context object itself)\n\n* inquire about the total amount of memory allocated to the context\n  (the raw memory from which the context allocates chunks; not the\n  chunks themselves)\n\nGiven a chunk of memory previously allocated from a context, one can\nfree it or reallocate it larger or smaller (corresponding to standard C\nlibrary's free() and realloc() routines).  These operations return memory\nto or get more memory from the same context the chunk was originally\nallocated in.\n\nAt all times there is a \"current\" context denoted by the\nCurrentMemoryContext global variable.  palloc() implicitly allocates space\nin that context.  The MemoryContextSwitchTo() operation selects a new current\ncontext (and returns the previous context, so that the caller can restore the\nprevious context before exiting).\n\nThe main advantage of memory contexts over plain use of malloc/free is\nthat the entire contents of a memory context can be freed easily, without\nhaving to request freeing of each individual chunk within it.  This is\nboth faster and more reliable than per-chunk bookkeeping.  We use this\nfact to clean up at transaction end: by resetting all the active contexts\nof transaction or shorter lifespan, we can reclaim all transient memory.\nSimilarly, we can clean up at the end of each query, or after each tuple\nis processed during a query.\n\n\nSome Notes About the palloc API Versus Standard C Library\n---------------------------------------------------------\n\nThe behavior of palloc and friends is similar to the standard C library's\nmalloc and friends, but there are some deliberate differences too.  Here\nare some notes to clarify the behavior.\n\n* If out of memory, palloc and repalloc exit via elog(ERROR).  They\nnever return NULL, and it is not necessary or useful to test for such\na result.  With palloc_extended() that behavior can be overridden\nusing the MCXT_ALLOC_NO_OOM flag.\n\n* palloc(0) is explicitly a valid operation.  It does not return a NULL\npointer, but a valid chunk of which no bytes may be used.  However, the\nchunk might later be repalloc'd larger; it can also be pfree'd without\nerror.  Similarly, repalloc allows realloc'ing to zero size.\n\n* pfree and repalloc do not accept a NULL pointer.  This is intentional.\n(For repalloc, this is necessary: As mentioned above, repalloc does\nnot depend on the current memory context.  But then it needs to know\nwhich memory context to do the allocation in.  So the first allocation\nhas to be done outside of repalloc.  For pfree, this behavior is\nmostly historical and partially because the extra check would impact\nperformance.)\n\n\nThe Current Memory Context\n--------------------------\n\nBecause it would be too much notational overhead to always pass an\nappropriate memory context to called routines, there always exists the\nnotion of the current memory context CurrentMemoryContext.  Without it,\nfor example, the copyObject routines would need to be passed a context, as\nwould function execution routines that return a pass-by-reference\ndatatype.  Similarly for routines that temporarily allocate space\ninternally, but don't return it to their caller?  We certainly don't\nwant to clutter every call in the system with \"here is a context to\nuse for any temporary memory allocation you might want to do\".\n\nThe upshot of that reasoning, though, is that CurrentMemoryContext should\ngenerally point at a short-lifespan context if at all possible.  During\nquery execution it usually points to a context that gets reset after each\ntuple.  Only in *very* circumscribed code should it ever point at a\ncontext having greater than transaction lifespan, since doing so risks\npermanent memory leaks.\n\n\npfree/repalloc Do Not Depend On CurrentMemoryContext\n----------------------------------------------------\n\npfree() and repalloc() can be applied to any chunk whether it belongs\nto CurrentMemoryContext or not --- the chunk's owning context will be\ninvoked to handle the operation, regardless.\n\n\n\"Parent\" and \"Child\" Contexts\n-----------------------------\n\nIf all contexts were independent, it'd be hard to keep track of them,\nespecially in error cases.  That is solved by creating a tree of\n\"parent\" and \"child\" contexts.  When creating a memory context, the\nnew context can be specified to be a child of some existing context.\nA context can have many children, but only one parent.  In this way\nthe contexts form a forest (not necessarily a single tree, since there\ncould be more than one top-level context; although in current practice\nthere is only one top context, TopMemoryContext).\n\nDeleting a context deletes all its direct and indirect children as\nwell.  When resetting a context it's almost always more useful to\ndelete child contexts, thus MemoryContextReset() means that, and if\nyou really do want a tree of empty contexts you need to call\nMemoryContextResetOnly() plus MemoryContextResetChildren().\n\nThese features allow us to manage a lot of contexts without fear that\nsome will be leaked; we only need to keep track of one top-level\ncontext that we are going to delete at transaction end, and make sure\nthat any shorter-lived contexts we create are descendants of that\ncontext.  Since the tree can have multiple levels, we can deal easily\nwith nested lifetimes of storage, such as per-transaction,\nper-statement, per-scan, per-tuple.  Storage lifetimes that only\npartially overlap can be handled by allocating from different trees of\nthe context forest (there are some examples in the next section).\n\nFor convenience we also provide operations like \"reset/delete all children\nof a given context, but don't reset or delete that context itself\".\n\n\nMemory Context Reset/Delete Callbacks\n-------------------------------------\n\nA feature introduced in Postgres 9.5 allows memory contexts to be used\nfor managing more resources than just plain palloc'd memory.  This is\ndone by registering a \"reset callback function\" for a memory context.\nSuch a function will be called, once, just before the context is next\nreset or deleted.  It can be used to give up resources that are in some\nsense associated with an object allocated within the context.  Possible\nuse-cases include\n* closing open files associated with a tuplesort object;\n* releasing reference counts on long-lived cache objects that are held\n  by some object within the context being reset;\n* freeing malloc-managed memory associated with some palloc'd object.\nThat last case would just represent bad programming practice for pure\nPostgres code; better to have made all the allocations using palloc,\nin the target context or some child context.  However, it could well\ncome in handy for code that interfaces to non-Postgres libraries.\n\nAny number of reset callbacks can be established for a memory context;\nthey are called in reverse order of registration.  Also, callbacks\nattached to child contexts are called before callbacks attached to\nparent contexts, if a tree of contexts is being reset or deleted.\n\nThe API for this requires the caller to provide a MemoryContextCallback\nmemory chunk to hold the state for a callback.  Typically this should be\nallocated in the same context it is logically attached to, so that it\nwill be released automatically after use.  The reason for asking the\ncaller to provide this memory is that in most usage scenarios, the caller\nwill be creating some larger struct within the target context, and the\nMemoryContextCallback struct can be made \"for free\" without a separate\npalloc() call by including it in this larger struct.\n\n\nMemory Contexts in Practice\n===========================\n\nGlobally Known Contexts\n-----------------------\n\nThere are a few widely-known contexts that are typically referenced\nthrough global variables.  At any instant the system may contain many\nadditional contexts, but all other contexts should be direct or indirect\nchildren of one of these contexts to ensure they are not leaked in event\nof an error.\n\nTopMemoryContext --- this is the actual top level of the context tree;\nevery other context is a direct or indirect child of this one.  Allocating\nhere is essentially the same as \"malloc\", because this context will never\nbe reset or deleted.  This is for stuff that should live forever, or for\nstuff that the controlling module will take care of deleting at the\nappropriate time.  An example is fd.c's tables of open files.  Avoid\nallocating stuff here unless really necessary, and especially avoid\nrunning with CurrentMemoryContext pointing here.\n\nPostmasterContext --- this is the postmaster's normal working context.\nAfter a backend is spawned, it can delete PostmasterContext to free its\ncopy of memory the postmaster was using that it doesn't need.\nNote that in non-EXEC_BACKEND builds, the postmaster's copy of pg_hba.conf\nand pg_ident.conf data is used directly during authentication in backend\nprocesses; so backends can't delete PostmasterContext until that's done.\n(The postmaster has only TopMemoryContext, PostmasterContext, and\nErrorContext --- the remaining top-level contexts are set up in each\nbackend during startup.)\n\nCacheMemoryContext --- permanent storage for relcache, catcache, and\nrelated modules.  This will never be reset or deleted, either, so it's\nnot truly necessary to distinguish it from TopMemoryContext.  But it\nseems worthwhile to maintain the distinction for debugging purposes.\n(Note: CacheMemoryContext has child contexts with shorter lifespans.\nFor example, a child context is the best place to keep the subsidiary\nstorage associated with a relcache entry; that way we can free rule\nparsetrees and so forth easily, without having to depend on constructing\na reliable version of freeObject().)\n\nMessageContext --- this context holds the current command message from the\nfrontend, as well as any derived storage that need only live as long as\nthe current message (for example, in simple-Query mode the parse and plan\ntrees can live here).  This context will be reset, and any children\ndeleted, at the top of each cycle of the outer loop of PostgresMain.  This\nis kept separate from per-transaction and per-portal contexts because a\nquery string might need to live either a longer or shorter time than any\nsingle transaction or portal.\n\nTopTransactionContext --- this holds everything that lives until end of the\ntop-level transaction.  This context will be reset, and all its children\ndeleted, at conclusion of each top-level transaction cycle.  In most cases\nyou don't want to allocate stuff directly here, but in CurTransactionContext;\nwhat does belong here is control information that exists explicitly to manage\nstatus across multiple subtransactions.  Note: this context is NOT cleared\nimmediately upon error; its contents will survive until the transaction block\nis exited by COMMIT/ROLLBACK.\n\nCurTransactionContext --- this holds data that has to survive until the end\nof the current transaction, and in particular will be needed at top-level\ntransaction commit.  When we are in a top-level transaction this is the same\nas TopTransactionContext, but in subtransactions it points to a child context.\nIt is important to understand that if a subtransaction aborts, its\nCurTransactionContext is thrown away after finishing the abort processing;\nbut a committed subtransaction's CurTransactionContext is kept until top-level\ncommit (unless of course one of the intermediate levels of subtransaction\naborts).  This ensures that we do not keep data from a failed subtransaction\nlonger than necessary.  Because of this behavior, you must be careful to clean\nup properly during subtransaction abort --- the subtransaction's state must be\ndelinked from any pointers or lists kept in upper transactions, or you will\nhave dangling pointers leading to a crash at top-level commit.  An example of\ndata kept here is pending NOTIFY messages, which are sent at top-level commit,\nbut only if the generating subtransaction did not abort.\n\nPortalContext --- this is not actually a separate context, but a\nglobal variable pointing to the per-portal context of the currently active\nexecution portal.  This can be used if it's necessary to allocate storage\nthat will live just as long as the execution of the current portal requires.\n\nErrorContext --- this permanent context is switched into for error\nrecovery processing, and then reset on completion of recovery.  We arrange\nto have a few KB of memory available in it at all times.  In this way, we\ncan ensure that some memory is available for error recovery even if the\nbackend has run out of memory otherwise.  This allows out-of-memory to be\ntreated as a normal ERROR condition, not a FATAL error.\n\n\nContexts For Prepared Statements And Portals\n--------------------------------------------\n\nA prepared-statement object has an associated private context, in which\nthe parse and plan trees for its query are stored.  Because these trees\nare read-only to the executor, the prepared statement can be re-used many\ntimes without further copying of these trees.\n\nAn execution-portal object has a private context that is referenced by\nPortalContext when the portal is active.  In the case of a portal created\nby DECLARE CURSOR, this private context contains the query parse and plan\ntrees (there being no other object that can hold them).  Portals created\nfrom prepared statements simply reference the prepared statements' trees,\nand don't actually need any storage allocated in their private contexts.\n\n\nLogical Replication Worker Contexts\n-----------------------------------\n\nApplyContext --- permanent during whole lifetime of apply worker.  It\nis possible to use TopMemoryContext here as well, but for simplicity\nof memory usage analysis we spin up different context.\n\nApplyMessageContext --- short-lived context that is reset after each\nlogical replication protocol message is processed.\n\n\nTransient Contexts During Execution\n-----------------------------------\n\nWhen creating a prepared statement, the parse and plan trees will be built\nin a temporary context that's a child of MessageContext (so that it will\ngo away automatically upon error).  On success, the finished plan is\ncopied to the prepared statement's private context, and the temp context\nis released; this allows planner temporary space to be recovered before\nexecution begins.  (In simple-Query mode we don't bother with the extra\ncopy step, so the planner temp space stays around till end of query.)\n\nThe top-level executor routines, as well as most of the \"plan node\"\nexecution code, will normally run in a context that is created by\nExecutorStart and destroyed by ExecutorEnd; this context also holds the\n\"plan state\" tree built during ExecutorStart.  Most of the memory\nallocated in these routines is intended to live until end of query,\nso this is appropriate for those purposes.  The executor's top context\nis a child of PortalContext, that is, the per-portal context of the\nportal that represents the query's execution.\n\nThe main memory-management consideration in the executor is that\nexpression evaluation --- both for qual testing and for computation of\ntargetlist entries --- needs to not leak memory.  To do this, each\nExprContext (expression-eval context) created in the executor has a\nprivate memory context associated with it, and we switch into that context\nwhen evaluating expressions in that ExprContext.  The plan node that owns\nthe ExprContext is responsible for resetting the private context to empty\nwhen it no longer needs the results of expression evaluations.  Typically\nthe reset is done at the start of each tuple-fetch cycle in the plan node.\n\nNote that this design gives each plan node its own expression-eval memory\ncontext.  This appears necessary to handle nested joins properly, since\nan outer plan node might need to retain expression results it has computed\nwhile obtaining the next tuple from an inner node --- but the inner node\nmight execute many tuple cycles and many expressions before returning a\ntuple.  The inner node must be able to reset its own expression context\nmore often than once per outer tuple cycle.  Fortunately, memory contexts\nare cheap enough that giving one to each plan node doesn't seem like a\nproblem.\n\nA problem with running index accesses and sorts in a query-lifespan context\nis that these operations invoke datatype-specific comparison functions,\nand if the comparators leak any memory then that memory won't be recovered\ntill end of query.  The comparator functions all return bool or int32,\nso there's no problem with their result data, but there can be a problem\nwith leakage of internal temporary data.  In particular, comparator\nfunctions that operate on TOAST-able data types need to be careful\nnot to leak detoasted versions of their inputs.  This is annoying, but\nit appeared a lot easier to make the comparators conform than to fix the\nindex and sort routines, so that's what was done for 7.1.  This remains\nthe state of affairs in btree and hash indexes, so btree and hash support\nfunctions still need to not leak memory.  Most of the other index AMs\nhave been modified to run opclass support functions in short-lived\ncontexts, so that leakage is not a problem; this is necessary in view\nof the fact that their support functions tend to be far more complex.\n\nThere are some special cases, such as aggregate functions.  nodeAgg.c\nneeds to remember the results of evaluation of aggregate transition\nfunctions from one tuple cycle to the next, so it can't just discard\nall per-tuple state in each cycle.  The easiest way to handle this seems\nto be to have two per-tuple contexts in an aggregate node, and to\nping-pong between them, so that at each tuple one is the active allocation\ncontext and the other holds any results allocated by the prior cycle's\ntransition function.\n\nExecutor routines that switch the active CurrentMemoryContext may need\nto copy data into their caller's current memory context before returning.\nHowever, we have minimized the need for that, because of the convention\nof resetting the per-tuple context at the *start* of an execution cycle\nrather than at its end.  With that rule, an execution node can return a\ntuple that is palloc'd in its per-tuple context, and the tuple will remain\ngood until the node is called for another tuple or told to end execution.\nThis parallels the situation with pass-by-reference values at the table\nscan level, since a scan node can return a direct pointer to a tuple in a\ndisk buffer that is only guaranteed to remain good that long.\n\nA more common reason for copying data is to transfer a result from\nper-tuple context to per-query context; for example, a Unique node will\nsave the last distinct tuple value in its per-query context, requiring a\ncopy step.\n\n\nMechanisms to Allow Multiple Types of Contexts\n----------------------------------------------\n\nTo efficiently allow for different allocation patterns, and for\nexperimentation, we allow for different types of memory contexts with\ndifferent allocation policies but similar external behavior.  To\nhandle this, memory allocation functions are accessed via function\npointers, and we require all context types to obey the conventions\ngiven here.\n\nA memory context is represented by struct MemoryContextData (see\nmemnodes.h). This struct identifies the exact type of the context, and\ncontains information common between the different types of\nMemoryContext like the parent and child contexts, and the name of the\ncontext.\n\nThis is essentially an abstract superclass, and the behavior is\ndetermined by the \"methods\" pointer which references which set of\nMemoryContextMethods are to be used.  Specific memory context types will\nuse derived structs having these fields as their first fields.  All the\ncontexts of a specific type will have methods pointers that point to\nthe corresponding element in the mcxt_methods[] array as defined in mcxt.c.\n\nWhile operations like allocating from and resetting a context take the\nrelevant MemoryContext as a parameter, operations like free and\nrealloc are trickier.  To make those work, we require all memory\ncontext types to produce allocated chunks that are immediately,\nwithout any padding, preceded by a uint64 value of which the least\nsignificant 4 bits are set to the owning context's MemoryContextMethodID.\nThis allows the code to determine the correct MemoryContextMethods to\nuse by looking up the mcxt_methods[] array using the 4 bits as an index\ninto that array.\n\nIf a type of allocator needs additional information about its chunks,\nlike e.g. the size of the allocation, that information can in turn\neither be encoded into the remaining 60 bits of the preceding uint64 value\nor if more space is required, additional values may be stored directly prior\nto the uint64 value.  It is up to the context implementation to manage this.\n\nGiven this, routines like pfree can determine which set of\nMemoryContextMethods to call the free_p function for by calling\nGetMemoryChunkMethodID() and finding the corresponding MemoryContextMethods\nin the mcxt_methods[] array.  For convenience, the MCXT_METHOD() macro is\nprovided, making the code as simple as:\n\nvoid\npfree(void *pointer)\n{\n\tMCXT_METHOD(pointer, free_p)(pointer);\n}\n\nAll of the current memory contexts make use of the MemoryChunk header type\nwhich is defined in memutils_memorychunk.h.  This suits all of the existing\ncontext types well as it makes use of the remaining 60-bits of the uint64\nheader to efficiently encode the size of the chunk of memory (or freelist\nindex, in the case of aset.c) and the number of bytes which must be subtracted\nfrom the chunk in order to obtain a reference to the block that the chunk\nbelongs to.  30 bits are used for each of these, but only a total of 59 bits\nas the lowest bit for the chunk to block offset is the same bit as the highest\nbit of the chunk size.  This overlapping is possible as the relative offset\nbetween the block and the chunk is expected to be a MAXALIGNed value which\nguarantees the lowest bit is always 0.  If more than 30 bits are required for\neach of these fields then the memory context must manage that itself.  This\ncan be done by calling the MemoryChunkSetHdrMaskExternal() function on the\ngiven chunk.  Whether a chunk is an external chunk can be determined by the 1\nremaining bit from the 64-bit MemoryChunk.\n\nCurrently, each memory context type stores large allocations on dedicated\nblocks (which always contain only a single chunk).  For these, finding the\nblock is simple as we know that the chunk must be the first on the given\nblock, so the block is always at a fixed offset to the chunk.  For these,\nfinding the size of the chunk is also simple as the block always stores an\nendptr which we can use to calculate the size of the chunk.\n\nMore Control Over aset.c Behavior\n---------------------------------\n\nBy default aset.c always allocates an 8K block upon the first\nallocation in a context, and doubles that size for each successive\nblock request.  That's good behavior for a context that might hold\n*lots* of data.  But if there are dozens if not hundreds of smaller\ncontexts in the system, we need to be able to fine-tune things a\nlittle better.\n\nThe creator of a context is able to specify an initial block size and\na maximum block size.  Selecting smaller values can prevent wastage of\nspace in contexts that aren't expected to hold very much (an example\nis the relcache's per-relation contexts).\n\nAlso, it is possible to specify a minimum context size, in case for some\nreason that should be different from the initial size for additional\nblocks.  An aset.c context will always contain at least one block,\nof size minContextSize if that is specified, otherwise initBlockSize.\n\nWe expect that per-tuple contexts will be reset frequently and typically\nwill not allocate very much space per tuple cycle.  To make this usage\npattern cheap, the first block allocated in a context is not given\nback to malloc() during reset, but just cleared.  This avoids malloc\nthrashing.\n\n\nAlternative Memory Context Implementations\n------------------------------------------\n\naset.c (AllocSetContext) is our default general-purpose allocator.  Three other\nallocator types also exist which are special-purpose:\n\n* slab.c (SlabContext) is designed for allocations of fixed-sized\n  chunks.  The fixed chunk size must be specified when creating the context.\n  New chunks are allocated to the fullest block, keeping used chunks densely\n  packed together to avoid memory fragmentation.  This also increases the\n  chances that pfree'ing a chunk will result in a block becoming empty of all\n  chunks and allow it to be free'd back to the operating system.\n\n* generation.c (GenerationContext) is best suited for cases when chunks are\n  allocated in groups with similar lifespan (generations), or roughly in FIFO\n  order.  No attempt is made to reuse space left by pfree'd chunks.  Blocks\n  are returned to the operating system when all chunks on them have been\n  pfree'd.\n\n* bump.c (BumpContext) is best suited for use cases that require densely\n  allocated chunks of memory that never need to be individually pfree'd or\n  repalloc'd.  These operations are unsupported due to BumpContext chunks\n  having no chunk header.  No chunk header means more densely packed chunks,\n  which is especially useful for workloads that perform lots of small\n  allocations.  Blocks are only free'd back to the operating system when the\n  context is reset or deleted.\n\nFor further details, please read the header comment in the corresponding .c\nfile.\n\nMemory Accounting\n-----------------\n\nOne of the basic memory context operations is determining the amount of\nmemory used in the context (and its children). We have multiple places\nthat implement their own ad hoc memory accounting, and this is meant to\nprovide a unified approach. Ad hoc accounting solutions work for places\nwith tight control over the allocations or when it's easy to determine\nsizes of allocated chunks (e.g. places that only work with tuples).\n\nThe accounting built into the memory contexts is transparent and works\ntransparently for all allocations as long as they end up in the right\nmemory context subtree.\n\nConsider for example aggregate functions - the aggregate state is often\nrepresented by an arbitrary structure, allocated from the transition\nfunction, so the ad hoc accounting is unlikely to work. The built-in\naccounting will however handle such cases just fine.\n\nTo minimize overhead, the accounting is done at the block level, not for\nindividual allocation chunks.\n\nThe accounting is lazy - after a block is allocated (or freed), only the\ncontext owning that block is updated. This means that when inquiring\nabout the memory usage in a given context, we have to walk all children\ncontexts recursively. This means the memory accounting is not intended\nfor cases with too many memory contexts (in the relevant subtree).",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\utils\\mmgr\\README",
      "directory": "backend\\utils\\mmgr"
    }
  },
  {
    "title": "README: backend\\utils\\resowner",
    "url": "backend\\utils\\resowner\\README",
    "content": "src/backend/utils/resowner/README\n\nNotes About Resource Owners\n===========================\n\nResourceOwner objects are a concept invented to simplify management of\nquery-related resources, such as buffer pins and table locks.  These\nresources need to be tracked in a reliable way to ensure that they will\nbe released at query end, even if the query fails due to an error.\nRather than expecting the entire executor to have bulletproof data\nstructures, we localize the tracking of such resources into a single\nmodule.\n\nThe design of the ResourceOwner API is modeled on our MemoryContext API,\nwhich has proven very flexible and successful in preventing memory leaks.\nIn particular we allow ResourceOwners to have child ResourceOwner objects\nso that there can be forests of the things; releasing a parent\nResourceOwner acts on all its direct and indirect children as well.\n\n(It is tempting to consider unifying ResourceOwners and MemoryContexts\ninto a single object type, but their usage patterns are sufficiently\ndifferent that this is probably not really a helpful thing to do.)\n\nWe create a ResourceOwner for each transaction or subtransaction as\nwell as one for each Portal.  During execution of a Portal, the global\nvariable CurrentResourceOwner points to the Portal's ResourceOwner.\nThis causes operations such as ReadBuffer and LockAcquire to record\nownership of the acquired resources in that ResourceOwner object.\n\nWhen a Portal is closed, any remaining resources (typically only locks)\nbecome the responsibility of the current transaction.  This is represented\nby making the Portal's ResourceOwner a child of the current transaction's\nResourceOwner.  resowner.c automatically transfers the resources to the\nparent object when releasing the child.  Similarly, subtransaction\nResourceOwners are children of their immediate parent.\n\nWe need transaction-related ResourceOwners as well as Portal-related ones\nbecause transactions may initiate operations that require resources (such\nas query parsing) when no associated Portal exists yet.\n\n\nUsage\n-----\n\nThe basic operations on a ResourceOwner are:\n\n* create a ResourceOwner\n\n* associate or deassociate some resource with a ResourceOwner\n\n* release a ResourceOwner's assets (free all owned resources, but not the\n  owner object itself)\n\n* delete a ResourceOwner (including child owner objects); all resources\n  must have been released beforehand\n\nLocks are handled specially because in non-error situations a lock should\nbe held until end of transaction, even if it was originally taken by a\nsubtransaction or portal.  Therefore, the \"release\" operation on a child\nResourceOwner transfers lock ownership to the parent instead of actually\nreleasing the lock, if isCommit is true.\n\nWhenever we are inside a transaction, the global variable\nCurrentResourceOwner shows which resource owner should be assigned\nownership of acquired resources.  Note however that CurrentResourceOwner\nis NULL when not inside any transaction (or when inside a failed\ntransaction).  In this case it is not valid to acquire query-lifespan\nresources.\n\nWhen unpinning a buffer or releasing a lock or cache reference,\nCurrentResourceOwner must point to the same resource owner that was current\nwhen the buffer, lock, or cache reference was acquired.  It would be possible\nto relax this restriction given additional bookkeeping effort, but at present\nthere seems no need.\n\nAdding a new resource type\n--------------------------\n\nResourceOwner can track ownership of many different kinds of resources.  In\ncore PostgreSQL it is used for buffer pins, lmgr locks, and catalog cache\nreferences, to name a few examples.\n\nTo add a new kind of resource, define a ResourceOwnerDesc to describe it.\nFor example:\n\nstatic const ResourceOwnerDesc myresource_desc = {\n\t.name = \"My fancy resource\",\n\t.release_phase = RESOURCE_RELEASE_AFTER_LOCKS,\n\t.release_priority = RELEASE_PRIO_FIRST,\n\t.ReleaseResource = ReleaseMyResource,\n\t.DebugPrint = PrintMyResource\n};\n\nResourceOwnerRemember() and ResourceOwnerForget() functions take a pointer\nto that struct, along with a Datum to represent the resource.  The meaning\nof the Datum depends on the resource type.  Most resource types use it to\nstore a pointer to some struct, but it can also be a file descriptor or\nlibrary handle, for example.\n\nThe ReleaseResource callback is called when a resource owner is released or\ndeleted.  It should release any resources (e.g. close files, free memory)\nassociated with the resource.  Because the callback is called during\ntransaction abort, it must perform only low-level cleanup with no user\nvisible effects.  The callback should not perform operations that could\nfail, like allocate memory.\n\nThe optional DebugPrint callback is used in the warning at transaction\ncommit, if any resources are leaked.  If not specified, a generic\nimplementation that prints the resource name and the resource as a pointer\nis used.\n\nThere is another API for other modules to get control during ResourceOwner\nrelease, so that they can scan their own data structures to find the objects\nthat need to be deleted.  See RegisterResourceReleaseCallback function.\nThis used to be the only way for extensions to use the resource owner\nmechanism with new kinds of objects; nowadays it is easier to define a custom\nResourceOwnerDesc struct.\n\n\nReleasing\n---------\n\nReleasing the resources of a ResourceOwner happens in three phases:\n\n1. \"Before-locks\" resources\n\n2. Locks\n\n3. \"After-locks\" resources\n\nEach resource type specifies whether it needs to be released before or after\nlocks.  Each resource type also has a priority, which determines the order\nthat the resources are released in.  Note that the phases are performed fully\nfor the whole tree of resource owners, before moving to the next phase, but\nthe priority within each phase only determines the order within that\nResourceOwner.  Child resource owners are always handled before the parent,\nwithin each phase.\n\nFor example, imagine that you have two ResourceOwners, parent and child,\nas follows:\n\nParent\n  parent resource BEFORE_LOCKS priority 1\n  parent resource BEFORE_LOCKS priority 2\n  parent resource AFTER_LOCKS priority 10001\n  parent resource AFTER_LOCKS priority 10002\n  Child\n    child resource BEFORE_LOCKS priority 1\n    child resource BEFORE_LOCKS priority 2\n    child resource AFTER_LOCKS priority 10001\n    child resource AFTER_LOCKS priority 10002\n\nThese resources would be released in the following order:\n\nchild resource BEFORE_LOCKS priority 1\nchild resource BEFORE_LOCKS priority 2\nparent resource BEFORE_LOCKS priority 1\nparent resource BEFORE_LOCKS priority 2\n(locks)\nchild resource AFTER_LOCKS priority 10001\nchild resource AFTER_LOCKS priority 10002\nparent resource AFTER_LOCKS priority 10001\nparent resource AFTER_LOCKS priority 10002\n\nTo release all the resources, you need to call ResourceOwnerRelease() three\ntimes, once for each phase. You may perform additional tasks between the\nphases, but after the first call to ResourceOwnerRelease(), you cannot use\nthe ResourceOwner to remember any more resources. You also cannot call\nResourceOwnerForget on the resource owner to release any previously\nremembered resources \"in retail\", after you have started the release process.\n\nNormally, you are expected to call ResourceOwnerForget on every resource so\nthat at commit, the ResourceOwner is empty (locks are an exception). If there\nare any resources still held at commit, ResourceOwnerRelease will print a\nWARNING on each such resource. At abort, however, we truly rely on the\nResourceOwner mechanism and it is normal that there are resources to be\nreleased.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\utils\\resowner\\README",
      "directory": "backend\\utils\\resowner"
    }
  },
  {
    "title": "README: bin\\pgevent",
    "url": "bin\\pgevent\\README",
    "content": "src/bin/pgevent/README\n\npgevent\n=======\n\nMSG00001.bin is a binary file, result of Microsoft MC compiler. MC compiler\ncan be downloaded for free with MS Core SDK but it is not included with MSYS\ntools and I didn't find an alternative way to compile MC file.\n\nTo summarize: the command \"MC pgmsgevent.mc\" generates pgmsgevent.h,\npgmsgevent.rc, and MSG00001.bin files.  In MC file, we declare a string\nwith %s format, so we can write anything we want in the future without\nneeding to change the definition of this string.\n\nTo finish, because DllUnregisterServer and DllRegisterServer are system\ndefined entry points, we need to export these two functions with their names\nwithout \"decoration\", so we cannot use auto generated .def files without\nhandy modifications.\n\nLaurent Ballester",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\bin\\pgevent\\README",
      "directory": "bin\\pgevent"
    }
  },
  {
    "title": "README: bin\\pg_amcheck",
    "url": "bin\\pg_amcheck\\README",
    "content": "src/bin/pg_amcheck/README\n\npg_amcheck is a command-line tool for running the amcheck extension.\n\nRunning the regression tests\n============================\n\nNOTE: You must have given the --enable-tap-tests argument to configure.\nAlso, to use \"make installcheck\", you must have built and installed\ncontrib/amcheck and contrib/pageinspect in addition to the core code.\n\nRun\n    make check\nor\n    make installcheck\nYou can use \"make installcheck\" if you previously did \"make install\".\nIn that case, the code in the installation tree is tested.  With\n\"make check\", a temporary installation tree is built from the current\nsources and then tested.\n\nSee src/test/perl/README for more info about running these tests.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\bin\\pg_amcheck\\README",
      "directory": "bin\\pg_amcheck"
    }
  },
  {
    "title": "README: common\\unicode",
    "url": "common\\unicode\\README",
    "content": "This directory contains tools to download new Unicode data files and\ngenerate static tables. These tables are used to normalize or\ndetermine various properties of Unicode data.\n\nThe generated header files are copied to src/include/common/, and\nincluded in the source tree, so these tools are not normally required\nto build PostgreSQL.\n\nUpdate Unicode Version\n----------------------\n\nEdit src/Makefile.global.in and src/common/unicode/meson.build\nto update the UNICODE_VERSION.\n\nThen, generate the new header files with:\n\n    make update-unicode\n\nor if using meson:\n\n    ninja update-unicode\n\nfrom the top level of the source tree. Examine the result to make sure\nthe changes look reasonable (that is, that the diff size and scope is\ncomparable to the Unicode changes since the last update), and then\ncommit it.\n\nTests\n-----\n\nNormalization tests:\n\nThe Unicode consortium publishes a comprehensive test suite for the\nnormalization algorithm, in a file called NormalizationTest.txt. This\ndirectory also contains a perl script and some C code, to run our\nnormalization code with all the test strings in NormalizationTest.txt.\nTo download NormalizationTest.txt and run the tests:\n\n    make normalization-check\n\nThis is also run as part of the update-unicode target.\n\nCategory, Property and Case tests:\n\nThe files case_test.c and category_test.c test Unicode categories,\nproperties, and case mapping by exhaustively comparing results with\nICU. For these tests to be effective, the version of the Unicode data\nfiles must be similar to the version of Unicode on which ICU is\nbased. Mismatched Unicode versions will cause the tests to skip over\ncodepoints that are assigned in one version and not the other, and may\nfalsely report failures. This test is run as a part of the\nupdate-unicode target.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\common\\unicode\\README",
      "directory": "common\\unicode"
    }
  },
  {
    "title": "README: interfaces\\ecpg",
    "url": "interfaces\\ecpg\\README.dynSQL",
    "content": "src/interfaces/ecpg/README.dynSQL\n\ndescriptor statements have the following shortcomings\n\n- input descriptors (USING DESCRIPTOR <name>) are not supported\n\n  Reason: to fully support dynamic SQL the frontend/backend communication\n          should change to recognize input parameters.\n          Since this is not likely to happen in the near future and you\n          can cover the same functionality with the existing infrastructure\n          (using s[n]printf), I'll leave the work to someone else.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\interfaces\\ecpg\\README.dynSQL",
      "directory": "interfaces\\ecpg"
    }
  },
  {
    "title": "README: interfaces\\ecpg\\preproc",
    "url": "interfaces\\ecpg\\preproc\\README.parser",
    "content": "ECPG modifies and extends the core grammar in a way that\n1) every token in ECPG is <str> type. New tokens are\n   defined in ecpg.tokens, types are defined in ecpg.type\n2) most tokens from the core grammar are simply converted\n   to literals concatenated together to form the SQL string\n   passed to the server, this is done by parse.pl.\n3) some rules need side-effects, actions are either added\n   or completely overridden (compared to the basic token\n   concatenation) for them, these are defined in ecpg.addons,\n   the rules for ecpg.addons are explained below.\n4) new grammar rules are needed for ECPG metacommands.\n   These are in ecpg.trailer.\n5) ecpg.header contains common functions, etc. used by\n   actions for grammar rules.\n\nIn \"ecpg.addons\", every modified rule follows this pattern:\n       ECPG: dumpedtokens postfix\nwhere \"dumpedtokens\" is simply tokens from core gram.y's\nrules concatenated together. e.g. if gram.y has this:\n       ruleA: tokenA tokenB tokenC {...}\nthen \"dumpedtokens\" is \"ruleAtokenAtokenBtokenC\".\n\"postfix\" above can be:\na) \"block\" - the automatic rule created by parse.pl is completely\n    overridden, the code block has to be written completely as\n    it were in a plain bison grammar\nb) \"rule\" - the automatic rule is extended on, so new syntaxes\n    are accepted for \"ruleA\". E.g.:\n      ECPG: ruleAtokenAtokenBtokenC rule\n          | tokenD tokenE { action_code; }\n          ...\n    It will be substituted with:\n      ruleA: <original syntax forms and actions up to and including\n                    \"tokenA tokenB tokenC\">\n             | tokenD tokenE { action_code; }\n             ...\nc) \"addon\" - the automatic action for the rule (SQL syntax constructed\n    from the tokens concatenated together) is prepended with a new\n    action code part. This code part is written as is's already inside\n    the { ... }\n\nMultiple \"addon\" or \"block\" lines may appear together with the\nnew code block if the code block is common for those rules.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\interfaces\\ecpg\\preproc\\README.parser",
      "directory": "interfaces\\ecpg\\preproc"
    }
  },
  {
    "title": "README: interfaces\\ecpg\\test\\connect",
    "url": "interfaces\\ecpg\\test\\connect\\README",
    "content": "src/interfaces/ecpg/test/connect/README\n\nPrograms in this directory test all sorts of connections.\n\nAll other programs just use one standard connection method.\n\nIf any details of the regression database get changed (port, unix socket file,\nuser names, passwords, ...), these programs here have to be changed as well\nbecause they contain hardcoded values.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\interfaces\\ecpg\\test\\connect\\README",
      "directory": "interfaces\\ecpg\\test\\connect"
    }
  },
  {
    "title": "README: interfaces\\libpq",
    "url": "interfaces\\libpq\\README",
    "content": "src/interfaces/libpq/README\n\nThis directory contains the C version of Libpq, the POSTGRES frontend library.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\interfaces\\libpq\\README",
      "directory": "interfaces\\libpq"
    }
  },
  {
    "title": "README: pl\\plperl",
    "url": "pl\\plperl\\README",
    "content": "src/pl/plperl/README\n\nPL/Perl allows you to write PostgreSQL functions and procedures in\nPerl.  To include PL/Perl in the build use './configure --with-perl'.\nTo build from this directory use 'make all; make install'.  libperl\nmust have been built as a shared library, which is usually not the\ncase in standard installations.\n\nConsult the PostgreSQL User's Guide for more information.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\pl\\plperl\\README",
      "directory": "pl\\plperl"
    }
  },
  {
    "title": "README: pl\\plpython\\expected",
    "url": "pl\\plpython\\expected\\README",
    "content": "Guide to alternative expected files:\n\nplpython_error_5.out\t\t\tPython 3.5 and newer",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\pl\\plpython\\expected\\README",
      "directory": "pl\\plpython\\expected"
    }
  },
  {
    "title": "README: port",
    "url": "port\\README",
    "content": "src/port/README\n\nlibpgport\n=========\n\nlibpgport must have special behavior.  It supplies functions to both\nlibraries and applications.  However, there are two complexities:\n\n1)  Libraries need to use object files that are compiled with exactly\nthe same flags as the library.  libpgport might not use the same flags,\nso it is necessary to recompile the object files for individual\nlibraries.  This is done by removing -lpgport from the link line:\n\n        # Need to recompile any libpgport object files\n        LIBS := $(filter-out -lpgport, $(LIBS))\n\nand adding infrastructure to recompile the object files:\n\n        OBJS= execute.o typename.o descriptor.o data.o error.o prepare.o memory.o \\\n                connect.o misc.o path.o exec.o \\\n                $(filter strlcat.o, $(LIBOBJS))\n\nThe problem is that there is no testing of which object files need to be\nadded, but missing functions usually show up when linking user\napplications.\n\n2) For applications, we use -lpgport before -lpq, so the static files\nfrom libpgport are linked first.  This avoids having applications\ndependent on symbols that are _used_ by libpq, but not intended to be\nexported by libpq.  libpq's libpgport usage changes over time, so such a\ndependency is a problem.  Windows, Linux, and macOS use an export\nlist to control the symbols exported by libpq.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\port\\README",
      "directory": "port"
    }
  },
  {
    "title": "README: test",
    "url": "test\\README",
    "content": "PostgreSQL tests\n================\n\nThis directory contains a variety of test infrastructure as well as some of the\ntests in PostgreSQL. Not all tests are here -- in particular, there are more in\nindividual contrib/ modules and in src/bin.\n\nNot all these tests get run by \"make check\". Check src/test/Makefile to see\nwhich tests get run automatically.\n\nauthentication/\n  Tests for authentication (but see also below)\n\nexamples/\n  Demonstration programs for libpq that double as regression tests via\n  \"make check\"\n\nisolation/\n  Tests for concurrent behavior at the SQL level\n\nkerberos/\n  Tests for Kerberos/GSSAPI authentication and encryption\n\nldap/\n  Tests for LDAP-based authentication\n\nlocale/\n  Sanity checks for locale data, encodings, etc\n\nmb/\n  Tests for multibyte encoding (UTF-8) support\n\nmodules/\n  Extensions used only or mainly for test purposes, generally not suitable\n  for installing in production databases\n\nperl/\n  Infrastructure for Perl-based TAP tests\n\nrecovery/\n  Test suite for recovery and replication\n\nregress/\n  PostgreSQL's main regression test suite, pg_regress\n\nssl/\n  Tests to exercise and verify SSL certificate handling\n\nsubscription/\n  Tests for logical replication",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\README",
      "directory": "test"
    }
  },
  {
    "title": "README: test\\authentication",
    "url": "test\\authentication\\README",
    "content": "src/test/authentication/README\n\nRegression tests for authentication\n===================================\n\nThis directory contains a test suite for authentication. SSL certificate\nauthentication tests are kept separate, in src/test/ssl/, because they\nare more complicated, and are not safe to run in a multi-user system.\n\n\nRunning the tests\n=================\n\nNOTE: You must have given the --enable-tap-tests argument to configure.\n\nRun\n    make check\nor\n    make installcheck\nYou can use \"make installcheck\" if you previously did \"make install\".\nIn that case, the code in the installation tree is tested.  With\n\"make check\", a temporary installation tree is built from the current\nsources and then tested.\n\nEither way, this test initializes, starts, and stops a test Postgres\ncluster.\n\nSee src/test/perl/README for more info about running these tests.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\authentication\\README",
      "directory": "test\\authentication"
    }
  },
  {
    "title": "README: test\\icu",
    "url": "test\\icu\\README",
    "content": "src/test/icu/README\n\nRegression tests for ICU functionality\n======================================\n\nThis directory contains a test suite for ICU functionality.\n\nRunning the tests\n=================\n\nNOTE: You must have given the --enable-tap-tests argument to configure.\n\nRun\n    make check\nor\n    make installcheck\nYou can use \"make installcheck\" if you previously did \"make install\".\nIn that case, the code in the installation tree is tested.  With\n\"make check\", a temporary installation tree is built from the current\nsources and then tested.\n\nEither way, this test initializes, starts, and stops several test Postgres\nclusters.\n\nSee src/test/perl/README for more info about running these tests.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\icu\\README",
      "directory": "test\\icu"
    }
  },
  {
    "title": "README: test\\isolation",
    "url": "test\\isolation\\README",
    "content": "src/test/isolation/README\n\nIsolation tests\n===============\n\nThis directory contains a set of tests for concurrent behaviors in\nPostgreSQL.  These tests require running multiple interacting transactions,\nwhich requires management of multiple concurrent connections, and therefore\ncan't be tested using the normal pg_regress program.  The name \"isolation\"\ncomes from the fact that the original motivation was to test the\nserializable isolation level; but tests for other sorts of concurrent\nbehaviors have been added as well.\n\nYou can run the tests against the current build tree by typing\n    make check\nAlternatively, you can run against an existing installation by typing\n    make installcheck\n(This will contact a server at the default port expected by libpq.\nYou can set PGPORT and so forth in your environment to control this.)\n\nTo run just specific test(s) against an installed server,\nyou can do something like\n    ./pg_isolation_regress fk-contention fk-deadlock\n(look into the specs/ subdirectory to see the available tests).\n\nCertain tests require the server's max_prepared_transactions parameter to be\nset to at least 3; therefore they are not run by default.  To include them in\nthe test run, use\n    make check-prepared-txns\nor\n    make installcheck-prepared-txns\nafter making sure the server configuration is correct (see TEMP_CONFIG\nto adjust this in the \"check\" case).\n\nTo define tests with overlapping transactions, we use test specification\nfiles with a custom syntax, which is described in the next section.  To add\na new test, place a spec file in the specs/ subdirectory, add the expected\noutput in the expected/ subdirectory, and add the test's name to the\nisolation_schedule file.\n\nisolationtester is a program that uses libpq to open multiple connections,\nand executes a test specified by a spec file. A libpq connection string\nspecifies the server and database to connect to; defaults derived from\nenvironment variables are used otherwise.\n\npg_isolation_regress is a tool similar to pg_regress, but instead of using\npsql to execute a test, it uses isolationtester.  It accepts all the same\ncommand-line arguments as pg_regress.\n\nBy default, isolationtester will wait at most 360 seconds (6 minutes)\nfor any one test step to complete.  If you need to adjust this, set\nthe environment variable PG_TEST_TIMEOUT_DEFAULT to half the desired\ntimeout in seconds.\n\n\nTest specification\n==================\n\nEach isolation test is defined by a specification file, stored in the specs\nsubdirectory.  A test specification defines some SQL \"steps\", groups them\ninto \"sessions\" (where all the steps of one session will be run in the\nsame backend), and specifies the \"permutations\" or orderings of the steps\nthat are to be run.\n\nA test specification consists of four parts, in this order:\n\nsetup { <SQL> }\n\n  The given SQL block is executed once (per permutation) before running\n  the test.  Create any test tables or other required objects here.  This\n  part is optional.  Multiple setup blocks are allowed if needed; each is\n  run separately, in the given order.  (The reason for allowing multiple\n  setup blocks is that each block is run as a single PQexec submission,\n  and some statements such as VACUUM cannot be combined with others in such\n  a block.)\n\nteardown { <SQL> }\n\n  The teardown SQL block is executed once after the test is finished. Use\n  this to clean up in preparation for the next permutation, e.g dropping\n  any test tables created by setup. This part is optional.\n\nsession <name>\n\n  There are normally several \"session\" parts in a spec file. Each\n  session is executed in its own connection. A session part consists\n  of three parts: setup, teardown and one or more \"steps\". The per-session\n  setup and teardown parts have the same syntax as the per-test setup and\n  teardown described above, but they are executed in each session. The setup\n  part might, for example, contain a \"BEGIN\" command to begin a transaction.\n\n  Each step has the syntax\n\n  step <name> { <SQL> }\n\n  where <name> is a name identifying this step, and <SQL> is a SQL statement\n  (or statements, separated by semicolons) that is executed in the step.\n  Step names must be unique across the whole spec file.\n\npermutation <step name> ...\n\n  A permutation line specifies a list of steps that are run in that order.\n  Any number of permutation lines can appear.  If no permutation lines are\n  given, the test program automatically runs all possible interleavings\n  of the steps from each session (running the steps of any one session in\n  order).  Note that the list of steps in a manually specified\n  \"permutation\" line doesn't actually have to be a permutation of the\n  available steps; it could for instance repeat some steps more than once,\n  or leave others out.  Also, each step name can be annotated with some\n  parenthesized markers, which are described below.\n\nSession and step names are SQL identifiers, either plain or double-quoted.\nA difference from standard SQL is that no case-folding occurs, so that\nFOO and \"FOO\" are the same name while FOO and Foo are different,\nwhether you quote them or not.  You must use quotes if you want to use\nan isolation test keyword (such as \"permutation\") as a name.\n\nA # character begins a comment, which extends to the end of the line.\n(This does not work inside <SQL> blocks, however.  Use the usual SQL\ncomment conventions there.)\n\nThere is no way to include a \"}\" character in an <SQL> block.\n\nFor each permutation of the session steps (whether these are manually\nspecified in the spec file, or automatically generated), the isolation\ntester runs the main setup part, then per-session setup parts, then\nthe selected session steps, then per-session teardown, then the main\nteardown script.  Each selected step is sent to the connection associated\nwith its session.  The main setup and teardown scripts are run in a\nseparate \"control\" session.\n\n\nSupport for blocking commands\n=============================\n\nEach step may contain commands that block until further action has been taken\n(most likely, some other session runs a step that unblocks it or causes a\ndeadlock).  A test that uses this ability must manually specify valid\npermutations, i.e. those that would not expect a blocked session to execute a\ncommand.  If a test fails to follow that rule, isolationtester will cancel it\nafter 2 * PG_TEST_TIMEOUT_DEFAULT seconds.  If the cancel doesn't work,\nisolationtester will exit uncleanly after a total of 4 *\nPG_TEST_TIMEOUT_DEFAULT.  Testing invalid permutations should be avoided\nbecause they can make the isolation tests take a very long time to run, and\nthey serve no useful testing purpose.\n\nNote that isolationtester recognizes that a command has blocked by looking\nto see if it is shown as waiting in the pg_locks view; therefore, only\nblocks on heavyweight locks will be detected.\n\n\nDealing with race conditions\n============================\n\nIn some cases, the isolationtester's output for a test script may vary\ndue to timing issues.  One way to deal with that is to create variant\nexpected-files, which follow the usual PG convention that variants for\nfoo.spec are named foo_1.out, foo_2.out, etc.  However, this method is\ndiscouraged since the extra files are a nuisance for maintenance.\nInstead, it's usually possible to stabilize the test output by applying\nspecial markers to some of the step names listed in a permutation line.\n\nThe general form of a permutation entry is\n\n\t<step name> [ ( <marker> [ , <marker> ... ] ) ]\n\nwhere each marker defines a \"blocking condition\".  The step will not be\nreported as completed before all the blocking conditions are satisfied.\nThe possible markers are:\n\n\t*\n\t<other step name>\n\t<other step name> notices <n>\n\nAn asterisk marker, such as mystep(*), forces the isolationtester to\nreport the step as \"waiting\" as soon as it's been launched, regardless of\nwhether it would have been detected as waiting later.  This is useful for\nstabilizing cases that are sometimes reported as waiting and other times\nreported as immediately completing, depending on the relative speeds of\nthe step and the isolationtester's status-monitoring queries.\n\nA marker consisting solely of a step name indicates that this step may\nnot be reported as completing until that other step has completed.\nThis allows stabilizing cases where two queries might be seen to complete\nin either order.  Note that this step can be *launched* before the other\nstep has completed.  (If the other step is used more than once in the\ncurrent permutation, this step cannot complete while any of those\ninstances is active.)\n\nA marker of the form \"<other step name> notices <n>\" (where <n> is a\npositive integer) indicates that this step may not be reported as\ncompleting until the other step's session has returned at least <n>\nNOTICE messages, counting from when this step is launched.  This is useful\nfor stabilizing cases where a step can return NOTICE messages before it\nactually completes, and those messages must be synchronized with the\ncompletions of other steps.\n\nNotice that these markers can only delay reporting of the completion\nof a step, not the launch of a step.  The isolationtester will launch\nthe next step in a permutation as soon as (A) all prior steps of the\nsame session are done, and (B) the immediately preceding step in the\npermutation is done or deemed blocked.  For this purpose, \"deemed\nblocked\" means that it has been seen to be waiting on a database lock,\nor that it is complete but the report of its completion is delayed by\none of these markers.\n\nIn some cases it is important not to launch a step until after the\ncompletion of a step in another session that could have been deemed\nblocked.  An example is that if step s1 in session A is issuing a\ncancel for step s2 in session B, we'd better not launch B's next step\ntill we're sure s1 is done.  If s1 is blockable, trouble could ensue.\nThe best way to prevent that is to create an empty step in session A\nand run it, without any markers, just before the next session B step.\nThe empty step cannot be launched until s1 is done, and in turn the\nnext session B step cannot be launched until the empty step finishes.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\isolation\\README",
      "directory": "test\\isolation"
    }
  },
  {
    "title": "README: test\\kerberos",
    "url": "test\\kerberos\\README",
    "content": "src/test/kerberos/README\n\nTests for Kerberos/GSSAPI functionality\n=======================================\n\nThis directory contains a test suite for Kerberos/GSSAPI\nfunctionality.  This requires a full MIT Kerberos installation,\nincluding server and client tools, and is therefore kept separate and\nnot run by default.\n\nCAUTION: The test server run by this test is configured to listen for TCP\nconnections on localhost. Any user on the same host is able to log in to the\ntest server while the tests are running. Do not run this suite on a multi-user\nsystem where you don't trust all local users! Also, this test suite creates a\nKDC server that listens for TCP/IP connections on localhost without any real\naccess control.\n\nRunning the tests\n=================\n\nNOTE: You must have given the --enable-tap-tests argument to configure.\nAlso, to use \"make installcheck\", you must have built and installed\ncontrib/dblink and contrib/postgres_fdw in addition to the core code.\n\nRun\n    make check PG_TEST_EXTRA=kerberos\nor\n    make installcheck PG_TEST_EXTRA=kerberos\nYou can use \"make installcheck\" if you previously did \"make install\".\nIn that case, the code in the installation tree is tested.  With\n\"make check\", a temporary installation tree is built from the current\nsources and then tested.\n\nEither way, this test initializes, starts, and stops a test Postgres\ncluster, as well as a test KDC server.\n\nSee src/test/perl/README for more info about running these tests.\n\nRequirements\n============\n\nMIT Kerberos server and client tools are required.  Heimdal is not\nsupported.\n\nDebian/Ubuntu packages: krb5-admin-server krb5-kdc krb5-user\n\nRHEL/CentOS/Fedora packages: krb5-server krb5-workstation\n\nFreeBSD port: krb5 (base system has Heimdal)",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\kerberos\\README",
      "directory": "test\\kerberos"
    }
  },
  {
    "title": "README: test\\ldap",
    "url": "test\\ldap\\README",
    "content": "src/test/ldap/README\n\nTests for LDAP functionality\n============================\n\nThis directory contains a test suite for LDAP functionality.  This\nrequires a full OpenLDAP installation, including server and client\ntools, and is therefore kept separate and not run by default.  You\nmight need to adjust some paths in the test file to have it find\nOpenLDAP in a place that hadn't been thought of yet.\n\nAlso, this test suite creates an LDAP server that listens for TCP/IP\nconnections on localhost without any real access control, so it is not\nsafe to run this on a system where there might be untrusted local\nusers.\n\nRunning the tests\n=================\n\nNOTE: You must have given the --enable-tap-tests argument to configure.\n\nRun\n    make check PG_TEST_EXTRA=ldap\nor\n    make installcheck PG_TEST_EXTRA=ldap\nYou can use \"make installcheck\" if you previously did \"make install\".\nIn that case, the code in the installation tree is tested.  With\n\"make check\", a temporary installation tree is built from the current\nsources and then tested.\n\nEither way, this test initializes, starts, and stops a test Postgres\ncluster, as well as a test LDAP server.\n\nSee src/test/perl/README for more info about running these tests.\n\nRequirements\n============\n\nLDAP server and client tools are required.\n\nDebian/Ubuntu packages: slapd ldap-utils\n\nRHEL/CentOS/Fedora packages: openldap-clients openldap-servers\n(You will already have needed openldap and openldap-devel to build.)\n\nFreeBSD: openldap-server\n(You will already have needed openldap-client to build.  If building\nfrom the ports source tree, you want to build net/openldap24-client\nand net/openldap24-server.)\n\nmacOS: We do not recommend trying to use the Apple-provided version of\nOpenLDAP; it's very old, plus Apple seem to have changed the launching\nconventions for slapd.  The paths in the test file are set on the\nassumption that you installed OpenLDAP using Homebrew or MacPorts.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\ldap\\README",
      "directory": "test\\ldap"
    }
  },
  {
    "title": "README: test\\locale",
    "url": "test\\locale\\README",
    "content": "src/test/locale/README\n\nLocales\n=======\n\nThis directory contains a set of tests for locales.  I provided one C\nprogram test-ctype.c to test CTYPE support in libc and the installed\nlocale data.  Then there are test-sort.pl and test-sort.py that test\ncollating.\n\nTo run a test for some locale run\n    make check-$locale\nfor example\n    make check-koi8-r\n\nCurrently, there are only tests for a few locales available.  The script\n'runall' calls test-ctype to test libc and locale data, test-sort.pl\n(uncomment test-sort.py, if you have a Python interpreter installed), and\ndoes tests on PostgreSQL with the provided SQL script files.\n\nTo add locale tests one needs to create a directory $locale and create\na Makefile (and other files) similar to koi8-r/*.  Actually, the simplest\n(I think) method is just to copy the koi8-r directory and edit/replace\nthe files.\n\nOleg.\n----\n    Oleg Broytmann     http://members.xoom.com/phd2/     phd2@earthling.net",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\locale\\README",
      "directory": "test\\locale"
    }
  },
  {
    "title": "README: test\\locale\\de_DE.ISO8859-1",
    "url": "test\\locale\\de_DE.ISO8859-1\\README",
    "content": "src/test/locale/de_DE.ISO8859-1/README\n\nde_DE.ISO-8859-1 (German) locale test.\nCreated by Armin Diehl <diehl@net-connection.de>",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\locale\\de_DE.ISO8859-1\\README",
      "directory": "test\\locale\\de_DE.ISO8859-1"
    }
  },
  {
    "title": "README: test\\locale\\gr_GR.ISO8859-7",
    "url": "test\\locale\\gr_GR.ISO8859-7\\README",
    "content": "src/test/locale/gr_GR.ISO8859-7/README\n\ngr_GR.ISO8859-7 (Greek) locale test.\nCreated by Angelos Karageorgiou <angelos@awesome.incredible.com>",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\locale\\gr_GR.ISO8859-7\\README",
      "directory": "test\\locale\\gr_GR.ISO8859-7"
    }
  },
  {
    "title": "README: test\\locale\\koi8-to-win1251",
    "url": "test\\locale\\koi8-to-win1251\\README",
    "content": "src/test/locale/koi8-to-win1251/README\n\nkoi8-to-win1251 test. The database should be created in koi8 (createdb -E koi8),\ntest uses koi8-to-win1251 converting feature.\nCreated by Oleg Broytmann <phd2@earthling.net>. Code for encodings\nconverting created by Tatsuo Ishii <t-ishii@sra.co.jp>.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\locale\\koi8-to-win1251\\README",
      "directory": "test\\locale\\koi8-to-win1251"
    }
  },
  {
    "title": "README: test\\mb",
    "url": "test\\mb\\README",
    "content": "src/test/mb/README\n\nREADME for multibyte regression test\n\t\t\t\t\t\t\t1998/7/22\n\t\t\t\t\t\t\tTatsuo Ishii\n\nThis directory contains a set of tests for multibyte supporting\nextensions for PostgreSQL. To run the test, simply type:\n\n% sh mbregress.sh",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\mb\\README",
      "directory": "test\\mb"
    }
  },
  {
    "title": "README: test\\modules",
    "url": "test\\modules\\README",
    "content": "Test extensions and libraries\n=============================\n\nsrc/test/modules contains PostgreSQL extensions that are primarily or entirely\nintended for testing PostgreSQL and/or to serve as example code. The extensions\nhere aren't intended to be installed in a production server and aren't suitable\nfor \"real work\".\n\nFurthermore, while you can do \"make install\" and \"make installcheck\" in\nthis directory or its children, it is NOT ADVISABLE to do so with a server\ncontaining valuable data.  Some of these tests may have undesirable\nside-effects on roles or other global objects within the tested server.\n\"make installcheck-world\" at the top level does not recurse into this\ndirectory.\n\nMost extensions have their own pg_regress tests or isolationtester specs. Some\nare also used by tests elsewhere in the tree.\n\nIf you're adding new hooks or other functionality exposed as C-level API this\nis where to add the tests for it.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\README",
      "directory": "test\\modules"
    }
  },
  {
    "title": "README: test\\modules\\dummy_index_am",
    "url": "test\\modules\\dummy_index_am\\README",
    "content": "Dummy Index AM\n==============\n\nDummy index AM is a module for testing any facility usable by an index\naccess method, whose code is kept a maximum simple.\n\nThis includes tests for all relation option types:\n- boolean\n- enum\n- integer\n- real\n- strings (with and without NULL as default)",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\dummy_index_am\\README",
      "directory": "test\\modules\\dummy_index_am"
    }
  },
  {
    "title": "README: test\\modules\\dummy_seclabel",
    "url": "test\\modules\\dummy_seclabel\\README",
    "content": "The dummy_seclabel module exists only to support regression testing of\nthe SECURITY LABEL statement.  It is not intended to be used in production.\n\nRationale\n=========\n\nThe SECURITY LABEL statement allows the user to assign security labels to\ndatabase objects; however, security labels can only be assigned when\nspecifically allowed by a loadable module, so this module is provided to\nallow proper regression testing.\n\nSecurity label providers intended to be used in production will typically be\ndependent on a platform-specific feature such as SELinux.  This module is\nplatform-independent, and therefore better-suited to regression testing.\n\nUsage\n=====\n\nHere's a simple example of usage:\n\n# postgresql.conf\nshared_preload_libraries = 'dummy_seclabel'\n\npostgres=# CREATE TABLE t (a int, b text);\nCREATE TABLE\npostgres=# SECURITY LABEL ON TABLE t IS 'classified';\nSECURITY LABEL\n\nThe dummy_seclabel module provides only four hardcoded\nlabels: unclassified, classified,\nsecret, and top secret.\nIt does not allow any other strings as security labels.\n\nThese labels are not used to enforce access controls.  They are only used\nto check whether the SECURITY LABEL statement works as expected,\nor not.\n\nAuthor\n======\n\nKaiGai Kohei <kaigai@ak.jp.nec.com>",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\dummy_seclabel\\README",
      "directory": "test\\modules\\dummy_seclabel"
    }
  },
  {
    "title": "README: test\\modules\\libpq_pipeline",
    "url": "test\\modules\\libpq_pipeline\\README",
    "content": "Test programs and libraries for libpq\n\nIf you have Test::Differences installed, any differences in the trace files\nare displayed in a format that's easier to read than the standard format.\n=====================================\n\nThis module was developed to test libpq's \"pipeline\" mode, but it can\nbe used for any libpq test that requires specialized C code.\n\n\"make check\" will run all the tests in the module against a temporary\nserver installation.\n\nYou can manually run a specific test by running:\n\n    ./libpq_pipeline <name of test> [ <connection string> ]\n\nThis will not start a new server, but rather connect to the server\nspecified by the connection string, or your default server if you\nleave that out.  To discover the available test names, run:\n\n    ./libpq_pipeline tests\n\nTo add a new test to this module, you need to edit libpq_pipeline.c.\nAdd a function to perform the test, and arrange for main() to call it\nwhen the name of your new test is passed to the program.  Don't forget\nto add the name of your test to the print_test_list() function, else\nthe TAP test won't run it.\n\nIf the order in which Postgres protocol messages are sent is deterministic\nin your test, you should arrange for the message sequence to be verified\nby the TAP test.  First generate a reference trace file, using a command\nlike:\n\n   ./libpq_pipeline -t traces/mynewtest.trace mynewtest\n\nThen add your test's name to the list in the $cmptrace definition in the\nt/001_libpq_pipeline.pl file.  Run \"make check\" a few times to verify\nthat the trace output actually is stable.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\libpq_pipeline\\README",
      "directory": "test\\modules\\libpq_pipeline"
    }
  },
  {
    "title": "README: test\\modules\\plsample",
    "url": "test\\modules\\plsample\\README",
    "content": "PL/Sample\n=========\n\nPL/Sample is an example template of procedural-language handler.  It is\na simple implementation, yet demonstrates some of the things that can be done\nto build a fully functional procedural-language handler.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\plsample\\README",
      "directory": "test\\modules\\plsample"
    }
  },
  {
    "title": "README: test\\modules\\spgist_name_ops",
    "url": "test\\modules\\spgist_name_ops\\README",
    "content": "spgist_name_ops implements an SP-GiST operator class that indexes\ncolumns of type \"name\", but with storage identical to that used\nby SP-GiST text_ops.\n\nThis is not terribly useful in itself, perhaps, but it allows\ntesting cases where the indexed data type is different from the leaf\ndata type and yet we can reconstruct the original indexed value.\nThat situation is not tested by any built-in SP-GiST opclass.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\spgist_name_ops\\README",
      "directory": "test\\modules\\spgist_name_ops"
    }
  },
  {
    "title": "README: test\\modules\\test_bloomfilter",
    "url": "test\\modules\\test_bloomfilter\\README",
    "content": "test_bloomfilter overview\n=========================\n\ntest_bloomfilter is a test harness module for testing Bloom filter library set\nmembership operations.  It consists of a single SQL-callable function,\ntest_bloomfilter(), plus a regression test that calls test_bloomfilter().\nMembership tests are performed against a dataset that the test harness module\ngenerates.\n\nThe test_bloomfilter() function displays instrumentation at DEBUG1 elog level\n(WARNING when the false positive rate exceeds a 1% threshold).  This can be\nused to get a sense of the performance characteristics of the Postgres Bloom\nfilter implementation under varied conditions.\n\nBitset size\n-----------\n\nThe main bloomfilter.c criteria for sizing its bitset is that the false\npositive rate should not exceed 2% when sufficient bloom_work_mem is available\n(and the caller-supplied estimate of the number of elements turns out to have\nbeen accurate).  A 1% - 2% rate is currently assumed to be suitable for all\nBloom filter callers.\n\nWith an optimal K (number of hash functions), Bloom filters should only have a\n1% false positive rate with just 9.6 bits of memory per element.  The Postgres\nimplementation's 2% worst case guarantee exists because there is a need for\nsome slop due to implementation inflexibility in bitset sizing.  Since the\nbitset size is always actually kept to a power of two number of bits, callers\ncan have their bloom_work_mem argument truncated down by almost half.\nIn practice, callers that make a point of passing a bloom_work_mem that is an\nexact power of two bitset size (such as test_bloomfilter.c) will actually get\nthe \"9.6 bits per element\" 1% false positive rate.\n\nTesting strategy\n----------------\n\nOur approach to regression testing is to test that a Bloom filter has only a 1%\nfalse positive rate for a single bitset size (2 ^ 23, or 1MB).  We test a\ndataset with 838,861 elements, which works out at 10 bits of memory per\nelement.  We round up from 9.6 bits to 10 bits to make sure that we reliably\nget under 1% for regression testing.  Note that a random seed is used in the\nregression tests because the exact false positive rate is inconsistent across\nplatforms.  Inconsistent hash function behavior is something that the\nregression tests need to be tolerant of anyway.\n\ntest_bloomfilter() SQL-callable function\n========================================\n\nThe SQL-callable function test_bloomfilter() provides the following arguments:\n\n* \"power\" is the power of two used to size the Bloom filter's bitset.\n\nThe minimum valid argument value is 23 (2^23 bits), or 1MB of memory.  The\nmaximum valid argument value is 32, or 512MB of memory.\n\n* \"nelements\" is the number of elements to generate for testing purposes.\n\n* \"seed\" is a seed value for hashing.\n\nA value < 0 is interpreted as \"use random seed\".  Varying the seed value (or\nspecifying -1) should result in small variations in the total number of false\npositives.\n\n* \"tests\" is the number of tests to run.\n\nThis may be increased when it's useful to perform many tests in an interactive\nsession.  It only makes sense to perform multiple tests when a random seed is\nused.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_bloomfilter\\README",
      "directory": "test\\modules\\test_bloomfilter"
    }
  },
  {
    "title": "README: test\\modules\\test_ddl_deparse",
    "url": "test\\modules\\test_ddl_deparse\\README",
    "content": "test_ddl_deparse is an example of how to use the pg_ddl_command datatype.\nIt is not intended to do anything useful on its own; rather, it is a\ndemonstration of how to use the datatype, and to provide some unit tests for\nit.\n\nThe functions in this extension are intended to be able to process some\npart of the struct and produce some readable output, preferably handling\nall possible cases so that SQL test code can be written.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_ddl_deparse\\README",
      "directory": "test\\modules\\test_ddl_deparse"
    }
  },
  {
    "title": "README: test\\modules\\test_ginpostinglist",
    "url": "test\\modules\\test_ginpostinglist\\README",
    "content": "test_ginpostinglist contains unit tests for the GIN posting list code in\nsrc/backend/access/gin/ginpostinglist.c.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_ginpostinglist\\README",
      "directory": "test\\modules\\test_ginpostinglist"
    }
  },
  {
    "title": "README: test\\modules\\test_integerset",
    "url": "test\\modules\\test_integerset\\README",
    "content": "test_integerset contains unit tests for testing the integer set implementation\nin src/backend/lib/integerset.c.\n\nThe tests verify the correctness of the implementation, but they can also be\nused as a micro-benchmark.  If you set the 'intset_test_stats' flag in\ntest_integerset.c, the tests will print extra information about execution time\nand memory usage.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_integerset\\README",
      "directory": "test\\modules\\test_integerset"
    }
  },
  {
    "title": "README: test\\modules\\test_json_parser",
    "url": "test\\modules\\test_json_parser\\README",
    "content": "Module `test_json_parser`\n=========================\n\nThis module contains two programs for testing the json parsers.\n\n- `test_json_parser_incremental` is for testing the incremental parser, It\n  reads in a file and passes it in very small chunks (default is 60 bytes at a\n  time) to the incremental parser. It's not meant to be a speed test but to\n  test the accuracy of the incremental parser.  There are two option arguments,\n  \"-c nn\" specifies an alternative chunk size, and \"-s\" specifies using\n  semantic routines. The semantic routines re-output the json, although not in\n  a very pretty form. The required non-option argument is the input file name.\n- `test_json_parser_perf` is for speed testing both the standard\n  recursive descent parser and the non-recursive incremental\n  parser. If given the `-i` flag it uses the non-recursive parser,\n  otherwise the standard parser. The remaining flags are the number of\n  parsing iterations and the file containing the input. Even when\n  using the non-recursive parser, the input is passed to the parser in a\n  single chunk. The results are thus comparable to those of the\n  standard parser.\n\nThe sample input file is a small, sanitized extract from a list of `delicious`\nbookmarks taken some years ago, all wrapped in a single json\narray.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_json_parser\\README",
      "directory": "test\\modules\\test_json_parser"
    }
  },
  {
    "title": "README: test\\modules\\test_misc",
    "url": "test\\modules\\test_misc\\README",
    "content": "This directory doesn't actually contain any extension module.\n\nWhat it is is a home for otherwise-unclassified TAP tests that exercise core\nserver features.  We might equally well have called it, say, src/test/misc.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_misc\\README",
      "directory": "test\\modules\\test_misc"
    }
  },
  {
    "title": "README: test\\modules\\test_oat_hooks",
    "url": "test\\modules\\test_oat_hooks\\README",
    "content": "OVERVIEW\n========\n\nThis test module, \"test_oat_hooks\", is an example of how to use the object\naccess hooks (OAT) to enforce mandatory access controls (MAC).\n\nThe testing strategy is as follows:  When this module loads, it registers hooks\nof various types.  (See below.)  GUCs are defined to control each hook,\ndetermining whether the hook allows or denies actions for which it fires.  A\nsingle additional GUC controls the verbosity of the hooks.  GUCs default to\npermissive/quiet, which allows the module to load without generating noise in\nthe log or denying any activity in the run-up to the regression test beginning.\nWhen the test begins, it uses SET commands to turn on logging and to control\neach hook's permissive/restrictive behavior.  Various SQL statements are run\nunder both superuser and ordinary user permissions.  The output is compared\nagainst the expected output to verify that the hooks behaved and fired in the\norder by expect.\n\nBecause users may care about the firing order of other system hooks relative to\nOAT hooks, ProcessUtility hooks and ExecutorCheckPerms hooks are also\nregistered by this module, with their own logging and allow/deny behavior.\n\n\nSUSET test configuration GUCs\n=============================\n\nThe following configuration parameters (GUCs) control this test module's Object\nAccess Type (OAT), Process Utility and Executor Check Permissions hooks.  The\ngeneral pattern is that each hook has a corresponding GUC which controls\nwhether the hook will allow or deny operations for which the hook gets called.\nA real-world OAT hook should certainly provide more fine-grained control than\nmerely \"allow-all\" vs. \"deny-all\", but for testing this is sufficient.\n\nNote that even when these hooks allow an action, the core permissions system\nmay still refuse the action.  The firing order of the hooks relative to the\ncore permissions system can be inferred from which NOTICE messages get emitted\nbefore an action is refused.\n\nEach hook applies the allow vs. deny setting to all operations performed by\nnon-superusers.\n\n- test_oat_hooks.deny_set_variable\n\n  Controls whether the object_access_hook_str MAC function rejects attempts to\n  set a configuration parameter.\n\n- test_oat_hooks.deny_alter_system\n\n  Controls whether the object_access_hook_str MAC function rejects attempts to\n  alter system set a configuration parameter.\n\n- test_oat_hooks.deny_object_access\n\n  Controls whether the object_access_hook MAC function rejects all operations\n  for which it is called.\n\n- test_oat_hooks.deny_exec_perms\n\n  Controls whether the exec_check_perms MAC function rejects all operations for\n  which it is called.\n\n- test_oat_hooks.deny_utility_commands\n\n  Controls whether the ProcessUtility_hook function rejects all operations for\n  which it is called.\n\n- test_oat_hooks.audit\n\n  Controls whether each hook logs NOTICE messages for each attempt, along with\n  success or failure status.  Note that clearing or setting this GUC may itself\n  generate NOTICE messages appearing before but not after, or after but not\n  before, the new setting takes effect.\n\n\nFunctions\n=========\n\nThe module registers hooks by the following names:\n\n- REGRESS_object_access_hook\n\n- REGRESS_object_access_hook_str\n\n- REGRESS_exec_check_perms\n\n- REGRESS_utility_command",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_oat_hooks\\README",
      "directory": "test\\modules\\test_oat_hooks"
    }
  },
  {
    "title": "README: test\\modules\\test_parser",
    "url": "test\\modules\\test_parser\\README",
    "content": "test_parser is an example of a custom parser for full-text\nsearch.  It doesn't do anything especially useful, but can serve as\na starting point for developing your own parser.\n\ntest_parser recognizes words separated by white space,\nand returns just two token types:\n\nmydb=# SELECT * FROM ts_token_type('testparser');\n tokid | alias |  description\n-------+-------+---------------\n     3 | word  | Word\n    12 | blank | Space symbols\n(2 rows)\n\nThese token numbers have been chosen to be compatible with the default\nparser's numbering.  This allows us to use its headline()\nfunction, thus keeping the example simple.\n\nUsage\n=====\n\nInstalling the test_parser extension creates a text search\nparser testparser.  It has no user-configurable parameters.\n\nYou can test the parser with, for example,\n\nmydb=# SELECT * FROM ts_parse('testparser', 'That''s my first own parser');\n tokid | token\n-------+--------\n     3 | That's\n    12 |\n     3 | my\n    12 |\n     3 | first\n    12 |\n     3 | own\n    12 |\n     3 | parser\n\nReal-world use requires setting up a text search configuration\nthat uses the parser.  For example,\n\nmydb=# CREATE TEXT SEARCH CONFIGURATION testcfg ( PARSER = testparser );\nCREATE TEXT SEARCH CONFIGURATION\n\nmydb=# ALTER TEXT SEARCH CONFIGURATION testcfg\nmydb-#   ADD MAPPING FOR word WITH english_stem;\nALTER TEXT SEARCH CONFIGURATION\n\nmydb=#  SELECT to_tsvector('testcfg', 'That''s my first own parser');\n          to_tsvector\n-------------------------------\n 'that':1 'first':3 'parser':5\n(1 row)\n\nmydb=# SELECT ts_headline('testcfg', 'Supernovae stars are the brightest phenomena in galaxies',\nmydb(#                    to_tsquery('testcfg', 'star'));\n                           ts_headline\n-----------------------------------------------------------------\n Supernovae <b>stars</b> are the brightest phenomena in galaxies\n(1 row)",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_parser\\README",
      "directory": "test\\modules\\test_parser"
    }
  },
  {
    "title": "README: test\\modules\\test_pg_dump",
    "url": "test\\modules\\test_pg_dump\\README",
    "content": "test_pg_dump is an extension explicitly to test pg_dump when\nextensions are present in the system.\n\nWe also make use of this module to test ALTER EXTENSION ADD/DROP.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_pg_dump\\README",
      "directory": "test\\modules\\test_pg_dump"
    }
  },
  {
    "title": "README: test\\modules\\test_predtest",
    "url": "test\\modules\\test_predtest\\README",
    "content": "test_predtest is a module for checking the correctness of the optimizer's\npredicate-proof logic, in src/backend/optimizer/util/predtest.c.\n\nThe module provides a function that allows direct application of\npredtest.c's exposed functions, predicate_implied_by() and\npredicate_refuted_by(), to arbitrary boolean expressions, with direct\ninspection of the results.  This could be done indirectly by checking\nplanner results, but it can be difficult to construct end-to-end test\ncases that prove that the expected results were obtained.\n\nIn general, the use of this function is like\n\tselect * from test_predtest('query string')\nwhere the query string must be a SELECT returning two boolean\ncolumns, for example\n\n\tselect * from test_predtest($$\n\tselect x, not x\n\tfrom (values (false), (true), (null)) as v(x)\n\t$$);\n\nThe function parses and plans the given query, and then applies the\npredtest.c code to the two boolean expressions in the SELECT list, to see\nif the first expression can be proven or refuted by the second.  It also\nexecutes the query, and checks the resulting rows to see whether any\nclaimed implication or refutation relationship actually holds.  If the\nquery is designed to exercise the expressions on a full set of possible\ninput values, as in the example above, then this provides a mechanical\ncross-check as to whether the proof code has given a correct answer.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_predtest\\README",
      "directory": "test\\modules\\test_predtest"
    }
  },
  {
    "title": "README: test\\modules\\test_rbtree",
    "url": "test\\modules\\test_rbtree\\README",
    "content": "test_rbtree is a test module for checking the correctness of red-black\ntree operations.\n\nThese tests are performed on red-black trees that store integers.\nSince the rbtree logic treats the comparison function as a black\nbox, it shouldn't be important exactly what the key type is.\n\nChecking the correctness of traversals is based on the fact that a red-black\ntree is a binary search tree, so the elements should be visited in increasing\n(for Left-Current-Right) or decreasing (for Right-Current-Left) order.\n\nAlso, this module does some checks of the correctness of the find, delete\nand leftmost operations.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_rbtree\\README",
      "directory": "test\\modules\\test_rbtree"
    }
  },
  {
    "title": "README: test\\modules\\test_regex",
    "url": "test\\modules\\test_regex\\README",
    "content": "test_regex is a module for testing the regular expression package.\nIt is mostly meant to allow us to absorb Tcl's regex test suite.\nTherefore, there are provisions to exercise regex features that\naren't currently exposed at the SQL level by PostgreSQL.\n\nCurrently, one function is provided:\n\ntest_regex(pattern text, string text, flags text) returns setof text[]\n\nReports an error if the pattern is an invalid regex.  Otherwise,\nthe first row of output contains the number of subexpressions,\nfollowed by words reporting set bit(s) in the regex's re_info field.\nIf the pattern doesn't match the string, that's all.\nIf the pattern does match, the next row contains the whole match\nas the first array element.  If there are parenthesized subexpression(s),\nfollowing array elements contain the matches to those subexpressions.\nIf the \"g\" (glob) flag is set, then additional row(s) of output similarly\nreport any additional matches.\n\nThe \"flags\" argument is a string of zero or more single-character\nflags that modify the behavior of the regex package or the test\nfunction.  As described in Tcl's reg.test file:\n\nThe flag characters are complex and a bit eclectic.  Generally speaking,\nlowercase letters are compile options, uppercase are expected re_info\nbits, and nonalphabetics are match options, controls for how the test is\nrun, or testing options.  The one small surprise is that AREs are the\ndefault, and you must explicitly request lesser flavors of RE.  The flags\nare as follows.  It is admitted that some are not very mnemonic.\n\n\t-\tno-op (placeholder)\n\t0\treport indices not actual strings\n\t\t(This substitutes for Tcl's -indices switch)\n\t!\texpect partial match, report start position anyway\n\t%\tforce small state-set cache in matcher (to test cache replace)\n\t^\tbeginning of string is not beginning of line\n\t$\tend of string is not end of line\n\t*\ttest is Unicode-specific, needs big character set\n\t+\tprovide fake xy equivalence class and ch collating element\n\t\t(Note: the equivalence class is implemented, the\n\t\tcollating element is not; so references to [.ch.] fail)\n\t,\tset REG_PROGRESS (only useful in REG_DEBUG builds)\n\t.\tset REG_DUMP (only useful in REG_DEBUG builds)\n\t:\tset REG_MTRACE (only useful in REG_DEBUG builds)\n\t;\tset REG_FTRACE (only useful in REG_DEBUG builds)\n\n\t&\ttest as both ARE and BRE\n\t\t(Not implemented in Postgres, we use separate tests)\n\tb\tBRE\n\te\tERE\n\ta\tturn advanced-features bit on (error unless ERE already)\n\tq\tliteral string, no metacharacters at all\n\n\tg\tglobal match (find all matches)\n\ti\tcase-independent matching\n\to\t(\"opaque\") do not return match locations\n\tp\tnewlines are half-magic, excluded from . and [^ only\n\tw\tnewlines are half-magic, significant to ^ and $ only\n\tn\tnewlines are fully magic, both effects\n\tx\texpanded RE syntax\n\tt\tincomplete-match reporting\n\tc\tcanmatch (equivalent to \"t0!\", in Postgres implementation)\n\ts\tmatch only at start (REG_BOSONLY)\n\n\tA\tbackslash-_a_lphanumeric seen\n\tB\tERE/ARE literal-_b_race heuristic used\n\tE\tbackslash (_e_scape) seen within []\n\tH\tlooka_h_ead constraint seen\n\tI\t_i_mpossible to match\n\tL\t_l_ocale-specific construct seen\n\tM\tunportable (_m_achine-specific) construct seen\n\tN\tRE can match empty (_n_ull) string\n\tP\tnon-_P_OSIX construct seen\n\tQ\t{} _q_uantifier seen\n\tR\tback _r_eference seen\n\tS\tPOSIX-un_s_pecified syntax seen\n\tT\tprefers shortest (_t_iny)\n\tU\tsaw original-POSIX botch: unmatched right paren in ERE (_u_gh)",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_regex\\README",
      "directory": "test\\modules\\test_regex"
    }
  },
  {
    "title": "README: test\\modules\\test_rls_hooks",
    "url": "test\\modules\\test_rls_hooks\\README",
    "content": "test_rls_hooks is an example of how to use the hooks provided for RLS to\ndefine additional policies to be used.\n\nFunctions\n=========\ntest_rls_hooks_permissive(CmdType cmdtype, Relation relation)\n    RETURNS List*\n\nReturns a list of policies which should be added to any existing\npolicies on the relation, combined with OR.\n\ntest_rls_hooks_restrictive(CmdType cmdtype, Relation relation)\n    RETURNS List*\n\nReturns a list of policies which should be added to any existing\npolicies on the relation, combined with AND.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_rls_hooks\\README",
      "directory": "test\\modules\\test_rls_hooks"
    }
  },
  {
    "title": "README: test\\modules\\test_shm_mq",
    "url": "test\\modules\\test_shm_mq\\README",
    "content": "test_shm_mq is an example of how to use dynamic shared memory\nand the shared memory message queue facilities to coordinate a user backend\nwith the efforts of one or more background workers.  It is not intended to\ndo anything useful on its own; rather, it is a demonstration of how these\nfacilities can be used, and a unit test of those facilities.\n\nThe function is this extension send the same message repeatedly through\na loop of processes.  The message payload, the size of the message queue\nthrough which it is sent, and the number of processes in the loop are\nconfigurable.  At the end, the message may be verified to ensure that it\nhas not been corrupted in transmission.\n\nFunctions\n=========\n\n\ntest_shm_mq(queue_size int8, message text,\n            repeat_count int4 default 1, num_workers int4 default 1)\n    RETURNS void\n\nThis function sends and receives messages synchronously.  The user\nbackend sends the provided message to the first background worker using\na message queue of the given size.  The first background worker sends\nthe message to the second background worker, if the number of workers\nis greater than one, and so forth.  Eventually, the last background\nworker sends the message back to the user backend.  If the repeat count\nis greater than one, the user backend then sends the message back to\nthe first worker.  Once the message has been sent and received by all\nthe coordinating processes a number of times equal to the repeat count,\nthe user backend verifies that the message finally received matches the\none originally sent and throws an error if not.\n\n\ntest_shm_mq_pipelined(queue_size int8, message text,\n                      repeat_count int4 default 1, num_workers int4 default 1,\n                      verify bool default true)\n    RETURNS void\n\nThis function sends the same message multiple times, as specified by the\nrepeat count, to the first background worker using a queue of the given\nsize.  These messages are then forwarded to each background worker in\nturn, in each case using a queue of the given size.  Finally, the last\nbackground worker sends the messages back to the user backend.  The user\nbackend uses non-blocking sends and receives, so that it may begin receiving\ncopies of the message before it has finished sending all copies of the\nmessage.  The 'verify' argument controls whether or not the\nreceived copies are checked against the message that was sent.  (This\ntakes nontrivial time so it may be useful to disable it for benchmarking\npurposes.)",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\test_shm_mq\\README",
      "directory": "test\\modules\\test_shm_mq"
    }
  },
  {
    "title": "README: test\\modules\\unsafe_tests",
    "url": "test\\modules\\unsafe_tests\\README",
    "content": "This directory doesn't actually contain any extension module.\n\nInstead it is a home for regression tests that we don't want to run\nduring \"make installcheck\" because they could have side-effects that\nseem undesirable for a production installation.\n\nAn example is that rolenames.sql tests ALTER USER ALL and so could\nhave effects on pre-existing roles.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\unsafe_tests\\README",
      "directory": "test\\modules\\unsafe_tests"
    }
  },
  {
    "title": "README: test\\modules\\xid_wraparound",
    "url": "test\\modules\\xid_wraparound\\README",
    "content": "This module contains tests for XID wraparound. The tests use two\nhelper functions to quickly consume lots of XIDs, to reach XID\nwraparound faster.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\modules\\xid_wraparound\\README",
      "directory": "test\\modules\\xid_wraparound"
    }
  },
  {
    "title": "README: test\\perl",
    "url": "test\\perl\\README",
    "content": "Perl-based TAP tests\n====================\n\nsrc/test/perl/ contains shared infrastructure that's used by Perl-based tests\nacross the source tree, particularly tests in src/bin and src/test. It's used\nto drive tests for backup and restore, replication, etc - anything that can't\nreally be expressed using pg_regress or the isolation test framework.\n\nThe tests are invoked via perl's 'prove' command, wrapped in PostgreSQL\nmakefiles to handle instance setup etc. See the $(prove_check) and\n$(prove_installcheck) targets in Makefile.global. By default every test in the\nt/ subdirectory is run. Individual test(s) can be run instead by passing\nsomething like PROVE_TESTS=\"t/001_testname.pl t/002_othertestname.pl\" to make.\n\nBy default, to keep the noise low during runs, we do not set any flags via\nPROVE_FLAGS, but this can be done on the 'make' command line if desired, eg:\n\nmake check-world PROVE_FLAGS='--verbose'\n\nWhen a test fails, the terminal output from 'prove' is usually not sufficient\nto diagnose the problem.  Look into the log files that are left under\ntmp_check/log/ to get more info.  Files named 'regress_log_XXX' are log\noutput from the perl test scripts themselves, and should be examined first.\nOther files are postmaster logs, and may be helpful as additional data.\n\nThe tests default to a timeout of 180 seconds for many individual operations.\nSlow hosts may avoid load-induced, spurious failures by setting environment\nvariable PG_TEST_TIMEOUT_DEFAULT to some number of seconds greater than 180.\nDevelopers may see faster failures by setting that environment variable to\nsome lesser number of seconds.\n\nData directories will also be left behind for analysis when a test fails;\nthey are named according to the test filename.  But if the environment\nvariable PG_TEST_NOCLEAN is set, the data directories will be retained\nregardless of test status.  This environment variable also prevents the\ntest's temporary directories from being removed.\n\n\nWriting tests\n-------------\n\nYou should prefer to write tests using pg_regress in src/test/regress, or\nisolation tester specs in src/test/isolation, if possible. If not, check to\nsee if your new tests make sense under an existing tree in src/test, like\nsrc/test/ssl, or should be added to one of the suites for an existing utility.\n\nNote that all tests and test tools should have perltidy run on them before\npatches are submitted, using perltidy --profile=src/tools/pgindent/perltidyrc\n\nTests are written using Perl's Test::More with some PostgreSQL-specific\ninfrastructure from src/test/perl providing node management, support for\ninvoking 'psql' to run queries and get results, etc. You should read the\ndocumentation for Test::More before trying to write tests.\n\nTest scripts in the t/ subdirectory of a suite are executed in alphabetical\norder.\n\nEach test script should begin with:\n\n    use strict;\n    use warnings FATAL => 'all';\n    use PostgreSQL::Test::Cluster;\n    use PostgreSQL::Test::Utils;\n    use Test::More;\n\nthen it will generally need to set up one or more nodes, run commands\nagainst them and evaluate the results. For example:\n\n    my $node = PostgreSQL::Test::Cluster->new('primary');\n    $node->init;\n    $node->start;\n\n    my $ret = $node->safe_psql('postgres', 'SELECT 1');\n    is($ret, '1', 'SELECT 1 returns 1');\n\n    $node->stop('fast');\n\nEach test script should end with:\n\n\tdone_testing();\n\nTest::Builder::Level controls how far up in the call stack a test will look\nat when reporting a failure.  This should be incremented by any subroutine\nwhich directly or indirectly calls test routines from Test::More, such as\nok() or is():\n\n    local $Test::Builder::Level = $Test::Builder::Level + 1;\n\nRead the documentation for more on how to write tests:\n\n    perldoc Test::More\n    perldoc Test::Builder\n\nFor available PostgreSQL-specific test methods and some example tests read the\nperldoc for the test modules, e.g.:\n\n    perldoc src/test/perl/PostgreSQL/Test/Cluster.pm\n\nPortability\n-----------\n\nAvoid using any bleeding-edge Perl features.  We have buildfarm animals\nrunning Perl versions as old as 5.14, so your tests will be expected\nto pass on that.\n\nAlso, do not use any non-core Perl modules except IPC::Run.  Or, if you\nmust do so for a particular test, arrange to skip the test when the needed\nmodule isn't present.  If unsure, you can consult Module::CoreList to find\nout whether a given module is part of the Perl core, and which module\nversions shipped with which Perl releases.\n\nOne way to test for compatibility with old Perl versions is to use\nperlbrew; see http://perlbrew.pl .  After installing that, do\n\n    export PERLBREW_CONFIGURE_FLAGS='-de -Duseshrplib'\n    perlbrew --force install 5.14.0\n    perlbrew use 5.14.0\n    perlbrew install-cpanm\n    cpanm install Test::Simple@0.98\n    cpanm install IPC::Run@0.79\n    cpanm install ExtUtils::MakeMaker@6.50  # downgrade\n\nTIP: if Test::Simple's utf8 regression test hangs up, try setting a\nUTF8-compatible locale, e.g. \"export LANG=en_US.utf8\".\n\nThen re-run Postgres' configure to ensure the correct Perl is used when\nrunning tests.  To verify that the right Perl was found:\n\n    grep ^PERL= config.log\n\nDue to limitations of cpanm, this recipe doesn't exactly duplicate the\nmodule list of older buildfarm animals.  The discrepancies should seldom\nmatter, but if you want to be sure, bypass cpanm and instead manually\ninstall the desired versions of Test::Simple and IPC::Run.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\perl\\README",
      "directory": "test\\perl"
    }
  },
  {
    "title": "README: test\\recovery",
    "url": "test\\recovery\\README",
    "content": "src/test/recovery/README\n\nRegression tests for recovery and replication\n=============================================\n\nThis directory contains a test suite for recovery and replication.\n\nRunning the tests\n=================\n\nNOTE: You must have given the --enable-tap-tests argument to configure.\nAlso, to use \"make installcheck\", you must have built and installed\ncontrib/pg_prewarm, contrib/pg_stat_statements and contrib/test_decoding\nin addition to the core code.\n\nRun\n    make check\nor\n    make installcheck\nYou can use \"make installcheck\" if you previously did \"make install\".\nIn that case, the code in the installation tree is tested.  With\n\"make check\", a temporary installation tree is built from the current\nsources and then tested.\n\nEither way, this test initializes, starts, and stops several test Postgres\nclusters.\n\nIf you want to test WAL consistency checking, add\nPG_TEST_EXTRA=wal_consistency_checking\nto the \"make\" command.  This is resource-intensive, so it's not done\nby default.\n\nSee src/test/perl/README for more info about running these tests.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\recovery\\README",
      "directory": "test\\recovery"
    }
  },
  {
    "title": "README: test\\regress",
    "url": "test\\regress\\README",
    "content": "Documentation concerning how to run these regression tests and interpret\nthe results can be found in the PostgreSQL manual, in the chapter\n\"Regression Tests\".",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\regress\\README",
      "directory": "test\\regress"
    }
  },
  {
    "title": "README: test\\ssl",
    "url": "test\\ssl\\README",
    "content": "src/test/ssl/README\n\nSSL regression tests\n====================\n\nThis directory contains a test suite for SSL support. It tests both\nclient-side functionality, i.e. verifying server certificates, and\nserver-side functionality, i.e. certificate authorization.\n\nCAUTION: The test server run by this test is configured to listen for\nTCP connections on localhost. Any user on the same host is able to\nlog in to the test server while the tests are running. Do not run this\nsuite on a multi-user system where you don't trust all local users!\n\nRunning the tests\n=================\n\nNOTE: You must have given the --enable-tap-tests argument to configure.\nAlso, to use \"make installcheck\", you must have built and installed\ncontrib/sslinfo in addition to the core code.\n\nRun\n    make check PG_TEST_EXTRA=ssl\nor\n    make installcheck PG_TEST_EXTRA=ssl\nYou can use \"make installcheck\" if you previously did \"make install\".\nIn that case, the code in the installation tree is tested.  With\n\"make check\", a temporary installation tree is built from the current\nsources and then tested.\n\nEither way, this test initializes, starts, and stops a test Postgres\ncluster that is accessible to other local users!\n\nSee src/test/perl/README for more info about running these tests.\n\nCertificates\n============\n\nThe test suite needs a set of public/private key pairs and certificates to\nrun:\n\nroot_ca\n\troot CA, use to sign the server and client CA certificates.\n\nserver_ca\n\tCA used to sign server certificates.\n\nclient_ca\n\tCA used to sign client certificates.\n\nserver-cn-only\nserver-cn-and-alt-names\nserver-single-alt-name\nserver-multiple-alt-names\nserver-no-names\n\tserver certificates, with small variations in the hostnames present\n        in the certificate. Signed by server_ca.\n\nserver-password\n\tsame as server-cn-only, but password-protected.\n\nclient\n\ta client certificate, for user \"ssltestuser\". Signed by client_ca.\n\nclient-revoked\n\tlike \"client\", but marked as revoked in the client CA's CRL.\n\nIn addition, there are a few files that combine various certificates together\nin the same file:\n\nboth-cas-1\n\tContains root_ca.crt, client_ca.crt and server_ca.crt, in that order.\n\nboth-cas-2\n\tContains root_ca.crt, server_ca.crt and client_ca.crt, in that order.\n\nroot+server_ca\n\tContains root_crt and server_ca.crt. For use as client's \"sslrootcert\"\n\toption.\n\nroot+client_ca\n\tContains root_crt and client_ca.crt. For use as server's \"ssl_ca_file\".\n\nclient+client_ca\n\tContains client.crt and client_ca.crt in that order. For use as client's\n\tcertificate chain.\n\nThere are also CRLs for each of the CAs: root.crl, server.crl and client.crl.\n\nFor convenience, all of these keypairs and certificates are included in the\nssl/ subdirectory. The Makefile also contains a rule, \"make sslfiles\", to\nrecreate them if you need to make changes. \"make sslfiles-clean\" is required\nin order to recreate the full set of keypairs and certificates. To rebuild\nseparate files, touch (or remove) the files in question and run \"make sslfiles\".\nThis step requires at least OpenSSL 1.1.1.\n\nNote\n====\n\nThese certificates are also used in other tests, e.g. the LDAP tests.\n\nTODO\n====\n\n* Allow the client-side of the tests to be run on different host easily.\n  Currently, you have to manually set up the certificates for the right\n  hostname, and modify the test file to skip setting up the server. And you\n  have to modify the server to accept connections from the client host.\n\n* Test having multiple server certificates, so that the private key chooses\n  the certificate to present to clients. (And the same in the client-side.)",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\ssl\\README",
      "directory": "test\\ssl"
    }
  },
  {
    "title": "README: test\\subscription",
    "url": "test\\subscription\\README",
    "content": "src/test/subscription/README\n\nRegression tests for subscription/logical replication\n=====================================================\n\nThis directory contains a test suite for subscription/logical replication.\n\nRunning the tests\n=================\n\nNOTE: You must have given the --enable-tap-tests argument to configure.\nAlso, to use \"make installcheck\", you must have built and installed\ncontrib/hstore in addition to the core code.\n\nRun\n    make check\nor\n    make installcheck\nYou can use \"make installcheck\" if you previously did \"make install\".\nIn that case, the code in the installation tree is tested.  With\n\"make check\", a temporary installation tree is built from the current\nsources and then tested.\n\nEither way, this test initializes, starts, and stops several test Postgres\nclusters.\n\nSee src/test/perl/README for more info about running these tests.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\test\\subscription\\README",
      "directory": "test\\subscription"
    }
  },
  {
    "title": "README: timezone",
    "url": "timezone\\README",
    "content": "src/timezone/README\n\nThis is a PostgreSQL adapted version of the IANA timezone library from\n\n\thttps://www.iana.org/time-zones\n\nThe latest version of the timezone data and library source code is\navailable right from that page.  It's best to get the merged file\ntzdb-NNNNX.tar.lz, since the other archive formats omit tzdata.zi.\nHistorical versions, as well as release announcements, can be found\nelsewhere on the site.\n\nSince time zone rules change frequently in some parts of the world,\nwe should endeavor to update the data files before each PostgreSQL\nrelease.  The code need not be updated as often, but we must track\nchanges that might affect interpretation of the data files.\n\n\nTime Zone data\n==============\n\nWe distribute the time zone source data as-is under src/timezone/data/.\nCurrently, we distribute just the abbreviated single-file format\n\"tzdata.zi\", to reduce the size of our tarballs as well as churn\nin our git repo.  Feeding that file to zic produces the same compiled\noutput as feeding the bulkier individual data files would do.\n\nWhile data/tzdata.zi can just be duplicated when updating, manual effort\nis needed to update the time zone abbreviation lists under tznames/.\nThese need to be changed whenever new abbreviations are invented or the\nUTC offset associated with an existing abbreviation changes.  To detect\nif this has happened, after installing new files under data/ do\n\tmake abbrevs.txt\nwhich will produce a file showing all abbreviations that are in current\nuse according to the data/ files.  Compare this to known_abbrevs.txt,\nwhich is the list that existed last time the tznames/ files were updated.\nUpdate tznames/ as seems appropriate, then replace known_abbrevs.txt\nin the same commit.  Usually, if a known abbreviation has changed meaning,\nthe appropriate fix is to make it refer to a long-form zone name instead\nof a fixed GMT offset.\n\nThe core regression test suite does some simple validation of the zone\ndata and abbreviations data (notably by checking that the pg_timezone_names\nand pg_timezone_abbrevs views don't throw errors).  It's worth running it\nas a cross-check on proposed updates.\n\nWhen there has been a new release of Windows (probably including Service\nPacks), findtimezone.c's mapping from Windows zones to IANA zones may\nneed to be updated.  We have two approaches to doing this:\n1. Consult the CLDR project's windowsZones.xml file, and add any zones\n   listed there that we don't have.  Use their \"territory=001\" mapping\n   if there's more than one IANA zone listed.\n2. Run the script in src/tools/win32tzlist.pl on a Windows machine\n   running the new release, and add any new timezones that it detects.\n   (This is not a full substitute for #1, though, as win32tzlist.pl\n   can't tell you which IANA zone to map to.)\nIn either case, never remove any zone names that have disappeared from\nWindows, since we still need to match properly on older versions.\n\n\nTime Zone code\n==============\n\nThe code in this directory is currently synced with tzcode release 2020d.\nThere are many cosmetic (and not so cosmetic) differences from the\noriginal tzcode library, but diffs in the upstream version should usually\nbe propagated to our version.  Here are some notes about that.\n\nFor the most part we want to use the upstream code as-is, but there are\nseveral considerations preventing an exact match:\n\n* For readability/maintainability we reformat the code to match our own\nconventions; this includes pgindent'ing it and getting rid of upstream's\noveruse of \"register\" declarations.  (It used to include conversion of\nold-style function declarations to C89 style, but thank goodness they\nfixed that.)\n\n* We need the code to follow Postgres' portability conventions; this\nincludes relying on configure's results rather than hand-hacked\n#defines (see private.h in particular).\n\n* Similarly, avoid relying on <stdint.h> features that may not exist on old\nsystems.  In particular this means using Postgres' definitions of the int32\nand int64 typedefs, not int_fast32_t/int_fast64_t.  Likewise we use\nPG_INT32_MIN/MAX not INT32_MIN/MAX.  (Once we desupport all PG versions\nthat don't require C99, it'd be practical to rely on <stdint.h> and remove\nthis set of diffs; but that day is not yet.)\n\n* Since Postgres is typically built on a system that has its own copy\nof the <time.h> functions, we must avoid conflicting with those.  This\nmandates renaming typedef time_t to pg_time_t, and similarly for most\nother exposed names.\n\n* zic.c's typedef \"lineno\" is renamed to \"lineno_t\", because having\n\"lineno\" in our typedefs list would cause unfortunate pgindent behavior\nin some other files where we have variables named that.\n\n* We have exposed the tzload() and tzparse() internal functions, and\nslightly modified the API of the former, in part because it now relies\non our own pg_open_tzfile() rather than opening files for itself.\n\n* tzparse() is adjusted to never try to load the TZDEFRULES zone.\n\n* There's a fair amount of code we don't need and have removed,\nincluding all the nonstandard optional APIs.  We have also added\na few functions of our own at the bottom of localtime.c.\n\n* In zic.c, we have added support for a -P (print_abbrevs) switch, which\nis used to create the \"abbrevs.txt\" summary of currently-in-use zone\nabbreviations that was described above.\n\n\nThe most convenient way to compare a new tzcode release to our code is\nto first run the tzcode source files through a sed filter like this:\n\n    sed -r \\\n        -e 's/^([ \\t]*)\\*\\*([ \\t])/\\1 *\\2/' \\\n        -e 's/^([ \\t]*)\\*\\*$/\\1 */' \\\n        -e 's|^\\*/| */|' \\\n        -e 's/\\bregister[ \\t]//g' \\\n        -e 's/\\bATTRIBUTE_PURE[ \\t]//g' \\\n        -e 's/int_fast32_t/int32/g' \\\n        -e 's/int_fast64_t/int64/g' \\\n        -e 's/intmax_t/int64/g' \\\n        -e 's/INT32_MIN/PG_INT32_MIN/g' \\\n        -e 's/INT32_MAX/PG_INT32_MAX/g' \\\n        -e 's/INTMAX_MIN/PG_INT64_MIN/g' \\\n        -e 's/INTMAX_MAX/PG_INT64_MAX/g' \\\n        -e 's/struct[ \\t]+tm\\b/struct pg_tm/g' \\\n        -e 's/\\btime_t\\b/pg_time_t/g' \\\n        -e 's/lineno/lineno_t/g' \\\n\nand then run them through pgindent.  (The first three sed patterns deal\nwith conversion of their block comment style to something pgindent\nwon't make a hash of; the remainder address other points noted above.)\nAfter that, the files can be diff'd directly against our corresponding\nfiles.  Also, it's typically helpful to diff against the previous tzcode\nrelease (after processing that the same way), and then try to apply the\ndiff to our files.  This will take care of most of the changes\nmechanically.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\timezone\\README",
      "directory": "timezone"
    }
  },
  {
    "title": "README: timezone\\tznames",
    "url": "timezone\\tznames\\README",
    "content": "src/timezone/tznames/README\n\ntznames\n=======\n\nThis directory contains files with timezone sets for PostgreSQL.  The problem\nis that time zone abbreviations are not unique throughout the world and you\nmight find out that a time zone abbreviation in the `Default' set collides\nwith the one you wanted to use.  This can be fixed by selecting a timezone\nset that defines the abbreviation the way you want it.  There might already\nbe a file here that serves your needs.  If not, you can create your own.\n\nIn order to use one of these files, you need to set\n\n   timezone_abbreviations = 'xyz'\n\nin any of the usual ways for setting a parameter, where xyz is the filename\nthat contains the desired time zone abbreviations.\n\nIf you do not find an appropriate set of abbreviations for your geographic\nlocation supplied here, please report this to <pgsql-hackers@lists.postgresql.org>.\nYour set of time zone abbreviations can then be included in future releases.\nFor the time being you can always add your own set.\n\nTypically a custom abbreviation set is made by including the `Default' set\nand then adding or overriding abbreviations as necessary.  For examples,\nsee the `Australia' and `India' files.\n\nThe files named Africa.txt, etc, are not intended to be used directly as\ntime zone abbreviation files. They contain reference definitions of time zone\nabbreviations that can be copied into a custom abbreviation file as needed.\nThese files contain most of the time zone abbreviations that were shown\nin the IANA timezone database circa 2010.\n\nHowever, it turns out that many of these abbreviations had simply been\ninvented by the IANA timezone group, and do not have currency in real-world\nuse.  The IANA group have changed their policy about that, and now prefer to\nuse numeric UTC offsets whenever there's not an abbreviation with known\nreal-world popularity.  A lot of these abbreviations therefore no longer\nappear in the IANA data, and so are marked \"obsolete\" in these data files.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\timezone\\tznames\\README",
      "directory": "timezone\\tznames"
    }
  },
  {
    "title": "README: tools\\ci",
    "url": "tools\\ci\\README",
    "content": "Postgres Continuous Integration (CI)\n====================================\n\nPostgres has two forms of CI:\n\n1) All supported branches in the main postgres repository are continuously\n   tested via the buildfarm. As this covers only the main repository, it\n   cannot be used during development of features.\n\n   For details see https://buildfarm.postgresql.org/\n\n2) For not yet merged development work, CI can be enabled for some git hosting\n   providers. This allows developers to test patches on a number of platforms\n   before they are merged (or even submitted).\n\n\nConfiguring CI on personal repositories\n=======================================\n\nCurrently postgres contains CI support utilizing cirrus-ci. cirrus-ci\ncurrently is only available for github.\n\n\nEnabling cirrus-ci in a github repository\n=========================================\n\nTo enable cirrus-ci on a repository, go to\nhttps://github.com/marketplace/cirrus-ci and select \"Public\nRepositories\". Then \"Install it for free\" and \"Complete order\". The next page\nallows to configure which repositories cirrus-ci has access to. Choose the\nrelevant repository and \"Install\".\n\nSee also https://cirrus-ci.org/guide/quick-start/\n\nOnce enabled on a repository, future commits and pull-requests in that\nrepository will automatically trigger CI builds. These are visible from the\ncommit history / PRs, and can also be viewed in the cirrus-ci UI at\nhttps://cirrus-ci.com/github/<username>/<reponame>/\n\nHint: all build log files are uploaded to cirrus-ci and can be downloaded\nfrom the \"Artifacts\" section from the cirrus-ci UI after clicking into a\nspecific task on a build's summary page.\n\n\nImages used for CI\n==================\n\nTo keep CI times tolerable, most platforms use pre-generated images. Some\nplatforms use containers, others use full VMs. Images for both are generated\nseparately from CI runs, otherwise each git repository that is being tested\nwould need to build its own set of containers, which would be wasteful (both\nin space and time.\n\nThese images are built, on a daily basis, from the specifications in\ngithub.com/anarazel/pg-vm-images/\n\n\nControlling CI via commit messages\n==================================\n\nThe behavior of CI can be controlled by special content in commit\nmessages. Currently the following controls are available:\n\n- ci-os-only: {(freebsd|linux|macos|windows|mingw)}\n\n  Only runs CI on operating systems specified. This can be useful when\n  addressing portability issues affecting only a subset of platforms.\n\n\nUsing custom compute resources for CI\n=====================================\n\nWhen running a lot of tests in a repository, cirrus-ci's free credits do not\nsuffice. In those cases a repository can be configured to use other\ninfrastructure for running tests. To do so, the REPO_CI_CONFIG_GIT_URL\nvariable can be configured for the repository in the cirrus-ci web interface,\nat https://cirrus-ci.com/github/<user or organization>. The file referenced\n(see https://cirrus-ci.org/guide/programming-tasks/#fs) by the variable can\noverwrite the default execution method for different operating systems,\ndefined in .cirrus.yml, by redefining the relevant yaml anchors.\n\nCustom compute resources can be provided using\n- https://cirrus-ci.org/guide/supported-computing-services/\n- https://cirrus-ci.org/guide/persistent-workers/",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\tools\\ci\\README",
      "directory": "tools\\ci"
    }
  },
  {
    "title": "README: tools\\ifaddrs",
    "url": "tools\\ifaddrs\\README",
    "content": "src/tools/ifaddrs/README\n\ntest_ifaddrs\n============\n\nThis program prints the addresses and netmasks of all the IPv4 and IPv6\ninterfaces on the local machine.  It is useful for testing that this\nfunctionality works on various platforms.  If \"samehost\" and \"samenet\"\nin pg_hba.conf don't seem to work right, run this program to see what\nis happening.\n\nUsage:\ttest_ifaddrs",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\tools\\ifaddrs\\README",
      "directory": "tools\\ifaddrs"
    }
  },
  {
    "title": "README: tools\\pginclude",
    "url": "tools\\pginclude\\README",
    "content": "src/tools/pginclude/README\n\nNOTE: headerscheck and headerscheck --cplusplus are in current use,\nand any problems they find should generally get fixed.  The other\nscripts in this directory have not been used in some time, and have\nissues.  pgrminclude in particular has a history of creating more\nproblems than it fixes.  Be very wary of applying their results\nblindly.\n\n\npginclude\n=========\n\nThese utilities help clean up #include file usage.  They should be run\nin this order so that the include files have the proper includes before\nthe C files are tested.\n\npgfixinclude\tchange #include's to <> or \"\"\n\npgcompinclude [-v]\n\t\treport which #include files can not compile on their own\n\npgrminclude [-v]\n\t\tremove extra #include's\n\npgcheckdefines\n\t\tcheck for #ifdef tests on symbols defined in files that\n\t\tweren't included --- this is a necessary sanity check on\n\t\tpgrminclude\n\npgdefine\tcreate macro calls for all defines in the file (used by\n\t\tthe above routines)\n\nIt is also a good idea to sort the pg-specific include files in\nalphabetic order.  This is best done with a text editor. Typical usage\norder would be:\n\n\tpgfixinclude\n\tsort include references\n\trun multiple times:\n\t\tpgcompinclude\n\t\tpgrminclude /src/include\n\tpgrminclude /\n\tpgcheckdefines\n\nThere is a complexity when modifying /src/include.  If include file 1\nincludes file 2, and file 2 includes file 3, then when file 1 is\nprocessed, it needs only file 2, not file 3.  However, if later, include\nfile 2 is processed, and file 3 is not needed by file 2 and is removed,\nfile 1 might then need to include file 3.  For this reason, the\npgcompinclude and pgrminclude /src/include steps must be run several\ntimes until all includes compile cleanly.\n\nAlso, tests should be done with configure settings of --enable-cassert\nand EXEC_BACKEND on and off.  It is also wise to test a WIN32 compile.\n\nAnother tools that does a similar task is at:\n\n\thttp://code.google.com/p/include-what-you-use/\n\nAn include file visualizer script is available at:\n\n\thttp://archives.postgresql.org/pgsql-hackers/2011-09/msg00311.php\n\n\nheaderscheck\n============\n\nThis script can be run to verify that all Postgres include files meet\nthe project convention that they will compile \"standalone\", that is\nwith no prerequisite headers other than postgres.h (or postgres_fe.h\nor c.h, as appropriate).\n\nA small number of header files are exempted from this requirement,\nand are skipped by the headerscheck script.\n\nThe easy way to run the script is to say \"make -s headerscheck\" in\nthe top-level build directory after completing a build.  You should\nhave included \"--with-perl --with-python\" in your configure options,\nelse you're likely to get errors about related headers not being found.\n\nA limitation of the current script is that it doesn't know exactly which\nheaders are for frontend or backend; when in doubt it uses postgres.h as\nprerequisite, even if postgres_fe.h or c.h would be more appropriate.\nAlso note that the contents of macros are not checked; this is intentional.\n\n\nheaderscheck --cplusplus\n========================\n\nThe headerscheck in --cplusplus mode can be run to verify that all\nPostgres include files meet the project convention that they will\ncompile as C++ code.  Although the project's coding language is C,\nsome people write extensions in C++, so it's helpful for include files\nto be C++-clean.\n\nA small number of header files are exempted from this requirement,\nand are skipped by the script in the --cplusplus mode.\n\nThe easy way to run the script is to say \"make -s cpluspluscheck\" in\nthe top-level build directory after completing a build.  You should\nhave included \"--with-perl --with-python\" in your configure options,\nelse you're likely to get errors about related headers not being found.\n\nIf you are using a non-g++-compatible C++ compiler, you may need to\noverride the script's CXXFLAGS setting by setting a suitable environment\nvalue.\n\nA limitation of the current script is that it doesn't know exactly which\nheaders are for frontend or backend; when in doubt it uses postgres.h as\nprerequisite, even if postgres_fe.h or c.h would be more appropriate.\nAlso note that the contents of macros are not checked; this is intentional.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\tools\\pginclude\\README",
      "directory": "tools\\pginclude"
    }
  },
  {
    "title": "README: tools\\pgindent",
    "url": "tools\\pgindent\\README",
    "content": "pgindent'ing the PostgreSQL source tree\n=======================================\n\npgindent is used to maintain uniform layout style in our C and Perl code,\nand should be run for every commit. There are additional code beautification\ntasks which should be performed at least once per release cycle.\n\nYou might find this blog post interesting:\nhttp://adpgtech.blogspot.com/2015/05/running-pgindent-on-non-core-code-or.html\n\n\nPREREQUISITES:\n\n1) Install pg_bsd_indent in your PATH.  Its source code is in the\n   sibling directory src/tools/pg_bsd_indent; see the directions\n   in that directory's README file.\n\n2) Install perltidy.  Please be sure it is version 20230309 (older and newer\n   versions make different formatting choices, and we want consistency).\n   You can get the correct version from\n   https://cpan.metacpan.org/authors/id/S/SH/SHANCOCK/\n   To install, follow the usual install process for a Perl module\n   (\"man perlmodinstall\" explains it).  Or, if you have cpan installed,\n   this should work:\n   cpan SHANCOCK/Perl-Tidy-20230309.tar.gz\n   Or if you have cpanm installed, you can just use:\n   cpanm https://cpan.metacpan.org/authors/id/S/SH/SHANCOCK/Perl-Tidy-20230309.tar.gz\n\n\nDOING THE INDENT RUN BEFORE A NORMAL COMMIT:\n\n1) Change directory to the top of the source tree.\n\n2) Run pgindent on the C files:\n\n\tsrc/tools/pgindent/pgindent .\n\n   If any files generate errors, restore their original versions with\n   \"git checkout\", and see below for cleanup ideas.\n\n3) Check for any newly-created files using \"git status\"; there shouldn't\n   be any.  (pgindent leaves *.BAK files behind if it has trouble, while\n   perltidy leaves *.LOG files behind.)\n\n4) If pgindent wants to change anything your commit wasn't touching,\n   stop and figure out why.  If it is making ugly whitespace changes\n   around typedefs your commit adds, you need to add those typedefs\n   to src/tools/pgindent/typedefs.list.\n\n5) If you have the patience, it's worth eyeballing the \"git diff\" output\n   for any egregiously ugly changes.  See below for cleanup ideas.\n\n6) Do a full test build:\n\n\tmake -s clean\n\tmake -s all\t# look for unexpected warnings, and errors of course\n\tmake check-world\n\n   Your configure switches should include at least --enable-tap-tests\n   or else much of the Perl code won't get exercised.\n   The ecpg regression tests may well fail due to pgindent's updates of\n   header files that get copied into ecpg output; if so, adjust the\n   expected-files to match.\n\n\nAT LEAST ONCE PER RELEASE CYCLE:\n\n1) Download the latest typedef file from the buildfarm:\n\n\twget -O src/tools/pgindent/typedefs.list https://buildfarm.postgresql.org/cgi-bin/typedefs.pl\n\n   This step resolves any differences between the incrementally updated\n   version of the file and a clean, autogenerated one.\n   (See https://buildfarm.postgresql.org/cgi-bin/typedefs.pl?show_list for\n   a full list of typedef files, if you want to indent some back branch.)\n\n2) Run pgindent as above.\n\n3) Indent the Perl code using perltidy:\n\n\tsrc/tools/pgindent/pgperltidy .\n\n   If you want to use some perltidy version that's not in your PATH,\n   first set the PERLTIDY environment variable to point to it.\n\n4) Reformat the bootstrap catalog data files:\n\n\t./configure     # \"make\" will not work in an unconfigured tree\n\tcd src/include/catalog\n\tmake reformat-dat-files\n\tcd ../../..\n\n5) When you're done, \"git commit\" everything including the typedefs.list file\n   you used.\n\n6) Add the newly created commit(s) to the .git-blame-ignore-revs file so\n   that \"git blame\" ignores the commits (for anybody that has opted-in\n   to using the ignore file).  Follow the instructions that appear at\n   the top of the .git-blame-ignore-revs file.\n\nAnother \"git commit\" will be required for your ignore file changes.\n\n---------------------------------------------------------------------------\n\nCleaning up in case of failure or ugly output\n---------------------------------------------\n\nIf you don't like the results for any particular file, \"git checkout\"\nthat file to undo the changes, patch the file as needed, then repeat\nthe indent process.\n\npgindent will reflow any comment block that's not at the left margin.\nIf this messes up manual formatting that ought to be preserved, protect\nthe comment block with some dashes:\n\n\t/*----------\n\t * Text here will not be touched by pgindent.\n\t */\n\nOdd spacing around typedef names might indicate an incomplete typedefs list.\n\npgindent will mangle both declaration and definition of a C function whose\nname matches a typedef.  Currently the best workaround is to choose\nnon-conflicting names.\n\npgindent can get confused by #if sequences that look correct to the compiler\nbut have mismatched braces/parentheses when considered as a whole.  Usually\nthat looks pretty unreadable to humans too, so best practice is to rearrange\nthe #if tests to avoid it.\n\nSometimes, if pgindent or perltidy produces odd-looking output, it's because\nof minor bugs like extra commas.  Don't hesitate to clean that up while\nyou're at it.\n\n---------------------------------------------------------------------------\n\nBSD indent\n----------\n\nWe have standardized on FreeBSD's indent, and renamed it pg_bsd_indent.\npg_bsd_indent does differ slightly from FreeBSD's version, mostly in\nbeing more easily portable to non-BSD platforms.  Find it in the\nsibling directory src/tools/pg_bsd_indent.\n\nGNU indent, version 2.2.6, has several problems, and is not recommended.\nThese bugs become pretty major when you are doing >500k lines of code.\nIf you don't believe me, take a directory and make a copy.  Run pgindent\non the copy using GNU indent, and do a diff -r. You will see what I\nmean. GNU indent does some things better, but mangles too.  For details,\nsee:\n\n\thttp://archives.postgresql.org/pgsql-hackers/2003-10/msg00374.php\n\thttp://archives.postgresql.org/pgsql-hackers/2011-04/msg01436.php\n\n---------------------------------------------------------------------------\n\nWhich files are processed\n-------------------------\n\nThe pgindent run processes (nearly) all PostgreSQL *.c and *.h files,\nbut we currently exclude *.y and *.l files, as well as *.c and *.h files\nderived from *.y and *.l files.  Additional exceptions are listed\nin exclude_file_patterns; see the notes therein for rationale.\n\nNote that we do not exclude ecpg's header files from the run.  Some of them\nget copied verbatim into ecpg's output, meaning that ecpg's expected files\nmay need to be updated to match.\n\nThe perltidy run processes all *.pl and *.pm files, plus a few\nexecutable Perl scripts that are not named that way.  See the \"find\"\nrules in pgperltidy for details.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\tools\\pgindent\\README",
      "directory": "tools\\pgindent"
    }
  },
  {
    "title": "README: tools\\pg_bsd_indent",
    "url": "tools\\pg_bsd_indent\\README",
    "content": "src/tools/pg_bsd_indent/README\n\nThis is a lightly modified version of the \"indent\" program maintained\nby the FreeBSD project.  The modifications are mostly to make it portable\nto non-BSD-ish platforms, though we do have one formatting switch we\ncouldn't convince upstream to take.\n\nTo build it, configure the surrounding Postgres source tree,\nthen run \"make\" in this directory.\nOptionally, run \"make test\" for some simple tests.\n\nYou'll need to install pg_bsd_indent somewhere in your PATH before\nusing it.  Most likely, if you're a developer, you don't want to\nput it in the same place as where the surrounding Postgres build\ngets installed.  Therefore, do this part with something like\n\n\tmake install prefix=/usr/local\n\nIf you are using Meson to build, the standard build targets will\nbuild pg_bsd_indent and also test it, but there is not currently\nprovision for installing it anywhere.  Manually copy the built\nexecutable from build/src/tools/pg_bsd_indent/pg_bsd_indent to\nwherever you want to put it.\n\n\nIf you happen to be hacking upon the indent source code, the closest\napproximation to the existing indentation style seems to be\n\n\t./pg_bsd_indent -i4 -l79 -di12 -nfc1 -nlp -sac somefile.c\n\nalthough this has by no means been rigorously adhered to.\n(What was that saw about the shoemaker's children?)\nWe're not planning to re-indent to Postgres style, because that\nwould make it difficult to compare to the FreeBSD sources.\n\n----------\n\nThe FreeBSD originals of the files in this directory bear the\n\"4-clause\" version of the BSD license.  We have removed the\n\"advertising\" clauses, as per UC Berkeley's directive here:\nftp://ftp.cs.berkeley.edu/pub/4bsd/README.Impt.License.Change\nwhich reads:\n\nJuly 22, 1999\n\nTo All Licensees, Distributors of Any Version of BSD:\n\nAs you know, certain of the Berkeley Software Distribution (\"BSD\") source\ncode files require that further distributions of products containing all or\nportions of the software, acknowledge within their advertising materials\nthat such products contain software developed by UC Berkeley and its\ncontributors.\n\nSpecifically, the provision reads:\n\n\"     * 3. All advertising materials mentioning features or use of this software\n      *    must display the following acknowledgement:\n      *    This product includes software developed by the University of\n      *    California, Berkeley and its contributors.\"\n\nEffective immediately, licensees and distributors are no longer required to\ninclude the acknowledgement within advertising materials.  Accordingly, the\nforegoing paragraph of those BSD Unix files containing it is hereby deleted\nin its entirety.\n\nWilliam Hoskins\nDirector, Office of Technology Licensing\nUniversity of California, Berkeley\n\n----------\n\nWhat follows is the README file as maintained by FreeBSD indent.\n\n----------\n\n  $FreeBSD: head/usr.bin/indent/README 105244 2002-10-16 13:58:39Z charnier $\n\nThis is the C indenter, it originally came from the University of Illinois\nvia some distribution tape for PDP-11 Unix.  It has subsequently been\nhacked upon by James Gosling @ CMU.  It isn't very pretty, and really needs\nto be completely redone, but it is probably the nicest C pretty printer\naround.\n\nFurther additions to provide \"Kernel Normal Form\" were contributed\nby the folks at Sun Microsystems.\n\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n> From mnetor!yunexus!oz@uunet.UU.NET Wed Mar  9 15:30:55 1988\n> Date: Tue, 8 Mar 88 18:36:25 EST\n> From: yunexus!oz@uunet.UU.NET (Ozan Yigit)\n> To: bostic@okeeffe.berkeley.edu\n> Cc: ccvaxa!willcox@uunet.UU.NET, jag@sun.com, rsalz@uunet.UU.NET\n> In-Reply-To: Keith Bostic's message of Tue, 16 Feb 88 16:09:06 PST \n> Subject: Re: Indent...\n\nThank you for your response about indent. I was wrong in my original\nobservation (or mis-observation :-). UCB did keep the Illinois\ncopyright intact.\n\nThe issue still is whether we can distribute indent, and if we can, which\nversion. David Willcox (the author) states that:\n\n| Several people have asked me on what basis I claim that indent is in\n| the public domain.  I knew I would be sorry I made that posting.\n| \n| Some history.  Way back in 1976, the project I worked on at the\n| University of Illinois Center for Advanced Computation had a huge\n| battle about how to format C code.  After about a week of fighting, I\n| got disgusted and wrote a program, which I called indent, to reformat C\n| code.  It had a bunch of different options that would let you format\n| the output the way you liked.  In particular, all of the different\n| formats being championed were supported.\n| \n| It was my first big C program.  It was ugly.  It wasn't designed, it\n| just sort of grew.  But it pretty much worked, and it stopped most of\n| the fighting.\n| \n| As a matter of form, I included a University of Illinois Copyright\n| notice.  However, my understanding was that, since the work was done\n| on an ARPA contract, it was in the public domain.\n| \n| Time passed.  Some years later, indent showed up on one of the early\n| emacs distributions.\n| \n| Later still, someone from UC Berkeley called the UofI and asked if\n| indent was in the public domain.  They wanted to include it in their\n| UNIX distributions, along with the emacs stuff.  I was no longer at the\n| UofI, but Rob Kolstad, who was, asked me about it.  I told him I didn't\n| care if they used it, and since then it has been on the BSD distributions.\n| \n| Somewhere along the way, several other unnamed people have had their\n| hands in it.  It was converted to understand version 7 C.  (The\n| original was version 6.)  It was converted from its original filter\n| interface to its current \"blow away the user's file\" interface.\n| The $HOME/.indent.pro file parsing was added.  Some more formatting\n| options were added.\n| \n| The source I have right now has two copyright notices.  One is the\n| original from the UofI.  One is from Berkeley.\n| \n| I am not a lawyer, and I certainly do not understand copyright law.  As\n| far as I am concerned, the bulk of this program, everything covered by\n| the UofI copyright, is in the public domain, and worth every penny.\n| Berkeley's copyright probably should only cover their changes, and I\n| don't know their feelings about sending it out.  \n\nIn any case, there appears to be none at UofI to clarify/and change\nthat copyright, but I am confident (based on the statements of its\nauthor) that the code, as it stands with its copyright, is\ndistributable, and will not cause any legal problems.\n\nHence, the issue reduces to *which* one to distribute through\ncomp.sources.unix. I would suggest that with the permission of you\nfolks (given that you have parts copyrighted), we distribute the 4.3\nversion of indent, which appears to be the most up-to-date version. I\nhappen to have just about every known version of indent, including the\nvery original submission from the author to a unix tape, later the\nG-Emacs version, any 4.n version, sun version and the Unipress\nversion.  I still think we should not have to \"go-back-in-time\" and\nre-do all the work you people have done.\n\nI hope to hear from you as to what you think about this. You may of\ncourse send 4.3 version to the moderator directly, or you can let me\nknow of your permission, and I will send the sources, or you can let\nme know that 4.3 version is off-limits, in which case we would probably\nhave to revert to an older version. One way or another, I hope to get\na version of indent to comp.sources.unix.\n\nregards..\toz\n\ncc: ccvaxa!willcox\n    sun.com!jar\n    uunet!rsalz",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\tools\\pg_bsd_indent\\README",
      "directory": "tools\\pg_bsd_indent"
    }
  },
  {
    "title": "README: tutorial",
    "url": "tutorial\\README",
    "content": "src/tutorial/README\n\ntutorial\n========\n\nThis directory contains SQL tutorial scripts.  To look at them, first do a\n\t% make\nto compile all the scripts and C files for the user-defined functions\nand types.  (make needs to be GNU make --- it may be named something\ndifferent on your system, often 'gmake')\n\nThen, run psql with the -s (single-step) flag:\n\t% psql -s\n\nFrom within psql, you can try each individual script file by using\npsql's \\i <filename> command.",
    "source_type": "README",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\tutorial\\README",
      "directory": "tutorial"
    }
  },
  {
    "title": "Comment from brin.c:1",
    "url": "backend\\access\\brin\\brin.c:1",
    "content": "brin.c\n\tImplementation of BRIN indexes for Postgres\n\nSee src/backend/access/brin/README for details.\n\nPortions Copyright (c) 1996-2024, PostgreSQL Global Development Group\nPortions Copyright (c) 1994, Regents of the University of California\n\nIDENTIFICATION\n  src/backend/access/brin/brin.c\n\nTODO\n\t* ScalarArrayOpExpr (amsearcharray -> SK_SEARCHARRAY)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1
    }
  },
  {
    "title": "Comment from brin.c:53",
    "url": "backend\\access\\brin\\brin.c:53",
    "content": "Status for index builds performed in parallel.  This is allocated in a\ndynamic shared memory segment.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 53
    }
  },
  {
    "title": "Comment from brin.c:59",
    "url": "backend\\access\\brin\\brin.c:59",
    "content": "These fields are not modified during the build.  They primarily exist\nfor the benefit of worker processes that need to create state\ncorresponding to that used by the leader.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 59
    }
  },
  {
    "title": "Comment from brin.c:70",
    "url": "backend\\access\\brin\\brin.c:70",
    "content": "workersdonecv is used to monitor the progress of workers.  All parallel\nparticipants must indicate that they are done before leader can use\nresults built by the workers (and before leader can write the data into\nthe index).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 70
    }
  },
  {
    "title": "Comment from brin.c:78",
    "url": "backend\\access\\brin\\brin.c:78",
    "content": "mutex protects all fields before heapdesc.\n\nThese fields contain status information of interest to BRIN index\nbuilds that must work just the same when an index is built in parallel.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 78
    }
  },
  {
    "title": "Comment from brin.c:86",
    "url": "backend\\access\\brin\\brin.c:86",
    "content": "Mutable state that is maintained by workers, and reported back to\nleader at end of the scans.\n\nnparticipantsdone is number of worker processes finished.\n\nreltuples is the total number of input heap tuples.\n\nindtuples is the total number of tuples that made it into the index.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 86
    }
  },
  {
    "title": "Comment from brin.c:100",
    "url": "backend\\access\\brin\\brin.c:100",
    "content": "ParallelTableScanDescData data follows. Can't directly embed here, as\nimplementations of the parallel table scan desc interface might need\nstronger alignment.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 100
    }
  },
  {
    "title": "Comment from brin.c:107",
    "url": "backend\\access\\brin\\brin.c:107",
    "content": "Return pointer to a BrinShared's parallel table scan.\n\nc.f. shm_toc_allocate as to why BUFFERALIGN is used, rather than just\nMAXALIGN.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 107
    }
  },
  {
    "title": "Comment from brin.c:124",
    "url": "backend\\access\\brin\\brin.c:124",
    "content": "nparticipanttuplesorts is the exact number of worker processes\nsuccessfully launched, plus one leader process if it participates as a\nworker (only DISABLE_LEADER_PARTICIPATION builds avoid leader\nparticipating as a worker).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 124
    }
  },
  {
    "title": "Comment from brin.c:132",
    "url": "backend\\access\\brin\\brin.c:132",
    "content": "Leader process convenience pointers to shared state (leader avoids TOC\nlookups).\n\nbrinshared is the shared state for entire build.  sharedsort is the\nshared, tuplesort-managed state passed to each process tuplesort.\nsnapshot is the snapshot used by the scan iff an MVCC snapshot is\nrequired.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 132
    }
  },
  {
    "title": "Comment from brin.c:148",
    "url": "backend\\access\\brin\\brin.c:148",
    "content": "We use a BrinBuildState during initial construction of a BRIN index.\nThe running state is kept in a BrinMemTuple.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 148
    }
  },
  {
    "title": "Comment from brin.c:169",
    "url": "backend\\access\\brin\\brin.c:169",
    "content": "bs_leader is only present when a parallel index build is performed, and\nonly in the leader process. (Actually, only the leader process has a\nBrinBuildState.)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 169
    }
  },
  {
    "title": "Comment from brin.c:177",
    "url": "backend\\access\\brin\\brin.c:177",
    "content": "The sortstate is used by workers (including the leader). It has to be\npart of the build state, because that's the only thing passed to the\nbuild callback etc.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 177
    }
  },
  {
    "title": "Comment from brin.c:185",
    "url": "backend\\access\\brin\\brin.c:185",
    "content": "We use a BrinInsertState to capture running state spanning multiple\nbrininsert invocations, within the same command.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 185
    }
  },
  {
    "title": "Comment from brin.c:242",
    "url": "backend\\access\\brin\\brin.c:242",
    "content": "BRIN handler function: return IndexAmRoutine with access method parameters\nand callbacks.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 242
    }
  },
  {
    "title": "Comment from brin.c:301",
    "url": "backend\\access\\brin\\brin.c:301",
    "content": "Initialize a BrinInsertState to maintain state to be used across multiple\ntuple inserts, within the same command.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 301
    }
  },
  {
    "title": "Comment from brin.c:322",
    "url": "backend\\access\\brin\\brin.c:322",
    "content": "A tuple in the heap is being inserted.  To keep a brin index up to date,\nwe need to obtain the relevant index tuple and compare its stored values\nwith those of the new tuple.  If the tuple values are not consistent with\nthe summary tuple, we need to update the index tuple.\n\nIf autosummarization is enabled, check if we need to summarize the previous\npage range.\n\nIf the range is not currently summarized (i.e. the revmap returns NULL for\nit), there's nothing to do for this tuple.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 322
    }
  },
  {
    "title": "Comment from brin.c:352",
    "url": "backend\\access\\brin\\brin.c:352",
    "content": "If first time through in this statement, initialize the insert state\nthat we keep for all the inserts in the command.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 352
    }
  },
  {
    "title": "Comment from brin.c:363",
    "url": "backend\\access\\brin\\brin.c:363",
    "content": "origHeapBlk is the block number where the insertion occurred.  heapBlk\nis the first block in the corresponding page range.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 363
    }
  },
  {
    "title": "Comment from brin.c:379",
    "url": "backend\\access\\brin\\brin.c:379",
    "content": "If auto-summarization is enabled and we just inserted the first\ntuple into the first block of a new non-first page range, request a\nsummarization run of the previous range.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 379
    }
  },
  {
    "title": "Comment from brin.c:435",
    "url": "backend\\access\\brin\\brin.c:435",
    "content": "The tuple is consistent with the new values, so there's nothing\nto do.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 435
    }
  },
  {
    "title": "Comment from brin.c:451",
    "url": "backend\\access\\brin\\brin.c:451",
    "content": "Make a copy of the old tuple, so that we can compare it after\nre-acquiring the lock.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 451
    }
  },
  {
    "title": "Comment from brin.c:458",
    "url": "backend\\access\\brin\\brin.c:458",
    "content": "Before releasing the lock, check if we can attempt a same-page\nupdate.  Another process could insert a tuple concurrently in\nthe same page though, so downstream we must be prepared to cope\nif this turns out to not be possible after all.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 458
    }
  },
  {
    "title": "Comment from brin.c:468",
    "url": "backend\\access\\brin\\brin.c:468",
    "content": "Try to update the tuple.  If this doesn't work for whatever\nreason, we need to restart from the top; the revmap might be\npointing at a different tuple for this block now, so we need to\nrecompute to ensure both our new heap tuple and the other\ninserter's are covered by the combined tuple.  It might be that\nwe don't need to update at all.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 468
    }
  },
  {
    "title": "Comment from brin.c:499",
    "url": "backend\\access\\brin\\brin.c:499",
    "content": "Callback to clean up the BrinInsertState once all tuple inserts are done.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 499
    }
  },
  {
    "title": "Comment from brin.c:511",
    "url": "backend\\access\\brin\\brin.c:511",
    "content": "do this first to avoid dangling pointer if we fail partway through",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 511
    }
  },
  {
    "title": "Comment from brin.c:514",
    "url": "backend\\access\\brin\\brin.c:514",
    "content": "Clean up the revmap. Note that the brinDesc has already been cleaned up\nas part of its own memory context.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 514
    }
  },
  {
    "title": "Comment from brin.c:522",
    "url": "backend\\access\\brin\\brin.c:522",
    "content": "Initialize state for a BRIN index scan.\n\nWe read the metapage here to determine the pages-per-range number that this\nindex was built with.  Note that since this cannot be changed while we're\nholding lock on index, it's not necessary to recompute it during brinrescan.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 522
    }
  },
  {
    "title": "Comment from brin.c:545",
    "url": "backend\\access\\brin\\brin.c:545",
    "content": "Execute the index scan.\n\nThis works by reading index TIDs from the revmap, and obtaining the index\ntuples pointed to by them; the summary values in the index tuples are\ncompared to the scan keys.  We return into the TID bitmap all the pages in\nranges corresponding to index tuples that match the scan keys.\n\nIf a TID from the revmap is read as InvalidTID, we know that range is\nunsummarized.  Pages in those ranges need to be returned regardless of scan\nkeys.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 545
    }
  },
  {
    "title": "Comment from brin.c:587",
    "url": "backend\\access\\brin\\brin.c:587",
    "content": "We need to know the size of the table so that we know how long to\niterate on the revmap.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 587
    }
  },
  {
    "title": "Comment from brin.c:596",
    "url": "backend\\access\\brin\\brin.c:596",
    "content": "Make room for the consistent support procedures of indexed columns.  We\ndon't look them up here; we do that lazily the first time we see a scan\nkey reference each of them.  We rely on zeroing fn_oid to InvalidOid.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 596
    }
  },
  {
    "title": "Comment from brin.c:603",
    "url": "backend\\access\\brin\\brin.c:603",
    "content": "Make room for per-attribute lists of scan keys that we'll pass to the\nconsistent support procedure. We don't know which attributes have scan\nkeys, so we allocate space for all attributes. That may use more memory\nbut it's probably cheaper than determining which attributes are used.\n\nWe keep null and regular keys separate, so that we can pass just the\nregular keys to the consistent function easily.\n\nTo reduce the allocation overhead, we allocate one big chunk and then\ncarve it into smaller arrays ourselves. All the pieces have exactly the\nsame lifetime, so that's OK.\n\nXXX The widest index can have 32 attributes, so the amount of wasted\nmemory is negligible. We could invent a more compact approach (with\njust space for used attributes) but that would make the matching more\ncomplex so it's not a good trade-off.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 603
    }
  },
  {
    "title": "Comment from brin.c:659",
    "url": "backend\\access\\brin\\brin.c:659",
    "content": "Preprocess the scan keys - split them into per-attribute arrays.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 659
    }
  },
  {
    "title": "Comment from brin.c:665",
    "url": "backend\\access\\brin\\brin.c:665",
    "content": "The collation of the scan key must match the collation used in the\nindex column (but only if the search is not IS NULL/ IS NOT NULL).\nOtherwise we shouldn't be using this index ...",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 665
    }
  },
  {
    "title": "Comment from brin.c:675",
    "url": "backend\\access\\brin\\brin.c:675",
    "content": "First time we see this index attribute, so init as needed.\n\nThis is a bit of an overkill - we don't know how many scan keys are\nthere for this attribute, so we simply allocate the largest number\npossible (as if all keys were for this attribute). This may waste a\nbit of memory, but we only expect small number of scan keys in\ngeneral, so this should be negligible, and repeated repalloc calls\nare not free either.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 675
    }
  },
  {
    "title": "Comment from brin.c:689",
    "url": "backend\\access\\brin\\brin.c:689",
    "content": "First time we see this attribute, so no key/null keys.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 689
    }
  },
  {
    "title": "Comment from brin.c:712",
    "url": "backend\\access\\brin\\brin.c:712",
    "content": "allocate an initial in-memory tuple, out of the per-range memcxt",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 712
    }
  },
  {
    "title": "Comment from brin.c:715",
    "url": "backend\\access\\brin\\brin.c:715",
    "content": "Setup and use a per-range memory context, which is reset every time we\nloop below.  This avoids having to free the tuples within the loop.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 715
    }
  },
  {
    "title": "Comment from brin.c:724",
    "url": "backend\\access\\brin\\brin.c:724",
    "content": "Now scan the revmap.  We start by querying for heap page 0,\nincrementing by the number of pages per range; this gives us a full\nview of the table.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 724
    }
  },
  {
    "title": "Comment from brin.c:750",
    "url": "backend\\access\\brin\\brin.c:750",
    "content": "For page ranges with no indexed tuple, we must return the whole\nrange; otherwise, compare it to the scan keys.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 750
    }
  },
  {
    "title": "Comment from brin.c:763",
    "url": "backend\\access\\brin\\brin.c:763",
    "content": "Placeholder tuples are always returned, regardless of the\nvalues stored in them.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 763
    }
  },
  {
    "title": "Comment from brin.c:773",
    "url": "backend\\access\\brin\\brin.c:773",
    "content": "Compare scan keys with summary values stored for the range.\nIf scan keys are matched, the page range must be added to\nthe bitmap.  We initially assume the range needs to be\nadded; in particular this serves the case where there are\nno keys.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 773
    }
  },
  {
    "title": "Comment from brin.c:787",
    "url": "backend\\access\\brin\\brin.c:787",
    "content": "skip attributes without any scan keys (both regular and\nIS [NOT] NULL)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 787
    }
  },
  {
    "title": "Comment from brin.c:796",
    "url": "backend\\access\\brin\\brin.c:796",
    "content": "If the BRIN tuple indicates that this range is empty,\nwe can skip it: there's nothing to match.  We don't\nneed to examine the next columns.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 796
    }
  },
  {
    "title": "Comment from brin.c:807",
    "url": "backend\\access\\brin\\brin.c:807",
    "content": "First check if there are any IS [NOT] NULL scan keys,\nand if we're violating them. In that case we can\nterminate early, without invoking the support function.\n\nAs there may be more keys, we can only determine\nmismatch within this loop.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 807
    }
  },
  {
    "title": "Comment from brin.c:819",
    "url": "backend\\access\\brin\\brin.c:819",
    "content": "If any of the IS [NOT] NULL keys failed, the page\nrange as a whole can't pass. So terminate the loop.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 819
    }
  },
  {
    "title": "Comment from brin.c:827",
    "url": "backend\\access\\brin\\brin.c:827",
    "content": "So either there are no IS [NOT] NULL keys, or all\npassed. If there are no regular scan keys, we're done -\nthe page range matches. If there are regular keys, but\nthe page range is marked as 'all nulls' it can't\npossibly pass (we're assuming the operators are\nstrict).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 827
    }
  },
  {
    "title": "Comment from brin.c:836",
    "url": "backend\\access\\brin\\brin.c:836",
    "content": "No regular scan keys - page range as a whole passes.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 836
    }
  },
  {
    "title": "Comment from brin.c:843",
    "url": "backend\\access\\brin\\brin.c:843",
    "content": "If it is all nulls, it cannot possibly be consistent.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 843
    }
  },
  {
    "title": "Comment from brin.c:850",
    "url": "backend\\access\\brin\\brin.c:850",
    "content": "Collation from the first key (has to be the same for\nall keys for the same attribute).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 850
    }
  },
  {
    "title": "Comment from brin.c:856",
    "url": "backend\\access\\brin\\brin.c:856",
    "content": "Check whether the scan key is consistent with the page\nrange values; if so, have the pages in the range added\nto the output bitmap.\n\nThe opclass may or may not support processing of\nmultiple scan keys. We can determine that based on the\nnumber of arguments - functions with extra parameter\n(number of scan keys) do support this, otherwise we\nhave to simply pass the scan keys one by one.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 856
    }
  },
  {
    "title": "Comment from brin.c:880",
    "url": "backend\\access\\brin\\brin.c:880",
    "content": "Check keys one by one\n\nWhen there are multiple scan keys, failure to meet\nthe criteria for a single one of them is enough to\ndiscard the range as a whole, so break out of the\nloop as soon as a false return value is obtained.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 880
    }
  },
  {
    "title": "Comment from brin.c:903",
    "url": "backend\\access\\brin\\brin.c:903",
    "content": "If we found a scan key eliminating the range, no need\nto check additional ones.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 903
    }
  },
  {
    "title": "Comment from brin.c:913",
    "url": "backend\\access\\brin\\brin.c:913",
    "content": "add the pages in the range to the output bitmap, if needed",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 913
    }
  },
  {
    "title": "Comment from brin.c:936",
    "url": "backend\\access\\brin\\brin.c:936",
    "content": "XXX We have an approximation of the number of *pages* that our scan\nreturns, but we don't have a precise idea of the number of heap tuples\ninvolved.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 936
    }
  },
  {
    "title": "Comment from brin.c:951",
    "url": "backend\\access\\brin\\brin.c:951",
    "content": "Other index AMs preprocess the scan keys at this point, or sometime\nearly during the scan; this lets them optimize by removing redundant\nkeys, or doing early returns when they are impossible to satisfy; see\n_bt_preprocess_keys for an example.  Something like that could be added\nhere someday, too.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 951
    }
  },
  {
    "title": "Comment from brin.c:977",
    "url": "backend\\access\\brin\\brin.c:977",
    "content": "Per-heap-tuple callback for table_index_build_scan.\n\nNote we don't worry about the page range at the end of the table here; it is\npresent in the build state struct after we're called the last time, but not\ninserted into the index.  Caller must ensure to do so, if appropriate.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 977
    }
  },
  {
    "title": "Comment from brin.c:997",
    "url": "backend\\access\\brin\\brin.c:997",
    "content": "If we're in a block that belongs to a future range, summarize what\nwe've got and start afresh.  Note the scan might have skipped many\npages, if they were devoid of live tuples; make sure to insert index\ntuples for those too.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 997
    }
  },
  {
    "title": "Comment from brin.c:1021",
    "url": "backend\\access\\brin\\brin.c:1021",
    "content": "Accumulate the current tuple into the running state",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1021
    }
  },
  {
    "title": "Comment from brin.c:1026",
    "url": "backend\\access\\brin\\brin.c:1026",
    "content": "Per-heap-tuple callback for table_index_build_scan with parallelism.\n\nA version of the callback used by parallel index builds. The main difference\nis that instead of writing the BRIN tuples into the index, we write them\ninto a shared tuplesort, and leave the insertion up to the leader (which may\nreorder them a bit etc.). The callback also does not generate empty ranges,\nthose will be added by the leader when merging results from workers.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1026
    }
  },
  {
    "title": "Comment from brin.c:1048",
    "url": "backend\\access\\brin\\brin.c:1048",
    "content": "If we're in a block that belongs to a different range, summarize what\nwe've got and start afresh.  Note the scan might have skipped many\npages, if they were devoid of live tuples; we do not create empty BRIN\nranges here - the leader is responsible for filling them in.\n\nUnlike serial builds, parallel index builds allow synchronized seqscans\n(because that's what parallel scans do). This means the block may wrap\naround to the beginning of the relation, so the condition needs to\ncheck for both future and past ranges.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1048
    }
  },
  {
    "title": "Comment from brin.c:1068",
    "url": "backend\\access\\brin\\brin.c:1068",
    "content": "create the index tuple and write it into the tuplesort",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1068
    }
  },
  {
    "title": "Comment from brin.c:1071",
    "url": "backend\\access\\brin\\brin.c:1071",
    "content": "Set state to correspond to the next range (for this block).\n\nThis skips ranges that are either empty (and so we don't get any\ntuples to summarize), or processed by other workers. We can't\ndifferentiate those cases here easily, so we leave it up to the\nleader to fill empty ranges where needed.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1071
    }
  },
  {
    "title": "Comment from brin.c:1086",
    "url": "backend\\access\\brin\\brin.c:1086",
    "content": "Accumulate the current tuple into the running state",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1086
    }
  },
  {
    "title": "Comment from brin.c:1105",
    "url": "backend\\access\\brin\\brin.c:1105",
    "content": "We expect to be called exactly once for any index relation.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1105
    }
  },
  {
    "title": "Comment from brin.c:1112",
    "url": "backend\\access\\brin\\brin.c:1112",
    "content": "Critical section not required, because on error the creation of the\nwhole relation will be rolled back.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1112
    }
  },
  {
    "title": "Comment from brin.c:1146",
    "url": "backend\\access\\brin\\brin.c:1146",
    "content": "Initialize our state, including the deformed tuple state.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1146
    }
  },
  {
    "title": "Comment from brin.c:1153",
    "url": "backend\\access\\brin\\brin.c:1153",
    "content": "Attempt to launch parallel worker scan when required\n\nXXX plan_create_index_workers makes the number of workers dependent on\nmaintenance_work_mem, requiring 32MB for each worker. That makes sense\nfor btree, but not for BRIN, which can do with much less memory. So\nmaybe make that somehow less strict, optionally?",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1153
    }
  },
  {
    "title": "Comment from brin.c:1165",
    "url": "backend\\access\\brin\\brin.c:1165",
    "content": "If parallel build requested and at least one worker process was\nsuccessfully launched, set up coordination state, wait for workers to\ncomplete. Then read all tuples from the shared tuplesort and insert\nthem into the index.\n\nIn serial mode, simply scan the table and build the index one index\ntuple at a time.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1165
    }
  },
  {
    "title": "Comment from brin.c:1184",
    "url": "backend\\access\\brin\\brin.c:1184",
    "content": "Begin leader tuplesort.\n\nIn cases where parallelism is involved, the leader receives the\nsame share of maintenance_work_mem as a serial sort (it is\ngenerally treated in the same way as a serial sort once we return).\nParallel worker Tuplesortstates will have received only a fraction\nof maintenance_work_mem, though.\n\nWe rely on the lifetime of the Leader Tuplesortstate almost not\noverlapping with any worker Tuplesortstate's lifetime.  There may\nbe some small overlap, but that's okay because we rely on leader\nTuplesortstate only allocating a small, fixed amount of memory\nhere. When its tuplesort_performsort() is called (by our caller),\nand significant amounts of memory are likely to be used, all\nworkers must have already freed almost all memory held by their\nTuplesortstates (they are about to go away completely, too).  The\noverall effect is that maintenance_work_mem always represents an\nabsolute high watermark on the amount of memory used by a CREATE\nINDEX operation, regardless of the use of parallelism or any other\nfactor.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1184
    }
  },
  {
    "title": "Comment from brin.c:1217",
    "url": "backend\\access\\brin\\brin.c:1217",
    "content": "Now scan the relation.  No syncscan allowed here because we want\nthe heap blocks in physical order (we want to produce the ranges\nstarting from block 0, and the callback also relies on this to not\ngenerate summary for the same range twice).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1217
    }
  },
  {
    "title": "Comment from brin.c:1226",
    "url": "backend\\access\\brin\\brin.c:1226",
    "content": "process the final batch\n\nXXX Note this does not update state->bs_currRangeStart, i.e. it\nstays set to the last range added to the index. This is OK, because\nthat's what brin_fill_empty_ranges expects.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1226
    }
  },
  {
    "title": "Comment from brin.c:1235",
    "url": "backend\\access\\brin\\brin.c:1235",
    "content": "Backfill the final ranges with empty data.\n\nThis saves us from doing what amounts to full table scans when the\nindex with a predicate like WHERE (nonnull_column IS NULL), or\nother very selective predicates.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1235
    }
  },
  {
    "title": "Comment from brin.c:1283",
    "url": "backend\\access\\brin\\brin.c:1283",
    "content": "brinbulkdelete\n\tSince there are no per-heap-tuple index tuples in BRIN indexes,\n\tthere's not a lot we can do here.\n\nXXX we could mark item tuples as \"dirty\" (when a minimum or maximum heap\ntuple is deleted), meaning the need to re-run summarization on the affected\nrange.  Would need to add an extra flag in brintuples for that.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1283
    }
  },
  {
    "title": "Comment from brin.c:1296",
    "url": "backend\\access\\brin\\brin.c:1296",
    "content": "allocate stats if first time through, else re-use existing struct",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1296
    }
  },
  {
    "title": "Comment from brin.c:1303",
    "url": "backend\\access\\brin\\brin.c:1303",
    "content": "This routine is in charge of \"vacuuming\" a BRIN index: we just summarize\nranges that are currently unsummarized.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1303
    }
  },
  {
    "title": "Comment from brin.c:1351",
    "url": "backend\\access\\brin\\brin.c:1351",
    "content": "SQL-callable function to scan through an index and summarize all ranges\nthat are not currently summarized.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1351
    }
  },
  {
    "title": "Comment from brin.c:1365",
    "url": "backend\\access\\brin\\brin.c:1365",
    "content": "SQL-callable function to summarize the indicated page range, if not already\nsummarized.  If the second argument is BRIN_ALL_BLOCKRANGES, all\nunsummarized ranges are summarized.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1365
    }
  },
  {
    "title": "Comment from brin.c:1397",
    "url": "backend\\access\\brin\\brin.c:1397",
    "content": "We must lock table before index to avoid deadlocks.  However, if the\npassed indexoid isn't an index then IndexGetRelation() will fail.\nRather than emitting a not-very-helpful error message, postpone\ncomplaining, expecting that the is-it-an-index test below will fail.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1397
    }
  },
  {
    "title": "Comment from brin.c:1408",
    "url": "backend\\access\\brin\\brin.c:1408",
    "content": "Autovacuum calls us.  For its benefit, switch to the table owner's\nuserid, so that any index functions are run as that user.  Also\nlock down security-restricted operations and arrange to make GUC\nvariable changes local to this command.  This is harmless, albeit\nunnecessary, when called from SQL, because we fail shortly if the\nuser does not own the index.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1408
    }
  },
  {
    "title": "Comment from brin.c:1425",
    "url": "backend\\access\\brin\\brin.c:1425",
    "content": "Set these just to suppress \"uninitialized variable\" warnings",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1425
    }
  },
  {
    "title": "Comment from brin.c:1441",
    "url": "backend\\access\\brin\\brin.c:1441",
    "content": "User must own the index (comparable to privileges needed for VACUUM)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1441
    }
  },
  {
    "title": "Comment from brin.c:1446",
    "url": "backend\\access\\brin\\brin.c:1446",
    "content": "Since we did the IndexGetRelation call above without any lock, it's\nbarely possible that a race against an index drop/recreation could have\nnetted us the wrong table.  Recheck.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1446
    }
  },
  {
    "title": "Comment from brin.c:1466",
    "url": "backend\\access\\brin\\brin.c:1466",
    "content": "Roll back any GUC changes executed by index functions",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1466
    }
  },
  {
    "title": "Comment from brin.c:1478",
    "url": "backend\\access\\brin\\brin.c:1478",
    "content": "SQL-callable interface to mark a range as no longer summarized",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1478
    }
  },
  {
    "title": "Comment from brin.c:1505",
    "url": "backend\\access\\brin\\brin.c:1505",
    "content": "We must lock table before index to avoid deadlocks.  However, if the\npassed indexoid isn't an index then IndexGetRelation() will fail.\nRather than emitting a not-very-helpful error message, postpone\ncomplaining, expecting that the is-it-an-index test below will fail.\n\nUnlike brin_summarize_range(), autovacuum never calls this.  Hence, we\ndon't switch userid.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1505
    }
  },
  {
    "title": "Comment from brin.c:1530",
    "url": "backend\\access\\brin\\brin.c:1530",
    "content": "User must own the index (comparable to privileges needed for VACUUM)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1530
    }
  },
  {
    "title": "Comment from brin.c:1535",
    "url": "backend\\access\\brin\\brin.c:1535",
    "content": "Since we did the IndexGetRelation call above without any lock, it's\nbarely possible that a race against an index drop/recreation could have\nnetted us the wrong table.  Recheck.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1535
    }
  },
  {
    "title": "Comment from brin.c:1568",
    "url": "backend\\access\\brin\\brin.c:1568",
    "content": "Build a BrinDesc used to create or scan a BRIN index",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1568
    }
  },
  {
    "title": "Comment from brin.c:1589",
    "url": "backend\\access\\brin\\brin.c:1589",
    "content": "Obtain BrinOpcInfo for each indexed column.  While at it, accumulate\nthe number of columns stored, since the number is opclass-defined.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1589
    }
  },
  {
    "title": "Comment from brin.c:1656",
    "url": "backend\\access\\brin\\brin.c:1656",
    "content": "Initialize a BrinBuildState appropriate to create tuples on the given index.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1656
    }
  },
  {
    "title": "Comment from brin.c:1684",
    "url": "backend\\access\\brin\\brin.c:1684",
    "content": "Remember the memory context to use for an empty tuple, if needed.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1684
    }
  },
  {
    "title": "Comment from brin.c:1689",
    "url": "backend\\access\\brin\\brin.c:1689",
    "content": "Calculate the start of the last page range. Page numbers are 0-based,\nso to calculate the index we need to subtract one. The integer division\ngives us the index of the page range.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1689
    }
  },
  {
    "title": "Comment from brin.c:1703",
    "url": "backend\\access\\brin\\brin.c:1703",
    "content": "Release resources associated with a BrinBuildState.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1703
    }
  },
  {
    "title": "Comment from brin.c:1709",
    "url": "backend\\access\\brin\\brin.c:1709",
    "content": "Release the last index buffer used.  We might as well ensure that\nwhatever free space remains in that page is available in FSM, too.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1709
    }
  },
  {
    "title": "Comment from brin.c:1732",
    "url": "backend\\access\\brin\\brin.c:1732",
    "content": "On the given BRIN index, summarize the heap page range that corresponds\nto the heap block number given.\n\nThis routine can run in parallel with insertions into the heap.  To avoid\nmissing those values from the summary tuple, we first insert a placeholder\nindex tuple into the index, then execute the heap scan; transactions\nconcurrent with the scan update the placeholder tuple.  After the scan, we\nunion the placeholder tuple with the one computed by this routine.  The\nupdate of the index value happens in a loop, so that if somebody updates\nthe placeholder tuple after we read it, we detect the case and try again.\nThis ensures that the concurrently inserted tuples are not lost.\n\nA further corner case is this routine being asked to summarize the partial\nrange at the end of the table.  heapNumBlocks is the (possibly outdated)\ntable size; if we notice that the requested range lies beyond that size,\nwe re-compute the table size after inserting the placeholder tuple, to\navoid missing pages that were appended recently.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1732
    }
  },
  {
    "title": "Comment from brin.c:1770",
    "url": "backend\\access\\brin\\brin.c:1770",
    "content": "Compute range end.  We hold ShareUpdateExclusive lock on table, so it\ncannot shrink concurrently (but it can grow).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1770
    }
  },
  {
    "title": "Comment from brin.c:1777",
    "url": "backend\\access\\brin\\brin.c:1777",
    "content": "If we're asked to scan what we believe to be the final range on the\ntable (i.e. a range that might be partial) we need to recompute our\nidea of what the latest page is after inserting the placeholder\ntuple.  Anyone that grows the table later will update the\nplaceholder tuple, so it doesn't matter that we won't scan these\npages ourselves.  Careful: the table might have been extended\nbeyond the current range, so clamp our result.\n\nFortunately, this should occur infrequently.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1777
    }
  },
  {
    "title": "Comment from brin.c:1797",
    "url": "backend\\access\\brin\\brin.c:1797",
    "content": "Execute the partial heap scan covering the heap blocks in the specified\npage range, summarizing the heap tuples in it.  This scan stops just\nshort of brinbuildCallback creating the new index entry.\n\nNote that it is critical we use the \"any visible\" mode of\ntable_index_build_range_scan here: otherwise, we would miss tuples\ninserted by transactions that are still in progress, among other corner\ncases.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1797
    }
  },
  {
    "title": "Comment from brin.c:1812",
    "url": "backend\\access\\brin\\brin.c:1812",
    "content": "Now we update the values obtained by the scan with the placeholder\ntuple.  We do this in a loop which only terminates if we're able to\nupdate the placeholder tuple successfully; if we are not, this means\nsomebody else modified the placeholder tuple after we read it.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1812
    }
  },
  {
    "title": "Comment from brin.c:1844",
    "url": "backend\\access\\brin\\brin.c:1844",
    "content": "If the update didn't work, it might be because somebody updated the\nplaceholder tuple concurrently.  Extract the new version, union it\nwith the values we have from the scan, and start over.  (There are\nother reasons for the update to fail, but it's simple to treat them\nthe same.)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1844
    }
  },
  {
    "title": "Comment from brin.c:1866",
    "url": "backend\\access\\brin\\brin.c:1866",
    "content": "Summarize page ranges that are not already summarized.  If pageRange is\nBRIN_ALL_BLOCKRANGES then the whole table is scanned; otherwise, only the\npage range containing the given heap page number is scanned.\nIf include_partial is true, then the partial range at the end of the table\nis summarized, otherwise not.\n\nFor each new index tuple inserted, *numSummarized (if not NULL) is\nincremented; for each existing tuple, *numExisting (if not NULL) is\nincremented.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1866
    }
  },
  {
    "title": "Comment from brin.c:1902",
    "url": "backend\\access\\brin\\brin.c:1902",
    "content": "Nothing to do if start point is beyond end of table",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1902
    }
  },
  {
    "title": "Comment from brin.c:1916",
    "url": "backend\\access\\brin\\brin.c:1916",
    "content": "Unless requested to summarize even a partial range, go away now if\nwe think the next range is partial.  Caller would pass true when it\nis typically run once bulk data loading is done\n(brin_summarize_new_values), and false when it is typically the\nresult of arbitrarily-scheduled maintenance command (vacuuming).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1916
    }
  },
  {
    "title": "Comment from brin.c:1933",
    "url": "backend\\access\\brin\\brin.c:1933",
    "content": "no revmap entry for this heap range. Summarize it.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1933
    }
  },
  {
    "title": "Comment from brin.c:1971",
    "url": "backend\\access\\brin\\brin.c:1971",
    "content": "Given a deformed tuple in the build state, convert it into the on-disk\nformat and insert it into the index, making the revmap point to it.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1971
    }
  },
  {
    "title": "Comment from brin.c:1991",
    "url": "backend\\access\\brin\\brin.c:1991",
    "content": "Given a deformed tuple in the build state, convert it into the on-disk\nformat and write it to a (shared) tuplesort (the leader will insert it\ninto the index later).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 1991
    }
  },
  {
    "title": "Comment from brin.c:2017",
    "url": "backend\\access\\brin\\brin.c:2017",
    "content": "Given two deformed tuples, adjust the first one so that it's consistent\nwith the summary values in both.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2017
    }
  },
  {
    "title": "Comment from brin.c:2037",
    "url": "backend\\access\\brin\\brin.c:2037",
    "content": "Check if the ranges are empty.\n\nIf at least one of them is empty, we don't need to call per-key union\nfunctions at all. If \"b\" is empty, we just use \"a\" as the result (it\nmight be empty fine, but that's fine). If \"a\" is empty but \"b\" is not,\nwe use \"b\" as the result (but we have to copy the data into \"a\" first).\n\nOnly when both ranges are non-empty, we actually do the per-key merge.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2037
    }
  },
  {
    "title": "Comment from brin.c:2048",
    "url": "backend\\access\\brin\\brin.c:2048",
    "content": "If \"b\" is empty - ignore it and just use \"a\" (even if it's empty etc.).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2048
    }
  },
  {
    "title": "Comment from brin.c:2056",
    "url": "backend\\access\\brin\\brin.c:2056",
    "content": "Now we know \"b\" is not empty. If \"a\" is empty, then \"b\" is the result.\nBut we need to copy the data from \"b\" to \"a\" first, because that's how\nwe pass result out.\n\nWe have to copy all the global/per-key flags etc. too.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2056
    }
  },
  {
    "title": "Comment from brin.c:2086",
    "url": "backend\\access\\brin\\brin.c:2086",
    "content": "\"a\" started empty, but \"b\" was not empty, so remember that",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2086
    }
  },
  {
    "title": "Comment from brin.c:2111",
    "url": "backend\\access\\brin\\brin.c:2111",
    "content": "If there are no values in B, there's nothing left to do.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2111
    }
  },
  {
    "title": "Comment from brin.c:2115",
    "url": "backend\\access\\brin\\brin.c:2115",
    "content": "Adjust \"allnulls\".  If A doesn't have values, just copy the\nvalues from B into A, and we're done.  We cannot run the\noperators in this case, because values in A might contain\ngarbage.  Note we already established that B contains values.\n\nAlso adjust \"hasnulls\" in order not to forget the summary\nrepresents NULL values. This is not redundant with the earlier\nupdate, because that only happens when allnulls=false.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2115
    }
  },
  {
    "title": "Comment from brin.c:2154",
    "url": "backend\\access\\brin\\brin.c:2154",
    "content": "brin_vacuum_scan\n\tDo a complete scan of the index during VACUUM.\n\nThis routine scans the complete index looking for uncataloged index pages,\ni.e. those that might have been lost due to a crash after index extension\nand such.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2154
    }
  },
  {
    "title": "Comment from brin.c:2168",
    "url": "backend\\access\\brin\\brin.c:2168",
    "content": "Scan the index in physical order, and clean up any possible mess in\neach page.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2168
    }
  },
  {
    "title": "Comment from brin.c:2187",
    "url": "backend\\access\\brin\\brin.c:2187",
    "content": "Update all upper pages in the index's FSM, as well.  This ensures not\nonly that we propagate leaf-page FSM updates made by brin_page_cleanup,\nbut also that any pre-existing damage or out-of-dateness is repaired.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2187
    }
  },
  {
    "title": "Comment from brin.c:2201",
    "url": "backend\\access\\brin\\brin.c:2201",
    "content": "If the range starts empty, we're certainly going to modify it.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2201
    }
  },
  {
    "title": "Comment from brin.c:2204",
    "url": "backend\\access\\brin\\brin.c:2204",
    "content": "Compare the key values of the new tuple to the stored index values; our\ndeformed tuple will get updated if the new tuple doesn't fit the\noriginal range (note this means we can't break out of the loop early).\nMake a note of whether this happens, so that we know to insert the\nmodified tuple later.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2204
    }
  },
  {
    "title": "Comment from brin.c:2220",
    "url": "backend\\access\\brin\\brin.c:2220",
    "content": "Does the range have actual NULL values? Either of the flags can be\nset, but we ignore the state before adding first row.\n\nWe have to remember this, because we'll modify the flags and we\nneed to know if the range started as empty.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2220
    }
  },
  {
    "title": "Comment from brin.c:2230",
    "url": "backend\\access\\brin\\brin.c:2230",
    "content": "If the value we're adding is NULL, handle it locally. Otherwise\ncall the BRIN_PROCNUM_ADDVALUE procedure.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2230
    }
  },
  {
    "title": "Comment from brin.c:2236",
    "url": "backend\\access\\brin\\brin.c:2236",
    "content": "If the new value is null, we record that we saw it if it's the\nfirst one; otherwise, there's nothing to do.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2236
    }
  },
  {
    "title": "Comment from brin.c:2257",
    "url": "backend\\access\\brin\\brin.c:2257",
    "content": "if that returned true, we need to insert the updated tuple",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2257
    }
  },
  {
    "title": "Comment from brin.c:2260",
    "url": "backend\\access\\brin\\brin.c:2260",
    "content": "If the range was had actual NULL values (i.e. did not start empty),\nmake sure we don't forget about the NULL values. Either the\nallnulls flag is still set to true, or (if the opclass cleared it)\nwe need to set hasnulls=true.\n\nXXX This can only happen when the opclass modified the tuple, so\nthe modified flag should be set.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2260
    }
  },
  {
    "title": "Comment from brin.c:2276",
    "url": "backend\\access\\brin\\brin.c:2276",
    "content": "After updating summaries for all the keys, mark it as not empty.\n\nIf we're actually changing the flag value (i.e. tuple started as\nempty), we should have modified the tuple. So we should not see empty\nrange that was not modified.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2276
    }
  },
  {
    "title": "Comment from brin.c:2294",
    "url": "backend\\access\\brin\\brin.c:2294",
    "content": "First check if there are any IS [NOT] NULL scan keys, and if we're\nviolating them.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2294
    }
  },
  {
    "title": "Comment from brin.c:2316",
    "url": "backend\\access\\brin\\brin.c:2316",
    "content": "For IS NOT NULL, we can only skip ranges that are known to have\nonly nulls.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2316
    }
  },
  {
    "title": "Comment from brin.c:2325",
    "url": "backend\\access\\brin\\brin.c:2325",
    "content": "Neither IS NULL nor IS NOT NULL was used; assume all indexable\noperators are strict and thus return false with NULL value in\nthe scan key.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2325
    }
  },
  {
    "title": "Comment from brin.c:2337",
    "url": "backend\\access\\brin\\brin.c:2337",
    "content": "Create parallel context, and launch workers for leader.\n\nbuildstate argument should be initialized (with the exception of the\ntuplesort states, which may later be created based on shared\nstate initially set up here).\n\nisconcurrent indicates if operation is CREATE INDEX CONCURRENTLY.\n\nrequest is the target number of parallel worker processes to launch.\n\nSets buildstate's BrinLeader, which caller must use to shut down parallel\nmode by passing it to _brin_end_parallel() at the very end of its index\nbuild.  If not even a single worker process can be launched, this is\nnever set, and caller should proceed with a serial index build.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2337
    }
  },
  {
    "title": "Comment from brin.c:2374",
    "url": "backend\\access\\brin\\brin.c:2374",
    "content": "Enter parallel mode, and create context for parallel build of brin\nindex",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2374
    }
  },
  {
    "title": "Comment from brin.c:2385",
    "url": "backend\\access\\brin\\brin.c:2385",
    "content": "Prepare for scan of the base relation.  In a normal index build, we use\nSnapshotAny because we must retrieve all tuples and do our own time\nqual checks (because we have to index RECENTLY_DEAD tuples).  In a\nconcurrent build, we take a regular MVCC snapshot and index whatever's\nlive according to that.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2385
    }
  },
  {
    "title": "Comment from brin.c:2397",
    "url": "backend\\access\\brin\\brin.c:2397",
    "content": "Estimate size for our own PARALLEL_KEY_BRIN_SHARED workspace.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2397
    }
  },
  {
    "title": "Comment from brin.c:2407",
    "url": "backend\\access\\brin\\brin.c:2407",
    "content": "Estimate space for WalUsage and BufferUsage -- PARALLEL_KEY_WAL_USAGE\nand PARALLEL_KEY_BUFFER_USAGE.\n\nIf there are no extensions loaded that care, we could skip this.  We\nhave no way of knowing whether anyone's looking at pgWalUsage or\npgBufferUsage, so do it unconditionally.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2407
    }
  },
  {
    "title": "Comment from brin.c:2432",
    "url": "backend\\access\\brin\\brin.c:2432",
    "content": "Everyone's had a chance to ask for space, so now create the DSM",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2432
    }
  },
  {
    "title": "Comment from brin.c:2435",
    "url": "backend\\access\\brin\\brin.c:2435",
    "content": "If no DSM segment was available, back out (do serial build)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2435
    }
  },
  {
    "title": "Comment from brin.c:2445",
    "url": "backend\\access\\brin\\brin.c:2445",
    "content": "Store shared build state, for which we reserved space",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2445
    }
  },
  {
    "title": "Comment from brin.c:2465",
    "url": "backend\\access\\brin\\brin.c:2465",
    "content": "Store shared tuplesort-private state, for which we reserved space.\nThen, initialize opaque state using tuplesort routine.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2465
    }
  },
  {
    "title": "Comment from brin.c:2473",
    "url": "backend\\access\\brin\\brin.c:2473",
    "content": "Store shared tuplesort-private state, for which we reserved space.\nThen, initialize opaque state using tuplesort routine.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2473
    }
  },
  {
    "title": "Comment from brin.c:2490",
    "url": "backend\\access\\brin\\brin.c:2490",
    "content": "Allocate space for each worker's WalUsage and BufferUsage; no need to\ninitialize.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2490
    }
  },
  {
    "title": "Comment from brin.c:2513",
    "url": "backend\\access\\brin\\brin.c:2513",
    "content": "If no workers were successfully launched, back out (do serial build)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2513
    }
  },
  {
    "title": "Comment from brin.c:2520",
    "url": "backend\\access\\brin\\brin.c:2520",
    "content": "Save leader state now that it's clear build will be parallel",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2520
    }
  },
  {
    "title": "Comment from brin.c:2527",
    "url": "backend\\access\\brin\\brin.c:2527",
    "content": "Caller needs to wait for all launched workers when we return.  Make\nsure that the failure-to-start case will not hang forever.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2527
    }
  },
  {
    "title": "Comment from brin.c:2534",
    "url": "backend\\access\\brin\\brin.c:2534",
    "content": "Shut down workers, destroy parallel context, and end parallel mode.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2534
    }
  },
  {
    "title": "Comment from brin.c:2545",
    "url": "backend\\access\\brin\\brin.c:2545",
    "content": "Next, accumulate WAL usage.  (This must wait for the workers to finish,\nor we might get incomplete data.)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2545
    }
  },
  {
    "title": "Comment from brin.c:2552",
    "url": "backend\\access\\brin\\brin.c:2552",
    "content": "Free last reference to MVCC snapshot, if one was used",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2552
    }
  },
  {
    "title": "Comment from brin.c:2559",
    "url": "backend\\access\\brin\\brin.c:2559",
    "content": "Within leader, wait for end of heap scan.\n\nWhen called, parallel heap scan started by _brin_begin_parallel() will\nalready be underway within worker processes (when leader participates\nas a worker, we should end up here just as workers are finishing).\n\nReturns the total number of heap tuples scanned.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2559
    }
  },
  {
    "title": "Comment from brin.c:2598",
    "url": "backend\\access\\brin\\brin.c:2598",
    "content": "Within leader, wait for end of heap scan and merge per-worker results.\n\nAfter waiting for all workers to finish, merge the per-worker results into\nthe complete index. The results from each worker are sorted by block number\n(start of the page range). While combining the per-worker results we merge\nsummaries for the same page range, and also fill-in empty summaries for\nranges without any tuples.\n\nReturns the total number of heap tuples scanned.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2598
    }
  },
  {
    "title": "Comment from brin.c:2620",
    "url": "backend\\access\\brin\\brin.c:2620",
    "content": "wait for workers to scan table and produce partial results",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2620
    }
  },
  {
    "title": "Comment from brin.c:2626",
    "url": "backend\\access\\brin\\brin.c:2626",
    "content": "Initialize BrinMemTuple we'll use to union summaries from workers (in\ncase they happened to produce parts of the same page range).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2626
    }
  },
  {
    "title": "Comment from brin.c:2632",
    "url": "backend\\access\\brin\\brin.c:2632",
    "content": "Create a memory context we'll reset to combine results for a single\npage range (received from the workers). We don't expect huge number of\noverlaps under regular circumstances, because for large tables the\nchunk size is likely larger than the BRIN page range), but it can\nhappen, and the union functions may do all kinds of stuff. So we better\nreset the context once in a while.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2632
    }
  },
  {
    "title": "Comment from brin.c:2645",
    "url": "backend\\access\\brin\\brin.c:2645",
    "content": "Read the BRIN tuples from the shared tuplesort, sorted by block number.\nThat probably gives us an index that is cheaper to scan, thanks to\nmostly getting data from the same index page as before.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2645
    }
  },
  {
    "title": "Comment from brin.c:2652",
    "url": "backend\\access\\brin\\brin.c:2652",
    "content": "Ranges should be multiples of pages_per_range for the index.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2652
    }
  },
  {
    "title": "Comment from brin.c:2655",
    "url": "backend\\access\\brin\\brin.c:2655",
    "content": "Do we need to union summaries for the same page range?\n\nIf this is the first brin tuple we read, then just deform it into\nthe memtuple, and continue with the next one from tuplesort. We\nhowever may need to insert empty summaries into the index.\n\nIf it's the same block as the last we saw, we simply union the brin\ntuple into it, and we're done - we don't even need to insert empty\nranges, because that was done earlier when we saw the first brin\ntuple (for this range).\n\nFinally, if it's not the first brin tuple, and it's not the same\npage range, we need to do the insert and then deform the tuple into\nthe memtuple. Then we'll insert empty ranges before the new brin\ntuple, if needed.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2655
    }
  },
  {
    "title": "Comment from brin.c:2681",
    "url": "backend\\access\\brin\\brin.c:2681",
    "content": "Not the first brin tuple, but same page range as the previous\none, so we can merge it into the memtuple.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2681
    }
  },
  {
    "title": "Comment from brin.c:2693",
    "url": "backend\\access\\brin\\brin.c:2693",
    "content": "We got brin tuple for a different page range, so form a brin\ntuple from the memtuple, insert it, and re-init the memtuple\nfrom the new brin tuple.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2693
    }
  },
  {
    "title": "Comment from brin.c:2704",
    "url": "backend\\access\\brin\\brin.c:2704",
    "content": "Reset the per-output-range context. This frees all the memory\npossibly allocated by the union functions, and also the BRIN\ntuple we just formed and inserted.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2704
    }
  },
  {
    "title": "Comment from brin.c:2716",
    "url": "backend\\access\\brin\\brin.c:2716",
    "content": "Fill empty ranges for all ranges missing in the tuplesort.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2716
    }
  },
  {
    "title": "Comment from brin.c:2724",
    "url": "backend\\access\\brin\\brin.c:2724",
    "content": "Fill the BRIN tuple for the last page range with data.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2724
    }
  },
  {
    "title": "Comment from brin.c:2739",
    "url": "backend\\access\\brin\\brin.c:2739",
    "content": "Fill empty ranges at the end, for all ranges missing in the tuplesort.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2739
    }
  },
  {
    "title": "Comment from brin.c:2742",
    "url": "backend\\access\\brin\\brin.c:2742",
    "content": "Switch back to the original memory context, and destroy the one we\ncreated to isolate the union_tuple calls.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2742
    }
  },
  {
    "title": "Comment from brin.c:2752",
    "url": "backend\\access\\brin\\brin.c:2752",
    "content": "Returns size of shared memory required to store state for a parallel\nbrin index build based on the snapshot its parallel scan will use.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2752
    }
  },
  {
    "title": "Comment from brin.c:2759",
    "url": "backend\\access\\brin\\brin.c:2759",
    "content": "c.f. shm_toc_allocate as to why BUFFERALIGN is used",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2759
    }
  },
  {
    "title": "Comment from brin.c:2773",
    "url": "backend\\access\\brin\\brin.c:2773",
    "content": "Might as well use reliable figure when doling out maintenance_work_mem\n(when requested number of workers were not launched, this will be\nsomewhat higher than it is for other workers).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2773
    }
  },
  {
    "title": "Comment from brin.c:2785",
    "url": "backend\\access\\brin\\brin.c:2785",
    "content": "Perform a worker's portion of a parallel sort.\n\nThis generates a tuplesort for the worker portion of the table.\n\nsortmem is the amount of working memory to use within each worker,\nexpressed in KBs.\n\nWhen this returns, workers are done, and need only release resources.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2785
    }
  },
  {
    "title": "Comment from brin.c:2867",
    "url": "backend\\access\\brin\\brin.c:2867",
    "content": "The only possible status flag that can be set to the parallel worker is\nPROC_IN_SAFE_IC.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2867
    }
  },
  {
    "title": "Comment from brin.c:2874",
    "url": "backend\\access\\brin\\brin.c:2874",
    "content": "Set debug_query_string for individual workers first",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2874
    }
  },
  {
    "title": "Comment from brin.c:2884",
    "url": "backend\\access\\brin\\brin.c:2884",
    "content": "Open relations using lock modes known to be obtained by index.c",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2884
    }
  },
  {
    "title": "Comment from brin.c:2908",
    "url": "backend\\access\\brin\\brin.c:2908",
    "content": "Prepare to track buffer usage during parallel execution",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2908
    }
  },
  {
    "title": "Comment from brin.c:2911",
    "url": "backend\\access\\brin\\brin.c:2911",
    "content": "Might as well use reliable figure when doling out maintenance_work_mem\n(when requested number of workers were not launched, this will be\nsomewhat higher than it is for other workers).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2911
    }
  },
  {
    "title": "Comment from brin.c:2931",
    "url": "backend\\access\\brin\\brin.c:2931",
    "content": "brin_build_empty_tuple\n\tMaybe initialize a BRIN tuple representing empty range.\n\nReturns a BRIN tuple representing an empty page range starting at the\nspecified block number. The empty tuple is initialized only once, when it's\nneeded for the first time, stored in the memory context bs_context to ensure\nproper life span, and reused on following calls. All empty tuples are\nexactly the same except for the bt_blkno field, which is set to the value\nin blkno parameter.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2931
    }
  },
  {
    "title": "Comment from brin.c:2945",
    "url": "backend\\access\\brin\\brin.c:2945",
    "content": "First time an empty tuple is requested? If yes, initialize it.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2945
    }
  },
  {
    "title": "Comment from brin.c:2951",
    "url": "backend\\access\\brin\\brin.c:2951",
    "content": "Allocate the tuple in context for the whole index build.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2951
    }
  },
  {
    "title": "Comment from brin.c:2961",
    "url": "backend\\access\\brin\\brin.c:2961",
    "content": "If we already have an empty tuple, just update the block.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2961
    }
  },
  {
    "title": "Comment from brin.c:2966",
    "url": "backend\\access\\brin\\brin.c:2966",
    "content": "brin_fill_empty_ranges\n\tAdd BRIN index tuples representing empty page ranges.\n\nprevRange/nextRange determine for which page ranges to add empty summaries.\nBoth boundaries are exclusive, i.e. only ranges starting at blkno for which\n(prevRange < blkno < nextRange) will be added to the index.\n\nIf prevRange is InvalidBlockNumber, this means there was no previous page\nrange (i.e. the first empty range to add is for blkno=0).\n\nThe empty tuple is built only once, and then reused for all future calls.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2966
    }
  },
  {
    "title": "Comment from brin.c:2985",
    "url": "backend\\access\\brin\\brin.c:2985",
    "content": "If we already summarized some ranges, we need to start with the next\none. Otherwise start from the first range of the table.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2985
    }
  },
  {
    "title": "Comment from brin.c:2991",
    "url": "backend\\access\\brin\\brin.c:2991",
    "content": "Generate empty ranges until we hit the next non-empty range.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2991
    }
  },
  {
    "title": "Comment from brin.c:2994",
    "url": "backend\\access\\brin\\brin.c:2994",
    "content": "Did we already build the empty tuple? If not, do it now.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin.c",
      "relative_path": "backend\\access\\brin\\brin.c",
      "line_start": 2994
    }
  },
  {
    "title": "Comment from brin_bloom.c:1",
    "url": "backend\\access\\brin\\brin_bloom.c:1",
    "content": "brin_bloom.c\n\tImplementation of Bloom opclass for BRIN\n\nPortions Copyright (c) 1996-2024, PostgreSQL Global Development Group\nPortions Copyright (c) 1994, Regents of the University of California\n\n\nA BRIN opclass summarizing page range into a bloom filter.\n\nBloom filters allow efficient testing whether a given page range contains\na particular value. Therefore, if we summarize each page range into a small\nbloom filter, we can easily (and cheaply) test whether it contains values\nwe get later.\n\nThe index only supports equality operators, similarly to hash indexes.\nBloom indexes are however much smaller, and support only bitmap scans.\n\nNote: Don't confuse this with bloom indexes, implemented in a contrib\nmodule. That extension implements an entirely new AM, building a bloom\nfilter on multiple columns in a single row. This opclass works with an\nexisting AM (BRIN) and builds bloom filter on a column.\n\n\nvalues vs. hashes\n-----------------\n\nThe original column values are not used directly, but are first hashed\nusing the regular type-specific hash function, producing a uint32 hash.\nAnd this hash value is then added to the summary - i.e. it's hashed\nagain and added to the bloom filter.\n\nThis allows the code to treat all data types (byval/byref/...) the same\nway, with only minimal space requirements, because we're working with\nhashes and not the original values. Everything is uint32.\n\nOf course, this assumes the built-in hash function is reasonably good,\nwithout too many collisions etc. But that does seem to be the case, at\nleast based on past experience. After all, the same hash functions are\nused for hash indexes, hash partitioning and so on.\n\n\nhashing scheme\n--------------\n\nBloom filters require a number of independent hash functions. There are\ndifferent schemes how to construct them - for example we might use\nhash_uint32_extended with random seeds, but that seems fairly expensive.\nWe use a scheme requiring only two functions described in this paper:\n\nLess Hashing, Same Performance:Building a Better Bloom Filter\nAdam Kirsch, Michael Mitzenmacher, Harvard School of Engineering and\nApplied Sciences, Cambridge, Massachusetts [DOI 10.1002/rsa.20208]\n\nThe two hash functions h1 and h2 are calculated using hard-coded seeds,\nand then combined using (h1 + i * h2) to generate the hash functions.\n\n\nsizing the bloom filter\n-----------------------\n\nSize of a bloom filter depends on the number of distinct values we will\nstore in it, and the desired false positive rate. The higher the number\nof distinct values and/or the lower the false positive rate, the larger\nthe bloom filter. On the other hand, we want to keep the index as small\nas possible - that's one of the basic advantages of BRIN indexes.\n\nAlthough the number of distinct elements (in a page range) depends on\nthe data, we can consider it fixed. This simplifies the trade-off to\njust false positive rate vs. size.\n\nAt the page range level, false positive rate is a probability the bloom\nfilter matches a random value. For the whole index (with sufficiently\nmany page ranges) it represents the fraction of the index ranges (and\nthus fraction of the table to be scanned) matching the random value.\n\nFurthermore, the size of the bloom filter is subject to implementation\nlimits - it has to fit onto a single index page (8kB by default). As\nthe bitmap is inherently random (when \"full\" about half the bits is set\nto 1, randomly), compression can't help very much.\n\nTo reduce the size of a filter (to fit to a page), we have to either\naccept higher false positive rate (undesirable), or reduce the number\nof distinct items to be stored in the filter. We can't alter the input\ndata, of course, but we may make the BRIN page ranges smaller - instead\nof the default 128 pages (1MB) we may build index with 16-page ranges,\nor something like that. This should reduce the number of distinct values\nin the page range, making the filter smaller (with fixed false positive\nrate). Even for random data sets this should help, as the number of rows\nper heap page is limited (to ~290 with very narrow tables, likely ~20\nin practice).\n\nOf course, good sizing decisions depend on having the necessary data,\ni.e. number of distinct values in a page range (of a given size) and\ntable size (to estimate cost change due to change in false positive\nrate due to having larger index vs. scanning larger indexes). We may\nnot have that data - for example when building an index on empty table\nit's not really possible. And for some data we only have estimates for\nthe whole table and we can only estimate per-range values (ndistinct).\n\nAnother challenge is that while the bloom filter is per-column, it's\nthe whole index tuple that has to fit into a page. And for multi-column\nindexes that may include pieces we have no control over (not necessarily\nbloom filters, the other columns may use other BRIN opclasses). So it's\nnot entirely clear how to distribute the space between those columns.\n\nThe current logic, implemented in brin_bloom_get_ndistinct, attempts to\nmake some basic sizing decisions, based on the size of BRIN ranges, and\nthe maximum number of rows per range.\n\n\nIDENTIFICATION\n  src/backend/access/brin/brin_bloom.c",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 1
    }
  },
  {
    "title": "Comment from brin_bloom.c:135",
    "url": "backend\\access\\brin\\brin_bloom.c:135",
    "content": "Additional SQL level support functions. We only have one, which is\nused to calculate hash of the input value.\n\nProcedure numbers must not use values reserved for BRIN itself; see\nbrin_internal.h.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 135
    }
  },
  {
    "title": "Comment from brin_bloom.c:145",
    "url": "backend\\access\\brin\\brin_bloom.c:145",
    "content": "Subtract this from procnum to obtain index in BloomOpaque arrays\n(Must be equal to minimum of private procnums).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 145
    }
  },
  {
    "title": "Comment from brin_bloom.c:161",
    "url": "backend\\access\\brin\\brin_bloom.c:161",
    "content": "The current min value (16) is somewhat arbitrary, but it's based\non the fact that the filter header is ~20B alone, which is about\nthe same as the filter bitmap for 16 distinct items with 1% false\npositive rate. So by allowing lower values we'd not gain much. In\nany case, the min should not be larger than MaxHeapTuplesPerPage\n(~290), which is the theoretical maximum for single-page ranges.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 161
    }
  },
  {
    "title": "Comment from brin_bloom.c:171",
    "url": "backend\\access\\brin\\brin_bloom.c:171",
    "content": "Used to determine number of distinct items, based on the number of rows\nin a page range. The 10% is somewhat similar to what estimate_num_groups\ndoes, so we use the same factor here.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 171
    }
  },
  {
    "title": "Comment from brin_bloom.c:178",
    "url": "backend\\access\\brin\\brin_bloom.c:178",
    "content": "Allowed range and default value for the false positive range. The exact\nvalues are somewhat arbitrary, but were chosen considering the various\nparameters (size of filter vs. page size, etc.).\n\nThe lower the false-positive rate, the more accurate the filter is, but\nit also gets larger - at some point this eliminates the main advantage\nof BRIN indexes, which is the tiny size. At 0.01% the index is about\n10% of the table (assuming 290 distinct values per 8kB page).\n\nOn the other hand, as the false-positive rate increases, larger part of\nthe table has to be scanned due to mismatches - at 25% we're probably\nclose to sequential scan being cheaper.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 178
    }
  },
  {
    "title": "Comment from brin_bloom.c:206",
    "url": "backend\\access\\brin\\brin_bloom.c:206",
    "content": "And estimate of the largest bloom we can fit onto a page. This is not\na perfect guarantee, for a couple of reasons. For example, the row may\nbe larger because the index has multiple columns.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 206
    }
  },
  {
    "title": "Comment from brin_bloom.c:218",
    "url": "backend\\access\\brin\\brin_bloom.c:218",
    "content": "Seeds used to calculate two hash functions h1 and h2, which are then used\nto generate k hashes using the (h1 + i * h2) scheme.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 218
    }
  },
  {
    "title": "Comment from brin_bloom.c:225",
    "url": "backend\\access\\brin\\brin_bloom.c:225",
    "content": "Bloom Filter\n\nRepresents a bloom filter, built on hashes of the indexed values. That is,\nwe compute a uint32 hash of the value, and then store this hash into the\nbloom filter (and compute additional hashes on it).\n\nXXX We could implement \"sparse\" bloom filters, keeping only the bytes that\nare not entirely 0. But while indexes don't support TOAST, the varlena can\nstill be compressed. So this seems unnecessary, because the compression\nshould do the same job.\n\nXXX We can also watch the number of bits set in the bloom filter, and then\nstop using it (and not store the bitmap, to save space) when the false\npositive rate gets too high. But even if the false positive rate exceeds the\ndesired value, it still can eliminate some page ranges.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 225
    }
  },
  {
    "title": "Comment from brin_bloom.c:259",
    "url": "backend\\access\\brin\\brin_bloom.c:259",
    "content": "bloom_filter_size\n\tCalculate Bloom filter parameters (nbits, nbytes, nhashes).\n\nGiven expected number of distinct values and desired false positive rate,\ncalculates the optimal parameters of the Bloom filter.\n\nThe resulting parameters are returned through nbytesp (number of bytes),\nnbitsp (number of bits) and nhashesp (number of hash functions). If a\npointer is NULL, the parameter is not returned.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 259
    }
  },
  {
    "title": "Comment from brin_bloom.c:285",
    "url": "backend\\access\\brin\\brin_bloom.c:285",
    "content": "round(log(2.0) * m / ndistinct), but assume round() may not be\navailable on Windows",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 285
    }
  },
  {
    "title": "Comment from brin_bloom.c:302",
    "url": "backend\\access\\brin\\brin_bloom.c:302",
    "content": "bloom_init\n\t\tInitialize the Bloom Filter, allocate all the memory.\n\nThe filter is initialized with optimal size for ndistinct expected values\nand the requested false positive rate. The filter is stored as varlena.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 302
    }
  },
  {
    "title": "Comment from brin_bloom.c:326",
    "url": "backend\\access\\brin\\brin_bloom.c:326",
    "content": "Reject filters that are obviously too large to store on a page.\n\nInitially the bloom filter is just zeroes and so very compressible, but\nas we add values it gets more and more random, and so less and less\ncompressible. So initially everything fits on the page, but we might\nget surprising failures later - we want to prevent that, so we reject\nbloom filter that are obviously too large.\n\nXXX It's not uncommon to oversize the bloom filter a bit, to defend\nagainst unexpected data anomalies (parts of table with more distinct\nvalues per range etc.). But we still need to make sure even the\noversized filter fits on page, if such need arises.\n\nXXX This check is not perfect, because the index may have multiple\nfilters that are small individually, but too large when combined.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 326
    }
  },
  {
    "title": "Comment from brin_bloom.c:347",
    "url": "backend\\access\\brin\\brin_bloom.c:347",
    "content": "We allocate the whole filter. Most of it is going to be 0 bits, so the\nvarlena is easy to compress.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 347
    }
  },
  {
    "title": "Comment from brin_bloom.c:388",
    "url": "backend\\access\\brin\\brin_bloom.c:388",
    "content": "if the bit is not set, set it and remember we did that",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 388
    }
  },
  {
    "title": "Comment from brin_bloom.c:402",
    "url": "backend\\access\\brin\\brin_bloom.c:402",
    "content": "bloom_contains_value\n\t\tCheck if the bloom filter contains a particular value.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 402
    }
  },
  {
    "title": "Comment from brin_bloom.c:436",
    "url": "backend\\access\\brin\\brin_bloom.c:436",
    "content": "XXX At this point we only need a single proc (to compute the hash), but\nlet's keep the array just like inclusion and minmax opclasses, for\nconsistency. We may need additional procs in the future.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 436
    }
  },
  {
    "title": "Comment from brin_bloom.c:453",
    "url": "backend\\access\\brin\\brin_bloom.c:453",
    "content": "opaque->strategy_procinfos is initialized lazily; here it is set to\nall-uninitialized by palloc0 which sets fn_oid to InvalidOid.\n\nbloom indexes only store the filter as a single BYTEA column",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 453
    }
  },
  {
    "title": "Comment from brin_bloom.c:471",
    "url": "backend\\access\\brin\\brin_bloom.c:471",
    "content": "brin_bloom_get_ndistinct\n\tDetermine the ndistinct value used to size bloom filter.\n\nAdjust the ndistinct value based on the pagesPerRange value. First,\nif it's negative, it's assumed to be relative to maximum number of\ntuples in the range (assuming each page gets MaxHeapTuplesPerPage\ntuples, which is likely a significant over-estimate). We also clamp\nthe value, not to over-size the bloom filter unnecessarily.\n\nXXX We can only do this when the pagesPerRange value was supplied.\nIf it wasn't, it has to be a read-only access to the index, in which\ncase we don't really care. But perhaps we should fall-back to the\ndefault pagesPerRange value?\n\nXXX We might also fetch info about ndistinct estimate for the column,\nand compute the expected number of distinct values in a range. But\nthat may be tricky due to data being sorted in various ways, so it\nseems better to rely on the upper estimate.\n\nXXX We might also calculate a better estimate of rows per BRIN range,\ninstead of using MaxHeapTuplesPerPage (which probably produces values\nmuch higher than reality).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 471
    }
  },
  {
    "title": "Comment from brin_bloom.c:509",
    "url": "backend\\access\\brin\\brin_bloom.c:509",
    "content": "Similarly to n_distinct, negative values are relative - in this case to\nmaximum number of tuples in the page range (maxtuples).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 509
    }
  },
  {
    "title": "Comment from brin_bloom.c:516",
    "url": "backend\\access\\brin\\brin_bloom.c:516",
    "content": "Positive values are to be used directly, but we still apply a couple of\nsafeties to avoid using unreasonably small bloom filters.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 516
    }
  },
  {
    "title": "Comment from brin_bloom.c:522",
    "url": "backend\\access\\brin\\brin_bloom.c:522",
    "content": "And don't use more than the maximum possible number of tuples, in the\nrange, which would be entirely wasteful.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 522
    }
  },
  {
    "title": "Comment from brin_bloom.c:531",
    "url": "backend\\access\\brin\\brin_bloom.c:531",
    "content": "Examine the given index tuple (which contains partial status of a certain\npage range) by comparing it to the given value that comes from another heap\ntuple.  If the new value is outside the bloom filter specified by the\nexisting tuple values, update the index tuple and return true.  Otherwise,\nreturn false and do not modify in this case.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 531
    }
  },
  {
    "title": "Comment from brin_bloom.c:557",
    "url": "backend\\access\\brin\\brin_bloom.c:557",
    "content": "If this is the first non-null value, we need to initialize the bloom\nfilter. Otherwise just extract the existing bloom filter from\nBrinValues.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 557
    }
  },
  {
    "title": "Comment from brin_bloom.c:573",
    "url": "backend\\access\\brin\\brin_bloom.c:573",
    "content": "Compute the hash of the new value, using the supplied hash function,\nand then add the hash value to the bloom filter.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 573
    }
  },
  {
    "title": "Comment from brin_bloom.c:588",
    "url": "backend\\access\\brin\\brin_bloom.c:588",
    "content": "Given an index tuple corresponding to a certain page range and a scan key,\nreturn whether the scan key is consistent with the index tuple's bloom\nfilter.  Return true if so, false otherwise.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 588
    }
  },
  {
    "title": "Comment from brin_bloom.c:613",
    "url": "backend\\access\\brin\\brin_bloom.c:613",
    "content": "Assume all scan keys match. We'll be searching for a scan key\neliminating the page range (we can stop on the first such key).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 613
    }
  },
  {
    "title": "Comment from brin_bloom.c:623",
    "url": "backend\\access\\brin\\brin_bloom.c:623",
    "content": "NULL keys are handled and filtered-out in bringetbitmap",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 623
    }
  },
  {
    "title": "Comment from brin_bloom.c:633",
    "url": "backend\\access\\brin\\brin_bloom.c:633",
    "content": "We want to return the current page range if the bloom\nfilter seems to contain the value.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 633
    }
  },
  {
    "title": "Comment from brin_bloom.c:657",
    "url": "backend\\access\\brin\\brin_bloom.c:657",
    "content": "Given two BrinValues, update the first of them as a union of the summary\nvalues contained in both.  The second one is untouched.\n\nXXX We assume the bloom filters have the same parameters for now. In the\nfuture we should have 'can union' function, to decide if we can combine\ntwo particular bloom filters.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 657
    }
  },
  {
    "title": "Comment from brin_bloom.c:710",
    "url": "backend\\access\\brin\\brin_bloom.c:710",
    "content": "Cache and return inclusion opclass support procedure\n\nReturn the procedure corresponding to the given function support number\nor null if it does not exist.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 710
    }
  },
  {
    "title": "Comment from brin_bloom.c:722",
    "url": "backend\\access\\brin\\brin_bloom.c:722",
    "content": "We cache these in the opaque struct, to avoid repetitive syscache\nlookups.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 722
    }
  },
  {
    "title": "Comment from brin_bloom.c:768",
    "url": "backend\\access\\brin\\brin_bloom.c:768",
    "content": "brin_bloom_summary_in\n\t- input routine for type brin_bloom_summary.\n\nbrin_bloom_summary is only used internally to represent summaries\nin BRIN bloom indexes, so it has no operations of its own, and we\ndisallow input too.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 768
    }
  },
  {
    "title": "Comment from brin_bloom.c:779",
    "url": "backend\\access\\brin\\brin_bloom.c:779",
    "content": "brin_bloom_summary stores the data in binary form and parsing text\ninput is not needed, so disallow this.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 779
    }
  },
  {
    "title": "Comment from brin_bloom.c:791",
    "url": "backend\\access\\brin\\brin_bloom.c:791",
    "content": "brin_bloom_summary_out\n\t- output routine for type brin_bloom_summary.\n\nBRIN bloom summaries are serialized into a bytea value, but we want\nto output something nicer humans can understand.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 791
    }
  },
  {
    "title": "Comment from brin_bloom.c:804",
    "url": "backend\\access\\brin\\brin_bloom.c:804",
    "content": "detoast the data to get value with a full 4B header",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 804
    }
  },
  {
    "title": "Comment from brin_bloom.c:818",
    "url": "backend\\access\\brin\\brin_bloom.c:818",
    "content": "brin_bloom_summary_recv\n\t- binary input routine for type brin_bloom_summary.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 818
    }
  },
  {
    "title": "Comment from brin_bloom.c:832",
    "url": "backend\\access\\brin\\brin_bloom.c:832",
    "content": "brin_bloom_summary_send\n\t- binary output routine for type brin_bloom_summary.\n\nBRIN bloom summaries are serialized in a bytea value (although the\ntype is named differently), so let's just send that.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_bloom.c",
      "relative_path": "backend\\access\\brin\\brin_bloom.c",
      "line_start": 832
    }
  },
  {
    "title": "Comment from brin_inclusion.c:1",
    "url": "backend\\access\\brin\\brin_inclusion.c:1",
    "content": "brin_inclusion.c\n\tImplementation of inclusion opclasses for BRIN\n\nThis module provides framework BRIN support functions for the \"inclusion\"\noperator classes.  A few SQL-level support functions are also required for\neach opclass.\n\nThe \"inclusion\" BRIN strategy is useful for types that support R-Tree\noperations.  This implementation is a straight mapping of those operations\nto the block-range nature of BRIN, with two exceptions: (a) we explicitly\nsupport \"empty\" elements: at least with range types, we need to consider\nemptiness separately from regular R-Tree strategies; and (b) we need to\nconsider \"unmergeable\" elements, that is, a set of elements for whose union\nno representation exists.  The only case where that happens as of this\nwriting is the INET type, where IPv6 values cannot be merged with IPv4\nvalues.\n\nPortions Copyright (c) 1996-2024, PostgreSQL Global Development Group\nPortions Copyright (c) 1994, Regents of the University of California\n\nIDENTIFICATION\n  src/backend/access/brin/brin_inclusion.c",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 1
    }
  },
  {
    "title": "Comment from brin_inclusion.c:40",
    "url": "backend\\access\\brin\\brin_inclusion.c:40",
    "content": "Additional SQL level support functions\n\nProcedure numbers must not use values reserved for BRIN itself; see\nbrin_internal.h.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 40
    }
  },
  {
    "title": "Comment from brin_inclusion.c:53",
    "url": "backend\\access\\brin\\brin_inclusion.c:53",
    "content": "Subtract this from procnum to obtain index in InclusionOpaque arrays\n(Must be equal to minimum of private procnums).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 53
    }
  },
  {
    "title": "Comment from brin_inclusion.c:59",
    "url": "backend\\access\\brin\\brin_inclusion.c:59",
    "content": "-\nThe values stored in the bv_values arrays correspond to:\n\nINCLUSION_UNION\n\tthe union of the values in the block range\nINCLUSION_UNMERGEABLE\n\twhether the values in the block range cannot be merged\n\t(e.g. an IPv6 address amidst IPv4 addresses)\nINCLUSION_CONTAINS_EMPTY\n\twhether an empty value is present in any tuple\n\tin the block range",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 59
    }
  },
  {
    "title": "Comment from brin_inclusion.c:100",
    "url": "backend\\access\\brin\\brin_inclusion.c:100",
    "content": "All members of opaque are initialized lazily; both procinfo arrays\nstart out as non-initialized by having fn_oid be InvalidOid, and\n\"missing\" to false, by zeroing here.  strategy_procinfos elements can\nbe invalidated when cached_subtype changes by zeroing fn_oid.\nextra_procinfo entries are never invalidated, but if a lookup fails\n(which is expected), extra_proc_missing is set to true, indicating not\nto look it up again.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 100
    }
  },
  {
    "title": "Comment from brin_inclusion.c:128",
    "url": "backend\\access\\brin\\brin_inclusion.c:128",
    "content": "BRIN inclusion add value function\n\nExamine the given index tuple (which contains partial status of a certain\npage range) by comparing it to the given value that comes from another heap\ntuple.  If the new value is outside the union specified by the existing\ntuple values, update the index tuple and return true.  Otherwise, return\nfalse and do not modify in this case.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 128
    }
  },
  {
    "title": "Comment from brin_inclusion.c:156",
    "url": "backend\\access\\brin\\brin_inclusion.c:156",
    "content": "If the recorded value is null, copy the new value (which we know to be\nnot null), and we're almost done.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 156
    }
  },
  {
    "title": "Comment from brin_inclusion.c:170",
    "url": "backend\\access\\brin\\brin_inclusion.c:170",
    "content": "No need for further processing if the block range is marked as\ncontaining unmergeable values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 170
    }
  },
  {
    "title": "Comment from brin_inclusion.c:177",
    "url": "backend\\access\\brin\\brin_inclusion.c:177",
    "content": "If the opclass supports the concept of empty values, test the passed\nnew value for emptiness; if it returns true, we need to set the\n\"contains empty\" flag in the element (unless already set).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 177
    }
  },
  {
    "title": "Comment from brin_inclusion.c:205",
    "url": "backend\\access\\brin\\brin_inclusion.c:205",
    "content": "Check if the new value is mergeable to the existing union.  If it is\nnot, mark the value as containing unmergeable elements and get out.\n\nNote: at this point we could remove the value from the union, since\nit's not going to be used any longer.  However, the BRIN framework\ndoesn't allow for the value not being present.  Improve someday.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 205
    }
  },
  {
    "title": "Comment from brin_inclusion.c:223",
    "url": "backend\\access\\brin\\brin_inclusion.c:223",
    "content": "Finally, merge the new value to the existing union.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 223
    }
  },
  {
    "title": "Comment from brin_inclusion.c:240",
    "url": "backend\\access\\brin\\brin_inclusion.c:240",
    "content": "BRIN inclusion consistent function\n\nWe're no longer dealing with NULL keys in the consistent function, that is\nnow handled by the AM code. That means we should not get any all-NULL ranges\neither, because those can't be consistent with regular (not [IS] NULL) keys.\n\nAll of the strategies are optional.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 240
    }
  },
  {
    "title": "Comment from brin_inclusion.c:263",
    "url": "backend\\access\\brin\\brin_inclusion.c:263",
    "content": "This opclass uses the old signature with only three arguments.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 263
    }
  },
  {
    "title": "Comment from brin_inclusion.c:269",
    "url": "backend\\access\\brin\\brin_inclusion.c:269",
    "content": "It has to be checked, if it contains elements that are not mergeable.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 269
    }
  },
  {
    "title": "Comment from brin_inclusion.c:279",
    "url": "backend\\access\\brin\\brin_inclusion.c:279",
    "content": "Placement strategies\n\nThese are implemented by logically negating the result of the\nconverse placement operator; for this to work, the converse\noperator must be part of the opclass.  An error will be thrown\nby inclusion_get_strategy_procinfo() if the required strategy\nis not part of the opclass.\n\nThese all return false if either argument is empty, so there is\nno need to check for empty elements.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 279
    }
  },
  {
    "title": "Comment from brin_inclusion.c:340",
    "url": "backend\\access\\brin\\brin_inclusion.c:340",
    "content": "Overlap and contains strategies\n\nThese strategies are simple enough that we can simply call the\noperator and return its result.  Empty elements don't change\nthe result.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 340
    }
  },
  {
    "title": "Comment from brin_inclusion.c:358",
    "url": "backend\\access\\brin\\brin_inclusion.c:358",
    "content": "Contained by strategies\n\nWe cannot just call the original operator for the contained by\nstrategies because some elements can be contained even though\nthe union is not; instead we use the overlap operator.\n\nWe check for empty elements separately as they are not merged\nto the union but contained by everything.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 358
    }
  },
  {
    "title": "Comment from brin_inclusion.c:380",
    "url": "backend\\access\\brin\\brin_inclusion.c:380",
    "content": "Adjacent strategy\n\nWe test for overlap first but to be safe we need to call the\nactual adjacent operator also.\n\nAn empty element cannot be adjacent to any other, so there is\nno need to check for it.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 380
    }
  },
  {
    "title": "Comment from brin_inclusion.c:402",
    "url": "backend\\access\\brin\\brin_inclusion.c:402",
    "content": "Basic comparison strategies\n\nIt is straightforward to support the equality strategies with\nthe contains operator.  Generally, inequality strategies do not\nmake much sense for the types which will be used with the\ninclusion BRIN family of opclasses, but it is possible to\nimplement them with logical negation of the left-of and\nright-of operators.\n\nNB: These strategies cannot be used with geometric datatypes\nthat use comparison of areas!  The only exception is the \"same\"\nstrategy.\n\nEmpty elements are considered to be less than the others.  We\ncannot use the empty support function to check the query is an\nempty element, because the query can be another data type than\nthe empty support function argument.  So we will return true,\nif there is a possibility that empty elements will change the\nresult.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 402
    }
  },
  {
    "title": "Comment from brin_inclusion.c:467",
    "url": "backend\\access\\brin\\brin_inclusion.c:467",
    "content": "BRIN inclusion union function\n\nGiven two BrinValues, update the first of them as a union of the summary\nvalues contained in both.  The second one is untouched.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 467
    }
  },
  {
    "title": "Comment from brin_inclusion.c:491",
    "url": "backend\\access\\brin\\brin_inclusion.c:491",
    "content": "If B includes empty elements, mark A similarly, if needed.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 491
    }
  },
  {
    "title": "Comment from brin_inclusion.c:496",
    "url": "backend\\access\\brin\\brin_inclusion.c:496",
    "content": "Check if A includes elements that are not mergeable.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 496
    }
  },
  {
    "title": "Comment from brin_inclusion.c:500",
    "url": "backend\\access\\brin\\brin_inclusion.c:500",
    "content": "If B includes elements that are not mergeable, mark A similarly.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 500
    }
  },
  {
    "title": "Comment from brin_inclusion.c:507",
    "url": "backend\\access\\brin\\brin_inclusion.c:507",
    "content": "Check if A and B are mergeable; if not, mark A unmergeable.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 507
    }
  },
  {
    "title": "Comment from brin_inclusion.c:536",
    "url": "backend\\access\\brin\\brin_inclusion.c:536",
    "content": "Cache and return inclusion opclass support procedure\n\nReturn the procedure corresponding to the given function support number\nor null if it is not exists.  If missing_ok is true and the procedure\nisn't set up for this opclass, return NULL instead of raising an error.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 536
    }
  },
  {
    "title": "Comment from brin_inclusion.c:550",
    "url": "backend\\access\\brin\\brin_inclusion.c:550",
    "content": "We cache these in the opaque struct, to avoid repetitive syscache\nlookups.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 550
    }
  },
  {
    "title": "Comment from brin_inclusion.c:556",
    "url": "backend\\access\\brin\\brin_inclusion.c:556",
    "content": "If we already searched for this proc and didn't find it, don't bother\nsearching again.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 556
    }
  },
  {
    "title": "Comment from brin_inclusion.c:587",
    "url": "backend\\access\\brin\\brin_inclusion.c:587",
    "content": "Cache and return the procedure of the given strategy\n\nReturn the procedure corresponding to the given sub-type and strategy\nnumber.  The data type of the index will be used as the left hand side of\nthe operator and the given sub-type will be used as the right hand side.\nThrows an error if the pg_amop row does not exist, but that should not\nhappen with a properly configured opclass.\n\nIt always throws an error when the data type of the opclass is different\nfrom the data type of the column or the expression.  That happens when the\ncolumn data type has implicit cast to the opclass data type.  We don't\nbother casting types, because this situation can easily be avoided by\nsetting storage data type to that of the opclass.  The same problem does not\napply to the data type of the right hand side, because the type in the\nScanKey always matches the opclass' one.\n\nNote: this function mirrors minmax_get_strategy_procinfo; if changes are\nmade here, see that function too.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 587
    }
  },
  {
    "title": "Comment from brin_inclusion.c:618",
    "url": "backend\\access\\brin\\brin_inclusion.c:618",
    "content": "We cache the procedures for the last sub-type in the opaque struct, to\navoid repetitive syscache lookups.  If the sub-type is changed,\ninvalidate all the cached entries.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_inclusion.c",
      "relative_path": "backend\\access\\brin\\brin_inclusion.c",
      "line_start": 618
    }
  },
  {
    "title": "Comment from brin_minmax.c:1",
    "url": "backend\\access\\brin\\brin_minmax.c:1",
    "content": "brin_minmax.c\n\tImplementation of Min/Max opclass for BRIN\n\nPortions Copyright (c) 1996-2024, PostgreSQL Global Development Group\nPortions Copyright (c) 1994, Regents of the University of California\n\nIDENTIFICATION\n  src/backend/access/brin/brin_minmax.c",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax.c",
      "relative_path": "backend\\access\\brin\\brin_minmax.c",
      "line_start": 1
    }
  },
  {
    "title": "Comment from brin_minmax.c:39",
    "url": "backend\\access\\brin\\brin_minmax.c:39",
    "content": "opaque->strategy_procinfos is initialized lazily; here it is set to\nall-uninitialized by palloc0 which sets fn_oid to InvalidOid.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax.c",
      "relative_path": "backend\\access\\brin\\brin_minmax.c",
      "line_start": 39
    }
  },
  {
    "title": "Comment from brin_minmax.c:56",
    "url": "backend\\access\\brin\\brin_minmax.c:56",
    "content": "Examine the given index tuple (which contains partial status of a certain\npage range) by comparing it to the given value that comes from another heap\ntuple.  If the new value is outside the min/max range specified by the\nexisting tuple values, update the index tuple and return true.  Otherwise,\nreturn false and do not modify in this case.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax.c",
      "relative_path": "backend\\access\\brin\\brin_minmax.c",
      "line_start": 56
    }
  },
  {
    "title": "Comment from brin_minmax.c:82",
    "url": "backend\\access\\brin\\brin_minmax.c:82",
    "content": "If the recorded value is null, store the new value (which we know to be\nnot null) as both minimum and maximum, and we're done.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax.c",
      "relative_path": "backend\\access\\brin\\brin_minmax.c",
      "line_start": 82
    }
  },
  {
    "title": "Comment from brin_minmax.c:94",
    "url": "backend\\access\\brin\\brin_minmax.c:94",
    "content": "Otherwise, need to compare the new value with the existing boundaries\nand update them accordingly.  First check if it's less than the\nexisting minimum.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax.c",
      "relative_path": "backend\\access\\brin\\brin_minmax.c",
      "line_start": 94
    }
  },
  {
    "title": "Comment from brin_minmax.c:127",
    "url": "backend\\access\\brin\\brin_minmax.c:127",
    "content": "Given an index tuple corresponding to a certain page range and a scan key,\nreturn whether the scan key is consistent with the index tuple's min/max\nvalues.  Return true if so, false otherwise.\n\nWe're no longer dealing with NULL keys in the consistent function, that is\nnow handled by the AM code. That means we should not get any all-NULL ranges\neither, because those can't be consistent with regular (not [IS] NULL) keys.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax.c",
      "relative_path": "backend\\access\\brin\\brin_minmax.c",
      "line_start": 127
    }
  },
  {
    "title": "Comment from brin_minmax.c:149",
    "url": "backend\\access\\brin\\brin_minmax.c:149",
    "content": "This opclass uses the old signature with only three arguments.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax.c",
      "relative_path": "backend\\access\\brin\\brin_minmax.c",
      "line_start": 149
    }
  },
  {
    "title": "Comment from brin_minmax.c:169",
    "url": "backend\\access\\brin\\brin_minmax.c:169",
    "content": "In the equality case (WHERE col = someval), we want to return\nthe current page range if the minimum value in the range <=\nscan key, and the maximum value >= scan key.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax.c",
      "relative_path": "backend\\access\\brin\\brin_minmax.c",
      "line_start": 169
    }
  },
  {
    "title": "Comment from brin_minmax.c:203",
    "url": "backend\\access\\brin\\brin_minmax.c:203",
    "content": "Given two BrinValues, update the first of them as a union of the summary\nvalues contained in both.  The second one is untouched.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax.c",
      "relative_path": "backend\\access\\brin\\brin_minmax.c",
      "line_start": 203
    }
  },
  {
    "title": "Comment from brin_minmax.c:238",
    "url": "backend\\access\\brin\\brin_minmax.c:238",
    "content": "Adjust maximum, if B's max is greater than A's max",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax.c",
      "relative_path": "backend\\access\\brin\\brin_minmax.c",
      "line_start": 238
    }
  },
  {
    "title": "Comment from brin_minmax.c:254",
    "url": "backend\\access\\brin\\brin_minmax.c:254",
    "content": "Cache and return the procedure for the given strategy.\n\nNote: this function mirrors inclusion_get_strategy_procinfo; see notes\nthere.  If changes are made here, see that function too.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax.c",
      "relative_path": "backend\\access\\brin\\brin_minmax.c",
      "line_start": 254
    }
  },
  {
    "title": "Comment from brin_minmax.c:271",
    "url": "backend\\access\\brin\\brin_minmax.c:271",
    "content": "We cache the procedures for the previous subtype in the opaque struct,\nto avoid repetitive syscache lookups.  If the subtype changed,\ninvalidate all the cached entries.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax.c",
      "relative_path": "backend\\access\\brin\\brin_minmax.c",
      "line_start": 271
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1",
    "content": "brin_minmax_multi.c\n\tImplementation of Multi Min/Max opclass for BRIN\n\nPortions Copyright (c) 1996-2024, PostgreSQL Global Development Group\nPortions Copyright (c) 1994, Regents of the University of California\n\n\nImplements a variant of minmax opclass, where the summary is composed of\nmultiple smaller intervals. This allows us to handle outliers, which\nusually make the simple minmax opclass inefficient.\n\nConsider for example page range with simple minmax interval [1000,2000],\nand assume a new row gets inserted into the range with value 1000000.\nDue to that the interval gets [1000,1000000]. I.e. the minmax interval\ngot 1000x wider and won't be useful to eliminate scan keys between 2001\nand 1000000.\n\nWith minmax-multi opclass, we may have [1000,2000] interval initially,\nbut after adding the new row we start tracking it as two interval:\n\n  [1000,2000] and [1000000,1000000]\n\nThis allows us to still eliminate the page range when the scan keys hit\nthe gap between 2000 and 1000000, making it useful in cases when the\nsimple minmax opclass gets inefficient.\n\nThe number of intervals tracked per page range is somewhat flexible.\nWhat is restricted is the number of values per page range, and the limit\nis currently 32 (see values_per_range reloption). Collapsed intervals\n(with equal minimum and maximum value) are stored as a single value,\nwhile regular intervals require two values.\n\nWhen the number of values gets too high (by adding new values to the\nsummary), we merge some of the intervals to free space for more values.\nThis is done in a greedy way - we simply pick the two closest intervals,\nmerge them, and repeat this until the number of values to store gets\nsufficiently low (below 50% of maximum values), but that is mostly\narbitrary threshold and may be changed easily).\n\nTo pick the closest intervals we use the \"distance\" support procedure,\nwhich measures space between two ranges (i.e. the length of an interval).\nThe computed value may be an approximation - in the worst case we will\nmerge two ranges that are slightly less optimal at that step, but the\nindex should still produce correct results.\n\nThe compactions (reducing the number of values) is fairly expensive, as\nit requires calling the distance functions, sorting etc. So when building\nthe summary, we use a significantly larger buffer, and only enforce the\nexact limit at the very end. This improves performance, and it also helps\nwith building better ranges (due to the greedy approach).\n\n\nIDENTIFICATION\n  src/backend/access/brin/brin_minmax_multi.c",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:86",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:86",
    "content": "Additional SQL level support functions\n\nProcedure numbers must not use values reserved for BRIN itself; see\nbrin_internal.h.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 86
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:95",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:95",
    "content": "Subtract this from procnum to obtain index in MinmaxMultiOpaque arrays\n(Must be equal to minimum of private procnums).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 95
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:101",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:101",
    "content": "Sizing the insert buffer - we use 10x the number of values specified\nin the reloption, but we cap it to 8192 not to get too large. When\nthe buffer gets full, we reduce the number of values by half.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 101
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:136",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:136",
    "content": "The summary of minmax-multi indexes has two representations - Ranges for\nconvenient processing, and SerializedRanges for storage in bytea value.\n\nThe Ranges struct stores the boundary values in a single array, but we\ntreat regular and single-point ranges differently to save space. For\nregular ranges (with different boundary values) we have to store both\nthe lower and upper bound of the range, while for \"single-point ranges\"\nwe only need to store a single value.\n\nThe 'values' array stores boundary values for regular ranges first (there\nare 2*nranges values to store), and then the nvalues boundary values for\nsingle-point ranges. That is, we have (2*nranges + nvalues) boundary\nvalues in the array.\n\n+-------------------------+----------------------------------+\n| ranges (2 * nranges of) | single point values (nvalues of) |\n+-------------------------+----------------------------------+\n\nThis allows us to quickly add new values, and store outliers without\nhaving to widen any of the existing range values.\n\n'nsorted' denotes how many of 'nvalues' in the values[] array are sorted.\nWhen nsorted == nvalues, all single point values are sorted.\n\nWe never store more than maxvalues values (as set by values_per_range\nreloption). If needed we merge some of the ranges.\n\nTo minimize palloc overhead, we always allocate the full array with\nspace for maxvalues elements. This should be fine as long as the\nmaxvalues is reasonably small (64 seems fine), which is the case\nthanks to values_per_range reloption being limited to 256.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 136
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:183",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:183",
    "content": "We simply add the values into a large buffer, without any expensive\nsteps (sorting, deduplication, ...). The buffer is a multiple of the\ntarget number of values, so the compaction happens less often,\namortizing the costs. We keep the actual target and compact to the\nrequested number of values at the very end, before serializing to\non-disk representation.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 183
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:194",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:194",
    "content": "values stored for this range - either raw values, or ranges",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 194
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:198",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:198",
    "content": "On-disk the summary is stored as a bytea value, with a simple header\nwith basic metadata, followed by the boundary values. It has a varlena\nheader, so can be treated as varlena directly.\n\nSee brin_range_serialize/brin_range_deserialize for serialization details.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 198
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:228",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:228",
    "content": "Used to represent ranges expanded to make merging and combining easier.\n\nEach expanded range is essentially an interval, represented by min/max\nvalues, along with a flag whether it's a collapsed range (in which case\nthe min and max values are equal). We have the flag to handle by-ref\ndata types - we can't simply compare the datums, and this saves some\ncalls to the type-specific comparator function.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 228
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:244",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:244",
    "content": "Represents a distance between two ranges (identified by index into\nan array of extended ranges).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 244
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:274",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:274",
    "content": "Check that the order of the array values is correct, using the cmp\nfunction (which should be BTLessStrategyNumber).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 274
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:308",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:308",
    "content": "First the ranges - there are 2*nranges boundary values, and the values\nhave to be strictly ordered (equal values would mean the range is\ncollapsed, and should be stored as a point). This also guarantees that\nthe ranges do not overlap.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 308
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:316",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:316",
    "content": "then the single-point ranges (with nvalues boundary values )",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 316
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:320",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:320",
    "content": "Check that none of the values are not covered by ranges (both sorted\nand unsorted)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 320
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:337",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:337",
    "content": "If the value is smaller than the lower bound in the first range\nthen it cannot possibly be in any of the ranges.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 337
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:346",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:346",
    "content": "Likewise, if the value is larger than the upper bound of the\nfinal range, then it cannot possibly be inside any of the\nranges.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 346
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:354",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:354",
    "content": "bsearch the ranges to see if 'value' fits within any of them",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 354
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:369",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:369",
    "content": "Is the value smaller than the minval? If yes, we'll recurse\nto the left side of range array.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 369
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:382",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:382",
    "content": "Is the value greater than the minval? If yes, we'll recurse\nto the right side of range array.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 382
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:401",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:401",
    "content": "and values in the unsorted part must not be in the sorted part",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 401
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:421",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:421",
    "content": "Check that the expanded ranges (built when reducing the number of ranges\nby combining some of them) are correctly sorted and do not overlap.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 421
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:441",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:441",
    "content": "Each range independently should be valid, i.e. that for the boundary\nvalues (lower <= upper).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 441
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:459",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:459",
    "content": "And the ranges should be ordered and must not overlap, i.e. upper <\nlower for boundaries of consecutive ranges.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 459
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:477",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:477",
    "content": "minmax_multi_init\n\t\tInitialize the deserialized range list, allocate all the memory.\n\nThis is only in-memory representation of the ranges, so we allocate\nenough space for the maximum number of values (so as not to have to do\nrepallocs as the ranges grow).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 477
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:504",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:504",
    "content": "range_deduplicate_values\n\tDeduplicate the part with values in the simple points.\n\nThis is meant to be a cheaper way of reducing the size of the ranges. It\ndoes not touch the ranges, and only sorts the other values - it does not\ncall the distance functions, which may be quite expensive, etc.\n\nWe do know the values are not duplicate with the ranges, because we check\nthat before adding a new value. Same for the sorted part of values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 504
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:523",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:523",
    "content": "If there are no unsorted values, we're done (this probably can't\nhappen, as we're adding values to unsorted part).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 523
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:534",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:534",
    "content": "the values start right after the ranges (which are always sorted)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 534
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:537",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:537",
    "content": "XXX This might do a merge sort, to leverage that the first part of the\narray is already sorted. If the sorted part is large, it might be quite\na bit faster.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 537
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:568",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:568",
    "content": "brin_range_serialize\n  Serialize the in-memory representation into a compact varlena value.\n\nSimply copy the header and then also the individual values, as stored\nin the in-memory value array.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 568
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:594",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:594",
    "content": "at this point the range should be compacted to the target size",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 594
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:615",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:615",
    "content": "The space needed depends on data type - for fixed-length data types\n(by-value and some by-reference) it's pretty simple, just multiply\n(attlen * nvalues) and we're done. For variable-length by-reference\ntypes we need to actually walk all the values and sum the lengths.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 615
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:646",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:646",
    "content": "Allocate the serialized object, copy the basic information. The\nserialized object is a varlena, so update the header.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 646
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:658",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:658",
    "content": "And now copy also the boundary values (like the length calculation this\ndepends on the particular data type).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 658
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:670",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:670",
    "content": "For byval types, we need to copy just the significant bytes -\nwe can't use memcpy directly, as that assumes little-endian\nbehavior.  store_att_byval does almost what we need, but it\nrequires a properly aligned buffer - the output buffer does not\nguarantee that. So we simply use a local Datum variable (which\nguarantees proper alignment), and then copy the value from it.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 670
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:713",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:713",
    "content": "brin_range_deserialize\n  Serialize the in-memory representation into a compact varlena value.\n\nSimply copy the header and then also the individual values, as stored\nin the in-memory value array.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 713
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:756",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:756",
    "content": "And now deconstruct the values into Datum array. We have to copy the\ndata because the serialized representation ignores alignment, and we\ndon't want to rely on it being kept around anyway.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 756
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:763",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:763",
    "content": "We don't want to allocate many pieces, so we just allocate everything\nin one chunk. How much space will we need?\n\nXXX We don't need to copy simple by-value data types.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 763
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:792",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:792",
    "content": "Restore the source pointer (might have been modified when calculating\nthe space we need to allocate).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 792
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:841",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:841",
    "content": "should have consumed the whole input value exactly",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 841
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:848",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:848",
    "content": "compare_expanded_ranges\n  Compare the expanded ranges - first by minimum, then by maximum.\n\nWe do guarantee that ranges in a single Ranges object do not overlap, so it\nmay seem strange that we don't order just by minimum. But when merging two\nRanges (which happens in the union function), the ranges may in fact\noverlap. So we do compare both.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 848
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:917",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:917",
    "content": "Check if the new value matches one of the existing ranges.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 917
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:942",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:942",
    "content": "Otherwise, need to compare the new value with boundaries of all the\nranges. First check if it's less than the absolute minimum, which is\nthe first value in the array.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 942
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:955",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:955",
    "content": "And now compare it to the existing maximum (last value in the data\narray). But only if we haven't already ruled out a possible match in\nthe minvalue check.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 955
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:967",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:967",
    "content": "So we know it's in the general min/max, the question is whether it\nfalls in one of the ranges or gaps. We'll do a binary search on\nindividual ranges - for each range we check equality (value falls into\nthe range), and then check ranges either above or below the current\nrange.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 967
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:988",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:988",
    "content": "Is the value smaller than the minval? If yes, we'll recurse to the\nleft side of range array.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 988
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1001",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1001",
    "content": "Is the value greater than the minval? If yes, we'll recurse to the\nright side of range array.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1001
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1022",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1022",
    "content": "range_contains_value\n\t\tSee if the new value is already contained in the range list.\n\nWe first inspect the list of intervals. We use a small trick - we check\nthe value against min/max of the whole range (min of the first interval,\nmax of the last one) first, and only inspect the individual intervals if\nthis passes.\n\nIf the value matches none of the intervals, we check the exact values.\nWe simply loop through them and invoke equality operator on them.\n\nThe last parameter (full) determines whether we need to search all the\nvalues, including the unsorted part. With full=false, the unsorted part\nis not searched, which may produce false negatives and duplicate values\n(in the unsorted part only), but when we're building the range that's\nfine - we'll deduplicate before serialization, and it can only happen\nif there already are unsorted values (so it was already modified).\n\nSerialized ranges don't have any unsorted values, so this can't cause\nfalse negatives during querying.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1022
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1053",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1053",
    "content": "First inspect the ranges, if there are any. We first check the whole\nrange, and only when there's still a chance of getting a match we\ninspect the individual ranges.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1053
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1064",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1064",
    "content": "There is no matching range, so let's inspect the sorted values.\n\nWe do a sequential search for small numbers of values, and binary\nsearch once we have more than 16 values. This threshold is somewhat\narbitrary, as it depends on how expensive the comparison function is.\n\nXXX If we use the threshold here, maybe we should do the same thing in\nhas_matching_range? Or maybe we should do the bin search all the time?\n\nXXX We could use the same optimization as for ranges, to check if the\nvalue is between min/max, to maybe rule out all sorted values without\nhaving to inspect all of them.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1064
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1104",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1104",
    "content": "If not asked to inspect the unsorted part, we're done.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1104
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1124",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1124",
    "content": "Expand ranges from Ranges into ExpandedRange array. This expects the\neranges to be pre-allocated and with the correct size - there needs to be\n(nranges + nvalues) elements.\n\nThe order of expanded ranges is arbitrary. We do expand the ranges first,\nand this part is sorted. But then we expand the values, and this part may\nbe unsorted.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1124
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1169",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1169",
    "content": "Sort and deduplicate expanded ranges.\n\nThe ranges may be deduplicated - we're simply appending values, without\nchecking for duplicates etc. So maybe the deduplication will reduce the\nnumber of ranges enough, and we won't have to compute the distances etc.\n\nReturns the number of expanded ranges.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1169
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1192",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1192",
    "content": "XXX We do qsort on all the values, but we could also leverage the fact\nthat some of the input data is already sorted (all the ranges and maybe\nsome of the points) and do merge sort.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1192
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1200",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1200",
    "content": "Deduplicate the ranges - simply compare each range to the preceding\none, and skip the duplicate ones.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1200
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1207",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1207",
    "content": "if the current range is equal to the preceding one, do nothing",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1207
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1211",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1211",
    "content": "otherwise, copy it to n-th place (if not already there)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1211
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1223",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1223",
    "content": "When combining multiple Range values (in union function), some of the\nranges may overlap. We simply merge the overlapping ranges to fix that.\n\nXXX This assumes the expanded ranges were previously sorted (by minval\nand then maxval). We leverage this when detecting overlap.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1223
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1242",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1242",
    "content": "comparing [?,maxval] vs. [minval,?] - the ranges overlap if (minval\n< maxval)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1242
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1250",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1250",
    "content": "Nope, maxval < minval, so no overlap. And we know the ranges are\nordered, so there are no more overlaps, because all the remaining\nranges have greater or equal minval.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1250
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1262",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1262",
    "content": "So ranges 'idx' and 'idx+1' do overlap, but we don't know if\n'idx+1' is contained in 'idx', or if they overlap only partially.\nSo compare the upper bounds and keep the larger one.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1262
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1274",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1274",
    "content": "The range certainly is no longer collapsed (irrespectively of the\nprevious state).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1274
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1280",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1280",
    "content": "Now get rid of the (idx+1) range entirely by shifting the remaining\nranges by 1. There are neranges elements, and we need to move\nelements from (idx+2). That means the number of elements to move is\n[ncranges - (idx+2)].",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1280
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1289",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1289",
    "content": "Decrease the number of ranges, and repeat (with the same range, as\nit might overlap with additional ranges thanks to the merge).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1289
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1299",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1299",
    "content": "Simple comparator for distance values, comparing the double value.\nThis is intentionally sorting the distances in descending order, i.e.\nthe longer gaps will be at the front.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1299
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1318",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1318",
    "content": "Given an array of expanded ranges, compute size of the gaps between each\nrange.  For neranges there are (neranges-1) gaps.\n\nWe simply call the \"distance\" function to compute the (max-min) for pairs\nof consecutive ranges. The function may be fairly expensive, so we do that\njust once (and then use it to pick as many ranges to merge as possible).\n\nSee reduce_expanded_ranges for details.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1318
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1338",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1338",
    "content": "If there's only a single range, there's no distance to calculate.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1338
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1345",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1345",
    "content": "Walk through the ranges once and compute the distance between the\nranges so that we can sort them once.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1345
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1366",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1366",
    "content": "Sort the distances in descending order, so that the longest gaps are at\nthe front.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1366
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1375",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1375",
    "content": "Builds expanded ranges for the existing ranges (and single-point ranges),\nand also the new value (which did not fit into the array).  This expanded\nrepresentation makes the processing a bit easier, as it allows handling\nranges and points the same way.\n\nWe sort and deduplicate the expanded ranges - this is necessary, because\nthe points may be unsorted. And moreover the two parts (ranges and\npoints) are sorted on their own.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1375
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1392",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1392",
    "content": "both ranges and points are expanded into a separate element",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1392
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1410",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1410",
    "content": "Counts boundary values needed to store the ranges. Each single-point\nrange is stored using a single value, each regular range needs two.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1410
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1433",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1433",
    "content": "reduce_expanded_ranges\n\treduce the ranges until the number of values is low enough\n\nCombines ranges until the number of boundary values drops below the\nthreshold specified by max_values. This happens by merging enough\nranges by the distance between them.\n\nReturns the number of result ranges.\n\nWe simply use the global min/max and then add boundaries for enough\nlargest gaps. Each gap adds 2 values, so we simply use (target/2-1)\ndistances. Then we simply sort all the values - each two values are\na boundary of a range (possibly collapsed).\n\nXXX Some of the ranges may be collapsed (i.e. the min/max values are\nequal), but we ignore that for now. We could repeat the process,\nadding a couple more gaps recursively.\n\nXXX The ranges to merge are selected solely using the distance. But\nthat may not be the best strategy, for example when multiple gaps\nare of equal (or very similar) length.\n\nConsider for example points 1, 2, 3, .., 64, which have gaps of the\nsame length 1 of course. In that case, we tend to pick the first\ngap of that length, which leads to this:\n\n   step 1:  [1, 2], 3, 4, 5, .., 64\n   step 2:  [1, 3], 4, 5,    .., 64\n   step 3:  [1, 4], 5,       .., 64\n   ...\n\nSo in the end we'll have one \"large\" range and multiple small points.\nThat may be fine, but it seems a bit strange and non-optimal. Maybe\nwe should consider other things when picking ranges to merge - e.g.\nlength of the ranges? Or perhaps randomize the choice of ranges, with\nprobability inversely proportional to the distance (the gap lengths\nmay be very close, but not exactly the same).\n\nXXX Or maybe we could just handle this by using random value as a\ntie-break, or by adding random noise to the actual distance.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1433
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1492",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1492",
    "content": "Maybe we have a sufficiently low number of ranges already?\n\nXXX This should happen before we actually do the expensive stuff like\nsorting, so maybe this should be just an assert.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1492
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1509",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1509",
    "content": "add the global min/max values, from the first/last range",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1509
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1516",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1516",
    "content": "index of the gap between (index) and (index+1) ranges",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1516
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1521",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1521",
    "content": "add max from the preceding range, minval from the next one",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1521
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1531",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1531",
    "content": "Sort the values using the comparator function, and form ranges from the\nsorted result.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1531
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1538",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1538",
    "content": "We have nvalues boundary values, which means nvalues/2 ranges.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1538
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1544",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1544",
    "content": "if the boundary values are the same, it's a collapsed range",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1544
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1553",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1553",
    "content": "Store the boundary values from ExpandedRanges back into 'ranges' (using\nonly the minimal number of values needed).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1553
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1594",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1594",
    "content": "Consider freeing space in the ranges. Checks if there's space for at least\none new value, and performs compaction if needed.\n\nReturns true if the value was actually modified.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1594
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1616",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1616",
    "content": "If there is free space in the buffer, we're done without having to\nmodify anything.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1616
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1623",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1623",
    "content": "we'll certainly need the comparator, so just look it up now",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1623
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1630",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1630",
    "content": "Did we reduce enough free space by just the deduplication?\n\nWe don't simply check against range->maxvalues again. The deduplication\nmight have freed very little space (e.g. just one value), forcing us to\ndo deduplication very often. In that case, it's better to do the\ncompaction and reduce more space.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1630
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1641",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1641",
    "content": "We need to combine some of the existing ranges, to reduce the number of\nvalues we have to store.\n\nThe distanceFn calls (which may internally call e.g. numeric_le) may\nallocate quite a bit of memory, and we must not leak it (we might have\nto do this repeatedly, even for a single BRIN page range). Otherwise\nwe'd have problems e.g. when building new indexes. So we use a memory\ncontext and make sure we free the memory at the end (so if we call the\ndistance function many times, it might be an issue, but meh).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1641
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1667",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1667",
    "content": "build array of gap distances and sort them in ascending order",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1667
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1670",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1670",
    "content": "Combine ranges until we release at least 50% of the space. This\nthreshold is somewhat arbitrary, perhaps needs tuning. We must not use\ntoo low or high value.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1670
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1679",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1679",
    "content": "Is the result of reducing expanded ranges correct?",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1679
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1682",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1682",
    "content": "Make sure we've sufficiently reduced the number of ranges.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1682
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1685",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1685",
    "content": "decompose the expanded ranges into regular ranges and single values",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1685
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1697",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1697",
    "content": "range_add_value\n\t\tAdd the new value to the minmax-multi range.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1697
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1709",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1709",
    "content": "we'll certainly need the comparator, so just look it up now",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1709
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1716",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1716",
    "content": "Make sure there's enough free space in the buffer. We only trigger this\nwhen the buffer is full, which means it had to be modified as we size\nit to be larger than what is stored on disk.\n\nThis needs to happen before we check if the value is contained in the\nrange, because the value might be in the unsorted part, and we don't\ncheck that in range_contains_value. The deduplication would then move\nit to the sorted part, and we'd add the value too, which violates the\nrule that we never have duplicates with the ranges or sorted values.\n\nWe might also deduplicate and recheck if the value is contained, but\nthat seems like overkill. We'd need to deduplicate anyway, so why not\ndo it now.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1716
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1734",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1734",
    "content": "Bail out if the value already is covered by the range.\n\nWe could also add values until we hit values_per_range, and then do the\ndeduplication in a batch, hoping for better efficiency. But that would\nmean we actually modify the range every time, which means having to\nserialize the value, which does palloc, walks the values, copies them,\netc. Not exactly cheap.\n\nSo instead we do the check, which should be fairly cheap - assuming the\ncomparator function is not very expensive.\n\nThis also implies the values array can't contain duplicate values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1734
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1754",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1754",
    "content": "If there's space in the values array, copy it in and we're done.\n\nWe do want to keep the values sorted (to speed up searches), so we do a\nsimple insertion sort. We could do something more elaborate, e.g. by\nsorting the values only now and then, but for small counts (e.g. when\nmaxvalues is 64) this should be fine.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1754
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1765",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1765",
    "content": "If we added the first value, we can consider it as sorted.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1765
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1769",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1769",
    "content": "Check we haven't broken the ordering of boundary values (checks both\nparts, but that doesn't hurt).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1769
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1782",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1782",
    "content": "Generate range representation of data collected during \"batch mode\".\nThis is similar to reduce_expanded_ranges, except that we can't assume\nthe values are sorted and there may be duplicate values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1782
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1801",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1801",
    "content": "Do we need to actually compactify anything?\n\nThere are two reasons why compaction may be needed - firstly, there may\nbe too many values, or some of the values may be unsorted.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1801
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1811",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1811",
    "content": "we'll certainly need the comparator, so just look it up now",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1811
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1818",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1818",
    "content": "The distanceFn calls (which may internally call e.g. numeric_le) may\nallocate quite a bit of memory, and we must not leak it. Otherwise,\nwe'd have problems e.g. when building indexes. So we create a local\nmemory context and make sure we free the memory before leaving this\nfunction (not after every call).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1818
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1834",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1834",
    "content": "build array of gap distances and sort them in ascending order",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1834
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1838",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1838",
    "content": "Combine ranges until we get below max_values. We don't use any scale\nfactor, because this is used during serialization, and we don't expect\nmore tuples to be inserted anytime soon.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1838
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1848",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1848",
    "content": "transform back into regular ranges and single values",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1848
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1863",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1863",
    "content": "opaque->strategy_procinfos is initialized lazily; here it is set to\nall-uninitialized by palloc0 which sets fn_oid to InvalidOid.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1863
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1879",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1879",
    "content": "Compute the distance between two float4 values (plain subtraction).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1879
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1888",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1888",
    "content": "if both values are NaN, then we consider them the same",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1888
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1896",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1896",
    "content": "We know the values are range boundaries, but the range may be collapsed\n(i.e. single points), with equal values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1896
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1905",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1905",
    "content": "Compute the distance between two float8 values (plain subtraction).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1905
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1914",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1914",
    "content": "if both values are NaN, then we consider them the same",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1914
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1922",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1922",
    "content": "We know the values are range boundaries, but the range may be collapsed\n(i.e. single points), with equal values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1922
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1931",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1931",
    "content": "Compute the distance between two int2 values (plain subtraction).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1931
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1940",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1940",
    "content": "We know the values are range boundaries, but the range may be collapsed\n(i.e. single points), with equal values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1940
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1949",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1949",
    "content": "Compute the distance between two int4 values (plain subtraction).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1949
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1958",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1958",
    "content": "We know the values are range boundaries, but the range may be collapsed\n(i.e. single points), with equal values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1958
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1967",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1967",
    "content": "Compute the distance between two int8 values (plain subtraction).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1967
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1976",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1976",
    "content": "We know the values are range boundaries, but the range may be collapsed\n(i.e. single points), with equal values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1976
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1985",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1985",
    "content": "Compute the distance between two tid values (by mapping them to float8 and\nthen subtracting them).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1985
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:1998",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:1998",
    "content": "We know the values are range boundaries, but the range may be collapsed\n(i.e. single points), with equal values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 1998
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2004",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2004",
    "content": "We use the no-check variants here, because user-supplied values may\nhave (ip_posid == 0). See ItemPointerCompare.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2004
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2017",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2017",
    "content": "Compute the distance between two numeric values (plain subtraction).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2017
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2027",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2027",
    "content": "We know the values are range boundaries, but the range may be collapsed\n(i.e. single points), with equal values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2027
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2038",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2038",
    "content": "Compute the approximate distance between two UUID values.\n\nXXX We do not need a perfectly accurate value, so we approximate the\ndeltas (which would have to be 128-bit integers) with a 64-bit float.\nThe small inaccuracies do not matter in practice, in the worst case\nwe'll decide to merge ranges that are not the closest ones.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2038
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2058",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2058",
    "content": "We know the values are range boundaries, but the range may be collapsed\n(i.e. single points), with equal values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2058
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2064",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2064",
    "content": "compute approximate delta as a double precision value",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2064
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2076",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2076",
    "content": "Compute the approximate distance between two dates.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2076
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2093",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2093",
    "content": "Compute the approximate distance between two time (without tz) values.\n\nTimeADT is just an int64, so we simply subtract the values directly.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2093
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2113",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2113",
    "content": "Compute the approximate distance between two timetz values.\n\nSimply subtracts the TimeADT (int64) values embedded in TimeTzADT.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2113
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2133",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2133",
    "content": "Compute the distance between two timestamp values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2133
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2165",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2165",
    "content": "Delta is (fractional) number of days between the intervals. Assume\nmonths have 30 days for consistency with interval_cmp_internal. We\ndon't need to be exact, in the worst case we'll build a bit less\nefficient ranges. But we should not contradict interval_cmp.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2165
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2184",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2184",
    "content": "Compute the distance between two pg_lsn values.\n\nLSN is just an int64 encoding position in the stream, so just subtract\nthose int64 values directly.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2184
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2205",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2205",
    "content": "Compute the distance between two macaddr values.\n\nmac addresses are treated as 6 unsigned chars, so do the same thing we\nalready do for UUID values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2205
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2242",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2242",
    "content": "Compute the distance between two macaddr8 values.\n\nmacaddr8 addresses are 8 unsigned chars, so do the same thing we\nalready do for UUID values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2242
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2285",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2285",
    "content": "Compute the distance between two inet values.\n\nThe distance is defined as the difference between 32-bit/128-bit values,\ndepending on the IP version. The distance is computed by subtracting\nthe bytes and normalizing it to [0,1] range for each IP family.\nAddresses from different families are considered to be in maximum\ndistance, which is 1.0.\n\nXXX Does this need to consider the mask (bits)?  For now, it's ignored.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2285
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2311",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2311",
    "content": "If the addresses are from different families, consider them to be in\nmaximal possible distance (which is 1.0).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2311
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2324",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2324",
    "content": "The length is calculated from the mask length, because we sort the\naddresses by first address in the range, so A.B.C.D/24 < A.B.C.1 (the\nfirst range starts at A.B.C.0, which is before A.B.C.1). We don't want\nto produce a negative delta in this case, so we just cut the extra\nbytes.\n\nXXX Maybe this should be a bit more careful and cut the bits, not just\nwhole bytes.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2324
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2385",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2385",
    "content": "In batch mode, we need to compress the accumulated values to the\nactually requested number of values/ranges.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2385
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2404",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2404",
    "content": "Examine the given index tuple (which contains the partial status of a\ncertain page range) by comparing it to the given value that comes from\nanother heap tuple.  If the new value is outside the min/max range\nspecified by the existing tuple values, update the index tuple and return\ntrue.  Otherwise, return false and do not modify in this case.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2404
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2434",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2434",
    "content": "If this is the first non-null value, we need to initialize the range\nlist. Otherwise, just extract the existing range list from BrinValues.\n\nWhen starting with an empty range, we assume this is a batch mode and\nwe use a larger buffer. The buffer size is derived from the BRIN range\nsize, number of rows per page, with some sensible min/max values. A\nsmall buffer would be bad for performance, but a large buffer might\nrequire a lot of memory (because of keeping all the values).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2434
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2455",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2455",
    "content": "Determine the insert buffer size - we use 10x the target, capped to\nthe maximum number of values in the heap range. This is more than\nenough, considering the actual number of rows per page is likely\nmuch lower, but meh.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2455
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2478",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2478",
    "content": "we'll certainly need the comparator, so just look it up now",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2478
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2501",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2501",
    "content": "Determine the insert buffer size - we use 10x the target, capped to\nthe maximum number of values in the heap range. This is more than\nenough, considering the actual number of rows per page is likely\nmuch lower, but meh.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2501
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2523",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2523",
    "content": "we'll certainly need the comparator, so just look it up now",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2523
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2533",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2533",
    "content": "Try to add the new value to the range. We need to update the modified\nflag, so that we serialize the updated summary later.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2533
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2543",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2543",
    "content": "Given an index tuple corresponding to a certain page range and a scan key,\nreturn whether the scan key is consistent with the index tuple's min/max\nvalues.  Return true if so, false otherwise.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2543
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2572",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2572",
    "content": "inspect the ranges, and for each one evaluate the scan keys",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2572
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2578",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2578",
    "content": "assume the range is matching, and we'll try to prove otherwise",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2578
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2586",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2586",
    "content": "NULL keys are handled and filtered-out in bringetbitmap",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2586
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2610",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2610",
    "content": "Otherwise, need to compare the new value with\nboundaries of all the ranges. First check if it's\nless than the absolute minimum, which is the first\nvalue in the array.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2610
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2632",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2632",
    "content": "We haven't managed to eliminate this range, so\nconsider it matching.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2632
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2663",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2663",
    "content": "have we found a range matching all scan keys? if yes, we're done",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2663
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2670",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2670",
    "content": "And now inspect the values. We don't bother with doing a binary search\nhere, because we're dealing with serialized / fully compacted ranges,\nso there should be only very few values.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2670
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2679",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2679",
    "content": "assume the range is matching, and we'll try to prove otherwise",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2679
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2687",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2687",
    "content": "we've already dealt with NULL keys at the beginning",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2687
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2722",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2722",
    "content": "have we found a range matching all scan keys? if yes, we're done",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2722
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2730",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2730",
    "content": "Given two BrinValues, update the first of them as a union of the summary\nvalues contained in both.  The second one is untouched.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2730
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2774",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2774",
    "content": "The distanceFn calls (which may internally call e.g. numeric_le) may\nallocate quite a bit of memory, and we must not leak it. Otherwise,\nwe'd have problems e.g. when building indexes. So we create a local\nmemory context and make sure we free the memory before leaving this\nfunction (not after every call).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2774
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2790",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2790",
    "content": "fill the expanded ranges with entries for the first range",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2790
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2805",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2805",
    "content": "We've loaded two different lists of expanded ranges, so some of them\nmay be overlapping. So walk through them and merge them.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2805
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2811",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2811",
    "content": "check that the combine ranges are correct (no overlaps, ordering)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2811
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2814",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2814",
    "content": "If needed, reduce some of the ranges.\n\nXXX This may be fairly expensive, so maybe we should do it only when\nit's actually needed (when we have too many ranges).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2814
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2821",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2821",
    "content": "build array of gap distances and sort them in ascending order",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2821
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2825",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2825",
    "content": "See how many values would be needed to store the current ranges, and if\nneeded combine as many of them to get below the threshold. The\ncollapsed ranges will be stored as a single value.\n\nXXX This does not apply the load factor, as we don't expect to add more\nvalues to the range, so we prefer to keep as many ranges as possible.\n\nXXX Can the maxvalues be different in the two ranges? Perhaps we should\nuse maximum of those?",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2825
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2840",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2840",
    "content": "Is the result of reducing expanded ranges correct?",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2840
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2856",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2856",
    "content": "Cache and return minmax multi opclass support procedure\n\nReturn the procedure corresponding to the given function support number\nor null if it does not exist.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2856
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2868",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2868",
    "content": "We cache these in the opaque struct, to avoid repetitive syscache\nlookups.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2868
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2892",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2892",
    "content": "Cache and return the procedure for the given strategy.\n\nNote: this function mirrors minmax_multi_get_strategy_procinfo; see notes\nthere.  If changes are made here, see that function too.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2892
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2909",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2909",
    "content": "We cache the procedures for the previous subtype in the opaque struct,\nto avoid repetitive syscache lookups.  If the subtype changed,\ninvalidate all the cached entries.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2909
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2967",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2967",
    "content": "brin_minmax_multi_summary_in\n\t- input routine for type brin_minmax_multi_summary.\n\nbrin_minmax_multi_summary is only used internally to represent summaries\nin BRIN minmax-multi indexes, so it has no operations of its own, and we\ndisallow input too.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2967
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2978",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2978",
    "content": "brin_minmax_multi_summary stores the data in binary form and parsing\ntext input is not needed, so disallow this.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2978
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:2990",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:2990",
    "content": "brin_minmax_multi_summary_out\n\t- output routine for type brin_minmax_multi_summary.\n\nBRIN minmax-multi summaries are serialized into a bytea value, but we\nwant to output something nicer humans can understand.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 2990
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:3013",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:3013",
    "content": "Detoast to get value with full 4B header (can't be stored in a toast\ntable, but can use 1B header).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 3013
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:3112",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:3112",
    "content": "brin_minmax_multi_summary_recv\n\t- binary input routine for type brin_minmax_multi_summary.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 3112
    }
  },
  {
    "title": "Comment from brin_minmax_multi.c:3126",
    "url": "backend\\access\\brin\\brin_minmax_multi.c:3126",
    "content": "brin_minmax_multi_summary_send\n\t- binary output routine for type brin_minmax_multi_summary.\n\nBRIN minmax-multi summaries are serialized in a bytea value (although\nthe type is named differently), so let's just send that.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_minmax_multi.c",
      "relative_path": "backend\\access\\brin\\brin_minmax_multi.c",
      "line_start": 3126
    }
  },
  {
    "title": "Comment from brin_pageops.c:1",
    "url": "backend\\access\\brin\\brin_pageops.c:1",
    "content": "brin_pageops.c\n\tPage-handling routines for BRIN indexes\n\nPortions Copyright (c) 1996-2024, PostgreSQL Global Development Group\nPortions Copyright (c) 1994, Regents of the University of California\n\nIDENTIFICATION\n  src/backend/access/brin/brin_pageops.c",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 1
    }
  },
  {
    "title": "Comment from brin_pageops.c:24",
    "url": "backend\\access\\brin\\brin_pageops.c:24",
    "content": "Maximum size of an entry in a BRIN_PAGETYPE_REGULAR page.  We can tolerate\na single item per page, unlike other index AMs.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 24
    }
  },
  {
    "title": "Comment from brin_pageops.c:40",
    "url": "backend\\access\\brin\\brin_pageops.c:40",
    "content": "Update tuple origtup (size origsz), located in offset oldoff of buffer\noldbuf, to newtup (size newsz) as summary tuple for the page range starting\nat heapBlk.  oldbuf must not be locked on entry, and is not locked at exit.\n\nIf samepage is true, attempt to put the new tuple in the same page, but if\nthere's no room, use some other one.\n\nIf the update is successful, return true; the revmap is updated to point to\nthe new tuple.  If the update is not done for whatever reason, return false.\nCaller may retry the update if this happens.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 40
    }
  },
  {
    "title": "Comment from brin_pageops.c:80",
    "url": "backend\\access\\brin\\brin_pageops.c:80",
    "content": "make sure the revmap is long enough to contain the entry we need",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 80
    }
  },
  {
    "title": "Comment from brin_pageops.c:93",
    "url": "backend\\access\\brin\\brin_pageops.c:93",
    "content": "Note: it's possible (though unlikely) that the returned newbuf is\nthe same as oldbuf, if brin_getinsertbuffer determined that the old\nbuffer does in fact have enough space.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 93
    }
  },
  {
    "title": "Comment from brin_pageops.c:115",
    "url": "backend\\access\\brin\\brin_pageops.c:115",
    "content": "Check that the old tuple wasn't updated concurrently: it might have\nmoved someplace else entirely, and for that matter the whole page\nmight've become a revmap page.  Note that in the first two cases\nchecked here, the \"oldlp\" we just calculated is garbage; but\nPageGetItemId() is simple enough that it was safe to do that\ncalculation anyway.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 115
    }
  },
  {
    "title": "Comment from brin_pageops.c:129",
    "url": "backend\\access\\brin\\brin_pageops.c:129",
    "content": "If this happens, and the new buffer was obtained by extending the\nrelation, then we need to ensure we don't leave it uninitialized or\nforget about it.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 129
    }
  },
  {
    "title": "Comment from brin_pageops.c:148",
    "url": "backend\\access\\brin\\brin_pageops.c:148",
    "content": "... or it might have been updated in place to different contents.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 148
    }
  },
  {
    "title": "Comment from brin_pageops.c:156",
    "url": "backend\\access\\brin\\brin_pageops.c:156",
    "content": "As above, initialize and record new page if we got one",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 156
    }
  },
  {
    "title": "Comment from brin_pageops.c:166",
    "url": "backend\\access\\brin\\brin_pageops.c:166",
    "content": "Great, the old tuple is intact.  We can proceed with the update.\n\nIf there's enough room in the old page for the new tuple, replace it.\n\nNote that there might now be enough space on the page even though the\ncaller told us there isn't, if a concurrent update moved another tuple\nelsewhere or replaced a tuple with a smaller one.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 166
    }
  },
  {
    "title": "Comment from brin_pageops.c:209",
    "url": "backend\\access\\brin\\brin_pageops.c:209",
    "content": "As above, initialize and record new page if we got one",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 209
    }
  },
  {
    "title": "Comment from brin_pageops.c:221",
    "url": "backend\\access\\brin\\brin_pageops.c:221",
    "content": "Not enough space, but caller said that there was. Tell them to\nstart over.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 221
    }
  },
  {
    "title": "Comment from brin_pageops.c:230",
    "url": "backend\\access\\brin\\brin_pageops.c:230",
    "content": "Not enough free space on the oldpage. Put the new tuple on the new\npage, and update the revmap.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 230
    }
  },
  {
    "title": "Comment from brin_pageops.c:244",
    "url": "backend\\access\\brin\\brin_pageops.c:244",
    "content": "We need to initialize the page if it's newly obtained.  Note we\nwill WAL-log the initialization as part of the update, so we don't\nneed to do that here.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 244
    }
  },
  {
    "title": "Comment from brin_pageops.c:319",
    "url": "backend\\access\\brin\\brin_pageops.c:319",
    "content": "Return whether brin_doupdate can do a samepage update.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 319
    }
  },
  {
    "title": "Comment from brin_pageops.c:330",
    "url": "backend\\access\\brin\\brin_pageops.c:330",
    "content": "Insert an index tuple into the index relation.  The revmap is updated to\nmark the range containing the given page as pointing to the inserted entry.\nA WAL record is written.\n\nThe buffer, if valid, is first checked for free space to insert the new\nentry; if there isn't enough, a new buffer is obtained and pinned.  No\nbuffer lock must be held on entry, no buffer lock is held on exit.\n\nReturn value is the offset number where the tuple was inserted.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 330
    }
  },
  {
    "title": "Comment from brin_pageops.c:366",
    "url": "backend\\access\\brin\\brin_pageops.c:366",
    "content": "Make sure the revmap is long enough to contain the entry we need",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 366
    }
  },
  {
    "title": "Comment from brin_pageops.c:369",
    "url": "backend\\access\\brin\\brin_pageops.c:369",
    "content": "Acquire lock on buffer supplied by caller, if any.  If it doesn't have\nenough space, unpin it to obtain a new one below.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 369
    }
  },
  {
    "title": "Comment from brin_pageops.c:375",
    "url": "backend\\access\\brin\\brin_pageops.c:375",
    "content": "It's possible that another backend (or ourselves!) extended the\nrevmap over the page we held a pin on, so we cannot assume that\nit's still a regular page.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 375
    }
  },
  {
    "title": "Comment from brin_pageops.c:388",
    "url": "backend\\access\\brin\\brin_pageops.c:388",
    "content": "If we still don't have a usable buffer, have brin_getinsertbuffer\nobtain one for us.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 388
    }
  },
  {
    "title": "Comment from brin_pageops.c:453",
    "url": "backend\\access\\brin\\brin_pageops.c:453",
    "content": "Tuple is firmly on buffer; we can release our locks",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 453
    }
  },
  {
    "title": "Comment from brin_pageops.c:469",
    "url": "backend\\access\\brin\\brin_pageops.c:469",
    "content": "Initialize a page with the given type.\n\nCaller is responsible for marking it dirty, as appropriate.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 469
    }
  },
  {
    "title": "Comment from brin_pageops.c:498",
    "url": "backend\\access\\brin\\brin_pageops.c:498",
    "content": "Note we cheat here a little.  0 is not a valid revmap block number\n(because it's the metapage buffer), but doing this enables the first\nrevmap page to be created when the index is.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 498
    }
  },
  {
    "title": "Comment from brin_pageops.c:505",
    "url": "backend\\access\\brin\\brin_pageops.c:505",
    "content": "Set pd_lower just past the end of the metadata.  This is essential,\nbecause without doing so, metadata will be lost if xlog.c compresses\nthe page.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 505
    }
  },
  {
    "title": "Comment from brin_pageops.c:514",
    "url": "backend\\access\\brin\\brin_pageops.c:514",
    "content": "Initiate page evacuation protocol.\n\nThe page must be locked in exclusive mode by the caller.\n\nIf the page is not yet initialized or empty, return false without doing\nanything; it can be used for revmap without any further changes.  If it\ncontains tuples, mark it for evacuation and return true.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 514
    }
  },
  {
    "title": "Comment from brin_pageops.c:543",
    "url": "backend\\access\\brin\\brin_pageops.c:543",
    "content": "Prevent other backends from adding more stuff to this page:\nBRIN_EVACUATE_PAGE informs br_page_get_freespace that this page\ncan no longer be used to add new tuples.  Note that this flag\nis not WAL-logged, except accidentally.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 543
    }
  },
  {
    "title": "Comment from brin_pageops.c:558",
    "url": "backend\\access\\brin\\brin_pageops.c:558",
    "content": "Move all tuples out of a page.\n\nThe caller must hold lock on the page. The lock and pin are released.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 558
    }
  },
  {
    "title": "Comment from brin_pageops.c:601",
    "url": "backend\\access\\brin\\brin_pageops.c:601",
    "content": "It's possible that someone extended the revmap over this page",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 601
    }
  },
  {
    "title": "Comment from brin_pageops.c:610",
    "url": "backend\\access\\brin\\brin_pageops.c:610",
    "content": "Given a BRIN index page, initialize it if necessary, and record its\ncurrent free space in the FSM.\n\nThe main use for this is when, during vacuuming, an uninitialized page is\nfound, which could be the result of relation extension followed by a crash\nbefore the page can be used.\n\nHere, we don't bother to update upper FSM pages, instead expecting that our\ncaller (brin_vacuum_scan) will fix them at the end of the scan.  Elsewhere\nin this file, it's generally a good idea to propagate additions of free\nspace into the upper FSM pages immediately.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 610
    }
  },
  {
    "title": "Comment from brin_pageops.c:628",
    "url": "backend\\access\\brin\\brin_pageops.c:628",
    "content": "If a page was left uninitialized, initialize it now; also record it in\nFSM.\n\nSomebody else might be extending the relation concurrently.  To avoid\nre-initializing the page before they can grab the buffer lock, we\nacquire the extension lock momentarily.  Since they hold the extension\nlock from before getting the page and after its been initialized, we're\nsure to see their initialization.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 628
    }
  },
  {
    "title": "Comment from brin_pageops.c:663",
    "url": "backend\\access\\brin\\brin_pageops.c:663",
    "content": "Return a pinned and exclusively locked buffer which can be used to insert an\nindex item of size itemsz (caller must ensure not to request sizes\nimpossible to fulfill).  If oldbuf is a valid buffer, it is also locked (in\nan order determined to avoid deadlocks).\n\nIf we find that the old page is no longer a regular index page (because\nof a revmap extension), the old buffer is unlocked and we return\nInvalidBuffer.\n\nIf there's no existing page with enough free space to accommodate the new\nitem, the relation is extended.  If this happens, *extended is set to true,\nand it is the caller's responsibility to initialize the page (and WAL-log\nthat fact) prior to use.  The caller should also update the FSM with the\npage's remaining free space after the insertion.\n\nNote that the caller is not expected to update FSM unless *extended is set\ntrue.  This policy means that we'll update FSM when a page is created, and\nwhen it's found to have too little space for a desired tuple insertion,\nbut not every single time we add a tuple to the page.\n\nNote that in some corner cases it is possible for this routine to extend\nthe relation and then not return the new page.  It is this routine's\nresponsibility to WAL-log the page initialization and to record the page in\nFSM if that happens, since the caller certainly can't do it.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 663
    }
  },
  {
    "title": "Comment from brin_pageops.c:706",
    "url": "backend\\access\\brin\\brin_pageops.c:706",
    "content": "Choose initial target page, re-using existing target if known",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 706
    }
  },
  {
    "title": "Comment from brin_pageops.c:711",
    "url": "backend\\access\\brin\\brin_pageops.c:711",
    "content": "Loop until we find a page with sufficient free space.  By the time we\nreturn to caller out of this loop, both buffers are valid and locked;\nif we have to restart here, neither page is locked and newblk isn't\npinned (if it's even valid).",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 711
    }
  },
  {
    "title": "Comment from brin_pageops.c:728",
    "url": "backend\\access\\brin\\brin_pageops.c:728",
    "content": "There's not enough free space in any existing index page,\naccording to the FSM: extend the relation to obtain a shiny new\npage.\n\nXXX: It's likely possible to use RBM_ZERO_AND_LOCK here,\nwhich'd avoid the need to hold the extension lock during buffer\nreclaim.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 728
    }
  },
  {
    "title": "Comment from brin_pageops.c:751",
    "url": "backend\\access\\brin\\brin_pageops.c:751",
    "content": "There's an odd corner-case here where the FSM is out-of-date,\nand gave us the old page.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 751
    }
  },
  {
    "title": "Comment from brin_pageops.c:762",
    "url": "backend\\access\\brin\\brin_pageops.c:762",
    "content": "We lock the old buffer first, if it's earlier than the new one; but\nthen we need to check that it hasn't been turned into a revmap page\nconcurrently.  If we detect that that happened, give up and tell\ncaller to start over.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 762
    }
  },
  {
    "title": "Comment from brin_pageops.c:775",
    "url": "backend\\access\\brin\\brin_pageops.c:775",
    "content": "It is possible that the new page was obtained from\nextending the relation.  In that case, we must be sure to\nrecord it in the FSM before leaving, because otherwise the\nspace would be lost forever.  However, we cannot let an\nuninitialized page get in the FSM, so we need to initialize\nit first.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 775
    }
  },
  {
    "title": "Comment from brin_pageops.c:809",
    "url": "backend\\access\\brin\\brin_pageops.c:809",
    "content": "We have a new buffer to insert into.  Check that the new page has\nenough free space, and return it if it does; otherwise start over.\n(br_page_get_freespace also checks that the FSM didn't hand us a\npage that has since been repurposed for the revmap.)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 809
    }
  },
  {
    "title": "Comment from brin_pageops.c:821",
    "url": "backend\\access\\brin\\brin_pageops.c:821",
    "content": "Lock the old buffer if not locked already.  Note that in this\ncase we know for sure it's a regular page: it's later than the\nnew page we just got, which is not a revmap page, and revmap\npages are always consecutive.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 821
    }
  },
  {
    "title": "Comment from brin_pageops.c:838",
    "url": "backend\\access\\brin\\brin_pageops.c:838",
    "content": "If an entirely new page does not contain enough free space for the\nnew item, then surely that item is oversized.  Complain loudly; but\nfirst make sure we initialize the page and record it as free, for\nnext time.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 838
    }
  },
  {
    "title": "Comment from brin_pageops.c:847",
    "url": "backend\\access\\brin\\brin_pageops.c:847",
    "content": "since this should not happen, skip FreeSpaceMapVacuum",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 847
    }
  },
  {
    "title": "Comment from brin_pageops.c:861",
    "url": "backend\\access\\brin\\brin_pageops.c:861",
    "content": "Update the FSM with the new, presumably smaller, freespace value\nfor this page, then search for a new target page.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 861
    }
  },
  {
    "title": "Comment from brin_pageops.c:869",
    "url": "backend\\access\\brin\\brin_pageops.c:869",
    "content": "Initialize a page as an empty regular BRIN page, WAL-log this, and record\nthe page in FSM.\n\nThere are several corner situations in which we extend the relation to\nobtain a new page and later find that we cannot use it immediately.  When\nthat happens, we don't want to leave the page go unrecorded in FSM, because\nthere is no mechanism to get the space back and the index would bloat.\nAlso, because we would not WAL-log the action that would initialize the\npage, the page would go uninitialized in a standby (or after recovery).\n\nWhile we record the page in FSM here, caller is responsible for doing FSM\nupper-page update if that seems appropriate.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 869
    }
  },
  {
    "title": "Comment from brin_pageops.c:899",
    "url": "backend\\access\\brin\\brin_pageops.c:899",
    "content": "We update the FSM for this page, but this is not WAL-logged.  This is\nacceptable because VACUUM will scan the index and update the FSM with\npages whose FSM records were forgotten in a crash.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 899
    }
  },
  {
    "title": "Comment from brin_pageops.c:909",
    "url": "backend\\access\\brin\\brin_pageops.c:909",
    "content": "Return the amount of free space on a regular BRIN index page.\n\nIf the page is not a regular page, or has been marked with the\nBRIN_EVACUATE_PAGE flag, returns 0.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_pageops.c",
      "relative_path": "backend\\access\\brin\\brin_pageops.c",
      "line_start": 909
    }
  },
  {
    "title": "Comment from brin_revmap.c:1",
    "url": "backend\\access\\brin\\brin_revmap.c:1",
    "content": "brin_revmap.c\n\tRange map for BRIN indexes\n\nThe range map (revmap) is a translation structure for BRIN indexes: for each\npage range there is one summary tuple, and its location is tracked by the\nrevmap.  Whenever a new tuple is inserted into a table that violates the\npreviously recorded summary values, a new tuple is inserted into the index\nand the revmap is updated to point to it.\n\nThe revmap is stored in the first pages of the index, immediately following\nthe metapage.  When the revmap needs to be expanded, all tuples on the\nregular BRIN page at that block (if any) are moved out of the way.\n\nPortions Copyright (c) 1996-2024, PostgreSQL Global Development Group\nPortions Copyright (c) 1994, Regents of the University of California\n\nIDENTIFICATION\n  src/backend/access/brin/brin_revmap.c",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 1
    }
  },
  {
    "title": "Comment from brin_revmap.c:35",
    "url": "backend\\access\\brin\\brin_revmap.c:35",
    "content": "In revmap pages, each item stores an ItemPointerData.  These defines let one\nfind the logical revmap page number and index number of the revmap item for\nthe given heap block number.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 35
    }
  },
  {
    "title": "Comment from brin_revmap.c:65",
    "url": "backend\\access\\brin\\brin_revmap.c:65",
    "content": "Initialize an access object for a range map.  This must be freed by\nbrinRevmapTerminate when caller is done with it.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 65
    }
  },
  {
    "title": "Comment from brin_revmap.c:96",
    "url": "backend\\access\\brin\\brin_revmap.c:96",
    "content": "Release resources associated with a revmap access object.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 96
    }
  },
  {
    "title": "Comment from brin_revmap.c:108",
    "url": "backend\\access\\brin\\brin_revmap.c:108",
    "content": "Extend the revmap to cover the given heap block number.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 108
    }
  },
  {
    "title": "Comment from brin_revmap.c:124",
    "url": "backend\\access\\brin\\brin_revmap.c:124",
    "content": "Prepare to insert an entry into the revmap; the revmap buffer in which the\nentry is to reside is locked and returned.  Most callers should call\nbrinRevmapExtend beforehand, as this routine does not extend the revmap if\nit's not long enough.\n\nThe returned buffer is also recorded in the revmap struct; finishing that\nreleases the buffer, therefore the caller needn't do it explicitly.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 124
    }
  },
  {
    "title": "Comment from brin_revmap.c:144",
    "url": "backend\\access\\brin\\brin_revmap.c:144",
    "content": "In the given revmap buffer (locked appropriately by caller), which is used\nin a BRIN index of pagesPerRange pages per range, set the element\ncorresponding to heap block number heapBlk to the given TID.\n\nOnce the operation is complete, the caller must update the LSN on the\nreturned buffer.\n\nThis is used both in regular operation and during WAL replay.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 144
    }
  },
  {
    "title": "Comment from brin_revmap.c:162",
    "url": "backend\\access\\brin\\brin_revmap.c:162",
    "content": "The correct page should already be pinned and locked",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 162
    }
  },
  {
    "title": "Comment from brin_revmap.c:176",
    "url": "backend\\access\\brin\\brin_revmap.c:176",
    "content": "Fetch the BrinTuple for a given heap block.\n\nThe buffer containing the tuple is locked, and returned in *buf.  The\nreturned tuple points to the shared buffer and must not be freed; if caller\nwants to use it after releasing the buffer lock, it must create its own\npalloc'ed copy.  As an optimization, the caller can pass a pinned buffer\n*buf on entry, which will avoid a pin-unpin cycle when the next tuple is on\nthe same page as a previous one.\n\nIf no tuple is found for the given heap range, returns NULL. In that case,\n*buf might still be updated (and pin must be released by caller), but it's\nnot locked.\n\nThe output tuple offset within the buffer is returned in *off, and its size\nis returned in *size.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 176
    }
  },
  {
    "title": "Comment from brin_revmap.c:207",
    "url": "backend\\access\\brin\\brin_revmap.c:207",
    "content": "normalize the heap block number to be the first page in the range",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 207
    }
  },
  {
    "title": "Comment from brin_revmap.c:210",
    "url": "backend\\access\\brin\\brin_revmap.c:210",
    "content": "Compute the revmap page number we need.  If Invalid is returned (i.e.,\nthe revmap page hasn't been created yet), the requested page range is\nnot summarized.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 210
    }
  },
  {
    "title": "Comment from brin_revmap.c:250",
    "url": "backend\\access\\brin\\brin_revmap.c:250",
    "content": "Check the TID we got in a previous iteration, if any, and save the\ncurrent TID we got from the revmap; if we loop, we can sanity-check\nthat the next one we get is different.  Otherwise we might be stuck\nlooping forever if the revmap is somehow badly broken.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 250
    }
  },
  {
    "title": "Comment from brin_revmap.c:267",
    "url": "backend\\access\\brin\\brin_revmap.c:267",
    "content": "Ok, got a pointer to where the BrinTuple should be. Fetch it.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 267
    }
  },
  {
    "title": "Comment from brin_revmap.c:280",
    "url": "backend\\access\\brin\\brin_revmap.c:280",
    "content": "If the offset number is greater than what's in the page, it's\npossible that the range was desummarized concurrently. Just\nreturn NULL to handle that case.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 280
    }
  },
  {
    "title": "Comment from brin_revmap.c:306",
    "url": "backend\\access\\brin\\brin_revmap.c:306",
    "content": "No luck. Assume that the revmap was updated concurrently.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 306
    }
  },
  {
    "title": "Comment from brin_revmap.c:315",
    "url": "backend\\access\\brin\\brin_revmap.c:315",
    "content": "Delete an index tuple, marking a page range as unsummarized.\n\nIndex must be locked in ShareUpdateExclusiveLock mode.\n\nReturn false if caller should retry.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 315
    }
  },
  {
    "title": "Comment from brin_revmap.c:344",
    "url": "backend\\access\\brin\\brin_revmap.c:344",
    "content": "revmap page doesn't exist: range not summarized, we're done",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 344
    }
  },
  {
    "title": "Comment from brin_revmap.c:349",
    "url": "backend\\access\\brin\\brin_revmap.c:349",
    "content": "Lock the revmap page, obtain the index tuple pointer from it",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 349
    }
  },
  {
    "title": "Comment from brin_revmap.c:370",
    "url": "backend\\access\\brin\\brin_revmap.c:370",
    "content": "if this is no longer a regular page, tell caller to start over",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 370
    }
  },
  {
    "title": "Comment from brin_revmap.c:391",
    "url": "backend\\access\\brin\\brin_revmap.c:391",
    "content": "Placeholder tuples only appear during unfinished summarization, and we\nhold ShareUpdateExclusiveLock, so this function cannot run concurrently\nwith that.  So any placeholder tuples that exist are leftovers from a\ncrashed or aborted summarization; remove them silently.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 391
    }
  },
  {
    "title": "Comment from brin_revmap.c:436",
    "url": "backend\\access\\brin\\brin_revmap.c:436",
    "content": "Given a heap block number, find the corresponding physical revmap block\nnumber and return it.  If the revmap page hasn't been allocated yet, return\nInvalidBlockNumber.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 436
    }
  },
  {
    "title": "Comment from brin_revmap.c:446",
    "url": "backend\\access\\brin\\brin_revmap.c:446",
    "content": "obtain revmap block number, skip 1 for metapage block",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 446
    }
  },
  {
    "title": "Comment from brin_revmap.c:456",
    "url": "backend\\access\\brin\\brin_revmap.c:456",
    "content": "Obtain and return a buffer containing the revmap page for the given heap\npage.  The revmap must have been previously extended to cover that page.\nThe returned buffer is also recorded in the revmap struct; finishing that\nreleases the buffer, therefore the caller needn't do it explicitly.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 456
    }
  },
  {
    "title": "Comment from brin_revmap.c:467",
    "url": "backend\\access\\brin\\brin_revmap.c:467",
    "content": "Translate the heap block number to physical index location.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 467
    }
  },
  {
    "title": "Comment from brin_revmap.c:477",
    "url": "backend\\access\\brin\\brin_revmap.c:477",
    "content": "Obtain the buffer from which we need to read.  If we already have the\ncorrect buffer in our access struct, use that; otherwise, release that,\n(if valid) and read the one we need.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 477
    }
  },
  {
    "title": "Comment from brin_revmap.c:494",
    "url": "backend\\access\\brin\\brin_revmap.c:494",
    "content": "Given a heap block number, find the corresponding physical revmap block\nnumber and return it. If the revmap page hasn't been allocated yet, extend\nthe revmap until it is.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 494
    }
  },
  {
    "title": "Comment from brin_revmap.c:504",
    "url": "backend\\access\\brin\\brin_revmap.c:504",
    "content": "obtain revmap block number, skip 1 for metapage block",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 504
    }
  },
  {
    "title": "Comment from brin_revmap.c:517",
    "url": "backend\\access\\brin\\brin_revmap.c:517",
    "content": "Try to extend the revmap by one page.  This might not happen for a number of\nreasons; caller is expected to retry until the expected outcome is obtained.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 517
    }
  },
  {
    "title": "Comment from brin_revmap.c:532",
    "url": "backend\\access\\brin\\brin_revmap.c:532",
    "content": "Lock the metapage. This locks out concurrent extensions of the revmap,\nbut note that we still need to grab the relation extension lock because\nanother backend can extend the index with regular BRIN pages.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 532
    }
  },
  {
    "title": "Comment from brin_revmap.c:541",
    "url": "backend\\access\\brin\\brin_revmap.c:541",
    "content": "Check that our cached lastRevmapPage value was up-to-date; if it\nwasn't, update the cached copy and have caller start over.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 541
    }
  },
  {
    "title": "Comment from brin_revmap.c:566",
    "url": "backend\\access\\brin\\brin_revmap.c:566",
    "content": "Very rare corner case: somebody extended the relation\nconcurrently after we read its length.  If this happens, give\nup and have caller start over.  We will have to evacuate that\npage from under whoever is using it.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 566
    }
  },
  {
    "title": "Comment from brin_revmap.c:579",
    "url": "backend\\access\\brin\\brin_revmap.c:579",
    "content": "Check that it's a regular block (or an empty page)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 579
    }
  },
  {
    "title": "Comment from brin_revmap.c:598",
    "url": "backend\\access\\brin\\brin_revmap.c:598",
    "content": "Ok, we have now locked the metapage and the target block. Re-initialize\nthe target block as a revmap page, and update the metapage.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 598
    }
  },
  {
    "title": "Comment from brin_revmap.c:604",
    "url": "backend\\access\\brin\\brin_revmap.c:604",
    "content": "the rm_tids array is initialized to all invalid by PageInit",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 604
    }
  },
  {
    "title": "Comment from brin_revmap.c:610",
    "url": "backend\\access\\brin\\brin_revmap.c:610",
    "content": "Set pd_lower just past the end of the metadata.  This is essential,\nbecause without doing so, metadata will be lost if xlog.c compresses\nthe page.  (We must do this here because pre-v11 versions of PG did not\nset the metapage's pd_lower correctly, so a pg_upgraded index might\ncontain the wrong value.)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_revmap.c",
      "relative_path": "backend\\access\\brin\\brin_revmap.c",
      "line_start": 610
    }
  },
  {
    "title": "Comment from brin_tuple.c:1",
    "url": "backend\\access\\brin\\brin_tuple.c:1",
    "content": "brin_tuple.c\n\tMethod implementations for tuples in BRIN indexes.\n\nIntended usage is that code outside this file only deals with\nBrinMemTuples, and convert to and from the on-disk representation through\nfunctions in this file.\n\nNOTES\n\nA BRIN tuple is similar to a heap tuple, with a few key differences.  The\nfirst interesting difference is that the tuple header is much simpler, only\ncontaining its total length and a small area for flags.  Also, the stored\ndata does not match the relation tuple descriptor exactly: for each\nattribute in the descriptor, the index tuple carries an arbitrary number\nof values, depending on the opclass.\n\nAlso, for each column of the index relation there are two null bits: one\n(hasnulls) stores whether any tuple within the page range has that column\nset to null; the other one (allnulls) stores whether the column values are\nall null.  If allnulls is true, then the tuple data area does not contain\nvalues for that column at all; whereas it does if the hasnulls is set.\nNote the size of the null bitmask may not be the same as that of the\ndatum array.\n\nPortions Copyright (c) 1996-2024, PostgreSQL Global Development Group\nPortions Copyright (c) 1994, Regents of the University of California\n\nIDENTIFICATION\n  src/backend/access/brin/brin_tuple.c",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 1
    }
  },
  {
    "title": "Comment from brin_tuple.c:45",
    "url": "backend\\access\\brin\\brin_tuple.c:45",
    "content": "This enables de-toasting of index entries.  Needed until VACUUM is\nsmart enough to rebuild indexes from scratch.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 45
    }
  },
  {
    "title": "Comment from brin_tuple.c:57",
    "url": "backend\\access\\brin\\brin_tuple.c:57",
    "content": "Return a tuple descriptor used for on-disk storage of BRIN tuples.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 57
    }
  },
  {
    "title": "Comment from brin_tuple.c:93",
    "url": "backend\\access\\brin\\brin_tuple.c:93",
    "content": "Generate a new on-disk tuple to be inserted in a BRIN index.\n\nSee brin_form_placeholder_tuple if you touch this.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 93
    }
  },
  {
    "title": "Comment from brin_tuple.c:131",
    "url": "backend\\access\\brin\\brin_tuple.c:131",
    "content": "Set up the values/nulls arrays for heap_fill_tuple",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 131
    }
  },
  {
    "title": "Comment from brin_tuple.c:139",
    "url": "backend\\access\\brin\\brin_tuple.c:139",
    "content": "\"allnulls\" is set when there's no nonnull value in any row in the\ncolumn; when this happens, there is no data to store.  Thus set the\nnullable bits for all data elements of this column and we're done.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 139
    }
  },
  {
    "title": "Comment from brin_tuple.c:154",
    "url": "backend\\access\\brin\\brin_tuple.c:154",
    "content": "The \"hasnulls\" bit is set when there are some null values in the\ndata.  We still need to store a real value, but the presence of\nthis means we need a null bitmap.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 154
    }
  },
  {
    "title": "Comment from brin_tuple.c:162",
    "url": "backend\\access\\brin\\brin_tuple.c:162",
    "content": "If needed, serialize the values before forming the on-disk tuple.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 162
    }
  },
  {
    "title": "Comment from brin_tuple.c:170",
    "url": "backend\\access\\brin\\brin_tuple.c:170",
    "content": "Now obtain the values of each stored datum.  Note that some values\nmight be toasted, and we cannot rely on the original heap values\nsticking around forever, so we must detoast them.  Also try to\ncompress them.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 170
    }
  },
  {
    "title": "Comment from brin_tuple.c:184",
    "url": "backend\\access\\brin\\brin_tuple.c:184",
    "content": "We must look at the stored type, not at the index descriptor.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 184
    }
  },
  {
    "title": "Comment from brin_tuple.c:190",
    "url": "backend\\access\\brin\\brin_tuple.c:190",
    "content": "For non-varlena types we don't need to do anything special",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 190
    }
  },
  {
    "title": "Comment from brin_tuple.c:197",
    "url": "backend\\access\\brin\\brin_tuple.c:197",
    "content": "Do nothing if value is not of varlena type. We don't need to\ncare about NULL values here, thanks to bv_allnulls above.\n\nIf value is stored EXTERNAL, must fetch it so we are not\ndepending on outside storage.\n\nXXX Is this actually true? Could it be that the summary is NULL\neven for range with non-NULL data? E.g. degenerate bloom filter\nmay be thrown away, etc.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 197
    }
  },
  {
    "title": "Comment from brin_tuple.c:215",
    "url": "backend\\access\\brin\\brin_tuple.c:215",
    "content": "If value is above size target, and is of a compressible\ndatatype, try to compress it in-line.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 215
    }
  },
  {
    "title": "Comment from brin_tuple.c:229",
    "url": "backend\\access\\brin\\brin_tuple.c:229",
    "content": "If the BRIN summary and indexed attribute use the same data\ntype and it has a valid compression method, we can use the\nsame compression method. Otherwise we have to use the\ndefault method.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 229
    }
  },
  {
    "title": "Comment from brin_tuple.c:253",
    "url": "backend\\access\\brin\\brin_tuple.c:253",
    "content": "If we untoasted / compressed the value, we need to free it\nafter forming the index tuple.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 253
    }
  },
  {
    "title": "Comment from brin_tuple.c:273",
    "url": "backend\\access\\brin\\brin_tuple.c:273",
    "content": "We need a double-length bitmap on an on-disk BRIN index tuple; the\nfirst half stores the \"allnulls\" bits, the second stores\n\"hasnulls\".",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 273
    }
  },
  {
    "title": "Comment from brin_tuple.c:296",
    "url": "backend\\access\\brin\\brin_tuple.c:296",
    "content": "The infomask and null bitmap as computed by heap_fill_tuple are useless\nto us.  However, that function will not accept a null infomask; and we\nneed to pass a valid null bitmap so that it will correctly skip\noutputting null attributes in the data area.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 296
    }
  },
  {
    "title": "Comment from brin_tuple.c:320",
    "url": "backend\\access\\brin\\brin_tuple.c:320",
    "content": "Now fill in the real null bitmasks.  allnulls first.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 320
    }
  },
  {
    "title": "Comment from brin_tuple.c:330",
    "url": "backend\\access\\brin\\brin_tuple.c:330",
    "content": "Note that we reverse the sense of null bits in this module: we\nstore a 1 for a null attribute rather than a 0.  So we must reverse\nthe sense of the att_isnull test in brin_deconstruct_tuple as well.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 330
    }
  },
  {
    "title": "Comment from brin_tuple.c:382",
    "url": "backend\\access\\brin\\brin_tuple.c:382",
    "content": "Generate a new on-disk tuple with no data values, marked as placeholder.\n\nThis is a cut-down version of brin_form_tuple.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 382
    }
  },
  {
    "title": "Comment from brin_tuple.c:438",
    "url": "backend\\access\\brin\\brin_tuple.c:438",
    "content": "Given a brin tuple of size len, create a copy of it.  If 'dest' is not\nNULL, its size is destsz, and can be used as output buffer; if the tuple\nto be copied does not fit, it is enlarged by repalloc, and the size is\nupdated to match.  This avoids palloc/free cycles when many brin tuples\nare being processed in loops.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 438
    }
  },
  {
    "title": "Comment from brin_tuple.c:461",
    "url": "backend\\access\\brin\\brin_tuple.c:461",
    "content": "Return whether two BrinTuples are bitwise identical.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 461
    }
  },
  {
    "title": "Comment from brin_tuple.c:474",
    "url": "backend\\access\\brin\\brin_tuple.c:474",
    "content": "Create a new BrinMemTuple from scratch, and initialize it to an empty\nstate.\n\nNote: we don't provide any means to free a deformed tuple, so make sure to\nuse a temporary memory context.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 474
    }
  },
  {
    "title": "Comment from brin_tuple.c:506",
    "url": "backend\\access\\brin\\brin_tuple.c:506",
    "content": "Reset a BrinMemTuple to initial state.  We return the same tuple, for\nnotational convenience.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 506
    }
  },
  {
    "title": "Comment from brin_tuple.c:540",
    "url": "backend\\access\\brin\\brin_tuple.c:540",
    "content": "Convert a BrinTuple back to a BrinMemTuple.  This is the reverse of\nbrin_form_tuple.\n\nAs an optimization, the caller can pass a previously allocated 'dMemtuple'.\nThis avoids having to allocate it here, which can be useful when this\nfunction is called many times in a loop.  It is caller's responsibility\nthat the given BrinMemTuple matches what we need here.\n\nNote we don't need the \"on disk tupdesc\" here; we rely on our own routine to\ndeconstruct the tuple from the on-disk format.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 540
    }
  },
  {
    "title": "Comment from brin_tuple.c:591",
    "url": "backend\\access\\brin\\brin_tuple.c:591",
    "content": "Iterate to assign each of the values to the corresponding item in the\nvalues array of each column.  The copies occur in the tuple's context.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 591
    }
  },
  {
    "title": "Comment from brin_tuple.c:606",
    "url": "backend\\access\\brin\\brin_tuple.c:606",
    "content": "We would like to skip datumCopy'ing the values datum in some cases,\ncaller permitting ...",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 606
    }
  },
  {
    "title": "Comment from brin_tuple.c:629",
    "url": "backend\\access\\brin\\brin_tuple.c:629",
    "content": "brin_deconstruct_tuple\n\tGuts of attribute extraction from an on-disk BRIN tuple.\n\nIts arguments are:\nbrdesc\t\tBRIN descriptor for the stored tuple\ntp\t\t\tpointer to the tuple data area\nnullbits\tpointer to the tuple nulls bitmask\nnulls\t\t\"has nulls\" bit in tuple infomask\nvalues\t\toutput values, array of size brdesc->bd_totalstored\nallnulls\toutput \"allnulls\", size brdesc->bd_tupdesc->natts\nhasnulls\toutput \"hasnulls\", size brdesc->bd_tupdesc->natts\n\nOutput arrays must have been allocated by caller.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 629
    }
  },
  {
    "title": "Comment from brin_tuple.c:654",
    "url": "backend\\access\\brin\\brin_tuple.c:654",
    "content": "First iterate to natts to obtain both null flags for each attribute.\nNote that we reverse the sense of the att_isnull test, because we store\n1 for a null value (rather than a 1 for a not null value as is the\natt_isnull convention used elsewhere.)  See brin_form_tuple.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 654
    }
  },
  {
    "title": "Comment from brin_tuple.c:662",
    "url": "backend\\access\\brin\\brin_tuple.c:662",
    "content": "the \"all nulls\" bit means that all values in the page range for\nthis column are nulls.  Therefore there are no values in the tuple\ndata area.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 662
    }
  },
  {
    "title": "Comment from brin_tuple.c:669",
    "url": "backend\\access\\brin\\brin_tuple.c:669",
    "content": "the \"has nulls\" bit means that some tuples have nulls, but others\nhave not-null values.  Therefore we know the tuple contains data\nfor this column.\n\nThe hasnulls bits follow the allnulls bits in the same bitmask.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 669
    }
  },
  {
    "title": "Comment from brin_tuple.c:680",
    "url": "backend\\access\\brin\\brin_tuple.c:680",
    "content": "Iterate to obtain each attribute's stored values.  Note that since we\nmay reuse attribute entries for more than one column, we cannot cache\noffsets here.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_tuple.c",
      "relative_path": "backend\\access\\brin\\brin_tuple.c",
      "line_start": 680
    }
  },
  {
    "title": "Comment from brin_validate.c:1",
    "url": "backend\\access\\brin\\brin_validate.c:1",
    "content": "-------------------------------------------------------------------------\n\nbrin_validate.c\n  Opclass validator for BRIN.\n\nPortions Copyright (c) 1996-2024, PostgreSQL Global Development Group\nPortions Copyright (c) 1994, Regents of the University of California\n\nIDENTIFICATION\n  src/backend/access/brin/brin_validate.c\n\n-------------------------------------------------------------------------",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_validate.c",
      "relative_path": "backend\\access\\brin\\brin_validate.c",
      "line_start": 1
    }
  },
  {
    "title": "Comment from brin_validate.c:28",
    "url": "backend\\access\\brin\\brin_validate.c:28",
    "content": "Validator for a BRIN opclass.\n\nSome of the checks done here cover the whole opfamily, and therefore are\nredundant when checking each opclass in a family.  But they don't run long\nenough to be much of a problem, so we accept the duplication rather than\ncomplicate the amvalidate API.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_validate.c",
      "relative_path": "backend\\access\\brin\\brin_validate.c",
      "line_start": 28
    }
  },
  {
    "title": "Comment from brin_validate.c:75",
    "url": "backend\\access\\brin\\brin_validate.c:75",
    "content": "Fetch all operators and support functions of the opfamily",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_validate.c",
      "relative_path": "backend\\access\\brin\\brin_validate.c",
      "line_start": 75
    }
  },
  {
    "title": "Comment from brin_validate.c:125",
    "url": "backend\\access\\brin\\brin_validate.c:125",
    "content": "Can't check signatures of optional procs, so assume OK",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_validate.c",
      "relative_path": "backend\\access\\brin\\brin_validate.c",
      "line_start": 125
    }
  },
  {
    "title": "Comment from brin_validate.c:141",
    "url": "backend\\access\\brin\\brin_validate.c:141",
    "content": "Track all valid procedure numbers seen in opfamily",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_validate.c",
      "relative_path": "backend\\access\\brin\\brin_validate.c",
      "line_start": 141
    }
  },
  {
    "title": "Comment from brin_validate.c:164",
    "url": "backend\\access\\brin\\brin_validate.c:164",
    "content": "The set of operators supplied varies across BRIN opfamilies.\nOur plan is to identify all operator strategy numbers used in\nthe opfamily and then complain about datatype combinations that\nare missing any operator(s).  However, consider only numbers\nthat appear in some non-cross-type case, since cross-type\noperators may have unique strategies.  (This is not a great\nheuristic, in particular an erroneous number used in a\ncross-type operator will not get noticed; but the core BRIN\nopfamilies are messy enough to make it necessary.)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_validate.c",
      "relative_path": "backend\\access\\brin\\brin_validate.c",
      "line_start": 164
    }
  },
  {
    "title": "Comment from brin_validate.c:191",
    "url": "backend\\access\\brin\\brin_validate.c:191",
    "content": "Check operator signature --- same for all brin strategies",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_validate.c",
      "relative_path": "backend\\access\\brin\\brin_validate.c",
      "line_start": 191
    }
  },
  {
    "title": "Comment from brin_validate.c:205",
    "url": "backend\\access\\brin\\brin_validate.c:205",
    "content": "Now check for inconsistent groups of operators/functions",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_validate.c",
      "relative_path": "backend\\access\\brin\\brin_validate.c",
      "line_start": 205
    }
  },
  {
    "title": "Comment from brin_validate.c:212",
    "url": "backend\\access\\brin\\brin_validate.c:212",
    "content": "Remember the group exactly matching the test opclass",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_validate.c",
      "relative_path": "backend\\access\\brin\\brin_validate.c",
      "line_start": 212
    }
  },
  {
    "title": "Comment from brin_validate.c:217",
    "url": "backend\\access\\brin\\brin_validate.c:217",
    "content": "Some BRIN opfamilies expect cross-type support functions to exist,\nand some don't.  We don't know exactly which are which, so if we\nfind a cross-type operator for which there are no support functions\nat all, let it pass.  (Don't expect that all operators exist for\nsuch cross-type cases, either.)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_validate.c",
      "relative_path": "backend\\access\\brin\\brin_validate.c",
      "line_start": 217
    }
  },
  {
    "title": "Comment from brin_validate.c:228",
    "url": "backend\\access\\brin\\brin_validate.c:228",
    "content": "Else complain if there seems to be an incomplete set of either\noperators or support functions for this datatype pair.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_validate.c",
      "relative_path": "backend\\access\\brin\\brin_validate.c",
      "line_start": 228
    }
  },
  {
    "title": "Comment from brin_validate.c:254",
    "url": "backend\\access\\brin\\brin_validate.c:254",
    "content": "Check that the originally-named opclass is complete",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_validate.c",
      "relative_path": "backend\\access\\brin\\brin_validate.c",
      "line_start": 254
    }
  },
  {
    "title": "Comment from brin_xlog.c:1",
    "url": "backend\\access\\brin\\brin_xlog.c:1",
    "content": "brin_xlog.c\n\tXLog replay routines for BRIN indexes\n\nPortions Copyright (c) 1996-2024, PostgreSQL Global Development Group\nPortions Copyright (c) 1994, Regents of the University of California\n\nIDENTIFICATION\n  src/backend/access/brin/brin_xlog.c",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_xlog.c",
      "relative_path": "backend\\access\\brin\\brin_xlog.c",
      "line_start": 1
    }
  },
  {
    "title": "Comment from brin_xlog.c:41",
    "url": "backend\\access\\brin\\brin_xlog.c:41",
    "content": "Common part of an insert or update. Inserts the new tuple and updates the\nrevmap.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_xlog.c",
      "relative_path": "backend\\access\\brin\\brin_xlog.c",
      "line_start": 41
    }
  },
  {
    "title": "Comment from brin_xlog.c:55",
    "url": "backend\\access\\brin\\brin_xlog.c:55",
    "content": "If we inserted the first and only tuple on the page, re-initialize the\npage from scratch.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_xlog.c",
      "relative_path": "backend\\access\\brin\\brin_xlog.c",
      "line_start": 55
    }
  },
  {
    "title": "Comment from brin_xlog.c:159",
    "url": "backend\\access\\brin\\brin_xlog.c:159",
    "content": "Then insert the new tuple and update revmap, like in an insertion.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_xlog.c",
      "relative_path": "backend\\access\\brin\\brin_xlog.c",
      "line_start": 159
    }
  },
  {
    "title": "Comment from brin_xlog.c:238",
    "url": "backend\\access\\brin\\brin_xlog.c:238",
    "content": "Set pd_lower just past the end of the metadata.  This is essential,\nbecause without doing so, metadata will be lost if xlog.c\ncompresses the page.  (We must do this here because pre-v11\nversions of PG did not set the metapage's pd_lower correctly, so a\npg_upgraded index might contain the wrong value.)",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_xlog.c",
      "relative_path": "backend\\access\\brin\\brin_xlog.c",
      "line_start": 238
    }
  },
  {
    "title": "Comment from brin_xlog.c:251",
    "url": "backend\\access\\brin\\brin_xlog.c:251",
    "content": "Re-init the target block as a revmap page.  There's never a full- page\nimage here.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_xlog.c",
      "relative_path": "backend\\access\\brin\\brin_xlog.c",
      "line_start": 251
    }
  },
  {
    "title": "Comment from brin_xlog.c:351",
    "url": "backend\\access\\brin\\brin_xlog.c:351",
    "content": "Regular brin pages contain unused space which needs to be masked.\nSimilarly for meta pages, but mask it only if pd_lower appears to have\nbeen set correctly.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_xlog.c",
      "relative_path": "backend\\access\\brin\\brin_xlog.c",
      "line_start": 351
    }
  },
  {
    "title": "Comment from brin_xlog.c:362",
    "url": "backend\\access\\brin\\brin_xlog.c:362",
    "content": "BRIN_EVACUATE_PAGE is not WAL-logged, since it's of no use in recovery.\nMask it.  See brin_start_evacuating_page() for details.",
    "source_type": "C_COMMENT",
    "metadata": {
      "file_path": "C:\\Users\\user\\postgres-REL_17_6\\src\\backend\\access\\brin\\brin_xlog.c",
      "relative_path": "backend\\access\\brin\\brin_xlog.c",
      "line_start": 362
    }
  }
]